task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Assume the CSV file is in the current directory
file_path = 'winequality-red.csv'

try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f""Error: The file '{file_path}' was not found. Please ensure it's in the current directory."")
    exit()

# 1. List all column names and identify the data type of each column
print(""1. Column Names and Data Types:"")
for col in df.columns:
    col_type = df[col].dtype
    if pd.api.types.is_numeric_dtype(col_ty...",9,"The generated code is very good and correctly addresses almost all aspects of the task. It properly loads the CSV, iterates through columns to identify data types, calculates descriptive statistics for numerical columns using `describe()`, and correctly reports dataset dimensions.

**Strong Points:**
- Excellent use of `pd.api.types.is_numeric_dtype` and `df.select_dtypes(include=['number'])` for robust type identification and selection of numerical columns.
- The `try-except` block for `FileNotFoundError` is crucial for good error handling.
- Clearly states that '50%' represents the median in the descriptive statistics output.
- The discussion around the 'quality' column being numerical but often treated as ordinal is highly insightful and directly addresses a nuance of this specific dataset.
- The heuristic for identifying potential categorical/ordinal columns (integer columns with few unique values) is a good practical approach beyond just looking at the dtype.

**Areas for Minor Improvement:**
- In point 1, when listing column data types, the `Categorical (Boolean)` and `Potentially Categorical/Text (object)` descriptions are based on `pd.api.types.is_bool_dtype` and `pd.api.types.is_object_dtype` respectively. While technically correct, the ground truth code used `pd.api.types.is_categorical_dtype` or `is_object_dtype` for a single, broader ""Categorical/Ordinal"" category. The generated code's more granular breakdown is fine, but it could also explicitly check for `pd.api.types.is_categorical_dtype` to be fully exhaustive in categorizing pandas' explicit categorical dtype. However, for this dataset, all 'object' types are numerical so it doesn't manifest.
- The Ground Truth used `df.shape` to get `num_rows, num_cols` immediately after loading, while the generated code stored `rows, cols = df.shape` and then printed it later. This is a minor style point and doesn't affect correctness.
- The output formatting could be slightly more consistent with the ground truth's use of headings (e.g., `--- Section Name ---`), but the current `1.`, `2.`, etc., is perfectly clear.

Overall, a very strong solution that goes beyond basic requirements with thoughtful analysis of the 'quality' column.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# 1. Load the dataset and display the first 10 rows
try:
    df = pd.read_csv('video_games_sales.csv')
    print(""1. First 10 rows of the dataset:"")
    print(df.head(10))
    print(""\n"" + ""=""*50 + ""\n"")

    # 2. Show the shape of the dataset (rows and columns)
    print(""2. Shape of the dataset (rows, columns):"")
    print(df.shape)
    print(""\n"" + ""=""*50 + ""\n"")

    # 3. List all column names and their data types
    print(""3. Column names and their data types...",8,"The generated code successfully completes almost all tasks specified in the prompt.

**Strong points:**
- Correctly loads the CSV and displays the first 10 rows.
- Accurately shows the dataset shape.
- Lists column data types using `df.dtypes`, which is a valid way to fulfill the requirement, although `df.info()` from the ground truth provides more comprehensive information (non-null counts, memory usage).
- Correctly identifies and displays the top 10 games by Global_Sales.
- Effectively shows unique values for 'Platform' and 'Genre'.
- Calculates summary statistics for all specified sales columns.
- Identifies missing values using `df.isnull().sum()`.
- Includes a basic `try-except` block for `FileNotFoundError`.

**Areas for improvement:**
- **Robustness:** The ground truth includes checks for column existence before attempting operations (e.g., `if 'Global_Sales' in df.columns:`). The generated code assumes these columns exist, which could lead to `KeyError` if the dataset structure changes.
- **Output Clarity (`df.dtypes` vs. `df.info()`):** While `df.dtypes` fulfills the requirement, `df.info()` (used in ground truth) gives a more complete picture of column types, non-null counts, and memory usage, which is typically more valuable for EDA.
- **Top 10 Games Columns:** The generated code displays 'Name', 'Platform', 'Global_Sales' for top 10 games, whereas the ground truth also includes 'Publisher'. This is a minor omission but worth noting.
- **Missing Values Presentation:** The ground truth filters to show only columns with *actual* missing values and provides a concluding statement. The generated code shows all columns, even those with zero missing values. This is not incorrect but less concise for large datasets.
- **Error Handling:** The `try-except` block is quite broad. The ground truth handles `FileNotFoundError` specifically and then allows subsequent errors to propagate more gracefully, making it easier to debug if the file loads but operations fail. The ground truth also uses `exit()` on `FileNotFoundError`, preventing subsequent operations on a non-existent DataFrame.
- **User Experience/Formatting:** The ground truth includes more verbose print statements (e.g., ""The dataset has X rows and Y columns."" for shape) and clearer section dividers, which improves readability for a user. The generated code's dividers are shorter (`=`*50 vs `=`*80) and less descriptive.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

def clean_video_games_sales(file_path='video_games_sales.csv'):
    """"""
    Cleans and preprocesses the video games sales dataset.

    Args:
        file_path (str): The path to the video_games_sales.csv file.

    Returns:
        pandas.DataFrame: The cleaned and preprocessed DataFrame.
    """"""

    # 1. Load the dataset
    df = pd.read_csv(file_path)

    # Convert 'Year' to numeric, coercing errors to NaN
    df['Year'] = pd.to_numeric(df['Year'], errors='coe...",9,"The generated code is very good and largely fulfills all requirements.

**Strong Points:**
*   **Correctness:** All tasks are handled largely correctly: loading, handling missing years, removing missing publishers, creating `Years_Since_Release`, removing duplicates, and filtering low-selling games.
*   **Code Quality:** The code is well-structured within a function, making it reusable. Docstrings are present, and variable names are clear.
*   **Completeness:** Addresses most of the requirements.
*   **Robustness:** Correctly uses `errors='coerce'` when converting 'Year' to numeric, which is good practice.

**Areas for Improvement (Minor):**
*   **No handling for negative `Years_Since_Release`:** The ground truth includes a `max(0, x)` step for `Years_Since_Release`. While less common with typical year data, it's a good defensive programming step if the `current_year` could be earlier than some `Year` values in the dataset due to data errors. The prompt implicitly suggests `2023 - Year`, implying positive results, but considering edge cases is good.
*   **No explicit handling for `Global_Sales` conversion errors:** Although the prompt doesn't explicitly ask for `Global_Sales` type conversion beyond filtering, if `Global_Sales` were not already numeric (which they usually are in such datasets), `pd.to_numeric` with `errors='coerce'` followed by `dropna` for `Global_Sales` would make the filtering more robust, similar to how 'Year' is handled. The current code assumes `Global_Sales` is numeric and filtering will work directly, but it would fail if it contained non-numeric strings. The ground truth does this.
*   **Lack of `try-except` block for `FileNotFoundError`:** The ground truth includes error handling for `FileNotFoundError` and general exceptions, which is robust. The generated code could benefit from this.
*   **`inplace=True` vs. reassignment:** The code mixes `inplace=True` with reassignment (`df = df[...]`). While functionally fine, consistency (either always using `inplace=True` or always reassigning) can sometimes make code clearer. For filtering, `inplace=True` is not available, so reassignment is necessary. But for `fillna` and `drop_duplicates`, `inplace=True` was used, then for `dropna` it was also used. This is a minor stylistic point.

Overall, it's a very strong submission, missing only a few minor robustness considerations that the ground truth included.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: 'video_games_sales.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Set a style for better aesthetics
plt.style.use('seaborn-v0_8-darkgrid')

# 1. Bar chart showing the top 10 gaming platforms by total global sales
platform_sales = df.groupby('Platform')['Global_Sales'].sum().nlargest(10...",8,"The generated code creates all the required visualizations with proper titles and labels, matching the task requirements. The data loading and basic cleaning for plotting (handling NaNs in sales and year columns) are also correctly implemented.

However, there are a couple of minor areas for improvement:
1.  **Year of Release Cleaning:** While the generated code cleans `Year_of_Release`, the ground truth's handling of `df.dropna(subset=['Year_of_Release'], inplace=True)` upfront, and then later filtering years for the line plot, is slightly more robust. The generated code re-cleans 'Year_of_Release' specifically for the line plot, which is redundant if it was cleaned once initially.
2.  **Pie Chart 'Other' Category:** The ground truth includes logic to group small categories into an 'Other' slice for the pie chart, which improves readability for datasets with many categories. The generated code simply plots all categories, which might make the pie chart cluttered if there are too many genres with very small counts.
3.  **Histogram Range:** The ground truth applies a `range` limit using `quantile(0.99)` for the histogram, which is a good practice to avoid plotting extreme outliers that can distort the visualization for heavily skewed data (like sales). The generated code plots the full range, which might not be optimal for skewed distribution.
4.  **Redundant Plotting:** The generated code uses `plt.bar(platform_sales.index, platform_sales.values, ...)` instead of directly calling `platform_sales.plot(kind='bar', ...)` which is idiomatic for pandas Series/DataFrames and often handles labels/indices more smoothly. Both approaches work, but the pandas `.plot()` method is often preferred for simplicity.

Overall, it's a strong solution that successfully addresses the core requirements. The `plt.style.use('seaborn-v0_8-darkgrid')` is a nice touch for aesthetics.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('diabetes.csv')
except FileNotFoundError:
    print(""Error: diabetes.csv not found. Please ensure the file is in the current directory."")
    exit()

# 1. Display the first 10 rows
print(""--- First 10 Rows ---"")
print(df.head(10))
print(""\n"")

# 2. Show the shape of the dataset (rows and columns)
print(""--- Dataset Shape (Rows, Columns) ---"")
print(df.shape)
print(""\n"")

# 3. List all column names and their data types
pr...",9,"The generated code is largely correct and complete, addressing almost all requirements.

**Strong Points:**
*   **Correctness:** All requested operations (loading data, shape, dtypes, outcome percentage, missing values, summary stats, zero values for specific columns) are performed accurately.
*   **Completeness:** It includes all the specified steps for EDA.
*   **Readability:** The output is well-separated with clear headers, making it easy to follow.
*   **Error Handling:** Includes basic `FileNotFoundError` handling for the CSV file.
*   **Good Identification of Zero-Value Columns:** Correctly identifies the common columns where zero values might indicate missing data.

**Areas for Minor Improvement (Why not a 10):**
*   **df.info() Output:** While `df.info()` does print the column names and dtypes, it also prints other information (memory usage, non-null counts) and returns `None`. The ground truth explicitly prints the return of `df.info()` within a `print` statement, which is a subtle difference in how the output is presented. For this specific task, directly calling `df.info()` is acceptable but the ground truth's way provides a consistent output style for all sections.
*   **Missing Values Output:** The ground truth code refined the display of missing values to only show columns with actual missing values, or explicitly state ""No missing values found."" The generated code prints `0` for all columns without missing values, which is less concise for reporting.
*   **Robustness in Zero Value Check:** The ground truth includes a check if columns in `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING` actually exist in the DataFrame, printing a warning if not. The generated code assumes all these columns will be present, which is a minor brittle point in a real-world scenario where a dataset might not have an expected column.
*   **Explicit `import numpy`:** While `numpy` wasn't directly used for calculations in the generated code, it's often a common import for data science tasks alongside pandas, which the ground truth included.

Overall, the generated code performs admirably for the task. The feedback points are minor refinements that would exist in highly robust and polished production code rather than critical failures in logic.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# 1. Load the dataset
df = pd.read_csv('diabetes.csv')

# 2. Identify columns with zero values that represent missing data
# 3. Replace zero values in these columns with NaN
columns_with_potential_zeros_as_nan = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

for col in columns_with_potential_zeros_as_nan:
    df[col] = df[col].replace(0, np.nan)

# 4. Handle missing values by fill...",7,"The generated code largely addresses all requirements of the task.

**Strong Points:**
- **Correct Zero Replacement:** It correctly identifies and replaces specified zero values with NaN.
- **Duplicate Removal:** It correctly removes duplicate rows.
- **BMI Categorization:** The `BMI_Category` creation is accurate and uses `pd.cut` effectively.
- **Standard Scaling:** It correctly identifies numerical features to scale and applies `StandardScaler` appropriately, importantly excluding the 'Outcome' column.
- **Display Output:** The final output of shape and head is correct.

**Areas for Improvement (Why not a 9 or 10):**
- **Robustness in File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness in real-world scenarios. The generated code assumes the file will always be present, which could lead to a hard crash.
- **Median Imputation Scope:** While `df.fillna(df.median(), inplace=True)` does fill NaNs in the specified columns, it also fills NaNs in *any* other numerical column present in the DataFrame. The prompt specifically asked to ""Handle missing values by filling them with the median of *each respective column*"" referring to the columns where zeros were replaced. While in this specific dataset it might not cause an issue (assuming other numeric columns don't have NaNs), it's less precise than iterating through `cols_to_replace_zero` for imputation. The ground truth's iterative approach is more aligned with the precise wording of the requirement.
- **Missing Column Handling:** The generated code does not include checks for whether a column in `columns_with_potential_zeros_as_nan` actually exists in the DataFrame before attempting to modify it (e.g., `if col in df.columns:`). This could lead to a `KeyError` if the dataset's column names change or are slightly different. The ground truth handles this gracefully with `if col in df.columns:`.
- **Clarity/Comments:** While the comments are present, they are less descriptive than the ground truth's, which aids in understanding the 'why' behind certain steps (e.g., why median was chosen, or original shape tracking).",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

# --- 1) Load the dataset ---
try:
    df = pd.read_csv('diabetes.csv')
except FileNotFoundError:
    print(""Error: diabetes.csv not found. Please make sure the file is in the current directory."")
    exit()

print(""Dataset loaded successfully. First 5 rows:"")
print(df.head())
print(""\nDataset info:"")
df.info()
print(""\n"")

# Identify numerical columns (excluding 'Outcome' if it's the target)
# Assum...",9,"The generated code successfully addresses all requirements of the task. It loads the dataset, and generates all requested visualizations (heatmap, histograms, box plots, pie chart, scatter plots, and a pair plot) with appropriate titles and labels. The code structure is clear, and imports are correct.

Strong points:
- All 7 visualization types are correctly generated.
- Proper use of `matplotlib.pyplot` and `seaborn` for plotting.
- Titles and labels are generally clear and informative.
- Handles the `FileNotFoundError` for the CSV.
- Correctly identifies numerical features for heatmap, histograms, and box plots.
- The use of `suptitle` in histogram and box plot sections to add a main title for the grid of subplots is a good touch.
- Scatter plots are correctly separated and individually titled, meeting the requirement.
- Pair plot is correctly implemented with hue for outcome.

Areas for minor improvement:
- **Consistency in `numerical_features`:** The ground truth code used `df.columns.drop('Outcome')`, which is a very direct and robust way to get all numerical columns except the target. The generated code uses `df.select_dtypes(include=['float64', 'int64']).columns.tolist()` and then removes 'Outcome'. While this works, it might be slightly less efficient or robust if there were other non-numeric columns that weren't ""object"" (e.g., boolean). However, for this dataset, it produces the same result.
- **Pie chart labels and colors:** The generated code places `Non-Diabetic (0)` as slice 1 and `Diabetic (1)` as slice 2, which is correct based on `value_counts()` default order. However, the ground truth explicitly sets the first label to be the non-diabetic one and explodes it, which is often a good practice to emphasize the larger group or non-event. The generated code explodes the 'Diabetic' slice. This is a stylistic choice and not incorrect.
- **Scatter plot y-axis for BMI vs Glucose:** In the generated code, it's `Glucose` vs `BMI` (y-axis is BMI). The ground truth has `BMI` vs `Glucose` (y-axis is Glucose). The prompt asked for ""BMI vs Glucose"" which typically implies BMI on the x-axis and Glucose on the y-axis. The generated code swapped these axes, though it's still a valid scatter plot. This is a minor deviation.
- **`sns.set_style`**: The ground truth code sets a seaborn style (`sns.set_style(""whitegrid"")`) at the beginning, which enhances aesthetics across all plots. The generated code doesn't include this, leading to default seaborn styling, which is fine but a missed opportunity for minor improvement.

Overall, the generated code met all requirements effectively and produced high-quality visualizations. The minor points are mostly about styling consistency or slight deviations from common conventions rather than correctness issues.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
try:
    df = pd.read_csv('world-happiness-report-2021.csv')
    print(""First 10 rows of the dataset:"")
    print(df.head(10))
except FileNotFoundError:
    print(""Error: 'world-happiness-report-2021.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) Show the shape of the dataset (rows and columns)
print(""\nShape of the dataset (rows, columns):"")
print(df.shape)

# 3) List all c...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task. It successfully loads the dataset, displays the first 10 rows, shows the shape, lists column names and data types, displays regional indicators and counts, identifies top 10 happiest countries, calculates summary statistics, and identifies missing values. The use of `try-except` for file loading is good practice.

Areas for minor improvement (leading to a score of 9 instead of 10):
1.  **Readability/Separation**: The ground truth code uses separators (`print(""\n"" + ""=""*70 + ""\n"")`) and numbered headings (`print(""1. First 10 rows..."")`, `print(""2. Shape of the dataset..."")`) for better visual separation and clarity of each task's output. The generated code uses only `\n` which is less distinct.
2.  **Robustness**: The ground truth code includes checks for the existence of specific columns (e.g., `'Regional indicator'`, `'Ladder score'`, `'Country name'`) before attempting operations on them. This makes the code more robust against potential `KeyError` if the dataset structure differs slightly or expected columns are missing. The generated code assumes these columns will always be present, which might lead to errors if the dataset schema changes.
3.  **Efficiency/`df.info()`**: While `df.info()` is correctly used, the ground truth explicitly mentions that it ""provides a concise summary including non-null counts and dtypes,"" which adds a useful comment.

Overall, it's a very strong submission that effectively solves the problem. The minor points are about robustness and presentation, not core functionality.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from pandas.api.types import is_numeric_dtype

# 1) Load the dataset
try:
    df = pd.read_csv('world-happiness-report-2021.csv')
except FileNotFoundError:
    print(""Error: 'world-happiness-report-2021.csv' not found. Make sure the file is in the current directory."")
    # Exit or handle the error appropriately
    exit()

# Create a copy to perform cleaning operations
df_cleaned = df...",8,"The generated code is largely correct and addresses most requirements, demonstrating a good understanding of the task.

**Strong points:**
- **Robust Missing Value Handling:** The use of `is_numeric_dtype` and separate handling for numerical (mean) and categorical (mode) missing values is a good practice.
- **Handling Coercion:** The code anticipates potential non-numeric entries by using `pd.to_numeric` with `errors='coerce'` and then re-filling NaNs, which is a good defensive programming technique, although the specific dataset for this task doesn't strictly require it.
- **Derived Features:** Correctly implements `Happiness_Category` using a function and apply, `GDP_Category` using `pd.qcut` (and `duplicates='drop'` is a thoughtful addition), and `Well_being_Score` using column aggregation.
- **Outlier Removal:** Correctly applies the IQR method for 'Ladder score'.
- **Standardization and Encoding:** Correctly applies `StandardScaler` to numerical features and `LabelEncoder` to the 'Regional indicator'.
- **Completeness:** Addresses almost all specific requirements of the prompt.
- **Clarity:** Code is well-structured and easy to follow.

**Areas for Improvement / Minor Issues:**
- **Order of Operations (Minor):** The ground truth performs standardization *then* encoding. The generated code does encoding *then* standardization. While for these specific operations it might not cause a critical issue, typically, encoding categorical features comes before standardization of numerical features if those numerical features were to be influenced by the encoding process (not relevant here, but good practice order). More importantly, the prompt asks for standardizing numerical features (step 5) and then encoding (step 6). The generated code performs 5) Encode, then 6) Standardize, which is a misordering of the prompt's steps.
- **Redundant `pandas.api.types.is_numeric_dtype` import:** While not incorrect, `df.select_dtypes(include=np.number)` as used in the ground truth is a more idiomatic and concise way to identify numerical columns. The current loop iterates through all columns and then checks type, which is less efficient than filtering columns first.
- **Column Name Consistency:** There's a slight inconsistency with ' Social support' (leading space) in `numerical_features_to_scale`. The ground truth directly relies on `select_dtypes` which inherently gets the correct column names from the DataFrame, avoiding such typos. The generated code has a `col for col in numerical_features_to_scale if col in df_cleaned.columns` safety check, which mitigates this, but still better to avoid manual lists prone to typos.
- **`df_cleaned = df.copy()`:** While a good practice to avoid modifying the original DataFrame, the prompt directly asks to clean `the` dataset and doesn't explicitly require retaining the original. For a benchmark context focused on the final output, this extra copy adds a tiny bit of overhead without a direct benefit for the specified task. The ground truth directly operates on `df`.
- **Missing Final Print for Shape/Head:** The prompt specifically asks to ""Display the cleaned dataset's shape and first 5 rows"", which the ground truth does at the very end. The generated code prints before standardization and encoding in its numbering, but then numbers standardization 6 and encoding 5, and the print is at 7, so the numbering is off slightly in the generated code itself (5 and 6 are swapped in the comments compared to the prompt). The final print happens correctly, but the numbering in the comments is swapped relative to the prompt's instructions.

The code is generally very good, fulfilling almost all requirements with correct logic. The points for improvement are minor details in best practice, efficiency, and adherence to the exact ordering of operations specified in the prompt.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# 1. Load the dataset
try:
    df = pd.read_csv('world-happiness-report-2021.csv')
except FileNotFoundError:
    print(""Error: 'world-happiness-report-2021.csv' not found. Please ensure the CSV file is in the same directory as the ...",9,"The generated code largely fulfills all requirements and creates good-quality visualizations.

**Strong Points:**
- **Correctness:** All visualization types (map, bar, heatmap, scatter, box, radar) are generated correctly and portray the requested data.
- **Completeness:** All 7 tasks are addressed.
- **Code Quality:**
    - Proper imports are included.
    - Error handling for file not found is present.
    - Column renaming is handled effectively.
    - Visualizations utilize appropriate libraries (Plotly for map, Seaborn/Matplotlib for others).
    - Titles and labels are generally clear and informative.
    - `tight_layout()` is used effectively for better presentation.
    - The radar chart includes Min-Max scaling for fair comparison, which is a good practice.
    - The radar chart's `subplot_kw=dict(polar=True)` and `ax.set_theta_offset` / `set_theta_direction` show good understanding of polar plots.

**Areas for Improvement (Minor):**
- **Consistency in Renaming:** The generated code renames 'Ladder score' implicitly by using it directly, but the ground truth explicitly renames it to 'Happiness Score' for consistency, which can improve readability in plots and code. The generated code renames 'Healthy life expectancy' to 'Life Expectancy' but uses 'Life Expectancy' as an axis label in the heatmap which is correct.
- **Scatter Plot Legend Placement:** While `bbox_to_anchor` is used, the legends on the scatter plots are still a bit too close to the plot area or slightly overlapping in some screen resolutions, which could be fine-tuned. The ground truth explicitly used `rect=[0, 0, 0.95, 1]` for `tight_layout` which provides a bit more space.
- **Radar Chart Y-axis Labels:** The generated code removes `yticklabels` from the radar chart, which makes it harder to interpret the normalized values directly. The ground truth keeps them visible (`ax.set_yticks`, `ax.set_yticklabels`). While it's a stylistic choice, showing them can be helpful for understanding the normalized scale.
- **Radar Chart Scaling:** The generated code scales the `top_5_countries` dataframe directly using `MinMaxScaler`. The ground truth scales the *entire* `df` first (`df_scaled`), then extracts the `top_5_scaled` from this already scaled dataframe. Scaling the entire dataset before isolating the top 5 ensures that the normalization is based on the global min/max for each feature, which is a more robust approach for comparison in a radar chart across subsets of data. The generated code's approach scales based only on the min/max within the top 5 countries for each feature, which might obscure the true relative strengths/weaknesses compared to the full dataset. This is a subtle but important detail for accurate comparative visualization.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# 1. Load the dataset and parse the 'date' column as datetime objects
try:
    df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
except FileNotFoundError:
    print(""Error: 'synthetic_covid19_data.csv' not found. Please ensure the file is in the current directory."")
    exit()

print(""--- 2. First 10 rows ---"")
# 2. Display the first 10 rows
print(df.head(10))

print(""\n--- 3. Shape of the dataset (rows, columns) ---"")
# 3. Show the shape of the ...",7,"The generated code provides a good attempt at the task and covers most of the requirements. It successfully loads the data, displays basic information, groups by continent, and calculates summary statistics.

However, there are a few areas where it falls short compared to the ground truth, mainly in robustness and precision:

1.  **Error Handling for File Not Found:** While it includes a `try-except` for `FileNotFoundError`, it doesn't handle other potential `Exception` types during file loading, which the ground truth does. This makes the data loading less robust.
2.  **Continental Summary Robustness:** The generated code directly `groupby('continent').sum()`. If 'continent' has NaN values, `groupby` will automatically create a group for NaN, which might not always be the desired behavior (often, NaNs should be excluded or imputed for aggregates). The ground truth explicitly handles this by dropping NaNs before grouping for this specific task.
3.  **Top 5 Locations Accuracy:** The `top_5_locations` logic is slightly less robust.
    *   It uses `df.sort_values('date').drop_duplicates('location', keep='last')` which is correct for getting the latest date.
    *   However, `nlargest` implicitly assumes `total_cases` is numeric or can be reliably coerced. The ground truth explicitly uses `pd.to_numeric` with `errors='coerce'` and then `dropna` to handle potential non-numeric values in `total_cases` more gracefully before identifying the top locations, preventing potential errors or unexpected results if the column contains dirty data.
    *   The `nlargest` call itself is fine, but the preparation of the `total_cases` column for this step is weaker.
4.  **Summary Statistics Detail:** The generated code uses `describe()` which provides mean, std, min, max, and quantiles (25%, 50%, 75%). The prompt specifically asked for ""mean, median, min, max, std"". While `describe()` includes these, it also includes extra quantiles. The ground truth uses `agg` to precisely select only the requested statistics, making it more aligned with the specific prompt. Also, it doesn't explicitly handle non-numeric values in 'new_cases' or 'new_deaths' before calling `describe()`, which could lead to NaNs or errors if the columns aren't clean. The ground truth explicitly uses `pd.to_numeric` and `dropna` for robustness.
5.  **Missing Value Display:** The generated code conditionally displays `missing_values[missing_values > 0]`. While this is often useful, the prompt asked to ""Count and display missing values for **each column**."" The ground truth `df.isnull().sum()` directly fulfills this, displaying all columns regardless of missing values, which is a more complete answer to the specific wording of the prompt. Having the ""No missing values found"" message is good, but the request was to show counts for *each* column.

Overall, the code is functional and addresses most aspects, but could be improved in terms of robustness, precision in specific calculations, and strict adherence to output format (especially for summary stats and missing values).",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Make sure the file is in the current directory."")
    exit()

# 1) Calculate and report the average (mean) 'quality' of the red wines
average_quality = df['quality'].mean()
print(f""Average 'quality' of red wines: {average_quality:.2f}"")

# 2) Group the wines by their 'q...",9,"The generated code successfully addresses all parts of the task: it calculates the mean quality, groups by quality to get average alcohol, and generates a bar chart. Imports are correct, and the structure is clean. The file handling is robust and includes error handling.

The main areas where it differs from the ground truth and could be slightly improved for completeness (but are not critical failures) are:
- The print statements in the generated code are slightly less detailed/formatted than the ground truth (e.g., missing numbering like ""1. Average..."").
- The visualization is good, but the ground truth includes additional aesthetic enhancements like `sns.set_style`, precise `xticks`/`yticks` font sizes, data labels on bars, and `plt.tight_layout()`, which make the plot more polished and readable. The generated code uses `plt.grid` which is a valid alternative to `sns.set_style` for a grid.
- The use of `average_alcohol_by_quality.index` and `.values` for plotting is correct and functional, but creating a DataFrame with `reset_index()` as in the ground truth can sometimes make `seaborn` plotting cleaner and more aligned with its common usage (e.g., `data=df_name`). However, for a simple Series, direct use of index/values is perfectly acceptable.

Overall, it's a very strong submission that fulfills the core requirements effectively.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# 1. Load the dataset and parse the 'date' column as datetime objects
try:
    df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
except FileNotFoundError:
    print(""Error: 'synthetic_covid19_data.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2. Handle missing values
# Fill 'hosp_patients' and 'icu_patients' with 0
df['hosp_patients'] = df['hosp_patients'].fillna(0)
df['icu_patients'] = df[...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
- **Correct Data Loading:** Uses `parse_dates=['date']` in `read_csv`, which is a more concise way to parse dates compared to a separate `pd.to_datetime` call.
- **Correct Handling of Missing Values:** Correctly fills `hosp_patients` and `icu_patients` with 0, and `positive_rate`, `reproduction_rate` with their respective means.
- **Robustness in Numeric Conversion:** Proactively converts columns to numeric using `pd.to_numeric(errors='coerce')` for 'population' and 'people_fully_vaccinated', which is good practice.
- **Correct Mortality Rate Calculation:** The calculation `(df['total_deaths'] / df['total_cases']) * 100` and subsequent handling of `np.inf`/`np.nan` by converting them to `np.nan` then filling with 0 is correct and robust.
- **Correct Vaccination Rate Calculation:** Similarly, calculates `vaccination_rate_per_capita` and handles division by zero robustly. The addition of `clip(upper=100)` is a sensible domain-specific cap.
- **Correct Filtering:** Accurately filters data for the year 2021 using `.dt.year` and `copy()` to avoid `SettingWithCopyWarning`.
- **Correct Display:** Prints the shape and head of the cleaned dataset as requested. The selective display of columns in `head()` is a nice touch, although not explicitly required, as it makes the new columns easily visible.
- **Error Handling:** Includes `try-except FileNotFoundError`, which is good practice.

**Minor Area for Improvement:**
- **Implicit type conversion for total_deaths/total_cases:** While operations like division often coerce types, the ground truth code explicitly converts `total_deaths` and `total_cases` to numeric before calculating `mortality_rate`. The generated code does not explicitly do this, relying on automatic type coercion or assuming these columns are already numeric. If these columns contained non-numeric strings, it could lead to `TypeError` or `ValueError` or incorrect results if they were `object` dtype. For robustness, explicit `pd.to_numeric` calls for `total_deaths` and `total_cases` would be beneficial, similar to how it was done for `population` and `people_fully_vaccinated`. However, for typical numerical data, pandas often handles this implicitly.

Overall, the generated code is excellent and provides a correct and robust solution to the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings that might arise from plotting
import warnings
warnings.filterwarnings('ignore')

# 1. Load the dataset and parse the 'date' column
try:
    df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
except FileNotFoundError:
    print(""Error: 'synthetic_covid19_data.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Set seaborn style for better aest...",9,"The generated code is highly correct, complete, and follows good coding practices. It successfully addresses all requirements of the task.

**Strong Points:**
- **Correctness:** All plots are generated accurately according to the task description.
- **Data Handling:** Correctly handles `date` parsing and `FileNotFoundError`.
- **Visualization Types:** All five requested visualization types (line plot, bar chart, scatter plot, histogram, box plot) are implemented.
- **Specific Requirements:**
    - **India line plot:** Correctly filters for 'India' and plots 'new_cases' over time.
    - **Total deaths by continent:** Correctly calculates the sum of 'total_deaths' for each continent by taking the latest value per location before summing. This is a crucial detail for cumulative metrics.
    - **GDP vs. Vaccinations scatter plot:** Correctly uses the latest entry per location, handles `dropna`, and uses `log` scales for both axes, which is appropriate for these types of skewed distributions. It also correctly uses `population` for sizing and colors by `continent`.
    - **Reproduction rate histogram:** Correctly plots the histogram and handles `dropna`.
    - **Stringency index box plot:** Correctly compares 'stringency_index' across continents and handles `dropna`.
- **Plot Formatting:** Each plot includes appropriate titles, x-labels, y-labels, and grid lines. The use of `sns.set_theme(style=""whitegrid"")` and `plt.tight_layout()` further improves aesthetics and readability.
- **Imports:** All necessary libraries are imported.
- **Clarity:** The code is well-structured with comments explaining each section.

**Minor Areas for Improvement (why not a 10):**
- **Explicit `plt.show()` for each plot:** While `plt.show()` often shows all buffered plots when called once at the end, it's generally good practice to call `plt.show()` immediately after `plt.tight_layout()` for each individual plot if you want them to appear sequentially or in separate windows (depending on the environment). In a script, the `plt.show()` at the very end will display all of them, but this could be slightly ambiguous if someone expected them to pop up one by one. The ground truth code demonstrates this approach.
- **Warning suppression:** While `warnings.filterwarnings('ignore')` can be useful, it's often better to be specific about what warnings are being ignored (`FutureWarning` from seaborn, etc.) if possible, or address the underlying cause. For a benchmark, it's a minor point, but good practice leans towards not broadly ignoring warnings.
- **Scatter plot GDP label:** The x-axis label is `GDP Per Capita (log scale)` which is accurate but doesn't need to explicitly state 'log scale' as `plt.xscale('log')` already implies it. This is a very minor stylistic point.

Overall, the generated code is excellent and fully meets the requirements of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')

    # 1) List all column names and identify the data type of each column
    print(""--- Column Names and Data Types ---"")
    df.info()
    print(""\n"")

    # 2) For the numerical columns elixirCost, Win Rate, and Usage, calculate and report basic descriptive statistics
    numerical_cols = ['elixirCost', 'Win Rate', 'Usage']
    print(""--- Descriptive Statistics for Numerical Columns ---"")
   ...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
*   **Correctness:**
    *   Loads the CSV file correctly.
    *   Identifies column names and data types using `df.info()`, which is a concise and effective way to achieve this.
    *   Calculates descriptive statistics for the specified numerical columns (`elixirCost`, `Win Rate`, `Usage`) using `df[numerical_cols].describe()`. This covers mean, min, max, and standard deviation, and also includes quartiles, which are a good addition.
    *   Reports the total number of rows accurately using `df.shape[0]`.
*   **Code Quality:**
    *   Properly imports pandas.
    *   Includes basic error handling for `FileNotFoundError`.
    *   The code is well-structured and easy to read.

**Areas for Improvement (Why not a 10):**
*   **Completeness/Specificity for Statistics:** The prompt specifically asked for ""mean, median, standard deviation, min, max"". While `df.describe()` provides these (except `median` directly, though it does give quartiles which imply median), it also provides other statistics (25th percentile, 75th percentile, count) that weren't explicitly requested. The ground truth code specifically selected `mean`, `median`, `std`, `min`, `max` using `agg`, which directly addresses the prompt's request. To be truly perfect against the prompt, the generated code would need to specifically pull out the median or use `agg`.
*   **Column Type Listing Clarity:** While `df.info()` does show data types, the prompt asked to ""List all column names and identify the data type of each column."" The ground truth code explicitly iterates and lists column names, then prints `df.dtypes` for clarity regarding data types. `df.info()` lumps this together. Both are valid, but the ground truth was slightly more explicit in its separation as per the prompt's wording.
*   **Error Handling for Missing Columns:** The ground truth code includes a check to ensure the specified numerical columns (`elixirCost`, `Win Rate`, `Usage`) actually exist in the DataFrame before attempting to calculate statistics on them. The generated code assumes these columns will always be present, which could lead to a `KeyError` if one or more are missing. This is a minor robustness point but worth noting for ""expert"" level evaluation.
*   **Output Formatting:** The ground truth code has more explicit print statements indicating what each section of output represents (e.g., ""--- Column Names and Data Types ---"", ""Column Names:"", ""Data Types of Each Column:""). The generated code is slightly less verbose in its output headers. This is a stylistic choice but contributes to readability and user understanding.

Overall, the generated code is a very good solution and performs all the required tasks. The minor deductions are for strict adherence to the requested statistics (median specifically) and robustness in column handling.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter for cards with elixirCost <= 3
filtered_df = df[df['elixirCost'] <= 3]

# Select 'Card' and 'Win Rate' columns
selected_columns_df = filtered_df[['Card', 'Win Rate']]

# Sort by 'Win Rate' in descending order
sorted_df = selected_columns_df.sort_values(by='Win Rate', ascending=False)

# Display the result
print(sorted_df)
```",9,"The generated code correctly implements all the requirements of the task: it loads the CSV, filters by 'elixirCost' less than or equal to 3, selects the specified columns ('Card' and 'Win Rate'), and sorts the result by 'Win Rate' in descending order. The imports are proper, and the code is concise and readable.

The only minor area where the ground truth is superior is in its robustness. The ground truth includes error handling for `FileNotFoundError` when loading the CSV and checks for the existence of required columns before selection. While not explicitly asked for in the prompt's requirements for the generated code, these are good practices for production-ready code. However, given the prompt asked for ""only the Python code without explanations"" and specified the task concisely, the generated code fully delivers on the core functional requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not found. Make sure the file is in the current directory."")
    exit()

# Generate the histogram for 'elixirCost'
plt.figure(figsize=(10, 6)) # Optional: Adjust figure size for better readability
sns.histplot(data=df, x='elixirCost', bins=range(1, df['elixirCost'].max() + 2...",9,"The generated code correctly addresses all parts of the task. It loads the CSV, generates a histogram for `elixirCost` using seaborn, includes a title and labeled axes, and handles `FileNotFoundError`. The use of `sns.histplot` is good practice for visualizing distributions. The dynamic `bins` and `xticks` based on `df['elixirCost'].max()` is a nice touch, making the code more robust to varying maximum elixir costs.

Minor improvements could include:
- Adding more robust error handling for cases where the 'elixirCost' column might be missing, similar to the ground truth.
- Explicitly setting `file_path` as a variable for better maintainability (though less critical for this simple script).
- Adding comments to explain specific choices (e.g., `kde=False`, `edgecolor`).
- While `plt.grid` at the end works, typically it should be placed earlier or after the plot creation but before `plt.show()`. However, this is a minor style point.

Overall, it's a very good and functional solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Assume 'iris.csv' is in the current directory
file_path = 'iris.csv'

# Load the dataset
df = pd.read_csv(file_path)

# 1) List all column names and identify the data type of each column
print(""--- Column Names and Data Types ---"")
print(df.info())
print(""\n"" + ""=""*50 + ""\n"")

# 2) For the numerical columns, calculate and report basic descriptive statistics
numerical_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
print(""--- Descripti...",7,"The generated code correctly performs most of the requested tasks. It loads the dataset, lists column info, calculates descriptive statistics for the specified columns, and reports unique species counts.

However, there are a few areas for improvement compared to the ground truth:

1.  **Robustness (Loading Data):** The generated code lacks error handling for `FileNotFoundError` or other potential issues when loading the CSV. The ground truth includes a `try-except` block, which is essential for robust code.
2.  **Descriptive Statistics Detail:** While `df.describe()` is a quick way to get stats, it provides more than just mean, median, Std, min, max (e.g., quartiles, count). The prompt specifically asked for ""mean, median, standard deviation, min, max"". The ground truth explicitly uses `.agg()` to select only those statistics, which is more precise. Although `describe()` does contain these, the `agg` approach is more specific to the prompt's request.
3.  **Column Info Output:** `df.info()` is a good way to get column names and data types, but its output also includes non-requested information like memory usage and non-null counts, which clutters the output for the specific request. The ground truth separates printing column names and data types, making the output more focused on the explicit requirements.
4.  **Column Existence Check:** The ground truth includes checks to ensure the specified numerical columns and the 'Species' column actually exist in the DataFrame before attempting operations. This makes the code more robust against variations in the dataset. The generated code assumes these columns will always be present, which could lead to `KeyError` if they are missing.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter for 'Iris-setosa' species
setosa_df = df[df['Species'] == 'Iris-setosa']

# Select 'SepalLengthCm' and 'SepalWidthCm' columns
selected_columns_df = setosa_df[['SepalLengthCm', 'SepalWidthCm']]

# Display the result
print(selected_columns_df)
```",8,"The generated code correctly addresses the prompt's core requirements: loading the CSV, filtering by 'Iris-setosa' species, and selecting/displaying the specified columns. It uses `pandas` correctly and is concise.

However, it lacks robust error handling for common issues like `FileNotFoundError` or `KeyError`, which are crucial for real-world applications where the dataset might be missing or malformed. While the prompt asks for ""only the Python code without explanations"" and does not explicitly demand error handling, an expert solution would typically include it for completeness and reliability. The ground truth provides excellent examples of such error handling using `try-except` blocks.

To achieve a higher score (9 or 10), the generated code should incorporate `try-except` blocks to handle potential `FileNotFoundError` if 'iris.csv' is not found, and `KeyError` if expected columns are missing, similar to the ground truth.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the iris dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: iris.csv not found. Please ensure the file is in the current directory."")
    exit()

# Create the scatter plot
plt.figure(figsize=(8, 6))

# Get unique species and a color palette
species_names = df['Species'].unique()
colors = plt.cm.get_cmap('viridis', len(species_names))

# Plot each species with a different color
for i, spec...",9,"The generated code successfully loads the dataset, creates the scatter plot with correct axes and species-based coloring, and includes a title and axis labels, fulfilling all task requirements. It demonstrates good code quality with proper imports and a clear structure.

Strengths:
- **Correctness**: All requirements of the task are met. The plot correctly visualizes `SepalLengthCm` vs `SepalWidthCm` and colors points by `Species`.
- **Imports**: Both `pandas` and `matplotlib.pyplot` are imported correctly.
- **Error Handling**: Includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice.
- **Clarity**: The code is well-structured and easy to understand.
- **Coloring**: Uses `matplotlib.cm.get_cmap` for dynamic coloring of species, which is robust.

Minor Improvements (not critical for a lower score but could make it a 10):
- The ground truth code wraps the plotting logic in a `main` function and includes `if __name__ == ""__main__"":`, which is good practice for larger scripts but not strictly necessary for this small task.
- The ground truth code includes more robust error handling for missing columns, which is a more complete solution for a ""production-ready"" script, but for this specific problem, it wasn't requested.
- The `plt.cm.get_cmap('viridis', len(species_names))` usage is good, but sometimes `tab10` or a more distinct Colormap could be preferred for categorical data where individual colors need to be very distinguishable. However, 'viridis' is still a valid choice.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# 1. List all column names and identify the data type of each column
print(""--- Column Names and Data Types ---"")
print(df.info()) # df.info() prints column names, non-null counts, and dtypes
print(""\n"")

# 2. Calculate and report basic descriptive statistics for numerical columns
numerical_cols = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
print(""--- Descriptive Statistics for Numerical Columns ---"")
# Using .desc...",7,"The generated code correctly loads the CSV, lists column names and data types (though `df.info()` includes more than just column names and types, which is acceptable), calculates descriptive statistics for the specified numerical columns, and reports value counts for 'HeartDisease'.

Areas for improvement:
1.  **Error Handling**: The generated code lacks explicit error handling for `FileNotFoundError` as present in the ground truth. This is a crucial aspect for robust code.
2.  **Specific Descriptive Statistics**: While `df.describe()` provides mean, std, min, max, it labels median as '50%'. The ground truth explicitly renames '50%' to 'median' for clarity and selects only the requested statistics, making the output more tailored. The ground truth also explicitly checks if the numerical columns exist before calculating statistics, making it more robust.
3.  **Clarity of `df.info()` output**: `df.info()` prints directly and includes non-null counts along with data types. While it fulfills the requirement of listing data types, the ground truth's approach of printing `df.dtypes` directly might be considered cleaner for just data types, and `for col in df.columns: print(f""- {col}"")` for column names specifically.
4.  **Output Readability**: The ground truth uses more explicit print statements and separators (`-` * 40) which improve the readability and structure of the output.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# Filter for patients with Age > 50 AND HeartDisease == 1
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]

# Select and display the specified columns
selected_columns_df = filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]
print(selected_columns_df)
```",9,"The generated code correctly performs all the required tasks: it loads the CSV, filters the DataFrame based on the age and HeartDisease criteria, and selects and displays the specified columns. The code is concise and directly addresses the problem statement. The imports are correct.

One minor improvement, though not strictly required by the prompt, would be to encapsulate the logic within a function, as done in the ground truth. This improves reusability and testability. Additionally, adding error handling (e.g., `try-except` for `FileNotFoundError`) would make the code more robust for production environments, but for a direct task completion, it's sufficient. Overall, a very good solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Make sure the file is in the current directory."")
    exit()

# 1. Filter wines that have an 'alcohol' content greater than 12% AND a 'pH' value less than 3.3
filtered_wines = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]

# Check if any wines match the criteria
if filtered_wines.empty:
    print(""No wines found matching the...",9,"The generated code correctly implements all the requirements of the task. It properly filters the DataFrame based on two conditions, identifies the row with the maximum 'quality' in the filtered subset, and displays all features of that specific wine. The code includes necessary imports and handles the `FileNotFoundError`.

**Strengths:**
*   **Correct Logic:** The boolean indexing for filtering `(df['alcohol'] > 12) & (df['pH'] < 3.3)` is correctly applied.
*   **Correct Max Identification:** `idxmax()` is the appropriate method to find the index of the highest quality wine.
*   **Clear Display:** The final print statement correctly shows the features of the selected wine.
*   **Error Handling:** Includes a basic `FileNotFoundError` check.
*   **Edge Case Handling:** Correctly checks if `filtered_wines` is empty.

**Areas for Minor Improvement (to reach a perfect 10):**
*   **Output Readability:** The ground truth uses `print(best_wine_in_subset.to_string())` which ensures all features are displayed without pandas default truncation (e.g., ellipses). The generated code just `print(wine_with_highest_quality)` which might truncate long series output.
*   **Function Encapsulation:** While not explicitly required by the prompt's output format, encapsulating the logic within a function (as in the ground truth) would improve reusability and modularity, especially for larger scripts or benchmarks where functions are expected.
*   **Error Handling Granularity:** The `try-except` block for `FileNotFoundError` is good. However, the ground truth also adds `KeyError` for missing columns and a general `Exception` for robustness, which is good practice for production-ready code.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# 1. Generate a histogram for the 'Age' column
plt.figure(figsize=(9, 6))
sns.histplot(data=df, x='Age', kde=True, bins=10) # Using kde for density estimation, bins for clarity
plt.title('Distribution of Age')
plt.xlabel('Age (Years)')
plt.ylabel('Frequency / Count')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# 2. Generate a bar chart for the 'Sex'...",7,"The generated code correctly addresses the core requirements of loading the data and generating both a histogram for `Age` and a bar chart for `Sex`, with titles and labeled axes. The use of `seaborn` for plotting is also in line with the instructions.

However, there are a few areas for improvement:
1.  **Readability of Sex Labels**: The generated code directly plots `Sex` as 0 and 1, relying on the user to interpret `Sex (0 = Female, 1 = Male)`. The ground truth code maps these numerical values to 'Female' and 'Male' strings, making the bar chart much more intuitive and readable, which is a better practice for visualization tasks.
2.  **Error Handling**: The generated code lacks any error handling (e.g., `FileNotFoundError` for 'heart.csv'). While the prompt didn't explicitly ask for it, robust code should include it. The ground truth demonstrates good practice in this regard.
3.  **Labeling Consistency/Clarity**: The y-axis label for the histogram is `Frequency / Count`. While not incorrect, `Number of Individuals` or `Count` as used in the ground truth is often more specific and clearer in this context.
4.  **Minor Details**: The ground truth includes `plt.tight_layout()` and `fontsize` adjustments, which improve the aesthetics and prevent potential overlapping issues, especially for more complex plots. While not critical for this specific simple task, they are good practices for robust plotting.

Overall, the code is functional and largely correct, but it misses some niceties and robustness features present in the ground truth.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# Load the spotify_2023.csv dataset
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 1. List all column names and identify the data type of each
print(""--- Column Names and Data Types ---"")
for col, dtype in df.dtypes.items():
    print(f""- {col}: {dtype}"")
print(""-"" * 40 + ""\n"")

# 2. Convert the `streams` column to a numeric d...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, lists column names and types, converts the 'streams' column to numeric handling NaNs, calculates descriptive statistics for the specified numerical columns, and reports unique artists.

Strong points:
- Proper imports and error handling for file not found.
- Correctly identifies data types and iterates through them.
- Handles the conversion of 'streams' to numeric, including a robust `replace(',', '')` step before `pd.to_numeric` which is good practice for common numerical string formats.
- Correctly uses `errors='coerce'` for the 'streams' conversion.
- Accurately reports NaNs in the 'streams' column.
- Calculates descriptive statistics for the specified columns using `.describe()` and then `.loc` for specific stats, which is efficient.
- Renaming '50%' to 'median' for better readability is a nice touch.
- Correctly counts unique artists.

Areas for minor improvement:
- The ground truth code uses `df.info()` for reporting column names and data types, which provides a more comprehensive summary (including non-null counts and memory usage) in one go, rather than a loop. While the generated code's loop is correct semantically, `df.info()` is often preferred in data analysis for its conciseness and completeness.
- The reporting of NaNs for 'streams' in the generated code only gives the count, whereas the ground truth also shows sample rows if NaNs are present, which can be very helpful for debugging or understanding the data quality.
- The print statements for section headers are a bit inconsistent with the use of dashes and `print(""-"" * 40 + ""\n"")`. It could be slightly cleaner.

Overall, it's a very solid solution that performs all required operations accurately.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Filter the dataset to include only songs where released_year is 2023
df_2023 = df[df['released_year'] == 2023].copy()

# Ensure the 'streams' column is numeric, coercing errors to NaN
df_2023.loc[:, 'streams'] = pd.to_numeric(df_2023['streams'], errors='coerce')

# Select the specified columns
selected_songs = df_2023[['track_name', 'artist(s)_name', 'streams']]

# Sort the resulting data by the 'streams' c...",9,"The generated code correctly implements all requirements of the task: loading the CSV, filtering by `released_year`, coercing the `streams` column to numeric, selecting the required columns, and sorting in descending order. The use of `.copy()` after filtering is good practice to prevent `SettingWithCopyWarning`. The prompt asked for only the Python code, and the generated code provided just that.

The only minor point to consider, which is handled in the ground truth but not explicitly required by the prompt, is error handling for `FileNotFoundError` or other potential issues with `pd.read_csv`. However, given the prompt's simplicity and focus on data manipulation, this is not a strict failing point. Another very minor difference is that the ground truth explicitly drops NaNs from the 'streams' column before sorting, which can be useful for a cleaner top-streams list. The generated code implicitly handles NaNs by placing them at the end when sorting in descending order, which is also a valid approach depending on the desired outcome. For the given task, both are acceptable.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Make sure the file is in the current directory."")
    exit()

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: Histogram of 'bpm'
axes[0].hist(df['bpm'], bins=20, edgecolor='black', color='skyblue')
axes[0].set_title('Distribution of Beats Per Minute (...",9,"The generated code correctly loads the dataset, generates both the histogram for 'bpm' and the scatter plot for 'danceability_%' vs 'energy_%', and includes appropriate titles and axis labels for both plots. It also handles the FileNotFoundError gracefully. The use of subplots is an efficient way to display multiple plots.

Areas for minor improvement:
- While not explicitly requested, ensuring the plots are distinct (e.g., separate figure calls as in the ground truth) can sometimes be preferred depending on the specific visualization goal, though subplots are perfectly valid and often desirable. The instruction ""Ensure both plots have titles and labeled axes"" was interpreted as individual plots, which subplots handle well.
- The `bins` argument for the histogram could be `auto` (as in the ground truth) or explicitly calculated for a more optimal distribution, rather than a fixed number like 20, but 20 is a reasonable default.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# Assume the CSV file is in the current directory
csv_file_path = 'youtube_trendings.csv'

try:
    # Load the dataset
    df = pd.read_csv(csv_file_path)

    # 1) List all column names and their inferred data types
    print(""--- 1) Column Names and Data Types ---"")
    print(df.info())
    print(""\n"")

    # 2) For the columns `Video views`, `Likes`, and `Dislikes`:
    #    Remove any commas, convert them to numeric types,
    #    treat conversion failures as ...",9,"The generated code is highly correct and complete, addressing all requirements of the task.

**Strong Points:**
- **Correct CSV Loading:** Properly uses `pd.read_csv` and includes `try-except` for `FileNotFoundError`, crucial for robust code.
- **Accurate Column Info:** Uses `df.info()` to display column names and data types as requested.
- **Correct Numeric Cleaning:** Successfully removes commas, converts to numeric with `errors='coerce'`, and fills NaNs with 0 for the specified columns. The use of `astype(str)` before `str.replace` is also a good practice to ensure handling of non-string types gracefully.
- **Correct Descriptive Statistics:** Calculates and prints mean, median, std, min, and max for the cleaned numeric columns. Using `agg` is a concise and effective way to do this.
- **Correct Category NaN Handling:** Fills missing 'Category' values with 'Unknown' using `fillna` and `inplace=True`. It also correctly reports initial and final NaN counts.
- **Correct Unique Category Count:** Accurately reports the count of unique values in the 'Category' column.
- **Robustness:** Includes checks (`if col in df.columns`) before processing columns, which is excellent for handling potential missing columns in the dataset.
- **Clear Output:** The print statements clearly delineate each step, making the output easy to follow.

**Minor Areas for Improvement (prevented a 10):**
- **`df.info()` output:** The `df.info()` command itself prints to console and returns `None`. Printing `print(df.info())` results in an extra `None` being printed to the console after the info table. While functionally correct, it's a minor aesthetic detail. `df.info()` alone would suffice.
- **Descriptive Statistics Presentation:** While using `agg` is good, the output for descriptive statistics (a DataFrame) might be less visually intuitive for a quick ""report"" than separate print statements for each statistic per column, as done in the ground truth. This is a matter of preference and interpretation of ""report basic descriptive statistics"". The current output is technically correct.
- **Redundant newline prints:** Multiple `print(""\n"")` statements between sections create significant vertical spacing. While not an error, it can make the output a bit ""chatty"".

Overall, this is a very strong submission that fully meets the requirements of the task with robust and clean code.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 1) For the 'Video views' column: remove commas, convert to numeric,
#    treat conversion failures as NaN, and fill NaNs with 0.
df['cleaned_video_views'] = (
    df['Video views']
    .astype(str)
    .str.replace(',', '', regex=False)
    .apply(pd.to_numeric, errors='coerce')
    .fillna(0)
)

# 2) Filter the dataset to include only videos where the cleaned 'Video views' is greater than 50,000,000
f...",9,"The generated code is almost entirely correct and effectively solves the given task.

Strong points:
- **Correctness**: It correctly loads the dataset, cleans the 'Video views' column by removing commas, converting to numeric, handling failed conversions with `coerce` to NaN, and filling NaNs with 0.
- **Clarity**: The chained operations for cleaning the 'Video views' column are clear and efficient.
- **Completeness**: It addresses all requirements: filtering by views > 50M, selecting specified columns, and sorting in descending order.
- **Code Quality**: Proper imports are included, and the code is well-structured with comments.

Areas for improvement (minor):
- The `try-except FileNotFoundError` block for loading the CSV is present in the ground truth but missing from the generated code. While the prompt assumes the file is in the current directory, robust code would include this. This is a best practice for real-world scenarios but not explicitly mandated by the prompt's functional requirements.
- The prompt explicitly asked for the *cleaned `Video views`* column to be displayed, which could imply the original column name should be preserved or reused. The generated code creates a *new* column `cleaned_video_views`. While functionally correct, using a new column name means the final output has a different column header than `Video views`. The ground truth code modifies the original column in place, which aligns slightly closer with the phrasing ""select and display the cleaned `Video views` columns"" if interpreted strictly to mean the original column name. However, creating a new column is often safer practice to avoid overwriting original data. This is a very minor stylistic difference and doesn't impact the functionality.

Overall, it's a very good solution. The creation of a new column `cleaned_video_views` is a reasonable and often preferred approach, even if the prompt's wording could be interpreted differently. The main minor omission is the error handling for `FileNotFoundError`.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found. Please make sure the file is in the current directory."")
    exit()

# List of columns to clean and convert
columns_to_clean = ['Likes', 'Video views']

# Process each specified column
for col in columns_to_clean:
    if col in df.columns:
        # Remove commas, convert to string first t...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans the 'Likes' and 'Video views' columns by removing commas, converting to numeric, and filling NaNs with 0. It then correctly generates a scatter plot with the specified title and axis labels.

Strong points:
- **Correct cleaning logic**: Uses `astype(str).str.replace(',', '')` to remove commas and `pd.to_numeric(errors='coerce')` to handle conversion errors, followed by `fillna(0)`. This is exactly as required.
- **Correct plotting**: Generates the scatter plot with the correct columns and sets the title and labels as specified.
- **Proper Imports**: Includes `pandas` and `matplotlib.pyplot`.
- **Error Handling**: Includes basic `FileNotFoundError` handling for the CSV load.

Areas for minor improvement (not critical but good practice):
- The ground truth includes more robust error handling for `pd.read_csv` and also checks if the columns actually exist before attempting to plot, which is good defensively. The generated code also checks if columns exist during cleaning, which is good, but doesn't explicitly check before plotting, assuming the cleaning loop would have identified issues.
- The ground truth defines constants for column names and labels, which makes the code more maintainable and less prone to typos, though for this specific task, direct string usage is acceptable.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

file_path = 'mobiles_2025.csv'
df = pd.read_csv(file_path)

# 1. Clean 'Mobile Weight' column
df['Mobile Weight'] = df['Mobile Weight'].astype(str).str.replace('g', '', regex=False).pipe(pd.to_numeric, errors='coerce')

# 2. Clean 'RAM' column
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '', regex=False).pipe(pd.to_numeric, errors='coerce')

# 3. Clean 'Battery Capacity' column
df['Battery Capacity'] = df['Battery Capacity'].astype(str).str.replace('mAh', ''...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

**Strong points:**
- **Correctness:** All cleaning steps are implemented correctly, converting relevant columns to numeric and handling errors with `coerce`.
- **Completeness:**
    - Loads the CSV.
    - Cleans `Mobile Weight`, `RAM`, `Battery Capacity`, and `Launched Price (USA)` as specified.
    - Lists column names and data types accurately.
    - Calculates the requested descriptive statistics.
    - Reports the count of unique company names.
- **Code Quality:**
    - Proper use of pandas functions for cleaning (`str.replace`, `pd.to_numeric`, `pipe`). Using `.pipe` is an elegant way to chain transformations.
    - Clear variable names.
    - Concise and effective statistical calculation using `.agg`.

**Areas for minor improvement:**
- **Robustness:** The code assumes the file exists and that the specified columns are always present. While the prompt implies this, a more robust solution would include `try-except` blocks for file loading (as in the ground truth) and checks for column existence before attempting operations on them (e.g., `if 'Column Name' in df.columns:`). This makes the code less prone to crashing if the CSV structure varies unexpectedly.
- **User Experience (Output):** While it prints the requested information, the descriptive statistics output is slightly less readable than the ground truth's `describe().loc[['mean', '50%', 'std', 'min', 'max']]` or individual prints with descriptive labels. The `.agg` output is correct but just a table. Reporting median as '50%' (from `describe`) instead of 'median' is a minor difference, but both are acceptable for ""median."" The prompt asked for ""median,"" and `df.agg(['mean', 'median', 'std', 'min', 'max'])` directly provides it, which is preferable to `describe`'s `50%` alias in this specific case.
- **`round(2)` for statistics:** The `round(2)` applied to the entire `agg` output might not be desirable for all statistics (e.g., standard deviation or min/max if they are integers). The ground truth lets pandas decide the precision, which is often fine. This is a stylistic choice.

Overall, it's a very solid solution that fulfills the core task effectively and efficiently. The minor suggestions are for making it even more production-ready or user-friendly in terms of output formatting.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean 'Launched Price (India)' column
# Remove 'INR ' and commas
df['Launched Price (India)'] = df['Launched Price (India)'].astype(str).str.replace('INR ', '', regex=False).str.replace(',', '', regex=False)
# Convert to numeric, setting errors to NaN
df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

# Clean 'RAM' column
# Remove 'GB'
df['RAM'] = df['RAM'].astype(s...",9,"The generated code correctly performs all the required steps: loading the data, cleaning both `Launched Price (India)` and `RAM` columns by removing specified strings and converting to numeric (handling errors with `coerce`), filtering for 'Apple' and price under 100,000, selecting the specified columns, and sorting by price. The code is clean, well-structured, and uses pandas effectively.

One minor improvement, though not critical for correctness in this specific task given the dataset, would be to chain the cleaning operations for `Launched Price (India)` into a single `.apply()` or chain the `str.replace` calls before `pd.to_numeric` to be slightly more concise. However, the current approach is perfectly readable and functional. The only slight point of improvement is the lack of error handling for file not found, which was present in the ground truth. While not explicitly asked for in the prompt, it's good practice. However, given the prompt asked only for the Python code without explanations, and focusing on the data manipulation tasks themselves, the code is very strong.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the `RAM` column: remove 'GB' and convert to numeric, handling errors
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '', regex=False).str.strip()
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

# Clean the `Battery Capacity` column: remove 'mAh' and convert to numeric, handling errors
df['Battery Capacity'] = df['Battery Capacity'].asty...",9,"The generated code is very good and correctly addresses almost all requirements of the task. It successfully loads the data, cleans both `RAM` and `Battery Capacity` columns, converts them to numeric, and handles conversion errors. It also correctly generates a scatter plot, colors points by `Company Name`, and sets the title and axis labels.

**Strong points:**
- Correctly uses `pd.read_csv`, `astype(str)`, `str.replace`, and `pd.to_numeric(errors='coerce')` for cleaning.
- Uses `seaborn.scatterplot` effectively for coloring by `Company Name`.
- Sets clear plot title and axis labels.
- Includes `plt.legend` and `plt.tight_layout` for better presentation.

**Areas for minor improvement:**
- **In-place cleaning vs. new columns:** The ground truth code creates new cleaned columns (`RAM_cleaned`, `Battery Capacity_cleaned`) while preserving the original columns. The generated code overwrites the original `RAM` and `Battery Capacity` columns. While this works for the task, creating new columns is generally better practice for data integrity, especially if the original, uncleaned data might be needed later (e.g., for debugging or other analyses). This is a stylistic choice more than a functional error for this specific task.
- **Error handling for file loading:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust practice. The generated code assumes the file will always be present. While not strictly required by the prompt, it's good practice for production-ready code.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 5 rows
df = pd.read_csv('titanic.csv')
print(""First 5 rows of the dataset:"")
print(df.head())
print(""\n"" + ""=""*50 + ""\n"")

# 2) Show the shape of the dataset (rows and columns)
print(""Shape of the dataset (rows, columns):"")
print(df.shape)
print(""\n"" + ""=""*50 + ""\n"")

# 3) List all column names and their data types
print(""Column names and their data types:"")
print(df.info()) # df.info() already prints column names and dty...",9,"The generated code successfully completes all tasks outlined in the prompt: loading data, displaying the first 5 rows, showing shape, listing column names and data types, calculating survival rate, and showing missing values. The calculations for survival rate and missing values are correct.

One minor area for improvement is the output from `df.info()`. Since `df.info()` already prints to console, wrapping it in `print(df.info())` results in `None` being printed after the info summary. While functionally correct, it's slightly redundant in terms of output.

Another minor point is the absence of error handling for `FileNotFoundError`. The ground truth includes a `try-except` block for robust file loading, which is a good practice in real-world scenarios.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: 'imdb_top_250.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 1) List all column names and their data types
print(""--- Column Names and Data Types ---"")
print(df.info())
print(""\n"")

# 2) Calculate and report basic descriptive statistics for numerical columns
print(""--- Descriptive Statistics for Year, Rating, and Runtime ...",6,"The generated code correctly loads the dataset and reports the total number of movies. It also attempts to list column names and data types, and calculate descriptive statistics. However, there are several areas for improvement.

**Correctness & Completeness:**
*   **`df.info()` output:** The line `print(df.info())` will print `df.info()` output (which is already printed to stdout by `df.info()` itself) and then `None` because `df.info()` returns `None`. This is not ideal; the ground truth simply calls `df.info()`.
*   **`Runtime` column handling:** The most critical missing piece is the preprocessing of the `Runtime` column. As stated in the task description (and common for this dataset), `Runtime` is often a string (e.g., ""142 min""). The generated code attempts to calculate statistics on it directly, which will likely raise a `TypeError` or produce `NaN` values for statistics if the column is not already numeric (which is often the case for `Runtime`). The ground truth includes robust preprocessing for this.
*   **Statistics for non-numeric columns:** If `Year`, `Rating`, or `Runtime` are not numeric (e.g., `Runtime` is still a string), calculating `mean`, `median`, `std` will fail or yield incorrect results. The ground truth explicitly ensures the columns are numeric before calculating statistics.
*   **Specific statistics request:** The task asked for specific statistics (mean, median, standard deviation, min, max). While `.agg()` can provide these, the output format using `.agg()` can be less readable than `.describe().loc[['mean', '50%', 'std', 'min', 'max']]` which gives a clean table. The generated code's `.agg()` output is less consolidated.

**Code Quality:**
*   **Error handling:** Using `exit()` for error handling is generally discouraged in Python scripts, as it terminates the entire process immediately. `sys.exit()` (as used in the ground truth) is preferred for a more graceful termination when part of a larger application, but for a simple script, `exit()` works albeit less elegantly.
*   **Redundant `print(""\n"")`:** While not an error, the extra `print(""\n"")` lines add empty lines unnecessarily.

To improve, the code needs to:
1.  Correctly handle the `Runtime` column conversion to numeric type.
2.  Ensure that `df.info()` is called correctly without printing its `None` return value.
3.  Improve the display of descriptive statistics to match the required format and readability.
4.  Robustly ensure numerical columns are indeed numerical before attempting calculations.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: 'imdb_top_250.csv' not found. Please make sure the file is in the current directory."")
    exit()

# Filter for movies with Rating >= 9.0
filtered_df = df[df['Rating'] >= 9.0]

# Select 'Title' and 'Rating' columns and sort by 'Rating' in descending order
result_df = filtered_df[['Title', 'Rating']].sort_values(by='Rating', ascending=False)

# Display the re...",9,"The generated code correctly addresses all the task requirements: it loads the CSV, filters by rating, selects the specified columns, and sorts in descending order. It also includes proper imports and handles the `FileNotFoundError` gracefully. The code is concise and efficient.

One minor point for improvement, which is reflected by the ground truth, is the handling of other potential `KeyError` exceptions (e.g., if 'Rating' or 'Title' columns are missing) or general `Exception` handling. While not strictly required by the prompt's explicit problem description, robust code often considers these scenarios, especially in a data science context where dataset schemas can vary. However, for the given task and constraints, the generated solution is very good.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Data preprocessing: Convert 'Runtime' column to numeric (assuming format like '142 min')
# This step is crucial for plotting 'Runtime' as a continuous numerical variable.
df['Runtime'] = df['Runtime'].str.replace(' min', '', regex=False).astype(int)

# --- 1. Create a histogram to visualize the distribution of the 'IMDB_Rating' column ---
plt.figure(figs...",6,"The generated code successfully creates both the histogram and the scatter plot with appropriate titles and labels. It correctly handles the 'Runtime' column conversion.

However, there is a critical assumption made about the `Rating` column name. The task description specifies a `Rating` column, but the generated code uses `IMDB_Rating`. While `imdb_top_250.csv` often contains an `IMDB_Rating` column, the task explicitly stated `Rating`. This disparity leads to an incorrect column reference if the CSV strictly adheres to the column name `Rating`.

Additionally, the generated code lacks robust error handling for `FileNotFoundError` or missing columns, which is present in the ground truth and is good practice for data loading scripts.

**Areas for Improvement:**
1.  **Column Name Adherence:** Strictly use the column name `Rating` as specified in the task, or provide a check/conditional logic if `IMDB_Rating` is also acceptable.
2.  **Error Handling:** Incorporate `try-except` for `pd.read_csv` and checks for column existence before attempting to plot, similar to the ground truth.
3.  **Preprocessing Robustness:** Although the `Runtime` conversion works for the expected format, the ground truth's `str.extract('(\d+)')[0]` is more robust to potential variations like leading/trailing spaces or other characters, and `errors='coerce'` handles unparseable values gracefully. The current `astype(int)` will raise an error if any value cannot be converted.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Please ensure the file is in the current directory."")
    exit()

# 1) Report the number of missing values (NA) for each column
print(""Number of missing values (NA) for each column:"")
print(df.isnull().sum())
print(""\n"")

# 2) For the numerical columns, first drop any rows where *any* of these four specific columns have a missing value.
#...",9,"The generated code correctly addresses almost all aspects of the task, demonstrating good understanding of pandas functionalities for data loading, missing value handling, descriptive statistics, and unique value counting.

**Strong Points:**
*   **Correct Loading:** Properly loads the `penguins.csv` dataset.
*   **NA Reporting:** Correctly reports missing values for all columns using `df.isnull().sum()`.
*   **Conditional NA Dropping:** Accurately drops rows only considering the specified numerical columns using `df.dropna(subset=...)`.
*   **Descriptive Statistics:** Correctly calculates and reports mean, median, std, min, and max for the filtered numerical columns using `.agg()`.
*   **Unique Value Counting:** Accurately counts unique values for 'species' and 'island' columns using `.nunique()`.
*   **Proper Imports:** Includes `import pandas as pd`.

**Areas for Minor Improvement (Why not a 10):**
*   **Error Handling (Minor):** The error handling for `FileNotFoundError` exits the program, which is acceptable for a script, but the ground truth adds a more general `Exception` catch, which is slightly more robust for unexpected loading issues.
*   **Clarity of NA Report:** The ground truth includes a filter (`columns_with_missing_values = missing_values_count[missing_values_count > 0]`) to only show columns with actual missing values and reports a message if none are found. The generated code simply prints all `isnull().sum()`, which might show '0' for many columns, slightly less focused for a ""report"".
*   **Reporting Verbosity:** The ground truth includes more verbose print statements (e.g., ""Successfully loaded..."", ""Original number of rows:"", etc.) which are helpful for understanding the processing steps, especially related to the row dropping. The prompt asked for ""reporting"" which implies clear, informative output.

Overall, it's a very good solution that competently solves the problem. The minor points are about output formatting and slightly more robust error handling which are good practices but don't detract significantly from the core functionality.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# 1) Filter the dataset
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# 2) Select specific columns
selected_columns_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# 3) Drop rows with missing values in the selected columns
cleaned_df = selected_columns_df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm'])

# 4) Display the first 5 rows
print(cleaned_df.head(5))
```",9,"The generated code correctly addresses all requirements of the task: it loads the CSV, filters by species and island, selects the specified columns, drops rows with missing values in those columns, and displays the first 5 rows. The imports are correct, and the code is well-structured and easy to read.

The only minor point of improvement compared to the ground truth is the lack of explicit error handling for file loading (e.g., `FileNotFoundError`). While the task prompt didn't explicitly ask for error handling, it's good practice in real-world scenarios. However, for a focused data processing task evaluation, this doesn't detract from its core correctness in ""solving"" the data manipulation problem.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure the file is in the current directory."")
    exit()

# Filter out rows with missing values in the specified columns
# We need 'culmen_length_mm', 'culmen_depth_mm', and 'species' to be non-missing.
df_filtered = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species...",9,"The generated code successfully loads the dataset, filters out rows with missing values in the specified columns (`culmen_length_mm`, `culmen_depth_mm`, and `species`), and generates a scatter plot with `culmen_length_mm` on the x-axis and `culmen_depth_mm` on the y-axis, colored by `species`. It correctly includes a title and axis labels.

Strengths:
- Correctly uses `pd.read_csv`.
- Accurately uses `df.dropna(subset=...)` to filter out NAs in the required columns.
- Correctly generates the scatter plot using `seaborn.scatterplot` with `hue='species'`.
- Includes a title and labels for both axes.
- Handles `FileNotFoundError` gracefully.
- Includes proper imports.

Areas for minor improvement (preventing a perfect 10):
- While the solution is contained within a single script and addresses the task requirements, wrapping the plotting logic within a function (as in the ground truth) would improve reusability and modularity, which is generally good practice for larger scripts or libraries.
- The `plt.grid(True, linestyle='--', alpha=0.7)` line was not explicitly requested, though it doesn't harm the solution.
- The task asked ""Provide only the Python code without explanations"", and while the generated code doesn't have extensive explanations, it does include a few comments like `# Load the dataset` and `# Filter out rows with missing values...` and `# Choose a color palette`. This is a minor point, as these comments are not ""explanations"" in the way a natural language paragraph would be.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

def analyze_penguins_data(file_path='penguins.csv'):
    """"""
    Loads the penguins.csv dataset, cleans specified numerical columns,
    calculates statistics, and prints dataset dimensions.

    Args:
        file_path (str): The path to the penguins.csv file.
    """"""
    try:
        # 1) Read the dataset
        df = pd.read_csv(file_path)
        print(f""Dataset '{file_path}' loaded successfully.\n"")

        # Define the numerical columns for imputation and an...",9,"The generated code is excellent. It correctly implements all requirements of the task: reading the dataset, filling NA values in specified columns with their respective means, calculating and printing mean, median, and standard deviation for the cleaned columns, and printing the total number of rows and columns.

**Strong Points:**
- **Encapsulation:** The code is well-encapsulated within a function `analyze_penguins_data`, making it reusable and modular.
- **Error Handling:** It includes robust error handling for `FileNotFoundError` and a general `Exception` catch, which is crucial for real-world applications.
- **Clarity and Readability:** The variable names are descriptive, and print statements provide clear feedback to the user on each step of the process (e.g., initial NA count, mean used for imputation, no NAs found).
- **Compliance:** It strictly adheres to all specified requirements in the prompt.
- **Efficiency:** Using `inplace=True` with `fillna` is efficient, avoiding unnecessary memory copies.
- **Docstrings and Comments:** The function includes a helpful docstring explaining its purpose and arguments.

**Minor Areas for Improvement (to reach a perfect 10):**
- **Slight Redundancy in Warnings:** The `Warning: Cannot calculate statistics for '{col}' as it was not found.` after the imputation step already handled missing columns, is a slight redundancy. If it's already confirmed `col not in df.columns` at the imputation step, then the later check for statistics will also correctly identify it as not present. However, the current implementation ensures robustness in case a column *magically* disappears or was not present during imputation.
- **Consistency in Output Formatting:** While good, it could be slightly more consistent in the formatting of numerical values across all print statements (e.g., always `:.2f`). (This is a very minor point).

Overall, this is a highly competent and well-structured solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Ensure plots don't overlap if running in an interactive environment
plt.close('all')

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure the CSV file is in the same directory."")
    exit()

# 2) Drop rows where flipper_length_mm is NA
df_cleaned = df.dropna(subset=['flipper_length_mm']).copy() # Using .copy() to avoid Se...",8,"The generated code correctly addresses almost all requirements of the task. It successfully loads the dataset, handles NA values, generates the specified histogram for `flipper_length_mm`, and creates a bar chart for `island` counts with appropriate titles and labels.

**Strengths:**
*   **Correct Data Loading and NA Handling:** Uses `pd.read_csv` with `try-except` for `FileNotFoundError` and `dropna(subset=['flipper_length_mm'])` effectively handles the NAs. The use of `.copy()` after dropping NAs is good practice to prevent `SettingWithCopyWarning`.
*   **Correct Plotting with `seaborn`:** Uses `sns.histplot` and `sns.countplot` as intended, which are suitable for the tasks.
*   **Correct Labels and Titles:** All plots have the required titles and x-axis labels.
*   **Clear and Concise:** The code is well-structured and easy to understand.

**Areas for Improvement:**
*   **Inconsistency in `df` vs `df_cleaned` for Island Plot:** The bar chart for `island` counts uses the original `df` (`sns.countplot(x='island', data=df)`), whereas the histogram uses `df_cleaned`. If `flipper_length_mm` NAs were dropped, then ideally, the island counts should reflect the data that *also* has valid `flipper_length_mm` for consistency, or the task should specify that the island counts should be from the original dataset. The ground truth code used `df_cleaned` for both plots, which is a more consistent approach. This isn't a critical error, but it's an inconsistency in data processing flow for the plots.
*   **Missing y-axis label for histogram:** The ground truth specified 'Count' as the y-axis label for the histogram, while the generated code uses 'Frequency' by default from seaborn. While 'Frequency' is acceptable for a histogram, 'Count' was explicitly requested in the common ground truth.
*   **Y-axis label for Bar Chart:** The generated code labels the y-axis 'Number of Penguins', which is good. The ground truth states ""label the axes appropriately"", and both the generated code and ground truth provide good labels.

Overall, the solution is very good and nearly perfectly matches the requirements. The single inconsistency in data usage for the second plot and a minor difference in a y-axis label are the only reasons it's not a perfect score.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

def analyze_penguins_torgersen(csv_file='penguins.csv'):
    """"""
    Loads the penguins dataset, filters for 'Torgersen' island,
    and counts species for this subset.
    """"""
    try:
        # 1. Load the dataset
        df = pd.read_csv(csv_file)

        # 2. Filter the data to include only penguins from the 'Torgersen' island
        torgersen_df = df[df['island'] == 'Torgersen']

        # 3. For this filtered subset, count and print the number of penguins b...",9,"The generated code successfully completes all aspects of the task: it loads the data, filters by 'Torgersen' island, and counts species within that subset. The code is encapsulated in a function, which is good practice. Error handling for `FileNotFoundError` and `KeyError` is implemented, covering common issues. The inclusion of a dummy CSV creation for demonstration is a thoughtful addition for self-contained testing.

Areas for minor improvement:
- While `KeyError` handles the case where 'island' or 'species' columns are missing, the ground truth explicitly checks for column existence with an `if 'column' not in df.columns:` statement, which can provide more precise error messages to the user. The current `KeyError` catch is more generic.
- The output format is good, but could be slightly more descriptive, similar to the ground truth's detailed print statements. For example, explicitly mentioning which island is being analyzed within the species count output.
- The use of `.copy()` when filtering, as seen in the ground truth, is a best practice to prevent `SettingWithCopyWarning` if subsequent modifications were to be made to `torgersen_df`. While not strictly necessary for this specific problem (as `torgersen_df` is only read), it's a good habit in data manipulation.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

# Create the pandas DataFrame
df = pd.DataFra...",10,"The generated code is entirely correct, complete, and follows good programming practices. It successfully defines the DataFrame as specified, prints its shape, and lists the data types of each column, fulfilling all requirements of the task. The code is well-structured and uses appropriate pandas functionalities.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv('titanic.csv')
except FileNotFoundError:
    print(""Error: titanic.csv not found. Please ensure the file is in the current directory."")
    exit()

# --- 1) Bar chart: Count of survivors vs non-survivors ---
plt.figure(figsize=(7, 5))
survived_counts = df['Survived'].value_counts()
survived_labels = {0: 'Did Not Survive', 1: 'Survived'}
survived_counts.index = survived_counts.index.map(sur...",8,"The generated code successfully addresses most of the requirements. It creates four distinct plots with proper titles and axis labels, covering survival, class, age, and gender distributions. Imports are correct and the structure is clear.

One minor point of improvement for handling missing age values is that `dropna()` removes rows with missing age values, which might discard valuable data if other columns are still relevant. The ground truth's approach of `fillna()` with the median is generally a more robust strategy for handling missing numerical data in histograms, as it preserves the sample size and provides an estimate for the missing data. While `dropna()` is a valid approach, it's worth noting the potential data loss.

Additionally, the generated code doesn't include the grid lines for the Pclass and Gender plots, which the ground truth included and typically adds to readability. It also lacks the initial and final print statements for user feedback. However, these are minor cosmetic or UX details and do not detract from the core functionality or correctness of the visualizations.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# 1. Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

# 2. Calculate and print the average ...",10,"The generated code is excellent. It correctly imports pandas, creates the DataFrame exactly as specified, and accurately performs all requested calculations: average midterm score, average final score, and identifying the subject with the highest average final score. The output formatting using f-strings and `.2f` for two decimal places is also correct. The code structure is clear and follows logical steps. It's a complete and correct solution to the problem.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# 1. Define the data
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [1000000, 1...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It correctly creates the DataFrame, sets 'Date' as the index, calculates the 'Daily_Range' column, identifies and prints the date with the highest 'Close' price, and finally prints the updated DataFrame. The code quality is good, with clear variable names and proper use of pandas functions.

The only minor point for improvement, which does not impact correctness or functionality, is that the `df = df.set_index('Date')` line is commonly written as `df.set_index('Date', inplace=True)` if one prefers to modify the DataFrame in place, though the current approach of reassigning the DataFrame is perfectly valid and arguably functionally cleaner for those who prefer immutability. The output format for printing the DataFrame differs slightly from the ground truth (`print(""DataFrame with Daily_Range:\n"", df)` vs `print(""DataFrame with 'Daily_Range' column:"")\nprint(df)`), but both correctly display the DataFrame.

Overall, a very strong submission.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004'...",10,"The generated code is correct, complete, and follows good programming practices. It successfully creates the DataFrame, calculates the 'Total_Revenue' column, correctly identifies the best-selling product by quantity, and prints the first 5 rows with the new column. The code is clean, well-commented, and directly addresses all aspects of the prompt.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# 1) Calculate and print the average ""SessionDurationMinutes...",10,"The generated code is excellent.
1.  **Correctness**: It correctly defines the DataFrame, calculates the average session duration, and counts users by device type, exactly as requested.
2.  **Code quality**: The code is well-structured, uses appropriate pandas functions (`.mean()`, `.value_counts()`), and has clear variable names. Imports are handled correctly.
3.  **Completeness**: All requirements of the task are fully addressed. The output format is also clear and understandable.

The output closely matches the ground truth in terms of functionality and clarity, demonstrating a comprehensive understanding of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# 1) Find an...",10,"The generated code is excellent. It correctly defines the DataFrame with the specified data, accurately finds the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"", and prints them as requested. The code is clean, well-structured, and directly addresses all parts of the task. It demonstrates a clear understanding of pandas DataFrame creation and basic aggregation methods.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts
total_likes = df[""Likes""].sum()
print(f""Total numb...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the total number of likes, and precisely identifies the PostType with the highest average number of shares. The code is well-structured, uses appropriate pandas functions, and directly addresses all requirements of the task. It perfectly matches the logic and output of the ground truth code.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gen...",10,"The generated code is excellent. It correctly defines the DataFrame with the specified data types, accurately calculates the average age, and correctly counts the number of patients for each gender. The code is well-structured, uses proper pandas methods, and includes clear print statements. It fully addresses all requirements of the task. The formatting of the print statements is also very good.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# 1) Print the...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies the title of the movie with the highest rating using `idxmax()`, which is a concise and efficient method. It also correctly calculates and prints the average year of all movies, properly formatted to two decimal places as exemplified in the ground truth. The code is well-structured, uses proper imports, and fully addresses all requirements of the task. Both parts of the task are solved efficiently and accurately.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# ...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the total number of cars using `len(df)` (which is perfectly valid and common alternative to `df.shape[0]`), and correctly calculates the average ""PriceUSD"" using `df[""PriceUSD""].mean()`. The output formatting for `PriceUSD` is also good, using `:.2f` for two decimal places. The code is well-structured, clean, and directly addresses all requirements of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

# 1) Print the name of the Pokemo...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense. The imports are correct, and the code is structured logically, making it easy to understand and maintain. It fully addresses all requirements of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# 1) Load the dataset and identify columns with missing values
df = pd.read_csv('titanic.csv')

# --- (Internal check for missing values - not part of final output display requirement) ---
# missing_before = df.isnull().sum()
# print(""Missing values before cleaning:\n"", missing_before[missing_before > 0])
# ----------------------------------------------------------------------------------------

# 2) Fill missing Age values with the median age
median_age = df['Age'...",9,"The generated code correctly performs all the specified data cleaning tasks. It loads the dataset, handles missing 'Age' and 'Embarked' values appropriately, drops the 'Cabin' column, and extracts the 'Title' using a very concise and effective pandas string method. The final output (shape and head) is also correctly displayed.

Strong points:
- Excellent use of `df['Name'].str.extract(' ([A-Za-z]+)\.')` for title extraction, which is more concise and arguably more idiomatic pandas than applying a custom function with `re.search`.
- Correct handling of missing values for 'Age' (median) and 'Embarked' (mode).
- Correctly drops the 'Cabin' column.
- Meets all the display requirements.
- The comments are helpful and indicate awareness of the broader context (e.g., standardizing titles, which was commented out but good to note).

Areas for improvement (minor):
- The `try-except` block for `FileNotFoundError` is missing. While not strictly required by the prompt's output, it's good practice for robust code when dealing with file I/O.
- The prompt explicitly asks to ""Include proper imports,"" and while `pandas` is imported, `re` is implicitly used by the `str.extract` method without a direct import if that method internally uses it. Since `re` wasn't explicitly imported, if the code were using `re.search` directly (as in the ground truth), it would raise an error. However, `pd.Series.str.extract` handles the regex internally, so `re` is not strictly needed as a direct import in this specific excellent solution.
- The prompt includes ""identify columns with missing values"" as part of task 1. While the code implicitly does this to process them, it doesn't *display* this identification to the user as a distinct step, which the ground truth does (commented out). Given the task was to ""identify columns with missing values"", a `df.isnull().sum()` or similar printout would have been a direct fulfillment of that part of the requirement. However, the subsequent cleaning steps confirm the identification was done internally. This is a very minor point.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}

# Cre...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies the title of the book with the highest price using `idxmax()` and `loc`, and correctly calculates the total number of books in stock using `sum()`. The code is well-structured, clean, and directly addresses all requirements of the task. The variable names are clear, and the print statements are informative.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('water_potability.csv')
except FileNotFoundError:
    print(""Error: water_potability.csv not found. Please ensure the file is in the current directory."")
    exit()

# 1) Display the first 10 rows
print(""--- First 10 Rows ---"")
print(df.head(10))
print(""\n"")

# 2) Show the shape of the dataset (rows and columns)
print(""--- Dataset Shape ---"")
print(f""Rows: {df.shape[0]}, Columns: {df.shape[1]}"")
print(""\n"")

# 3) List al...",8,"The generated code is functionally correct and addresses almost all requirements of the task. It successfully loads the dataset, displays the first 10 rows, shows the shape, lists column names and data types, calculates potability percentages, counts missing values, and provides summary statistics.

**Strong Points:**
*   Correctly uses `pandas` functions for all required EDA steps.
*   Clear and concise output for each section.
*   Handles `FileNotFoundError` an important robustness check.
*   Uses `normalize=True` for direct percentage calculation, which is efficient.

**Areas for Improvement (Why not a 10):**
*   **Column Names and Data Types (Task 3):** While `df.info()` does print the information, the prompt explicitly asked to ""List all column names and their data types"". `df.info()` prints a lot of extra information (non-null counts, memory usage, etc.) and also returns `None`, so `print(df.info())` is slightly redundant as `df.info()` already prints to `stdout`. Simply calling `df.info()` would be sufficient. For a more direct ""list"", `df.dtypes` would also work, but `df.info()` is generally preferred for a comprehensive view.
*   **Missing Values (Task 5):** The ground truth code specifically filtered to `missing_values[missing_values > 0]` and added a check if `missing_values.sum() == 0`. This is a nice touch for cleaner output when there are many columns without missing values. The generated code just prints all columns, even if their missing count is 0.
*   **Potability Interpretation (Task 4):** The ground truth provides context for 'Potability' (1 = Potable, 0 = Non-Potable), which is very helpful for understanding the output. The generated code doesn't include this.
*   **General Output Formatting:** The ground truth code uses more elaborate print statements (e.g., `""=""*50`) to clearly delineate sections, making the output more readable. The generated code's section headers are good but less aesthetically separated.
*   **Error Handling (Task 1):** The `try-except` for `FileNotFoundError` is good, but the Ground Truth adds a more generic `except Exception as e:` which is robust for other potential loading issues. However, for a simple EDA script, the current error handling is often sufficient.

Overall, a very good attempt that meets the core requirements effectively. The suggested improvements are minor refinements for better user experience and robust output.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd

# 1. Load the dataset and identify columns with missing values
try:
    df = pd.read_csv('water_potability.csv')
except FileNotFoundError:
    print(""Error: 'water_potability.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Store initial statistics for summary
initial_shape = df.shape
initial_missing_values = df.isnull().sum()

print(""--- Initial Dataset Status ---"")
print(f""Initial Dataset Shape: {initial_shape}"")
print(""Missing ...",7,"The generated code performs most of the required tasks correctly, including loading the data, handling missing values, and removing duplicates, and presenting before/after statistics. However, it falls short on the data range validation (Requirement 4). Instead of actively verifying and correcting/removing out-of-range values, it only displays min/max values and states that specific ranges depend on standards. The ground truth code actively applies physical range checks (e.g., pH between 0 and 14, other parameters non-negative) and removes rows that violate these, which is a more robust approach to ""Verifying that all values are within reasonable ranges"".

**Areas for Improvement:**
- **Data Range Validation:** Implement actual filtering/cleaning based on reasonable physical ranges for water quality parameters, as demonstrated in the ground truth, rather than just displaying min/max values. This is a critical step in data cleaning for this type of data.
- **Clarity of Output:** While the output is descriptive, aligning the before/after sections more closely with the ground truth for clearer comparison (e.g., all ""before"" data together, then all 'after' data together) could enhance readability, though this is a minor aesthetic point.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
try:
    df = pd.read_csv('water_potability.csv')
except FileNotFoundError:
    print(""Error: 'water_potability.csv' not found. Please ensure the CSV file is in the same directory."")
    exit()

# Handle missing values for consistent plotting.
# For visualization, dropping rows with NaNs is a simple approach.
# Alternatively, one could impute, but for this basic visualization request, dropping...",8,"The generated code largely fulfills the requirements of the task.

**Strong Points:**
*   **Correctness:** All four requested visualizations are generated.
*   **Proper Imports and Structure:** Imports are correct, and the code is well-structured with clear comments for each visualization section.
*   **Titles and Labels:** Most plots have appropriate titles and axis labels, enhancing readability.
*   **Handling Missing Values:** The code explicitly addresses missing values by dropping rows with `df.dropna()`. While the ground truth handles them more subtly for each plot (e.g., `dropna()` on series for histograms, `corr()` / `boxplot()` / `countplot()` handling NaNs internally), the generated code's approach is acceptable for basic visualization.
*   **Clarity of Potability Labels:** The use of `xticks` to label 'Potable' and 'Non-Potable' is good.

**Areas for Improvement (to achieve a higher score):**
*   **Missing Value Handling (subtlety):** The ground truth's approach of letting `seaborn` functions handle NaNs contextually (e.g., `histplot` ignoring them, `corr` calculating pairwise) is generally more robust than a blanket `df.dropna()` at the start. `df.dropna()` can discard a significant amount of data, especially if NaNs are sparse across different columns. For example, if 'ph' has NaNs but 'Hardness' doesn't, dropping rows removes valid 'Hardness' data that could be visualized.
*   **Consistency in X-axis labels for Potability:** The bar chart and boxplot both use 'Potability' as the x-axis label. While the `xticks` are correctly set, adding the ""(0: Non-Potable, 1: Potable)"" like in the ground truth makes it clearer for potentially ambiguous values.
*   **Heatmap Labels:** The x-axis labels on the heatmap would benefit from rotation (`plt.xticks(rotation=45, ha='right')`) to prevent overlap, especially if there were more features.
*   **Minor Customizations:** The ground truth includes `sns.set_style(""whitegrid"")` for a consistent aesthetic, which is a good practice. Also, minor gridline settings (`plt.grid(axis='y', linestyle='--', alpha=0.7)`) are implemented in the generated code on most plots, which is good. The ground truth adds them to all relevant plots.
*   **Re-using the `water_potability.csv` safety check:** The file not found check is good, but the generated code refers to the `df_cleaned` dataframe (after dropping NaNs) for all plots, which itself is a good decision. However, in comparison to the ground truth, the ground truth does not call `.copy()` on `df_cleaned` which is not strictly necessary unless `df_cleaned` was going to be modified in place extensively.

Overall, a very solid attempt that produced effective visualizations. The main point for improvement is around the nuance of missing value handling for visualization purposes.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE, SelectKBest, f_regression

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Define target variable and initial features
X = df.drop('quality', ...",7,"The generated code demonstrates a good understanding of feature engineering and selection techniques. It correctly implements two different feature selection methods (RFE and SelectKBest) and attempts to compare their results.

However, there are a few significant issues and areas for improvement:

**Strengths:**
*   **Correct Imports and Data Loading:** Standard libraries are imported, and data loading includes basic error handling.
*   **Feature Engineering:** Creates three new features, including a ratio and an interaction term, which is good.
*   **Multiple Feature Selection Methods:** Correctly uses both RFE and SelectKBest.
*   **Comparison of Results:** Attempts to compare the selected feature sets using set operations.
*   **Explanatory Output:** Provides good print statements explaining the engineered features, results from each method, and a conceptual comparison of the methods.

**Areas for Improvement / Issues:**
1.  **RFE with RandomForestClassifier:** The most critical flaw is the use of `RandomForestClassifier` for the `quality` target. The `quality` column is a numerical rating (e.g., 3-8), making it a regression problem, not a classification problem. Using `RandomForestRegressor` as in the ground truth is appropriate. This choice of classifier for a regression problem will likely lead to incorrect feature importance/selection.
2.  **RFE Feature Importance/Ranking Display:** While it correctly identifies selected features, the display of ""Full Feature Ranking"" from RFE (where rank 1 means selected) isn't the most informative way to compare importance. The ground truth's approach of retraining the `RandomForestRegressor` on the *selected* features and then extracting feature importances provides actual importance scores specific to that reduced set, which is more useful for comparison.
3.  **Scaling and `f_regression`:** The code notes that scaling is not necessary for `f_regression`. While technically true for `f_regression` specifically (which calculates F-statistics based on variance ratios, unaffected by scaling), scaling *all* features (`X_scaled_df`) and then passing `X_engineered` (unscaled) to `SelectKBest` introduces an inconsistency. It would be cleaner to either consistently use scaled data or explicitly state why scaling for one method but not the other. The ground truth consistently scales first, which is generally good practice before any modeling/feature selection unless the method is known to be invariant to scaling.
4.  **Display of Final Importance/Ranking:** The task asks to ""Display the final selected features with their importance scores or rankings."" While it prints the selected features, for RFE, it only shows ranks (1 for selected). For SelectKBest, it shows scores, which is good. Combining this more effectively (like the ground truth's dataframes) would improve readability and comparison.
5.  **Handling `NaN`/`inf` after Engineering:** The ground truth includes a robust step to handle potential `NaN` or `inf` values introduced by division in feature engineering. The generated code omits this. While it might not have occurred for *this specific* dataset/features, it's a critical robustness measure when creating ratio features.

**Overall:** The code is functional and addresses most parts of the prompt conceptually. However, the critical error of using a classifier for a regression task with RFE, and the less-than-optimal display of RFE results, prevents a higher score. Fixing these would lead to a very strong solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from xgboost import...",9,"The generated code provides a comprehensive solution that largely meets the requirements of the task.

**Strong Points:**
*   **Comprehensive Preprocessing:** The `load_and_preprocess_data` function is well-structured and handles missing values, data type conversions (including 'tbd' to NaN), and proper feature scaling/encoding for numerical and categorical features. It also correctly drops potentially leakage-inducing sales columns.
*   **Modular Evaluation:** The `evaluate_model` function is excellent. It encapsulates the cross-validation logic and metric calculation, making the main execution flow clean and readable. It also correctly uses `store_predictions=True` for later residual plotting.
*   **Multiple Ensemble Methods:** Random Forest, Gradient Boosting, XGBoost, and AdaBoost are all correctly implemented and evaluated.
*   **Voting and Stacking Ensembles:** Both ensemble types are correctly set up and evaluated using cross-validation.
*   **Hyperparameter Tuning:** GridSearchCV is correctly used for hyperparameter tuning on the best performing individual model. The `param_grid_selected` logic is a good attempt at adapting the tuning to the chosen best model.
*   **Comprehensive Evaluation Metrics:** MAE, MSE, RMSE, and R² are all calculated and displayed for individual and ensemble models.
*   **Feature Importance Plots:** The code successfully extracts and plots feature importances for tree-based models, including the handling of transformed feature names.
*   **Residual Plots:** Residuals are calculated and visualized against predicted values, and true vs. predicted plots are also included, providing deeper insights into model performance.

**Areas for Minor Improvement:**
*   **Data Leakage/Handling:** While dropping `NA_Sales`, `EU_Sales`, etc., is a good practice to avoid target leakage, it was not explicitly requested to drop these columns in the prompt, and the ground truth did not drop them. The generated code made a good analytical decision here, but it diverges from a strict interpretation of ""write Python code to implement..."". If the goal was to predict `Global_Sales` based *only* on descriptive features, then dropping regional sales IS correct. If `Global_Sales` was intended to be the sum of those, and the regional sales themselves were features contributing to the sum to be predicted, then keeping them might be considered. This is a nuanced point.
*   **Consistency in `XGBRegressor` `eval_metric`:** The `eval_metric='rmse'` is set in the main model definitions but not in the `XGBRegressor` within the `StackingRegressor`'s `level0_estimators` list or within the GridSearchCV `tuned_model` selection, which might lead to slightly different training behavior for XGBoost in those contexts compared to the initial individual evaluation. It's generally good practice to be consistent.
*   **Feature Importance for `AdaBoost`:** The code correctly notes that AdaBoost doesn't directly expose `feature_importances_` from its top-level estimator unless its base estimator is tree-based. However, the plotting loop still iterates over it. A more robust approach might be to check `hasattr(model, 'feature_importances_')` directly within the loop before attempting to plot. The current code does a basic check but could be more explicit in the plotting loop.
*   **Redundant Imputer in Pipeline:** The numerical transformer `SimpleImputer` is redundant since `Year_of_Release` and the score columns are already imputed with specific strategies in `load_and_preprocess_data`. While it doesn't harm, it adds an unnecessary step to the pipeline.
*   **`X_processed_df` use:** The `X_processed_df` is created but then not directly used in the `evaluate_model` calls. Instead, the raw `X` (DataFrame) is passed, and the `model` (which is assumed to have a pipeline) handles the preprocessing. This is fine, but the intermediate `X_processed_df` might be slightly confusing if it's not the exact input to the internal KFold splits. The ground truth builds the pipeline for each model evaluation, which is a cleaner pattern. The current code makes `X` the preprocessed X, and then the models evaluated are just the regressors, not full pipelines. This is an important distinction from the Ground Truth's approach, but it's a valid way to organize the code if `evaluate_model` expects preprocessed data. The `evaluate_model` expects `X.iloc` which suggests `X` is still a DataFrame.
    *   **Correction on `evaluate_model`:** The `evaluate_model` function uses `X.iloc[train_index]` and `model.fit(X_train, y_train)`. This implies that `model` passed to `evaluate_model` *is* the full pipeline of `Preprocessor + Regressor`, as found in the ground truth. However, the models instantiated in `models` dictionary (e.g., `RandomForestRegressor`) are *just* the regressors. This leads to an inconsistency. The `evaluate_model` function *should* receive a pipeline with the preprocessor, not just the regressor, or the preprocessing should happen *inside* the `evaluate_model` function for each fold. As it stands, the individual models are trained on the raw `X` (with some manual cleaning) and not the properly scaled/encoded `X_processed`. This is a significant point of failure.

Given the identified inconsistency in how `X` and `model` are handled within `evaluate_model` (where raw `X` is passed but the `model` is just the regressor, missing the preprocessing pipeline), the core functionality of machine learning pipeline is likely broken. The models will be trained on unscaled/unencoded data, which significantly impacts performance and correctness, especially for `XGBoost`, `GradientBoosting`, `AdaBoost` (less for RF, but still). This critical flaw brings the score down.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import rupture
import pymc as pm
import arviz as az
from lif...",6,"The generated code attempts to address all requirements of the prompt but contains several critical issues and omissions that significantly impact its correctness and completeness.

**Correctness Issues:**
*   **PyMC3 vs. PyMC:** The prompt explicitly asks for PyMC3 or similar. The generated code uses `pymc` (PyMC v4+), which has a different API than PyMC3. While `pymc` is a valid ""similar"" library, the code snippet provided for Bayesian regression would require adjustments for the exact API used (e.g., `pm.Normal` instead of `pm.HalfNormal` for sigma, and the `trace` handling). More importantly, the execution of the PyMC block without specific dependencies check for PyMC version 4/5, and assuming the correct installation, it's not a direct implementation of the prompt's `PyMC3` request.
*   **Survival Analysis Interpretation and Implementation:** The prompt asks to model 'game longevity' (years a game remains profitable). The generated code interprets 'longevity' as `Global_Sales` (`T = df_survival['Global_Sales']`). This is fundamentally incorrect. Traditional survival analysis defines 'T' as the duration (time) until an event, not the outcome measure itself. The ground truth correctly defines longevity as 'Years since release' (`Year_of_Release_last_observed - Year_of_Release_release + 1`). The implementation then fitting `cph.fit(surv_data_final, event_col=C_filtered, duration_col=T_filtered)` where `T_filtered` is `Global_Sales` is a misapplication of Cox Proportional Hazards.
*   **Change Point Detection Library:** The generated code uses `rupture`, but the way it's imported (`import rupture`) and used for `rupture.Binseg` is not standard. It should likely be `import ruptures as rpt` and then `rpt.Binseg`. While `rupture` is referenced, the usage might lead to an error or requires a specific installation beyond the typical. It also hardcodes the number of change points (`n_bkps = 3`) rather than using a penalty-based approach which is more robust and common for determining change points without prior knowledge of their count.
*   **Multivariate Regression `patsy` usage:** While `patsy` can generate design matrices, the prompt doesn't specifically ask for it, and it adds an extra dependency. More importantly, the ground truth uses `statsmodels.formula.api` which can achieve the same directly and often more idiomatically for `statsmodels` linear models. The selection of interaction terms is also quite limited (only two explicit interactions).
*   **Confidence/Prediction Intervals:** The prompt explicitly asks for CIs and PIs for *all* models. The generated code notes where these are available (ARIMA, OLS, Bayesian) and correctly extracts CIs for coefficients in some cases, but it largely omits explicit calculation or discussion of prediction intervals for models like ARIMA (beyond the basic `conf_int`) and completely for Ridge/Lasso (noting they are not directly available from scikit-learn but not providing an alternative or approximation). The ground truth provides an approximation for sklearn models.
*   **Time Series Cross-Validation:** The TimeSeriesSplit is applied to the raw `X_ts` (yearly_sales index) and `y_ts` (Global_Sales). However, the implementation for ARIMA within the CV loop seems to re-fit ARIMA from scratch, which is fine, but it uses `pd.to_datetime(X_train_ts.flatten(), format='%Y')` as the index for the series, which is potentially problematic. Most importantly, it only performs TS-CV for ARIMA and completely omits it for the multivariate regressions and polynomial regressions, whereas the prompt explicitly asks for ""all models"". The ground truth correctly implements TS-CV for ARIMA, OLS, Ridge, and Lasso.

**Completeness Issues:**
*   **Polynomial Regression Features:** The generated code uses `Year_of_Release`, `Critic_Score`, and `User_Score` for polynomial regression. The prompt implies using `Year` as the primary feature but doesn't strictly restrict it, so this might be acceptable. However, the ground truth simplifies to `Year_of_Release` for this specific part, which makes more sense for a ""polynomial regression *with regularization*"" section, as adding more features makes the impact of regularization harder to observe against simple polynomial curves.
*   **Plots:** The prompt mentions ""Create confidence intervals and prediction intervals for all models"" which often implies plotting these, and ""Check generated plots"" as a final instruction. The generated code has no plotting code, which the ground truth includes for all relevant models.
*   **Assumptions and Data Handling:** The generated code makes an arbitrary filter for `Year_of_Release` (1980-2016) without explanation, which might truncate data. It also hardcodes `arima_order = (1,1,1)` instead of using an auto-selection method like `pmdarima` from the ground truth, which is more robust for a general solution.

**Code Quality:**
*   **Imports:** Uses `import rupture` instead of `import ruptures as rpt` which is the common convention. Uses `import pymc as pm` which is for PyMC v4+, while the prompt specified PyMC3.
*   **Redundant Imports:** `train_test_split` is imported but not used in the final version of the TS-CV which is also generally problematic for time series.
*   **Readability/Structure:** Comments are present but sometimes lack depth. Some intermediate print statements or visualizations would help in debugging and understanding the steps, as present in the ground truth.

To improve, the code needs to:
1.  Correctly implement the survival analysis, defining longevity as a duration.
2.  Use `pmdarima` for auto-ARIMA or ensure the PyMC library is consistently used (e.g., PyMC3) and its API is correctly called.
3.  Implement time series cross-validation for all regression models (multivariate, polynomial), not just ARIMA.
4.  Provide and visualize confidence/prediction intervals where applicable for all models, even if approximate for sklearn models.
5.  Address the dependency choice and potentially plotting.
6.  Correct the `ruptures` import and usage.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score, mutual_info_score
from sklearn.decomposition import...",7,"The generated code provides a good attempt at the complex task, covering most of the requirements. It has a well-structured modular approach using functions for different parts of the analysis, which is excellent for code organization and readability.

**Strong Points:**
*   **Modular Design:** Good use of functions for preprocessing, clustering, dimensionality reduction, etc.
*   **Comprehensive Preprocessing:** Handles missing values, calculates regional sales distribution, market position, and publisher portfolio size correctly.
*   **Multiple Clustering Algorithms:** Implements K-Means, DBSCAN, Hierarchical, and GMM.
*   **Dimensionality Reduction:** Uses t-SNE and UMAP (with a check for UMAP availability).
*   **Optimal Cluster Determination:** Includes elbow method and silhouette analysis for K-Means.
*   **Consensus Clustering:** Attempts to implement consensus clustering based on co-occurrence.
*   **Cluster Profiling with Statistical Significance:** Uses ANOVA/Kruskal-Wallis and Chi-squared for profiling, which is a good addition.
*   **Anomaly Detection:** Correctly uses Isolation Forest.
*   **Association Rule Mining:** Uses `mlxtend` for Apriori and association rules.
*   **Recommendation System:** Implements a simple content-based recommender using clusters.
*   **Validation Metrics:** Includes internal metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin) and external (Adjusted Rand Index, Mutual Info Score) for comparison.
*   **Error Handling:** Basic `try-except` for file loading and checks for library imports.

**Areas for Improvement to achieve a higher score:**

1.  **DBSCAN Parameter Tuning:** The ground truth code provided a more robust approach to estimate `eps` for DBSCAN by plotting the k-distance graph, and recommending visual inspection or using `KneeLocator`. The generated code uses fixed `DBSCAN_EPS` and `DBSCAN_MIN_SAMPLES` which is not ideal for real-world application as these parameters are highly dataset-dependent. While it mentions the need for tuning, it doesn't implement it.
2.  **Optimal K Determination Refinement:**
    *   The ""Simplified Gap Statistic Insight"" is not a proper gap statistic implementation. The gap statistic is fundamentally about comparing within-cluster dispersion to a reference null distribution, which is not done here. This might be misleading.
    *   While elbow and silhouette are implemented, the ground truth uses `kneed` for automated elbow point detection, which is more robust than visual inspection.
3.  **Consensus Clustering Logic:**
    *   The `num_algorithms` used for normalization in consensus clustering is flawed. If both points are noise from DBSCAN, `num_algorithms` should still increment for DBSCAN for correctly calculating the average ""same cluster"" count. The current logic effectively excludes DBSCAN from the `num_algorithms` if points are noise, making the normalization inconsistent across pairs. A simpler and more robust approach often involves treating noise points as a separate ""cluster"" for co-occurrence calculation, or just normalizing by the total number of algorithms run.
    *   The `n_clusters_consensus` derivation is somewhat arbitrary. Ideally, consensus clustering should also determine its optimal number of clusters if it seeks to improve stability, or at least base it on a more robust optimal k from the prior step.
4.  **Feature Engineering Fidelity:** The ground truth calculates `Regional_Sales_Distribution` (proportions) on `Global_Sales` as the denominator. The generated code uses `Total_Regional_Sales` (sum of NA, EU, JP, Other sales). While this isn't strictly wrong, the prompt implies ""proportion of sales across regions"" which usually refers to a larger total, and `Global_Sales` is a better representation if available and comprehensive. Also, the handling of `Year_of_Release` in preprocessing was a bit simplified; the ground truth properly cleans and converts it. The `features_for_clustering` in the generated code explicitly exclude `Year_of_Release` (now 'Year') which might be a loss of information depending on the intent.
5.  **Robustness of Preprocessing:** The ground truth uses `ColumnTransformer` and `Pipeline` which is best practice for feature scaling and one-hot encoding categorical variables, ensuring consistency and preventing data leakage. The generated code scales numerical features manually and does not one-hot encode categorical features *before* clustering (except for ARM). Categorical features (like `Genre`, `Platform`, `Publisher`) are crucial for effective clustering and should be part of the scaled `data_scaled` input to clustering algorithms. This is a significant omission, leading to potentially less meaningful clusters. The prompt explicitly mentions creating features and then implementing clustering. The ground truth also mentions `categorical_features` and passes them through a preprocessor.
6.  **ESRB Rating:** `ESRB_Rating` was used in ARM without being mentioned anywhere else (preprocessing, cluster features). While not a critical error, it suggests an ad-hoc inclusion.

Overall, the code is well-structured and attempts to cover all aspects of the complex task. The primary area for improvement is ensuring all relevant features (numerical and categorical) are properly scaled and encoded for clustering, and refining the parameter selection/estimation for algorithms like DBSCAN and the consensus clustering normalization.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, brier_score_loss, roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.tree ...",7,"The generated code attempts to address all requirements and demonstrates an understanding of the concepts. However, there are several key areas where it falls short, especially when compared directly to the ground truth code and the prompt's nuances.

**Strengths:**
*   **Comprehensive Coverage:** Most of the requested advanced ensemble methods (Voting, Bagging, Boosting types, Stacking), Bayesian optimization (Optuna), SHAP interpretation, model calibration, and diversity analysis are implemented.
*   **Good Structure:** The code is logically organized with comments indicating different sections of the task requirements.
*   **Use of Libraries:** Correct libraries are imported and used for the specified tasks.
*   **Feature Engineering:** Includes `PolynomialFeatures` and `SimpleImputer` as requested.
*   **CatBoost:** Correctly handles CatBoost, including its `Pool` object.
*   **Visualizations:** Generates plots for SHAP and calibration, which is good.

**Weaknesses and Areas for Improvement:**

1.  **Nested Cross-Validation (Major Flaw):**
    *   **Incorrect Implementation:** The prompt specifically asks for ""nested cross-validation for unbiased model evaluation."" The generated code implements a typical cross-validation (using `cross_val_score`) using `StratifiedKFold` for `models_for_nested_cv`. This is *not* nested CV. Nested CV involves an outer loop for unbiased evaluation and an inner loop for hyperparameter tuning or model selection/training. The ground truth correctly implements an outer loop that splits data into train/test, and within each outer fold, it trains and evaluates models, or performs hyperparameter tuning, rather than directly using `cross_val_score` on the entire dataset. The generated code's Optuna optimization is performed on the *entire resampled dataset* (X_res, y_res) before nested CV, which introduces data leakage and biases the hyperparameter search.
    *   The prompt asked for nested CV for *unbiased model evaluation*, which means the models should be evaluated on unseen data for each fold within the outer loop that was not used for any training or hyperparameter tuning of *that specific model instance*.

2.  **Handling of Imbalance (Potential Issue):**
    *   The code applies `RandomOverSampler` to the `X_processed_df` and `y` *before* any train-test splitting or cross-validation. This means that duplicated samples created by oversampling are present in both training and test sets in cross-validation folds, leading to an optimistic and biased evaluation of model performance. Resampling should be done *within* the cross-validation folds (e.g., using `imblearn.pipeline` with `cross_val_score` or manually within loops) to prevent data leakage. The ground truth does not explicitly handle imbalance, but its nested CV setup would implicitly mitigate the impact if balancing was integrated carefully within each fold's training process.

3.  **Bayesian Optimization Scope:**
    *   While Optuna is used, the optimization is done on a single model (XGBoost) using `cross_val_score` on the full resampled dataset. As mentioned, this could lead to overly optimistic hyperparameters due to data leakage if not careful. The ground truth performs this on a single train-validation split *before* starting the nested CV, which is a safer approach for finding initial good hyperparameters, even if not fully nested. The generated code also didn't explicitly implement `eval_set` and `early_stopping_rounds` for early stopping during Optuna, which the ground truth did for more robust tuning.

4.  **Custom Ensemble:**
    *   The `CustomWeightedEnsembleClassifier` is a good attempt at dynamic weight assignment. However, its `fit` method performs a single `train_test_split` on the *entire training data* (`X`, `y`) passed to it (which itself is `X_res`, `y_res` - the *already resampled* dataset). This makes the ""validation"" split for weight learning not truly representative and can lead to overfitting of weights to the training data. A proper approach would be to calculate weights based on out-of-fold predictions within a cross-validation scheme.

5.  **Model Calibration:**
    *   The calibration is performed on the *same data* (`X_res`, `y_res`) that the model was trained on. This will drastically overestimate calibration performance. Calibration should be done on a separate validation set or using out-of-fold predictions from cross-validation splits. The ground truth uses the `CalibratedClassifierCV(..., cv='prefit')` and fits it on the training data of the *last outer fold*, then evaluates on the test data of that fold, which is a more appropriate (though still simplified) approach compared to calibrating on the data used for training.

6.  **SHAP Interpretation:**
    *   SHAP is directly applied to `X_res` (the oversampled full dataset) with the `tuned_xgb_clf` that was also trained on `X_res`. It's generally better to interpret models on a held-out test set or a representative subset to avoid insights biased by training data characteristics.

7.  **Efficiency/Redundancy:**
    *   Some models are trained multiple times in different sections (e.g., boosting models trained individually, then again within stacking, then again for diversity analysis). The ground truth's nested CV structure integrates training and evaluation more cohesively, avoiding unnecessary re-training for performance measurement.

8.  **Output Plots:**
    *   The generated code `plt.close()`s the plots immediately after saving them, which might prevent in-line display in environments like Jupyter notebooks. The ground truth generally uses `plt.show()` allowing for display.
    *   The generated code uses `plt.savefig()` but assumes the environment allows writing files, which wasn't specified. While not a functional error, `plt.show()` would be expected for direct feedback within an environment.

Overall, the generated code contains good concepts and attempts to implement all features, but the critical mistake in nested cross-validation and improper handling of data splits for resampling, calibration, and custom ensembles significantly impacts the unbiasedness and reliability of the evaluation, which was a core requirement.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import aesara.tensor as at
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans # Can be used for more structured age banding
from scipy.stats import weibull_min # For survival simulation
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings that might clutter the output
import warnings
warnings.simplefilter(action=""ignore"", category=FutureWarning)
warnings.si...",8,"The generated code provides a strong attempt at fulfilling the complex requirements of the task. It successfully implements several advanced statistical modeling techniques, including hierarchical Bayesian models, Bayesian logistic regression, MCMC sampling, Gaussian processes, and Bayesian model comparison. The code quality is generally good, with proper imports, clear variable names, and reasonable structuring.

However, there are a few significant areas where the code falls short or could be improved:

1.  **PyMC3 vs. PyMC4 (PyMC):** The ground truth uses `pymc3`, while the generated code uses `pymc` (which corresponds to PyMC v4+). While both are valid, the prompt specifically mentioned ""PyMC3 or PyMC4"", and consistency is key for benchmark evaluation. The generated code's use of `pymc` introduces API differences (e.g., `aesara.tensor as at` is used for mathematical operations, whereas `pymc3` used `theano.tensor`). This is not a direct error, but a different implementation context.

2.  **Survival Analysis:**
    *   The simulation of `time_to_event` and `event_observed` is done.
    *   The Bayesian survival model implementation is commendable for its complexity. However, the use of `pm.Potential` to manually add `logp` and `logcdf` for observed and censored events is an advanced and often necessary technique, but `lifelines` (used in ground truth) offers a more straightforward, established way for frequentist Cox models. The prompt asked for ""survival analysis techniques"", and while the Bayesian approach is more advanced, a direct Cox PH model implementation like the ground truth would have been a simpler, more direct fulfillment of that specific sub-requirement, especially since the Bayesian implementation is quite involved and correctness becomes harder to verify quickly. Its specific `Weibull` parameterization (`log_lambda = -pm.math.dot(X_survival_features, beta_s)`) correctly links higher `beta_s` to higher hazard (shorter time), but it's a very specific choice that might be less common than a direct proportional hazards formulation in a general context.

3.  **Causal Inference - Instrumental Variables:** The generated code correctly identifies the conceptual challenges of IV and explicitly states that a full implementation is not provided due to data limitations. This is a very pragmatic and honest assessment. However, the ground truth manages to provide a simplified (albeit frequentist 2SLS) demonstration using simulated IV, which goes beyond a conceptual outline. Given the prompt's high bar, a demonstrable, even if simplified, implementation would have been ideal.

4.  **Causal Inference - Propensity Score Matching:**
    *   It attempts to use `causalinference` library, which is excellent.
    *   However, it doesn't seem to account for `NaN` values before `CausalModel` instantiation, although it adds a `non_nan_mask` check. The dummy data creation fills 0s with NaN, then imputes these NaNs, so it should be clean.
    *   The ground truth also uses `sklearn.neighbors.NearestNeighbors` for PSM. The `causalinference` library is a good choice for more robust PSM, but it also adds an external dependency that PyMC users might not typically have installed or is not as generically expected as scikit-learn.

5.  **Data Preprocessing - Dummy Data:** The generated code includes a very robust dummy data creation and handling for `diabetes.csv` not found. While useful for execution, it adds significant complexity to the evaluation process, as the model behavior on real vs. dummy data can differ. The prompt assumed the CSV is present.

6.  **PyMC3 Specifics:** The `pm.model.PyMC3DeprecationWarning` suppression in ground truth implies it explicitly targets PyMC3, reinforcing the idea that PyMC3 was a primary expectation. The generated code uses `pymc` directly, making it implicitly PyMC v4+.

Overall, the code is very capable and demonstrates a deep understanding of Bayesian modeling. The score reflects that it is a *very good* solution, but some discrepancies (PyMC version, partial IV implementation, and the specific choices for survival analysis) prevent it from being a perfect match to the ground truth's approach and completeness for *all* specific sub-requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, QuantileTransformer
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, OPTICS, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.manifold import TSNE
from sklearn.ensembl...",7,"The generated code provides a comprehensive attempt at addressing the complex task. It successfully implements multiple clustering algorithms, advanced dimensionality reduction, and aspects of patient phenotyping, anomaly detection, and association rules. However, there are several areas where the implementation falls short compared to the ground truth and generally accepted best practices for such an advanced task, particularly regarding parameter tuning, robust consensus clustering, and detailed statistical validation.

**Strong Points:**

*   **Comprehensive Coverage:** Most direct requirements are addressed, including multiple clustering algorithms, several dimensionality reduction techniques, anomaly detection, association rule mining, and risk stratification.
*   **Feature Engineering:** Includes well-thought-out feature engineering for metabolic syndrome indicators, demonstrating an understanding of the domain.
*   **Dimensionality Reduction:** Correctly applies t-SNE, UMAP, and a basic Autoencoder for embeddings, and attempts to visualize cluster results on these.
*   **Clustering Algorithms:** Implements K-Means, Hierarchical, GMM, DBSCAN, OPTICS, and Spectral Clustering.
*   **Association Rule Mining:** Successfully loads `mlxtend` and performs ARM with sensible discretization.
*   **Pre-processing:** Handles missing values and scales data.

**Areas for Improvement (leading to a score of 7 rather than higher):**

1.  **Optimal Clustering Parameter Determination:**
    *   **Elbow/Silhouette/BIC/AIC:** While plots are generated for K-Means and GMM, the optimal K selection often relies on `np.argmax(silhouette_scores)` or `np.argmin(bics)`, which is a simplistic approach. The `k_range` definition is hardcoded `range(2, 11)`, which might not always be appropriate. The ground truth uses `N_CLUSTERS_MAX` config.
    *   **DBSCAN/OPTICS Parameter Tuning:** The code uses `eps=0.5, min_samples=5` for DBSCAN and fixed parameters for OPTICS as ""example parameters."" The ground truth includes a robust way to estimate `eps` for DBSCAN using the k-distance graph, which is crucial for density-based methods. This is a significant omission for a truly ""advanced"" clustering task.
    *   **Spectral Clustering:** The ground truth mentions `affinity='nearest_neighbors'` and `assign_labels='kmeans'` which are important parameters for performance and stability; the generated code also uses `affinity='nearest_neighbors'` but doesn't explain how `n_neighbors` is chosen, and the `assign_labels` is missing.

2.  **Consensus Clustering Robustness:**
    *   The consensus clustering implementation is a common co-occurrence matrix approach, which is good.
    *   However, the `AgglomerativeClustering` on `1 - consensus_similarity` with `metric='precomputed'` and `linkage='average'` implies clustering the *distance* rather than similarity. While conceptually similar, the ground truth's detailed handling of the distance matrix (`distance_matrix[np.triu_indices(n_samples, k=1)]`) for `linkage` is more precise for scipy's `linkage` function directly. The generated code's call to `silhouette_score` with `metric='precomputed'` on `1 - consensus_similarity` is correct for evaluating precomputed distance.
    *   The generated code handles `None` labels before consensus clustering, but the ground truth's filter for algorithms that produced ""only noise or a single cluster"" is a more explicit and robust check.

3.  **Patient Phenotype Profiles & Statistical Validation:**
    *   The phenotype profiling provides descriptive statistics (mean/std) and prevalence.
    *   **Statistical Significance Testing:** The use of Kruskal-Wallis for continuous features and Chi-squared for categorical is appropriate. However, the print statements for significance could be more aligned with the ground truth's concise ""Likely Significant"" or ""Not Significant"" messages. The ground truth also explicitly checks `len(valid_groups) > 1` and counts in the contingency table to prevent errors, which the generated code also attempts but less robustly.

4.  **Anomaly Detection:**
    *   Only Isolation Forest is implemented. The ground truth also uses One-Class SVM and LOF, demonstrating a more comprehensive approach to anomaly detection as required by ""implement anomaly detection to identify unusual patient profiles or outliers"". This represents a gap in ""completeness"".

5.  **Time-Series Clustering:**
    *   The generated code correctly identifies that time-series clustering is not applicable to the given dataset but provides a *conceptual outline* and example, which is a good effort to address the requirement. The ground truth also does this, but specifies slightly more detail (e.g., DTW importance, `tslearn` library).

6.  **Code Structure and Readability:**
    *   The code is generally well-structured with clear print statements for each step.
    *   The repeated plotting logic in the `'Plotting visualizations for each algorithm's best labels'` section could be refactored into a single loop or function call for better conciseness, similar to how the ground truth does for `plot_embedding`. The specific color mapping (`cmap = plt.cm.get_cmap('tab10', len(unique_labels))`) logic is good, but the common legend for all `plot_embedding` calls might obscure individual cluster labels if not handled carefully for algorithms like DBSCAN/OPTICS with many small clusters.

7.  **Minor Details / Refinements:**
    *   The ground truth explicitly defines `features`, `target`, `RANDOM_STATE`, and `N_CLUSTERS_MAX` at the beginning, which improves maintainability and clarity. The generated code scatters `random_state=42` and `range(2, 11)` throughout.
    *   The `df_processed` and `X_final` creation could be slightly cleaner in terms of naming intermediary dataframes.

In summary, the generated code performs well on many aspects of the task, meeting the core requirements. Its main weaknesses lie in the depth of parameter tuning for density-based methods, the missing comprehensive suite of anomaly detection algorithms, and some minor robustness and structure nuances compared to a truly expert-level solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import warnings
import optuna
import shap
import lime
import lime.lime_tabular
from sklearn.model_selection import (
    train_test_split,
    KFold,
    StratifiedKFold,
    cross_val_score,
)
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, ...",8,"The generated code comprehensively addresses most aspects of the task, demonstrating a strong understanding of advanced ensemble methods, hyperparameter tuning, model interpretation, and evaluation.

**Strengths:**

*   **Comprehensive Ensemble Methods:** Implements a wide range of ensemble techniques, including Voting Regressor, Bagging, Boosting algorithms (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost), and Stacking.
*   **Advanced Feature Engineering:** Attempts advanced feature engineering with interaction terms and polynomial features, though its integration with the `ColumnTransformer` could be slightly cleaner.
*   **Bayesian Optimization with Optuna:** Correctly sets up an Optuna study for hyperparameter tuning on multiple models, including XGBoost, LightGBM, and CatBoost.
*   **Nested Cross-Validation:** Demonstrates a nested cross-validation setup for robust evaluation of tuned models. While the inner loop isn't fully integrated (re-running Optuna in each fold), it correctly evaluates the *tuned* models using an outer loop.
*   **Custom Ensemble with Dynamic Weight Assignment:** Provides a good implementation of dynamic weight assignment based on inverse RMSE.
*   **Advanced Techniques (Blending & Multi-level Stacking):** Correctly implements blending, separating data for base model training and meta-model training. It also attempts a plausible multi-level stacking approach.
*   **Model Interpretation (SHAP & LIME):** Successfully integrates SHAP for global and local interpretation of tree-based models and LIME for local interpretation, including necessary data transformations for these tools.
*   **Model Calibration and Uncertainty Quantification:** Explores residual plots for calibration and provides a basic (though simplified) approach to prediction intervals, which is a good effort for uncertainty quantification in regression.
*   **Ensemble Diversity Analysis and Model Combination Optimization:** Computes correlation matrices for diversity analysis and uses a linear regression meta-learner on OOF predictions for combination optimization.
*   **Code Quality:** Generally well-structured, with proper imports and comments.

**Areas for Improvement:**

*   **Feature Engineering Integration:** The current ""Advanced Feature Engineering"" block modifies `X` directly *before* the `ColumnTransformer`. While functional, this can sometimes lead to issues if the new features aren't properly detected or if `ColumnTransformer` is expected to handle all transformations from scratch (which it usually is designed for). A more robust approach might be to wrap these operations inside a custom `Transformer` within the `Pipeline` or ensure that `numerical_features` and `categorical_features` are dynamically updated *after* the custom feature engineering step is applied. The current update of `numerical_features` and `categorical_features` after FE should largely mitigate this, but it could be clearer.
*   **Nested CV (Full Implementation):** While the nested CV section is good for evaluating *pre-tuned* models, the prompt asked for ""nested cross-validation for robust model evaluation,"" which implies running the hyperparameter optimization *within* each outer fold. The generated code explicitly states it's using pre-tuned models for nested CV, which is a common simplification but technically not a *full* re-run of the tuning process inside each outer loop. This is a very computationally intensive requirement, so the given solution is a practical compromise.
*   **Optuna Objective for Stacking:** The Optuna objective for Stacking is somewhat simplified. Tuning Stacking involves potentially tuning parameters of its base learners and its final estimator. The current implementation only tunes `final_estimator_alpha` for the `HuberRegressor`. A more advanced Optuna setup for stacking would involve conditional parameters based on base learner choices or a more complex parameter space.
*   **`get_feature_names_out()`:** The use of `pipeline.named_steps[""preprocessor""].get_feature_names_out()` for SHAP and LIME is correct for getting actual feature names after preprocessing, which is a crucial step for interpretability.
*   **`CalibratedRegressorCV`:** The prompt mentions `CalibratedRegressorCV`. While it's primarily for classification probability calibration, its inclusion in the ground truth code suggests considering regression calibration explicitly. The generated code adequately addresses regression calibration through residual analysis and uncertainty quantification with prediction intervals, which are more appropriate for regression. The ground truth mentioning it with a comment (""More for classification..."") indicates it's not strictly necessary for regression.

Overall, the code is very strong and demonstrates expert-level knowledge in applied machine learning, successfully tackling complex requirements. The identified areas for improvement are minor or concern highly demanding computational requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import semopy as sp
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.neighbors import NearestNeighbors
from sklearn.linear_model import LogisticRegression
from scipy import stats
import warnings
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings for cleaner output
warni...",6,"The generated code attempts to address all requirements of the task, which is commendable given the complexity. However, it falls short in several aspects regarding implementation correctness and robustness, particularly for the advanced statistical and causal inference methods.

**Strengths:**
*   **Comprehensive Task Coverage:** The code attempts to implement all 10 requested points, which is a significant undertaking.
*   **PyMC3 (PyMC) Usage:** Correctly uses PyMC for hierarchical and Bayesian regression models, including basic prior specification and sampling.
*   **Preprocessing:** Includes some missing value handling (KNNImputer) and scaling.
*   **Model Comparison:** Utilizes ArviZ for WAIC/LOO-CV comparison.
*   **Visualizations (Basic):** Includes a basic PPC plot.

**Weaknesses and Areas for Improvement:**

1.  **PyMC Version Mix-up (Critical):** The prompt specified PyMC3 or PyMC4. The generated code imports `pymc as pm`, which is the current PyMC (v5+), but many methods used, like `pm.set_data_buffer_size`, `return_nuts_data=False`, and the general API structure (e.g., `sigma=1` vs `sd=1` in `pm.Normal`), appear to be a mix incompatible between PyMC3 and PyMC (v5). This indicates a potential mismatch in environment setup or understanding of the current PyMC API, leading to non-functional or suboptimal code. The ground truth explicitly uses `pymc3` which is important for backward compatibility or specific user requirements. My evaluation assumed ""PyMC3 or PyMC4"" meant either of those specific versions, not latest `PyMC v5`.

2.  **Missing Value Handling and Data Consistency (Pre-processing):**
    *   `df_model.dropna(subset=categorical_cols, inplace=True)`: This drops rows if the regional indicator is missing. The ground truth used `SimpleImputer(strategy='most_frequent')` and `OneHotEncoder` which is more robust for categorical features.
    *   The categorical columns are not one-hot encoded before being used in PyMC models (`df_scaled['region_id']` is created, but no other categorical features are handled). The ground truth used `OneHotEncoder` and `ColumnTransformer` for a proper robust preprocessing pipeline.
    *   The scaling of `GDP_per_capita` after `simulated_iv_gdp` is introduced (`df_scaled['simulated_iv_gdp'] = scaler.fit_transform(...)`) may unintentionally alter the `GDP_per_capita` column used in other models if not handled carefully, potentially leading to inconsistencies.

3.  **Structural Equation Models (SEM):**
    *   The `semopy` implementation is very basic and lacks any detailed output or interpretation, which is crucial for SEM. The ground truth used a series of Bayesian regressions which, while an approximation, gave more interpretable and traceable results within the Bayesian framework. The generated code also `pass`es on exceptions, meaning SEM issues would be silently ignored.
    *   The prompt asked for SEM to analyze causal *pathways*, which `semopy` can do, but the implementation doesn't demonstrate any analysis of the fit, parameters, or pathways.

4.  **Causal Inference (IV and PSM):**
    *   **Instrumental Variables (IV):** The simulation of `simulated_iv_gdp` is arbitrary and doesn't explicitly link to a plausible mechanism (e.g., `df_scaled['Generosity'] * 0.2` as part of the IV, this is not a clear design choice). The ground truth provided a more reasonable simulation of an instrument. The `simulated_iv_gdp` is directly computed and then scaled, rather than being an external variable that influences the endogenous one.
    *   **Propensity Score Matching (PSM):** The matching logic, especially `if len(matched_treated_corresponding) > len(matched_control):` and subsequent sampling, seems overly complicated and potentially incorrect. It's unclear if this ensures a proper one-to-one or many-to-one match. The approach of `nn.kneighbors` for finding matches is correct, but the subsequent manipulation of `unique_control_indices` and balancing lengths may not yield ideal matched samples representative for ATE calculation.

5.  **Gaussian Processes (GP):**
    *   The `mean_function` for the GP includes *all* other predictors linearly, which partially defeats the purpose of using a GP for *non-parametric* modeling of complex relationships, particularly for the one variable (`GDP_per_capita`) it's meant to model non-linearly. The idea of GP is usually to capture the *non-linear* part of a relationship, possibly after accounting for linear effects of other variables. The ground truth models `happiness` as a non-linear function of *only* GDP using GP, effectively showing that complex relationship.

6.  **Difference-in-Differences (DiD):**
    *   The simulation of `Ladder score` change for treated and control groups is a bit simplified and assumes a fixed `Ladder score_base`. Real DiD settings usually involve more nuanced data generation or existing panel data. The use of `Q()` in `Q('Ladder score')` in the `ols` formula for column names with spaces is unconventional and might be unnecessary if columns are renamed properly at the start (as done in the ground truth). The result is silently `pass`ed if there's an exception, which shouldn't happen.

7.  **Output and Explanation:** The requested output was ""only the Python code without explanations"". However, the prompt also asked for ""proper imports"" and ""robust inference"" requiring ""credible intervals, posterior predictive checks, and sensitivity analysis"". While the code includes these, it largely comments out the `print` statements or `az.summary` calls that would actually *show* the inference results. The ground truth included these prints to clearly demonstrate that the inference results were accessible and interpreted, even if not explicitly defined in the task output format (which was only the code). The prompt implies that the code should be executable and demonstrate the results of the analyses.

8.  **Bayes Factors:** The prompt specifically asked for Bayes Factors in addition to WAIC/LOO-CV. The generated code does not include Bayes factors, stating that it `Not included in comparison due to different target structure` and `Not directly computed here due to complexity`. This is a missed requirement. While challenging for complex models, simpler comparisons might have been possible (e.g., using `pm.compare` with `method='bf'`).

Overall, the code attempts to cover all aspects but lacks the precision, robustness, and correct PyMC API usage expected for an expert-level solution in several key areas.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, QuantileTransformer, KBinsDiscretizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sk...",7,"The generated code makes a solid attempt at addressing the comprehensive requirements of the task, demonstrating a good understanding of various data science techniques.

**Strong Points:**
*   **Comprehensive Coverage:** It genuinely attempts to implement almost all requested components: multiple clustering algorithms (K-Means, DBSCAN, Hierarchical, GMM, Spectral, HDBSCAN), multiple dimensionality reduction techniques (t-SNE, UMAP, Autoencoder, Isomap, LLE), optimal parameter determination (silhouette, elbow, stability analysis), consensus clustering, detailed profiling with statistical tests, anomaly detection (Isolation Forest), network analysis (cosine similarity, NetworkX, centrality), and association rule mining.
*   **Advanced Feature Engineering:** Includes good advanced feature engineering beyond basic combinations, with ratios and inverse corruption perception.
*   **Autoencoder Implementation:** The Keras autoencoder implementation is appropriate.
*   **Clustering Parameter Optimization:** Attempts to optimize parameters using silhouette and elbow for various algorithms.
*   **Detailed Profiling:** The cluster profiling includes statistical significance testing (ANOVA/Kruskal-Wallis) and clearly prints mean values, which is excellent.
*   **Anomaly Detection:** Implements Isolation Forest and clearly highlights anomalous countries.
*   **Association Rule Mining:** Properly uses `mlxtend` for ARM, including sensible binning for continuous features.
*   **Recommendation System:** The cosine similarity-based recommendation system is a good approach.
*   **Policy Insights:** Connects findings from clustering and ARM to policy insights, addressing this complex requirement well.
*   **Modularity/Readability:** While not fully object-oriented like the ground truth, the code is logically sectioned with comments, making it relatively easy to follow the different analytical steps.

**Areas for Improvement (Why not higher score):**

*   **Code Structure/Modularity:** The code is presented as a single script without a class structure. While it functions, it's less maintainable and reusable compared to the class-based approach in the ground truth, which encapsulates related logic and data within a single object, common in more complex data science projects.
*   **Consensus Clustering:** The consensus clustering implementation is a ""meta-clustering"" approach, which is valid, but it could be more explicit in how it handles the potential misalignment of cluster labels from different algorithms (e.g., using a co-association matrix and then hierarchical clustering on it, as the ground truth does). The fallback logic is a bit crude.
*   **Stability Analysis:** The stability analysis for optimal parameters is limited to K-Means and uses ARI between two runs. A more robust stability analysis (e.g., assessing consistency across bootstrapped samples or multiple initializations for various algorithms, not just K-Means) would have been stronger. The ground truth conceptually outlines this, but doesn't fully implement it either due to complexity.
*   **Dimensionality Reduction Visualization:** While the DR techniques are implemented, there's no code provided for visualizing the embeddings with the cluster labels, which is a crucial part of using these techniques ""for visualization"" as requested.
*   **DBSCAN/HDBSCAN Parameter Tuning:** While it attempts to find optimal `eps` for DBSCAN using silhouette, the tuning process could be more exhaustive (e.g., varying `min_samples` as well, or using a grid search approach). Similarly, HDBSCAN tuning is less explicit.
*   **""Optimal Clustering Parameters"" Section:** The dedicated section for optimal clustering parameters is less detailed than the ground truth in terms of discussing gap statistics and conceptualizing stability. The ground truth at least explicitly states why certain advanced methods aren't fully implemented due to scope, giving a more complete picture.
*   **Minor Imbalances:** The autoencoder uses a simple linear activation for the output layer, which is okay if the input is scaled, but traditionally sigmoid/tanh is used if input is normalized to a specific range. Also, `prediction_data=True` in HDBSCAN is more for new data prediction than initial clustering but not a major issue.
*   **Geopolitical Validation:** While the policy insights section discusses this conceptually, it doesn't provide code that actively ""validates"" clusters geopolitically (e.g., by analyzing regional distributions within clusters as the ground truth does). It mostly provides a qualitative discussion.

Overall, the code is very capable and implements a wide array of advanced techniques. The primary points of improvement revolve around better code structure and a more refined or complete implementation for some of the more complex ""optimal parameter"" and ""consensus clustering"" aspects, and the missing visualizations.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
# Although not directly used for fitting/predicting in this specific implementation (as auto_arima wraps it),
# SARIMAX is the underlying model, so including for conceptual completeness.
from statsmodels.tsa.statespace.sarimax import SARIMAX 

# 1. Load data, parse 'date', filter for 'Germany', and set 'date' as index.
file_path = 'synthe...",9,"The generated code is largely correct and addresses almost all requirements of the task.

**Strengths:**
- **Correct Data Loading and Preprocessing:** Correctly loads data, parses dates, filters for Germany, and sets the index.
- **Handling Exogenous Variables:** Correctly identifies and uses `stringency_index` and `people_fully_vaccinated` as exogenous variables.
- **Missing Value Handling:** Implements `interpolate`, `ffill`, and `bfill` for missing value handling, which is appropriate for time series. It also includes a fallback `fillna(0)` and ensures non-negative 'new_cases'.
- **Correct Train-Test Split:** Splits data into training and testing sets appropriately based on the last 30 days.
- **SARIMA Model Selection:** Uses `pmdarima.auto_arima` to automatically determine SARIMA orders, which is robust and explicitly mentioned as an option in the prompt.
- **Forecasting:** Correctly generates forecasts for the test period using the fitted model and provides the exogenous variables for prediction.
- **Evaluation:** Calculates RMSE, meeting the evaluation requirement.
- **Visualization:** Generates a clear plot comparing actual vs. forecasted values, along with training data.
- **Code Structure and Readability:** The code is well-structured with comments explaining each step, making it easy to follow.

**Areas for Minor Improvement (Why not a 10):**
1.  **Redundant `SARIMAX` Import:** The generated code imports `SARIMAX` from `statsmodels.tsa.statespace.sarimax` and adds a comment that it's ""not directly used for fitting/predicting in this specific implementation (as auto_arima wraps it), but included for conceptual completeness."" While good for understanding, for a production-ready script, unneeded imports should ideally be removed, or the comment made more assertive that `auto_arima` *uses* it internally rather than just wrapping it, or if it were manually fitting a model after `auto_arima` suggests orders. The ground truth model explicitly trains `SARIMAX` after `auto_arima` finds parameters, which is a common and often more flexible approach for final model fitting (e.g., controlling `enforce_stationarity`, `enforce_invertibility`, or if you wanted to manually inspect summary after fitting). The generated code skips this explicit `SARIMAX` fitting step, relying solely on `auto_arima`'s internal fitting, which is acceptable but slightly less explicit in terms of the workflow desired by the prompt's point 5 (""Train the SARIMA model..."").
2.  **`auto_arima` fitted model:** The ground truth code uses `auto_arima` to find optimal orders, then explicitly creates and trains a `SARIMAX` model using those orders. The generated code directly uses the model returned by `auto_arima` for prediction. While `pmdarima`'s `auto_arima` returns a fitted model that can be directly used for prediction, the prompt's step 5 specifically says ""Train the SARIMA model on the training set,"" implying an explicit instantiation and fitting, which the ground truth does. The generated code implicitly fulfills this by `auto_arima` returning an already-fitted model, but it's not as explicitly broken down into separate steps as the prompt implied. This is a subtle difference in workflow but makes the ground truth slightly more aligned with breaking down the SARIMA steps explicitly.
3.  **Warning Suppression:** The ground truth code specifically mentions `warnings.filterwarnings(""ignore"")` at the beginning, anticipating warnings from `auto_arima` or `statsmodels`. While `auto_arima` has `suppress_warnings=True`, a general warning suppression might still be useful for other potential warnings from the broader script, making the solution slightly more robust in terms of clean output.
4.  **Handling of `file_path`:** The ground truth explicitly uses `try-except FileNotFoundError`, which is good practice for robustness. The generated code assumes the file exists.

Overall, it's a very strong submission that correctly implements the core requirements. The minor points are about adhering more precisely to the implied workflow in the prompt's steps and adding a touch more robustness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Please ensure the CSV file is in the same directory as the script."")
    exit()

# 1. Define quality tiers
def assign_quality_tier(quality):
    """"""Assigns a quality...",7,"The generated code correctly categorizes quality groups and applies the correct statistical tests based on normality. It successfully performs Tukey's HSD for ANOVA post-hoc analysis. The visualization part uses box plots for significant features and includes a summary.

However, there are a few areas for improvement:
1.  **Kruskal-Wallis Post-hoc (Dunn's Test):** The code attempts to implement pairwise Mann-Whitney U tests with Bonferroni correction as a proxy for Dunn's test, which is a common approach. However, there's a critical error in applying the Bonferroni correction: `adjusted_p_mw = p_mw * num_comparisons`. This only performs correction for *one* comparison. For a proper Bonferroni correction, you first collect all p-values from the pairwise comparisons, then apply `multipletests` from `statsmodels.stats.multitest` as done in the ground truth for a robust correction. The current implementation is incorrect for the entire set of comparisons.
2.  **Normality and Homogeneity of Variance Check:** While it checks for normality, it completely skips the homogeneity of variance check (Levene's test) even if normality is met. ANOVA assumes both normality *and* homogeneity of variance. If normality is met but homogeneity is violated, a Welch's ANOVA is often preferred over standard ANOVA, although the ground truth opted for Kruskal-Wallis if homogeneity was violated after normality. The generated code strictly defaults to Kruskal-Wallis if *any* normality assumption is violated, which is a reasonable robust approach, but the lack of Levene's check means it doesn't fully explore the conditions for standard ANOVA.
3.  **Visualization Plotting:** While it creates plots correctly, the generated code tries to create subplots for all significant features on a single figure (`plt.subplots`). This can lead to very crowded plots if there are many significant features, making them hard to read. The ground truth, by generating a separate figure for each significant feature, provides clearer individual visualizations.
4.  **Error Handling for Post-hoc:** The ground truth includes `try-except` blocks around `pairwise_tukeyhsd` and checks for insufficient data for pairwise comparisons (`len < 2`), which is more robust.
5.  **Output Clarity:** The ground truth provides more detailed and structured print statements for the statistical test decisions (e.g., specific Shapiro-Wilk and Levene's p-values) which helps in understanding the test selection logic.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"This Python code performs classification to predict 'high_daily_death_increase' events using a synthetic COVID-19 dataset. It includes data loading, target variable definition, extensive feature engineering, missing value handling, feature preprocessing (scaling and encoding), hyperparameter tuning for an XGBoost classifier using `GridSearchCV` with time-series cross-validation, model evaluation, and SHAP-based feature importance explanations.

```python
import pandas as pd
import numpy as np
fr...",9,"The generated code is very good and largely addresses all requirements of the task.

**Strong Points:**

*   **Correct Target Variable Definition:** The code correctly defines 'high\_daily\_death\_increase' based on the 90th percentile of *non-zero* 'new\_deaths' per location. The use of `df_positive_deaths` is a good way to handle this.
*   **Comprehensive Feature Engineering:** It successfully implements rolling averages/std deviations, lag features, and 'days\_since\_first\_case'.
*   **Appropriate Missing Value Handling:** It correctly distinguishes between newly engineered features and other numerical/categorical features for imputation (filling with 0 for engineered, ffill/bfill for static-ish).
*   **Correct Feature Selection:** Identifies and separates numerical and categorical features for preprocessing.
*   **Robust Preprocessing Pipeline:** Uses `ColumnTransformer` and `Pipeline` effectively for scaling and one-hot encoding. Using `drop='first'` for `OneHotEncoder` is good practice.
*   **Correct Time-Based Split:** Implements the train-test split correctly by sorting the data and taking the last 20%.
*   **Hyperparameter Tuning with `GridSearchCV`:** Correctly uses `GridSearchCV` with an `XGBClassifier` and the specified parameter grid.
*   **`TimeSeriesSplit` for CV:** This was an excellent addition that the prompt didn't explicitly ask for but significantly improves the robustness of the cross-validation for time-series data.
*   **Proper Evaluation Metrics:** Uses ROC-AUC and F1-score as requested.
*   **SHAP Implementation:** Correctly uses `shap.TreeExplainer` and generates a summary plot with the top 10 features, ensuring that feature names are correctly handled after preprocessing. Using `X_test_transformed_df` for SHAP values is also a good choice to see feature contributions on unseen data.
*   **Code Structure and Readability:** The code is well-structured with clear comments and print statements, making it easy to follow.

**Areas for Minor Improvement (to reach a perfect 10):**

*   **Definition of 'high\_daily\_death\_increase':** The ground truth uses `(df['new_deaths'] > df['location_90th_percentile_deaths'])` while the generated code uses `(df['new_deaths'] >= df['death_90th_percentile'])`. The original prompt specified ""is in the top 10%"", which generally implies strict inequality (greater than the 90th percentile) to put it *into* the top 10%. Using `>=` might include the 90th percentile value itself, slightly broadening the definition of ""top 10%"". This is a minor semantic point but worth noting for strict adherence to the prompt.
*   **`days_since_first_case` Calculation:** The logic for `days_since_first_case` is a bit complex. While it works, the ground truth's `(x - x.min()).dt.days` within a groupby transform is more direct and arguably cleaner if `x.min()` is indeed the *first ever* date for that location, not necessarily the first non-zero case. The prompt said ""days\_since\_first\_case"" implying difference from the start of data for that location, not necessarily based on `new_cases > 0`. However, the generated code's interpretation (based on first `new_cases > 0`) is also a valid and interesting feature. Given the prompt's wording ""days\_since\_first\_case"", the ground truth's approach might be slightly closer to the literal interpretation.
*   **Handling NaNs in `stringency_index`:** Filling `stringency_index` with 0 globally when there's no data might not be the most appropriate imputation for this specific feature. A mean or median would likely be more robust, or even removing rows/locations if the missingness is extensive. However, `ffill().bfill()` is good for within-location missingness.
*   **Consistency in `fillna` Strategy:** The prompt mentioned ""fill with 0 or backfill"". While the generated code mostly uses `fillna(0)` for engineered features and `ffill/bfill` for static features, some `fillna(0)` for static features at the very end (as a fallback) might be too simplistic if there are truly significant missing values for *static* features. The current overall strategy is defensible, but consistency could be improved or explained further.
*   **SHAP on `X_train` vs `X_test`:** The ground truth calculates SHAP on `X_train_processed_df` for global feature importance. The generated code calculates it on `X_test_transformed_df`. While both are valid for SHAP interpretation, calculating on the training set (or a representative sample of it) is more common for deriving overall feature importance (which a bar plot usually represents). Applying it to the test set is also fine and can show how features behave on unseen data, but it might reflect specific patterns in the test set rather than overall model behavior across all data points a bar plot is meant to generalize. This is a subtle point and not ""wrong.""

Overall, the generated code is highly functional, demonstrates a deep understanding of the requirements, and implements several best practices (like `TimeSeriesSplit` and proper `ColumnTransformer` usage).",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"To assess the impact of 'stringency_index' and 'people_fully_vaccinated' on 'reproduction_rate', considering 'location' and 'continent' effects, we will implement a mixed-effects linear regression model using `statsmodels`.

This code will perform the following steps:
1.  **Data Loading and Preprocessing**: Load the `synthetic_covid19_data.csv` file, parse dates, create a unique 'location\_id\_nested' for mixed-effects grouping, and calculate 'vaccination\_percentage'.
2.  **Missing Value Handli...",9,"The generated code is very good, demonstrating a strong understanding of mixed-effects modeling and robust data handling.

**Strong Points:**
*   **Correct Model Implementation:** The code correctly implements the mixed-effects linear regression model as specified in the task, including fixed effects, interaction term, and nested random intercepts.
*   **Thorough Data Preprocessing:** The data loading and parsing are handled well. The calculation of `vaccination_percentage` is accurate, including capping at 100%.
*   **Robust Missing Value Handling:** The imputation strategy is sophisticated and appropriate for time-series data, using `ffill`/`bfill`, `interpolate` within groups, and global median as a final fallback. This is a significant improvement over simpler imputation methods.
*   **Correct Random Effects Grouping:** The creation of `location_id_nested` correctly implements the ""location nested within continent"" random effect structure using `continent_location` ids.
*   **Comprehensive Output and Interpretation:** The code reports fixed effects coefficients, standard errors, and p-values clearly. The interpretation section is very helpful and explains the meaning of coefficients and the interaction term.
*   **Correct Random Effects Variance Reporting:** It correctly identifies and reports the variance of the random intercepts and residual variance, with good interpretations.
*   **Good Discussion of Assumptions:** The discussion of model assumptions is thorough and accurate, explaining each assumption and how it *could* be checked, even though full implementation is outside the scope.
*   **Synthetic Data Generation:** The inclusion of synthetic data generation for demonstration purposes is a very helpful addition for standalone execution and debugging, showing proactive thought.

**Areas for Minor Improvement (for a perfect 10):**
*   **Pandas SettingWithCopyWarning:** While not explicitly seen in the output, chained assignments during the imputation (e.g., `df.groupby('location')[col].transform(...)`) can sometimes lead to `SettingWithCopyWarning`. Using `.loc` for direct assignment after a copy or `df.copy()` at the start can prevent this, especially for more complex operations. However, for `transform`, it's generally safe because it returns a new Series/DataFrame.
*   **Warning Suppression (Minor):** The ground truth includes `warnings.filterwarnings` to suppress common `statsmodels` convergence warnings. While not critical for correctness, for production or robust scripting, this can be useful for cleaner output if convergence issues are expected but not critical. The generated code doesn't include this.
*   **Clarity on `group` variable naming:** The task requested ""random intercepts for 'location' nested within 'continent'"". The ground truth directly creates `continent_location_id` for the `groups` parameter. The generated code creates `location_id_nested`. Both achieve the same goal, which is good. However, the explanation for its creation could slightly reinforce that this represents the *nested* structure (e.g., ""This makes each unique (continent, location) combination a distinct group for random intercepts, effectively nesting location within continent.""). The current explanation is already good, but a slight emphasis on ""nesting"" could be added.
*   **Fixed effects formula and interpretation:** The prompt asked for `stringency_index`, `vaccination_percentage`, and *their interaction*. The formula `stringency_index * vaccination_percentage` correctly expands to `stringency_index + vaccination_percentage + stringency_index:vaccination_percentage`. The interpretation for the interaction term is excellent.

Overall, this is an exceptionally well-written and comprehensive solution that perfectly addresses the task requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not found. Make sure the file is in the same directory."")
    exit()

# 2) Select the features for clustering
features = ['elixirCost', 'Win Rate', 'Usage']
X = df[features...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** All steps (loading, feature selection, standardization, K-Means with K selection, PCA, plotting) are implemented correctly.
- **Error Handling:** Includes `try-except` for `FileNotFoundError` when loading the CSV, which is good practice. Also checks for missing columns, though the ground truth's handling is slightly more robust (checking all at once).
- **K-selection (Elbow Method):** Correctly implements the WCSS calculation and plots the elbow curve.
- **Clarity and Readability:** The code is well-structured with comments indicating each step, making it easy to follow.
- **`n_init` Parameter:** Properly includes `n_init=10` in `KMeans` to address newer scikit-learn warnings, which is excellent.
- **Plotting:** Creates a clear and informative scatter plot with PCA components, colored by cluster, using a good colormap (`viridis`). Adds appropriate labels and title.
- **Feature Check (Small Improvement):** While the generated code iterates through features to check for existence, the `ground_truth` code creates a list of `missing_features` first and then checks, which is slightly more efficient as it doesn't `exit()` on the first missing feature but rather reports all missing features at once. This is a minor point.
- **Plot Aesthetics:** The generated code adds some extra aesthetics to the plot like `edgecolors`, `linewidths`, `axvline`, `axhline` which are nice touches for visualization.

**Areas for Minor Improvement (Why not a 10):**
- **Robustness in Feature Selection Check:** The ground truth code checks for all missing features at once and reports them in a list. The generated code exits on the first missing feature it encounters. While functionally correct for this dataset, a more robust solution would identify all missing features before exiting.
- **No `numpy` Import:** Although not strictly used for direct `numpy` operations, it's a very common dependency in data science code, especially for numerical operations and often implicitly used by `matplotlib` or `sklearn`. The ground truth includes it, which is generally good practice to have for ML tasks. However, its absence doesn't impact the current functionality here.
- **`SettingWithCopyWarning`:** The generated code performs `X = df[features]` directly without `.copy()`. If `X` were ever modified later (which it isn't in this specific script as `fit_transform` returns a new array), it *could* lead to a `SettingWithCopyWarning`. The ground truth explicitly uses `.copy()` for this reason, which is a good habit.

These are minor points, and the generated code is very close to perfect for the given task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

# 1. Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# 2. Select features and target
features = ['elixirCost', 'Usage', 'rarity']
target = 'Win Rate'

X = df[features]
y = df[target]

# 3. Preprocess the 'rarity' column using one-hot encoding
X = pd.get_dummies(X, columns=['rarity'], drop_first...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, selects appropriate features and the target, preprocesses the `rarity` column using one-hot encoding (specifically `pd.get_dummies` with `drop_first=True` which is good practice), splits the data as specified, trains a `RandomForestRegressor` model, and evaluates it using R-squared and MSE. The imports are correct and sufficient.

Minor points for improvement (and why it's not a perfect 10):
1.  **Error Handling/Robustness**: The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV and checks for missing columns. While not strictly required by the prompt's core task (which assumes a valid CSV), robust production-ready code or code for a general benchmark would ideally include such checks.
2.  **Missing Value Handling**: The ground truth includes a basic `.dropna()` on the features (`X`), which is a good practice when creating a robust solution, even if the prompt doesn't explicitly state ""handle missing values"". The generated code implicitly assumes no missing values or that the model can handle them directly (which `RandomForestRegressor` can't for `NaN`s in features).
3.  **Clarity/Verbosity**: The ground truth adds print statements for training progress and clear headings for evaluation results, which enhances the user experience and understanding of the code's flow. The generated code simply prints the final metrics without context.

Overall, it's a very good solution that fulfills the core task's requirements accurately and efficiently. The omissions are more about robustness and user experience than correctness of the core ML pipeline.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) Identify the unique categories in the `rarity` column
unique_rarities = df['rarity'].unique()

# Prepare data for ANOVA: Create a list of Win Rate series, one for each rarity group
rarity_groups_win_rates = []
for rarity...",7,"The generated code correctly addresses most of the requirements but has a critical data preparation flaw that could lead to incorrect ANOVA results or errors.

**Strong points:**
*   Correctly loads the dataset and handles `FileNotFoundError`.
*   Identifies unique rarity categories.
*   Clearly states null and alternative hypotheses.
*   Performs the ANOVA test using `scipy.stats.f_oneway` correctly.
*   Prints F-statistic and p-value with appropriate formatting.
*   Correctly interprets the p-value against the significance level (alpha = 0.05) and states the conclusion.
*   Includes necessary imports (pandas, scipy.stats).

**Areas for improvement/Corrections:**
*   **Critical Data Cleaning Issue (`Win Rate` conversion and NaNs):** The generated code does not explicitly convert the 'Win Rate' column to numeric. If 'Win Rate' contains non-numeric strings, `df[df['rarity'] == rarity]['Win Rate'].dropna()` might drop legitimate string values or cause issues if the column is not already numeric. The ground truth explicitly uses `pd.to_numeric(df['Win Rate'], errors='coerce')` and `dropna` on pertinent columns, which is crucial for robust statistical analysis. This is a significant omission.
*   **Handling Empty or Insufficient Groups:** While the code checks `if not rarity_win_rates.empty:`, `f_oneway` expects *at least two* observations per group for a valid ANOVA. The generated code would include groups with only one observation, which `f_oneway` will handle without error but is statistically problematic (it treats a single value as having no variance, which can skew results). The ground truth specifically checks for `len(group_win_rates) >= 2`.
*   **Clarity in Output:** The ground truth separates the hypotheses and interpretation sections, making the output more readable. The generated code's print statements are a bit more scattered.
*   **Error Handling (General):** While `FileNotFoundError` is handled, other loading or processing errors aren't caught (e.g., if 'Win Rate' or 'rarity' columns don't exist).
*   **Hypothesis Wording:** The alternative hypothesis in the generated code ""At least one card rarity category has a different mean Win Rate"" is generally acceptable, but the ground truth's ""At least one rarity category has a significantly different mean Win Rate"" is slightly more precise for statistical testing.

The primary reason for a score of 7 instead of higher is the lack of robust data cleaning for the 'Win Rate' column and insufficient checks for minimum observations per group, which are fundamental for a reliable statistical test like ANOVA.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1) Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: iris.csv not found. Please ensure the file is in ...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** It correctly loads the dataset, defines features and target, performs label encoding, splits the data, trains all three specified models, and calculates the required evaluation metrics (accuracy, precision, recall, F1-score with macro averaging).
- **Completeness:** It identifies the best model based on the F1-score as requested.
- **Code Quality:** Imports are proper, code structure is logical, and variable names are clear.
- **Minor Improvements:**
    - The ground truth includes `stratify=y_encoded` in `train_test_split`. While not explicitly stated as a hard requirement in the prompt, it's good practice for imbalanced datasets and ensures uniform class distribution across splits, which is often crucial for robust model comparison. The generated code omits this.
    - The ground truth explicitly handles `LogisticRegression(max_iter=200)` and the generated code includes `LogisticRegression(random_state=42, solver='liblinear')`. While `max_iter` helps convergence for some datasets, `solver='liblinear'` and `random_state` are good additions for reproducibility and avoiding warnings, especially if the default solver might struggle. The prompt specified default parameters for LR, and both implementations deviate slightly but are functionally sound. The generated code's choice is reasonable.
    - The output formatting is slightly different (e.g., ""Training {name}..."" vs. ""--- Training and Evaluating: {name} ---""). Both are clear, but the ground truth's formatting is arguably a bit more distinct.

Overall, it's a very strong submission, missing only a minor detail in `train_test_split` that could improve robustness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: iris.csv not found. Please ensure the file is in the current directory."")
    exit()

# 2) Select the features
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[f...",10,"The generated code is excellent and perfectly addresses all requirements of the task.

**Strong points:**
- **Correctness:** All steps (loading, feature selection, standardization, K-Means, PCA, plotting) are implemented accurately.
- **K-Means and PCA Parameters:** Correctly uses K=3, `n_init='auto'`, `random_state=42` for K-Means and 2 components for PCA.
- **Plotting:** Both required scatter plots are generated with correct coloring (K-Means labels and true species labels). Titles and axis labels are appropriate.
- **Species Encoding:** Correctly uses `LabelEncoder` for mapping true species labels to numerical values for plotting.
- **Code Quality:**
    - Proper imports are included.
    - Code is well-structured and easy to read.
    - Error handling for `FileNotFoundError` is present.
    - Effective use of `plt.colorbar` for both plots, with the second plot demonstrating advanced use of `cbar.set_ticklabels` to show actual species names, which is a nice touch.
    - Use of `pd.DataFrame` for principal components before plotting is a good practice for readability, although not strictly necessary for this task.
    - `plt.tight_layout()` and `plt.grid()` with `linestyle` and `alpha` improve plot aesthetics.

Overall, the generated code is a complete and high-quality solution to the problem.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# 1) Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found. Please make sure the file is in the current directory."")
    exit()

# 2) Create two new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLeng...",10,"The generated code is excellent. It correctly loads the dataset, creates the two new features (`PetalArea` and `SepalArea`), defines `X` to include both original and new features, correctly encodes the `Species` target for `y`, initializes a Gradient Boosting Classifier with the specified parameters, performs 5-fold cross-validation, and reports the mean accuracy. All imports are correct and necessary. The code structure is clear and follows the requested steps logically. It perfectly meets all requirements of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1. Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: heart.csv no...",9,"The generated code is highly correct and addresses almost all requirements of the task. It correctly loads the dataset, defines features and target, performs one-hot encoding for categorical features and scaling for numerical features using `ColumnTransformer` within a `Pipeline`. It splits the data correctly, trains a `RandomForestClassifier` with the specified parameters, and evaluates it using all requested metrics (accuracy, macro precision, macro recall, macro F1-score, and ROC-AUC).

**Strong points:**
- Correct use of `ColumnTransformer` and `Pipeline` for preprocessing and model training, which is good practice for preventing data leakage and streamlining the workflow.
- Correct identification of categorical and numerical features.
- Correct parameters for `RandomForestClassifier` (`n_estimators=100`, `random_state=42`) and `train_test_split` (`test_size=0.2`, `random_state=42`).
- All required evaluation metrics are calculated and printed correctly.
- Includes error handling for file not found.

**Areas for improvement:**
- The `train_test_split` in the ground truth includes `stratify=y`. While not explicitly stated as a requirement in the prompt's evaluation criteria, stratifying the split is generally good practice for classification tasks, especially with imbalanced datasets, to ensure that the class distribution is maintained across training and testing sets. This might be a subtle but important detail for a robust benchmark.
- The prompt asked for ""comprehensive preprocessing"" and the ground truth added explicit print statements for model training and evaluation start/completion, which are good for usability and debugging in a real-world scenario. The generated code omits these.

Overall, the generated code is excellent and fully functional based on the explicit requirements. The only minor point is the absence of `stratify=y` in the data split, which is a best practice.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: 'heart.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2) Select numerical features for clustering
selected_features = ['Age', 'Resti...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, selects and standardizes features, implements both the elbow method (WCSS plot) and silhouette scores to determine K, applies K-Means with the specified parameters, performs PCA for dimensionality reduction, and visualizes the clusters.

One minor point for improvement:
- In the final PCA plot, using `plt.scatter` with `c=pca_df['KMeans_Cluster']` directly and then `plt.colorbar` is a valid approach. However, for a more explicit legend with cluster labels, iterating through each cluster and plotting them individually (as done in the ground truth) can sometimes be preferred as it allows for direct labeling in the legend rather than relying solely on the colorbar ticks. This is a stylistic choice and doesn't detract from correctness.
- The `print` statements giving status updates are slightly less verbose than the ground truth but still sufficient for the task.

Overall, the code is very good, correct, well-structured, and complete according to the requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Create binary target variable: 'good' if quality >= 7, 'bad' if quality < 7
df['good_quality'] = (df['quality'] >= 7).astype(int)

# Drop the original 'qualit...",9,"The generated code correctly addresses almost all aspects of the task.

**Strong points:**
- **Correct Target Variable:** The binary target variable is created correctly (`quality >= 7`).
- **Pre-processing:** Features are correctly scaled using `StandardScaler`.
- **Data Splitting:** Data is correctly split into 80/20 train/test sets with `stratify=y` and `random_state` for reproducibility.
- **Model Training:** A `LogisticRegression` model is trained with `random_state` and `max_iter`.
- **Evaluation:** Accuracy, F1-score, and classification report are correctly calculated and printed.
- **Feature Importance:** Feature coefficients are correctly extracted and sorted by absolute magnitude, which is a good indication of importance for logistic regression.

**Areas for minor improvement:**
- **Missing Value Handling:** The ground truth includes a check for missing values and a generic imputation strategy. While `winequality-red.csv` typically doesn't have missing values, providing a robust solution that assumes potential missing values (as the ground truth does) is better practice and part of the prompt's `2) Preprocess the data (handle missing values if any...)` requirement. The generated code omits this explicit check and handling.
- **User Feedback/Prints:** The generated code is more concise, which is good, but the ground truth includes more verbose print statements for progress tracking (e.g., ""Dataset loaded successfully"", ""No missing values found"", ""Features scaled successfully""). While not strictly required for functionality, these make the code more user-friendly for someone running it.
- **Displaying Feature Importance:** The ground truth explicitly shows 'Abs_Coefficient' column in the final output, which is helpful for understanding the sorting criterion directly. The generated code only shows 'Feature' and 'Coefficient'. The ground truth also adds a short interpretation of coefficients, which is an excellent contextual addition.

Overall, it's a very good solution that fulfills the core requirements. The lack of explicit missing value handling is the main reason for not giving a perfect score.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# 1. Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: 'heart.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2. Create a box plot showing the distribution of Oldpeak for patients with HeartDisease=0 and HeartDisease=1
plt.figure(figsize=(8, 6))
sns.boxplot(...",8,"The generated code addresses most of the requirements correctly.

**Strong Points:**
*   **Correct Data Loading and Handling:** Properly loads the dataset and handles `FileNotFoundError`.
*   **Correct Box Plot:** Generates the box plot correctly with appropriate labels and title.
*   **Correct Logistic Regression:** Accurately performs logistic regression, extracts the coefficient, and calculates the odds ratio.
*   **Correct Binning:** Uses `pd.qcut` as specified for quantile-based binning and handles the `duplicates='drop'` argument.
*   **Correct Proportion Calculation:** Calculates the proportion of `HeartDisease=1` within each bin correctly using `groupby().mean()`.

**Areas for Improvement:**
*   **Interpretation of Odds Ratio (Minor):** While it states the general meaning, the interpretation for the odds ratio is a generic text, not providing a specific numerical interpretation based on `odds_ratio` and `oldpeak_coefficient` value (e.g., ""For every one-unit increase in Oldpeak, the odds of having Heart Disease are multiplied by X.XX (i.e., they increase/decrease by Y.YY%)""). This was explicitly asked for in the prompt's interpretation requirement. The ground truth provides a much more detailed and numerical interpretation.
*   **Box Plot X-axis Labels (Minor):** The x-axis labels for the box plot are `0` and `1` which is correct, but the ground truth explicitly added more descriptive labels (""No Heart Disease"", ""Heart Disease""), which improves readability for the user.
*   **Missing Visualization for Binning Proportions (Minor):** The prompt asks to ""display"" the proportion of `HeartDisease=1` within each bin. While printing the pandas Series is a form of display, the ground truth includes a bar plot visualization, which is a more effective way to display such categorical proportions. This could be considered a ""display"" enhancement.
*   **Oldpeak Label in Box Plot (Minor):** The box plot y-axis label for `Oldpeak` is just ""Oldpeak (ST depression)"", whereas the ground truth includes the more complete description ""Oldpeak (ST depression induced by exercise relative to rest)"" as per the task description. This is a minor detail.

Overall, the core analytical steps are well-executed. The missing details are mostly around presentation and detailed interpretation, which are important for a comprehensive solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# 1) Load data
try:
    df = pd.read_csv('spotify_2023.csv')
    print(""Data loaded successfully."")
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Make sure the CSV file is in the current dire...",10,"The generated code is excellent and meets all requirements of the task.

**Strengths:**
- **Correctness:** All steps are implemented accurately, from data loading to model evaluation.
    - `streams` is correctly converted to numeric with `errors='coerce'` and NaN rows are dropped.
    - `is_popular` target variable is created using the 75th percentile of `streams`.
    - Specified numerical features are selected.
    - `key` and `mode` are correctly one-hot encoded using `pd.get_dummies` and combined with numerical features.
    - Data splitting is done with `test_size=0.2` and `random_state=42`, and importantly, `stratify=y` is used, which is good practice for imbalanced target variables.
    - Logistic Regression model is instantiated with `max_iter=1000` and `random_state=42`.
    - F1-score (macro), ROC AUC score, and confusion matrix are all calculated and displayed correctly.
- **Code Quality:**
    - Imports are proper and comprehensive.
    - Code is well-structured with clear comments for each step, improving readability.
    - Includes robust error handling for file not found and provides informative print statements for data loading, cleaning, and shape changes.
    - Includes additional checks for empty dataframes after cleaning or processing, which makes the code more robust.
    - Explicitly converts categorical features to `category` dtype before `get_dummies`, a good practice for consistency.
    - Handles potential NaNs in features after `get_dummies` (though unlikely), showing thoroughness.
    - The use of `ConfusionMatrixDisplay` with `matplotlib.pyplot` for plotting the confusion matrix is an excellent addition, making the output more visually informative.
- **Completeness:** All requirements outlined in the prompt are addressed comprehensively. The code not only satisfies the minimum requirements but also enhances them with good practices (stratified split, detailed print statements, plotting confusion matrix).",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load data
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Please make sure the file is in the current directory."")
    exit()

# 2) Select audio features and drop rows with any NaNs
# Define the list of audio features to be used
audio_fea...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
*   **Correctness:** All required steps are implemented correctly: data loading, feature selection, NaN handling, standardization, K-Means clustering (with correct K, `random_state`, and `n_init`), PCA, plotting clusters, and calculating mean original features per cluster.
*   **Completeness:** All specified requirements are met.
*   **Code Quality:**
    *   Proper imports are used.
    *   Code is well-structured with comments indicating each step.
    *   Appropriate use of `.copy()` to avoid `SettingWithCopyWarning`.
    *   Clear variable names.
    *   Includes basic error handling for `FileNotFoundError`.
    *   The use of `df_cleaned_features.index` when creating `scaled_df` ensures index alignment, which is good practice.
    *   The check `if df_cleaned_features.shape[0] < 4:` is a thoughtful addition to prevent issues if too many rows are dropped.

**Minor Area for Improvement (Not critical for score, but worth noting):**
*   The ground truth explicitly states: ""For each cluster, calculate and display the mean values of the original (unstandardized) selected audio features."" In the generated code, `original_data_for_cluster_means` is created *after* `dropna` and *before* `StandardScaler`. This means the mean values are calculated from the *cleaned* original (unstandardized) data, which is correct. However, `df_original_for_analysis` in the ground truth is also a copy *after* NaN drop, so both effectively achieve the same result. The phrasing in the ground truth ""Store the original (unstandardized) data for mean calculation later"" could slightly imply storing the *entire* original data without any cleaning initially, then applying filtering for analysis, but both codes correctly address the 'original (unstandardized)' part on the *cleaned* subset used for clustering.

Overall, the generated code is excellent and fully addresses the prompt.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def analyze_audio_feature_trends(filepath='spotify_2023.csv'):
    """"""
    Analyzes trends in audio features (danceability, energy, valence) of popular songs
    from 'spotify_2023.csv' over years (2010-2023).

    Args:
        filepath (str): The path to the spotify_2023.csv file.
    """"""

    # 1. Load data
    try:
        df = pd.read_csv(filepath)
    except FileNotFoundError:
        print(f""Error: The file '{filepath}' was no...",9,"The generated code correctly addresses all requirements of the task. It loads the data, correctly handles the `streams` column conversion and filtering (including dropping NaNs), filters by `released_year` and `streams` as specified, calculates the required yearly averages, and generates well-labeled plots.

**Strong points:**
*   **Encapsulation:** The code is well-structured within a function `analyze_audio_feature_trends`, making it reusable and organized.
*   **Error Handling:** It includes robust error handling for `FileNotFoundError` and checks for the existence of required columns (`streams`, `released_year`, `danceability_%`, etc.), which is excellent practice.
*   **Streams Conversion:** It explicitly handles potential commas in the `streams` column before conversion, which is a good foresight for messy data.
*   **Filtering Logic:** The filtering steps are clear and correct.
*   **Plotting:** Dynamic plotting (using `subplots` with `nrows=len(features)`) is an intelligent way to handle plotting multiple features and allows for easier extension if more features were to be analyzed. Labels, titles, and `tight_layout` are used correctly.
*   **Clarity:** Comments and print statements are helpful for understanding the execution flow and data transformations.

**Areas for minor improvement:**
*   **Redundant `initial_rows` / `rows_after_filtering` prints:** While useful for debugging, these print statements provide redundant information compared to the `df.shape` prints in the ground truth for a final solution.
*   **`df_filtered.copy()`:** While `copy()` is used to prevent `SettingWithCopyWarning`, it's not strictly necessary in the exact way it's used if `df_filtered` is immediately reassigned after the first filter, but it doesn't hurt.
*   **Minor Plot Customization:** The color selection for plots is hardcoded (`skyblue`, `lightcoral`, `mediumseagreen`) which works, but could be part of a more general palette if there were many features. The ground truth uses individual subplots, which works fine as well. The generated code chose to plot each feature on its own subplot which is also a valid approach. Spreading them across rows might be preferred for very few plots like this (3), but columns might be better for comparison.

Overall, it's a very high-quality solution that demonstrates attention to robustness and best practices.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: 'youtube_trendings.csv' not found. Make sure the file is in the current directory."")
    exit()

# Function to clean numeric columns
def clean_numeric_column(series):
    return pd.to_numeric...",9,"The generated code is largely correct and addresses almost all requirements of the task.

**Strong points:**
- All required libraries are imported.
- Data loading with error handling is present.
- The cleaning of 'Likes', 'Dislikes', and 'Video views' is correctly implemented, including comma removal, conversion to numeric, and filling NaNs with 0. The use of a helper function `clean_numeric_column` is a good practice for reusability and readability.
- Extraction of the year from `published` column is correct.
- Handling of missing 'Category' values by filling with 'Unknown' and then one-hot encoding is correct.
- Feature and target definition (`X` and `y`) is accurate.
- Data splitting with `random_state=42` and 80/20 ratio is correct.
- Linear Regression model training is correct.
- Model evaluation with R2 and MAE is correctly performed and printed.

**Areas for minor improvement/consideration:**
- **`published_year` NaN handling**: The generated code `fillna(df['published'].dt.year.median())` for `published_year` is a reasonable approach for robustness, especially if dates might have NaNs, but the prompt specifically stated to ""treat the `published` column (year) as a numeric feature"" and did not explicitly ask for robust NaN handling for the year itself if the date conversion failed. The interpretation of the `published` handling in the ground truth was simpler, focusing on the year as a numeric feature and assuming typical data quality where the year can be extracted directly without many NaNs after conversion. However, filling NaNs (if any) with median is not incorrect, it's just a slightly different interpretation of robustness. For this specific prompt, the ground truth's `fillna(0)` after `pd.to_datetime` for simple `published` year processing (if relevant) was a direct interpretation of ""fill resulting NaNs with 0"" for similar numeric handling. In this case, since `published_year` is derived, filling NaNs from `dt.year` with median is a reasonable approach.
- **Missing `sklearn.preprocessing` import**: Although `pd.get_dummies` was used for one-hot encoding (which is perfectly fine and often preferred for simplicity), if `OneHotEncoder` from `sklearn.preprocessing` is specifically mentioned in the prompt (as it was in the initial problem description), it's good practice to import it, even if not directly used, to indicate awareness of the sklearn tool for that task. The prompt did *not* explicitly state `OneHotEncoder` should be used for category encoding, it just listed it in the imports. The generated code did not import it, but it's not a functional error.

Overall, the code is highly functional and correctly implements the task. The only minor deviation is in handling potential NaNs after year extraction, which is a robust choice but not strictly following the exact `fillna(0)` approach specified for `Likes`, `Dislikes`, `Video views`.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: 'youtube_trendings.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) For the Video views column: remove commas, convert to numeric, and fill resulting NaNs with 0
df['Video views'] = df['Video views'].astype(str).str.replace(',', '', regex=False)
df['Video views'] = pd.to_numeric(df...",9,"The generated code successfully addresses most requirements of the task.

**Strong points:**
- **Data Loading:** Correctly loads the CSV and includes basic error handling for `FileNotFoundError`.
- **`Video views` cleaning:** Implements all steps correctly: removes commas, converts to numeric, and fills NaNs with 0.
- **`published` year handling:** Correctly converts a `published` column (assumed here to be `publishedAt` as in the ground truth, though the prompt only said `published`) to datetime and extracts the year to a numeric type. It also correctly handles potential NaNs in the year extraction.
- **Grouping and Aggregation:** Correctly groups by `published_year` and calculates both the average `Video views` and the count of unique `Video` titles.
- **Plotting:** Creates two subplots with line graphs as required, each with appropriate titles and axis labels. `ticklabel_format` for plain numbers on the y-axis is a good touch.
- **Imports:** Includes necessary imports (pandas, matplotlib.pyplot).

**Areas for minor improvement:**
- **Column Name Ambiguity (`published`):** The prompt specified ""the `published` column (year)"", which can be ambiguous if the column name is actually `publishedAt` (as is common in YouTube datasets and used in the ground truth). The generated code assumes a column literally named 'published' contains the date string. If the actual column name is `publishedAt`, this would cause an error. While the code is syntactically correct for a column named 'published', this is a potential point of failure based on the dataset structure. The ground truth explicitly states `publishedAt` which makes it more robust.
- **Handling of NaNs in `published_year`:** The code uses `df.dropna(subset=['published_year'], inplace=True)`. While this ensures valid year data, it might drop rows that could still be useful if their `publishedAt` date was valid but just ill-formatted. The ground truth's approach of `fillna(0).astype(int)` allows all rows to be retained, treating unparseable dates as a specific '0' year which can then be optionally filtered out during aggregation, giving more control. The `dropna` approach is simpler but potentially discards more data. For this specific task, dropping NaNs for `published_year` is acceptable as grouping on `NaN` is not meaningful.
- **Color choices in plots:** The default color for the first plot is blue, and the second is orange. While acceptable, using different colors or common colorblind-friendly palettes could improve readability. The ground truth provides specific `skyblue` and `lightcoral` which are good.
- **X-axis ticks:** The generated code doesn't explicitly set x-axis ticks to be integers for years, while the ground truth does (`set_xticks`). This can lead to non-integer ticks if the range is small or data points are sparse, which is less visually intuitive for years. However, with `yearly_trends['published_year']` directly on the x-axis, matplotlib often does a reasonable job.

Overall, the code is very good and effectively solves the problem with clear, well-structured logic. The points for improvement are minor refinements rather than critical errors.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords
import numpy as np # For potential NaN handling if needed, though pandas handles it well

# Ensure stopwords are downloaded (run once)
try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')

# 1) Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: 'youtube_trendi...",9,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
- **Correctness:** The code correctly loads the dataset, cleans the 'Video views' column, calculates word counts, computes the correlation, filters by year, processes titles (lowercase, punctuation removal, stop word removal, alphabetic filtering), and identifies the 5 most common words.
- **Completeness:** All the specified requirements of the task are addressed.
- **Code Quality:** Proper imports are used. Error handling for file not found and NLTK stopwords download is present. The steps are clearly separated and executed.
- **Robustness:** Handles potential non-string values gracefully using `astype(str)` when processing titles and views. Uses `errors='coerce'` for `pd.to_numeric` and `pd.to_datetime`, which is good practice.

**Areas for Minor Improvement (for a perfect score):**
- **Word Processing Iteration:** In step 6, the generated code concatenates all titles from 2023-2024 into a single string (`all_titles_2023_2024 = "" "".join(...)`) before processing words. While it yields the correct result for finding common words overall, it can be less memory-efficient for very large datasets and doesn't allow for per-title analysis if that were ever needed. The ground truth's approach of iterating through each title individually and processing it is perhaps slightly more robust and idiomatic in this context, though not strictly necessary for *this* specific word counting task.
- **Correlation Calculation Note:** The comment `df['title_word_count'] = pd.to_numeric(df['title_word_count'])` is redundant as `len()` always returns an integer, so `title_word_count` would already be numeric. This doesn't cause an issue, but it's an unnecessary line.
- **Filtering `published` column:** The line `df_filtered_dates = df.dropna(subset=['published'])` removes rows where the `published` date could not be parsed. While functionally correct for filtering, the ground truth's approach of only operating on `df_filtered_years` (which correctly handles `NaT` values by `dt.year.isin()` returning `False` for them) is slightly more direct and avoids an intermediate DataFrame specifically for dropping NaNs. Both achieve the same outcome for this specific filtering.

Overall, the generated code is excellent and fully addresses the prompt. The minor points mentioned are more about stylistic choices or very slight optimizations rather than outright errors or missing functionality.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
import re # For regex extraction in custom function
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score

# 1) Load data
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found. Please ensure the CSV file is in the same directory."")
    exit()

# Create a copy to avoid SettingWithCopyW...",9,"The generated code is highly correct, complete, and well-structured, successfully addressing all requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly implements all specified cleaning steps, target creation, feature engineering, and model training/evaluation.
    - `Launched Price (USA)` cleaning and `is_expensive_usa` target creation are accurate, including handling of NaNs and type conversion.
    - All numerical feature cleaning (`Mobile Weight`, `RAM`, `Battery Capacity`, `Screen Size`) is correctly performed.
    - The `extract_mp` function for `Front Camera` and `Back Camera` is robust, handling NaNs and extracting the first numerical MP value as required.
    - One-hot encoding for `Company Name` and `Processor` is done correctly.
    - Feature and target definition is precise.
    - Data splitting, model training, and evaluation metrics (F1-macro, ROC AUC) are correctly applied.
- **Completeness:** All eight steps outlined in the task are thoroughly implemented.
- **Code Quality:**
    - Proper imports are included.
    - Variable names are clear and descriptive.
    - Comments are used effectively to explain different sections of the code, aligning with the task's numbered steps.
    - Error handling for `FileNotFoundError` is present.
    - The use of `.copy()` prevents `SettingWithCopyWarning`.
    - Dynamic collection of one-hot encoded column names is good practice.
    - The handling of `Launched Year` column (checking existence, coercing to numeric) is a good addition for robustness.
    - `stratify=y` in `train_test_split` is a good choice for classification tasks to maintain class distribution.
    - The optional print statements for dataset information are helpful for debugging and understanding the process.

**Areas for Minor Improvement (to reach a perfect 10):**
- **Strict adherence to NaN handling for features:** The prompt states ""Drop rows with any NaNs in features or target."" The generated code first processes features and then drops NaNs *after* defining `X` and `y` by creating `df_final` and dropping NaNs there. This is functionally correct and effective. However, the ground truth explicitly drops NaNs in `is_expensive_usa` first, then implicitly handles remaining NaNs in features when `X` and `y` are concatenated and `dropna()` is called. The generated code also drops NaNs where `is_expensive_usa` is NaN, which is good. The subsequent dropna on `df_final` handles feature NaNs. The key difference is that the ground truth explicitly drops rows with `is_expensive_usa` as NaN then, at the end, drops NaNs in `X` and `y` together, which is subtly different but functionally similar. The prompt could be interpreted as ""handle NaNs for each feature during cleaning by coercing to NaN, then drop NaNs *globally* when defining X and y"". Both codes achieve this, but the ground truth's final `combined_df.dropna()` is a very explicit application of ""Drop rows with any NaNs in features or target"" at the very end. The generated code's approach of `df_final.dropna()` achieves the same, but the GT emphasizes the ""any NaNs"" removal as a final global step.
- **Redundant `astype(str)` or `regex=False` in some string operations:** While not strictly incorrect, some `.astype(str)` on columns that are likely already strings (e.g., `Mobile Weight`, `RAM`) or explicit `regex=False` when the pattern contains no special regex characters are sometimes redundant. Modern pandas often handles these without explicit `astype(str)` for `str` accessors, and `regex=False` is the default for `replace` in recent pandas versions for literal strings. This is a very minor style point.
- **Redundant `float` conversion for target:** `df_processed['is_expensive_usa'] = (df_processed['Launched Price (USA)_cleaned'] > 1000).astype(float)` creates a boolean series and then converts it to float (True=1.0, False=0.0). While it works, directly using `astype(int)` after dropping NaNs might be slightly cleaner, as `(df_processed['Launched Price (USA)_cleaned'] > 1000).astype(int)` would directly yield 0s and 1s, but would turn NaNs to -1 or raise an error if not handled. The current approach with `astype(float)` then `dropna` then `astype(int)` is robust.

These are minor points, and the generated code is excellent.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price_str):
    """"""
    Cleans a price string by removing currency symbols and commas,
    then converts it to a numeric type. Handles errors by returning NaN.
    """"""
    if pd.isna(price_str):
        return np.nan
    try:
        # Ensure it's a string before stripping
        price_str = str(price_str)
        # Remove currency symbols
        price_str = price_str.replace('PKR', '').replace('INR', '').replace('CNY', '').repl...",9,"The generated code is very good and largely correct.

**Strong Points:**
*   **Correctness:** It correctly loads the data, defines and applies the `clean_price` function, handles NaN values gracefully, calculates the `Average Global Price` using `skipna=True`, and identifies the top 5 models.
*   **Completeness:** All requirements from the prompt are addressed.
*   **Code Quality:** The code is well-structured, uses proper imports, and has clear variable names. Error handling for `FileNotFoundError` is present, and warnings for missing columns are also included.
*   **Price Cleaning Robustness:** The `clean_price` function handles `pd.isna` upfront and converts input to string before processing, which is robust. The multiple `replace` calls for currency symbols are functional, though a loop as in the ground truth might be slightly more scalable if many more symbols were added.
*   **Average Calculation:** Correctly uses `mean(axis=1, skipna=True)` to handle available data.
*   **Top 5 Identification:** Correctly sorts and selects the top 5 models.

**Areas for Minor Improvement (Why not a 10):**
*   **Display Formatting:** While `print(result.to_string(index=False))` works, the prompt specifically asked to ""Display the `Model Name` and their calculated `Average Global Price`...sorted by `Average Global Price` descending."" The ground truth includes a descriptive print statement (`print(""Top 5 Mobile Phone Models by Average Global Price:"")`) which enhances readability for the user. The generated code outputs just the DataFrame table.
*   **Redundancy in `df_filtered`:** The `df.dropna(subset=['Average Global Price'])` step before `sort_values` is functionally correct but somewhat redundant in this specific case. `sort_values` followed by `head(5)` would naturally only include models that have a non-NaN `Average Global Price` (unless all top 5 had NaN, which `dropna` prevents). However, `dropna` ensures that `sort_values` isn't attempting to sort NaNs if there were non-NaN values later in the dataset. The ground truth handles this via `dropna(subset=['Average Global Price'])` on the `top_5_models_display` *after* `head(5)`, which is also valid and perhaps slightly more efficient if there are many rows where the average is NaN but are not among the top 5. Both approaches are generally fine for correctness, it's a minor optimization/style point.

Overall, an excellent submission that correctly solves the problem and demonstrates strong pandas proficiency.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
except FileNotFoundError:
    print(""Error: mobiles_2025.csv not found. Please ensure the file is in the current directory."")
    exit()

# 2) Clean specified columns and handle conversion errors
# Clean 'RAM': remove 'GB', convert to numeric (coerce errors to NaN)
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '', regex=False).str.strip()
df['RAM'] = pd.to_nume...",9,"The generated code is largely correct and addresses most of the task requirements.

**Strong Points:**
- **Correct Data Loading and Cleaning:** The code correctly loads the CSV, cleans `RAM`, `Battery Capacity`, and `Launched Price (USA)` columns as specified, including handling conversion errors by coercing to NaN and dropping rows with NaNs in these critical columns. It also correctly handles `Launched Year` conversion and NaN dropping.
- **Correct Filtering:** It correctly filters the data to include only 'Apple' products.
- **Correct Grouping and Iteration:** It correctly groups by `Launched Year` and iterates through each unique year to generate plots.
- **Correct Plotting Logic:** It uses subplots for each year as requested, plotting RAM, Battery Capacity, and Launched Price.
- **Appropriate Titles and Labels:** The plots have correct titles (per metric) and y-axis labels, and a `suptitle` for the year.
- **Good Use of `tight_layout`:** `plt.tight_layout` is used to prevent overlapping elements.
- **Error Handling for File Not Found:** The `try-except` block for `FileNotFoundError` is a good practice.
- **Clarity in `astype(str)` for string operations.**

**Areas for Minor Improvement (Why not a 10):**
- **Plot Style/Aesthetics:** The ground truth code used `seaborn-v0_8-darkgrid` and `sns.boxplot` which provides a more professional and visually appealing look by default. The generated code uses `matplotlib.pyplot.boxplot` directly, which is functional but less aesthetically pleasing and does not use Seaborn, despite `import seaborn as sns` being present in both codebases. However, the generated code *only* imports `matplotlib.pyplot` and not `seaborn` (it does not import `seaborn` at all, which is a minor discrepancy to the implicit expectation to use it if it's the primary plotting library in the ground truth). The task description explicitly said ""Include proper imports (pandas, matplotlib.pyplot or seaborn)"", so `matplotlib.pyplot` is acceptable.
- **Redundant `unique_years` Check:** The `if not unique_years:` check is good, but the code implicitly assumes `apple_df` might be empty, and if `apple_df` is empty, `unique_years` would also be empty. This is a very minor point and not a functional flaw.
- **No `seaborn` import:** While not explicitly required for `matplotlib.pyplot.boxplot`, the ground truth used `seaborn` and the prompt implied it (""matplotlib.pyplot or seaborn""). The generated code does not import it, which is fine, but if the intention was to use `seaborn` for better aesthetics, it's missing.

Overall, the generated code successfully completes the core requirements of the task. The minor points are mostly about visual presentation and not functional correctness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats # For statistical tests
from sklearn.base import BaseEstimator, TransformerMixin

# Preprocessing and Feature Engineering
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer, IterativeImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Model Selection and T...",9,"The generated code is highly comprehensive and addresses almost all aspects of the prompt with good correctness and code quality.

**Strong Points:**
- **Comprehensive Data Preprocessing:** Includes advanced feature engineering from 'Name' (title extraction/consolidation), 'FamilySize', 'IsAlone', 'Deck' from 'Cabin', and 'Fare_Per_Person'. It correctly uses `IterativeImputer` for numerical and `SimpleImputer` for categorical missing values, and `PolynomialFeatures` for interaction terms.
- **Robust Pipeline Usage:** Excellent use of `Pipeline` and `ColumnTransformer` to encapsulate all preprocessing steps, preventing data leakage and ensuring consistency. The custom `FeatureEngineer` transformer is well-integrated.
- **Multiple Base Models with Tuning:** Implements Random Forest, Gradient Boosting, SVM, and Neural Networks (`MLPClassifier`). It uses `RandomizedSearchCV` for hyperparameter tuning with a comprehensive set of parameters and `roc_auc` as the refit metric.
- **Ensemble Methods:** Correctly implements both `VotingClassifier` (soft voting) and `StackingClassifier` with base models being the full pipelines, which is crucial for proper ensemble training.
- **Comprehensive Evaluation:** Uses `StratifiedKFold` for cross-validation, generates learning curves and validation curves, and presents a clear summary of model performance.
- **Feature Selection:** Implements `SelectKBest` and `SelectFromModel` and demonstrates their impact by re-evaluating a RandomForest model. It correctly identifies the need to extract feature names *after* preprocessing for these techniques.
- **Model Interpretability (SHAP):** Successfully integrates SHAP for the best RandomForest model, providing summary plots and dependence plots. It correctly handles the preprocessing steps to get the features in the right format for SHAP.
- **Statistical Significance Tests:** Includes both paired t-test and Wilcoxon signed-rank test to compare model performances, which is a strong point.
- **Code Quality:** Good imports, clear variable names, comments, and logical flow. Random state is set for reproducibility.

**Areas for Minor Improvement:**
- **LIME Implementation:** The prompt explicitly asks for ""SHAP or LIME"". The generated code provides SHAP but omits LIME. While SHAP is arguably more powerful for tree-based models, including LIME would complete the interpretability requirement. If LIME was intended to be covered by ""SHAP or LIME"" as an ""either/or"", it should still be present in the generated code as one of the options.
- **XGBoost:** The prompt specifically mentions ""XGBoost"". While Gradient Boosting (`GradientBoostingClassifier`) is included, `xgboost` as a separate library/model is not explicitly present in the base models. This is a minor omission, as `GradientBoostingClassifier` covers the same family of algorithms, but the prompt's wording suggests a distinct inclusion.
- **Feature Selection (RFE):** The prompt also mentions `RFE`. While `SelectKBest` and `SelectFromModel` are included, `RFE` is not implemented in the feature selection section.
- **Error Handling/Robustness for LIME:** The `Ground Truth` code provides a robust prediction function for LIME (`predict_proba_for_lime`) that handles `pd.DataFrame(data, columns=X_test.columns)`, ensuring LIME perturbed samples are correctly passed through the pipeline. The generated code also has this, but since LIME is missing, the robustness is not demonstrated.
- **Clarity in `RandomizedSearchCV` Iterations:** `n_iter=10` might be too small for effective hyperparameter tuning, especially compared to `n_iter=50` in the ground truth. While it demonstrates the functionality, for a ""comprehensive"" benchmark, a higher number would be more realistic.

Overall, the code is very well-structured and provides a strong solution to most requirements of the prompt. The missing components (LIME, XGBoost, RFE) are relatively minor given the overall complexity and depth achieved.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1) Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: 'imdb_top_250.csv' not found. Make sure the file is in the current directory."")
    exit()

# Clean 'Runtime' column by removing ' min' and converting to numeric
df['Runtime'] = df['Runtime'].str....",9,"The generated code correctly implements all specified requirements: loading the data, defining features and target, splitting the data, training a Linear Regression model, and evaluating it with R-squared and MAE. It includes all necessary imports and handles the `Runtime` column cleaning correctly.

One minor area for improvement, which is not a critical error for this specific dataset but good practice, is the order of data preprocessing steps. The generated code first processes `Runtime` and then immediately drops NaNs based on `Year`, `Runtime`, and `Rating`. It then processes `Year` separately. It would be slightly more robust to ensure all cleaning (like `str.replace` and `pd.to_numeric`) for all relevant columns (`Year`, `Runtime`, `Rating`) is done *before* a single `dropna` operation on the `subset` of these columns. While it works for `imdb_top_250.csv` because `Year` data is generally clean, if `Year` had string values that needed `pd.to_numeric` to resolve, dropping NaNs *before* that conversion for `Year` might prematurely remove rows if `Year` was treated as a string when `dropna` was called. However, given `Year` is generally numeric, it's a minor point. The ground truth code's handling of `Year` and `Rating` conversion before a single `dropna` is a bit more robust.

Overall, it's a very good solution that successfully addresses all prompt requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 1. Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2. Select the features Rating and Runtime for clustering
features_for_clustering = ['Rating', 'Runtime']
X_original = df[features_for_clustering].copy()

# 3. Standardize these features using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_original)

# 4. Apply K-...",7,"The generated code correctly addresses most of the requirements and demonstrates a good understanding of K-Means clustering in Python.

**Strong Points:**
*   Correctly loads the data.
*   Correctly selects the features.
*   Correctly standardizes the features using `StandardScaler`.
*   Applies K-Means with the specified parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`).
*   Generates a scatter plot with original (non-standardized) features, coloring by cluster labels, and includes title and axis labels.
*   Calculates and prints the mean `Rating` and `Runtime` for each cluster.
*   Uses proper imports.

**Areas for Improvement (Why not an 8 or higher):**

*   **Robustness/Data Cleaning:** The ground truth code includes crucial data cleaning steps for the 'Runtime' column (handling ' min' suffix and converting to numeric) and ensures both 'Rating' and 'Runtime' columns are numeric and handles `NaN` values. The generated code directly assumes these columns are clean and numeric upon loading, which is a common point of failure for real-world datasets. This makes the generated code less robust.
*   **Error Handling:** The ground truth code includes `try-except` blocks for `FileNotFoundError` and checks for the existence of `Runtime` and `Rating` columns. The generated code lacks these error handling mechanisms, making it prone to crashing if the CSV file isn't found or if the column names differ.
*   **Minor Plotting Detail:** The ground truth added a `colorbar` to the plot, which is good practice for understanding what the colors represent (cluster labels). *Correction: The generated code does include a colorbar, my initial review missed it. This is a point in its favor.*

The main reason for the score of 7, rather than higher, is the lack of robust data cleaning and error handling, which are critical in production-ready data science code. Without these, the code might fail unexpectedly for a slightly different dataset or if the 'Runtime' column is not perfectly clean.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re

# Ensure NLTK resources are available
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
try:
    word_tokenize(""test"")
except LookupError:
    nltk.download('punkt')


# 1) Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: 'imdb_top_...",8,"The generated code correctly addresses all aspects of the task, demonstrating good understanding of data loading, feature engineering, correlation calculation, and text preprocessing.

**Strong Points:**
- **Correct Data Loading:** Handles `FileNotFoundError` robustly.
- **`title_length` Calculation:** Correctly uses `apply(len)` or `str.len()`.
- **Correlation Calculation:** Accurately calculates and prints the Pearson correlation.
- **Comprehensive Text Preprocessing:**
    - Correctly converts to lowercase.
    - Uses `re.sub` for robust punctuation removal, which is better than `string.punctuation`.
    - Correctly tokenizes using `nltk.word_tokenize`.
    - Effectively removes stop words using NLTK's `stopwords.words('english')`.
    - Includes a sensible filter `len(word) > 1` during stopword removal, which helps remove single-character artifacts.
- **Most Common Words:** Accurately finds and prints the 10 most common words using `collections.Counter`.
- **NLTK Downloads:** Includes the necessary `try-except LookupError` blocks to download NLTK resources if not already present, which is a good practice for portability.
- **Proper Imports:** All required libraries are imported.

**Areas for Improvement (minor):**
- **Correlation Calculation with `NaN` considerations:** The ground truth code explicitly handles potential `NaN` values in 'Rating' (using `pd.to_numeric(errors='coerce')` and `dropna`) before calculating correlation. While the dataset might not have `NaN`s in 'Rating' by default, it's a good practice for robustness in real-world scenarios. The generated code assumes 'Rating' is clean or will handle `NaN`s implicitly (which `corr()` does by default, but it's good to be explicit about data cleaning).
- **Processing Loop vs. Pandas `apply`:** The text processing part (lowercase, punctuation removal, tokenization, stop word removal) is done in a `for` loop, while the ground truth uses `df.apply()`. While both achieve the correct result, `df.apply()` is generally more idiomatic and often more performant for larger datasets in Pandas. This is a minor stylistic point, as for 250 rows, the performance difference is negligible. The use of a loop storing all tokens in `all_processed_tokens` directly is efficient for the final `Counter` step.
- **Printed Output:** The format of the printed output for the correlation and most common words is slightly different from the ground truth, but this is a minor aesthetic difference and doesn't impact correctness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Make sure the file is in the current directory."")
...",8,"The generated code correctly addresses most of the task requirements and employs a robust pipeline architecture for preprocessing, which is a good practice.

**Strengths:**
*   **Correct Preprocessing:** The code correctly handles numerical imputation with median, categorical imputation with mode, one-hot encoding, and numerical scaling.
*   **Pipeline Usage:** Implementing a `ColumnTransformer` within a `Pipeline` is an excellent approach for managing complex preprocessing steps, making the code more organized and preventing data leakage during cross-validation.
*   **Target Label Encoding:** The `species` target is correctly label-encoded before model training.
*   **Cross-validation:** Uses `cross_val_score` for 5-fold cross-validation and prints the mean accuracy.
*   **Error Handling (File Not Found):** Includes a `try-except` block for `FileNotFoundError`, which is a good defensive programming practice.
*   **Handling `species` NA:** Explicitly drops rows with missing `species` target.

**Areas for Improvement (Why not a 10):**
*   **Imputation of `island` edge case:** The ground truth code specifically handles the tricky edge case where the `island` column might be *entirely* NA, leading to `mode()[0]` failing directly and requiring special attention for dropping rows. The generated code attempts to use `try-except` for `mode()[0]` but might not perfectly replicate the ground truth's strict dropping logic if `island` was entirely NA (it still attempts to `fillna(np.nan)` which then gets caught by `dropna`). The ground truth's logic for `island` is more robust in this specific extreme scenario.
*   **Data Copying and Alignment:** The generated code combines `X` and `y` into `combined_df` early on for preprocessing and then separates them. While this works, it adds a slight unnecessary complexity compared to keeping `X` and `y` separate and applying transformations, especially if `X` and `y` were already correctly aligned. The ground truth carefully manages `X` and `y` alignment after dropping rows, which is a common point of error.
*   **Explicit `drop_first` in OneHotEncoder:** While `handle_unknown='ignore'` is good, `drop_first=False` (default for `OneHotEncoder` if `handle_unknown='ignore'` is not specified, but good to be explicit here if required by the prompt's `pd.get_dummies` behavior) is not explicitly set, which `pd.get_dummies` does by default. The prompt implies `pd.get_dummies` was the expected method, which defaults to `drop_first=False`. For tasks where multicollinearity is a concern, `drop_first=True` often is preferred, but the prompt didn't specify.

Overall, it's a very good solution and demonstrates a strong understanding of data preprocessing and machine learning pipelines. The minor points are mostly about nuance in edge case handling and explicit parameter settings.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Please ensure the file is in the current directory."")
    exit()

# 2. Select the features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
targe...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
- **Correctness:** All steps (loading data, feature selection, NA handling, standardization, K-Means, PCA, and plotting) are implemented accurately.
- **NA Handling:** The `dropna(subset=features)` along with selecting `species` before dropping NAs correctly ensures that the `species_labels` align perfectly with the `pca_df` after NA removal, which is crucial for the second plot.
- **K-Means and PCA parameters:** Correctly uses K=3, `n_init='auto'`, and `random_state=42` for KMeans, and 2 components for PCA.
- **Plotting:** Both plots are generated as required, with correct coloring (K-Means labels and original species labels) and appropriate titles and labels.
- **Error Handling:** Includes `try-except` for `FileNotFoundError`.
- **Code Quality:** Imports are proper, and the code is well-structured with comments indicating each step.

**Areas for improvement (minor):**
- **Plot 1 Coloring:** While `plt.scatter` with `c=kmeans_labels` works, using `sns.scatterplot` as in the ground truth often provides better default aesthetics (like distinct legend entries) and is generally preferred for statistical plots due to its integration with pandas DataFrames and more control over categorical mapping. The generated code's current `colorbar` for K-Means labels is functional but less intuitive than a categorical legend.
- **Plot 2 Coloring (Original Species):** The manual loop for `plt.scatter` is functional, but `seaborn.scatterplot` can handle the categorical `hue` mapping automatically and often produces a more visually appealing and consistent result with less code, similar to how it would handle K-Means labels.
- **Consistency in Plotting Library:** The ground truth consistently uses `seaborn` for scatter plots which provides a more unified and aesthetically pleasing output. Mixing `matplotlib.pyplot.scatter` and `seaborn` calls is not strictly wrong but can lead to inconsistencies in plot styling if not managed carefully. The `sns.set_style(""whitegrid"")` in the ground truth is an example of setting a consistent aesthetic.

Overall, the generated code successfully achieves the task and is very robust. The suggestions above are mostly about leveraging more power from `seaborn` for cleaner plotting code and slightly better aesthetics.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Please ensure the file is in the current directory."")
    exit()

# 2) Impute missing values
# Drop rows ...",8,"The generated code correctly addresses most of the requirements.

**Strengths:**
*   **Correct Loading and Initial Imputation:** Successfully loads the dataset and correctly handles the initial `island` NA drop.
*   **Correct Imputation Logic:** Properly imputes numerical columns with means and 'sex' with mode.
*   **Correct Feature Engineering:** Correctly identifies numerical features and applies one-hot encoding to `island` and `sex`. Using `pandas.DataFrame` for one-hot encoded columns with `index=df.index` is a robust way to ensure alignment when concatenating.
*   **Correct Model Training:** Trains the `GradientBoostingClassifier` with the specified parameters.
*   **Correct Feature Importance Logic:** Extracts and prints feature importances correctly, associated with their names.
*   **Visualization Attempt:** Attempts to create the requested box plots.

**Areas for Improvement:**
*   **Box Plot Visualization:** The task specifically asked for ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column."" The ground truth code achieved this with a single `seaborn.boxplot` call using `hue='sex'`, which is more aligned with the ""grouped by"" phrasing and generally provides a more compact and comparable visualization across species than separate subplots for each species. While the generated code creates separate subplots per species, it then plots `flipper_length_mm` vs `sex` *within each species*, effectively showing the distribution of `flipper_length_mm` for each sex *for that specific species*. This is functionally correct, but the single plot with `hue` is a more direct interpretation of ""grouped by"" across all species for comparative purposes. The generated code's plotting approach makes it harder to directly compare `flipper_length_mm` distributions of the same sex across different species.

*   **Plot Titles:** The task required plots to have titles. The generated code uses `plt.title` for each subplot, which is good. It also adds a `plt.suptitle`, which is a nice touch.

Overall, the code is very good and largely fulfills the requirements. The minor deviation in the plotting method is the main reason for not giving a higher score. It's a valid way to visualize, but a single plot with `hue` would have been a more direct interpretation of ""grouped by"" across all species simultaneously as shown in the ground truth.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer

def clean_penguins_data(file_path='penguins.csv'):
    """"""
    Performs advanced data cleaning on the penguins dataset.

    Args:
        file_path (str): The path to the penguins CSV file.

    Returns:
        pandas.DataFrame: The cleaned DataFrame.
    """"""
    # 1. Load the dataset
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f""Error: The file '{file_path}' was not found. Please ...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs KNN imputation on the specified numerical columns, imputes the 'sex' column with its mode, and correctly prints the remaining NA values for verification.

**Strong points:**
*   **Encapsulation:** The code is encapsulated within a function `clean_penguins_data`, which is good practice for reusability.
*   **Clear Steps:** The code clearly delineates each step of the data cleaning process with comments (# 1. Load, # 2. Impute numerical, # 3. Impute categorical).
*   **Correct Imputation:** It correctly uses `KNNImputer` for numerical columns and `mode()` for the categorical column, as specified.
*   **Robustness:** Includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is excellent for handling potential issues.
*   **Verification:** Prints initial NA counts and final NA counts, fulfilling the verification requirement.
*   **Mode Handling:** The `if not df[categorical_col].isnull().all()` check for mode imputation is a good addition to prevent errors if the entire column were NA.

**Areas for minor improvement:**
*   The ground truth explicitly states to print the number of remaining NAs in *each* column **after imputation to verify**. The generated code does this but without further breaking down the print statements to confirm `0` NAs in the specific imputed columns, though `df.isna().sum()` is sufficient for verification.
*   The `print` statements in the generated code are within the function, which is generally acceptable for this kind of task but sometimes functions only return data and print outside. For this specific task, it aligns with requirement 4 ""Print the number of remaining NA values... to verify.""",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

def perform_penguin_ttest(file_path='penguins.csv', alpha=0.05):
    """"""
    Performs an independent two-sample t-test on culmen length between Adelie and Gentoo penguins.

    Args:
        file_path (str): The path to the penguins CSV dataset.
        alpha (float): The significance level for the hypothesis test.
    """"""
    try:
        # 1) Load the dataset
        df = pd.read_csv(file_path)

        # 2) Filter the dataset to include o...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It loads the data, filters species, handles NAs, performs the t-test, and interprets the results correctly. The code structure is good with clear comments.

**Strong points:**
*   Correctly loads data, filters, and handles NAs.
*   Performs the `ttest_ind` correctly.
*   Provides a clear interpretation of the p-value against the significance level.
*   Includes error handling for `FileNotFoundError`.
*   Includes print statements to show intermediate steps (e.g., number of samples, means), which is good for debugging and understanding.

**Areas for minor improvement:**
*   **Docstring/Comments for `ttest_ind`:** The generated code includes a comment about `equal_var=False` (Welch's t-test) and then states it's sticking to the default. While the default for `ttest_ind` is `equal_var=True` (Student's t-test), the ground truth explicitly sets `equal_var=True`. For clarity and to ensure the desired test is performed, explicitly setting `equal_var=True` (if that's the intention) in the `ttest_ind` call would be slightly more robust than relying on the default. The problem statement does not specify which t-test variety (Student's or Welch's) to use, so the default is acceptable, but being explicit can prevent ambiguity.
*   **Minor print statement difference:** The ground truth informs about how many rows were dropped *after* dropping them (e.g., `Dropped X rows... Remaining rows: Y`). The generated code tells you how many were dropped but doesn't explicitly state the remaining count (though `len(filtered_df)` could be added). This is a minor stylistic difference.

Overall, the generated code is excellent and fully meets the requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
  ...",9,"The generated code is highly correct, complete, and well-structured, achieving the vast majority of the task requirements.

**Strong Points:**
- **Correctness:** The code correctly covers all major requirements: data loading, feature definition, proper preprocessing (median imputation for numeric, mode imputation for 'sex', one-hot encoding for categorical features using `ColumnTransformer` and `Pipeline`), `LabelEncoder` for the target, stratified train-test split, RandomForestClassifier training, and evaluation with `accuracy_score` and `classification_report`.
- **Modularity and Robustness:** The use of `Pipeline` and `ColumnTransformer` for preprocessing is excellent practice, making the code clean, modular, and more robust. The `try-except` block for file loading is also a good addition.
- **Completeness:** All specified evaluation metrics (accuracy, classification report) are included. The `random_state` is correctly applied for reproducibility.
- **Readability:** Variable names are clear (`numerical_features`, `categorical_features`, `y_encoded`), and comments align with the task requirements.

**Areas for Minor Improvement (to reach a 10):**
- **Clarity in `categorical_transformer` for 'island' and 'sex'**: The comment `('imputer', SimpleImputer(strategy='most_frequent')), # Applies to all categorical, but only 'sex' might have NAs` is slightly confusing. While `SimpleImputer` will apply to both, it's true `island` doesn't have NAs in the typical dataset. However, explicitly stating that `island` doesn't strictly *need* imputation (or checking `df['island'].isnull().sum()` to confirm) could add more clarity, though the current setup is perfectly functional and safe. This is a very minor point.
- **Decoding `y_pred` and `y_test` outside the pipeline**: The code correctly decodes `y_pred_encoded` and `y_test_encoded` back to original species names for the `classification_report`, which is *exactly* what's needed for a human-readable report. The ground truth, by contrast, passes the `target_names` directly to `classification_report`, which is also correct and arguably slightly more streamlined by avoiding an extra `inverse_transform` step on `y_test` if `y_test_encoded` is acceptable for `accuracy_score` (though `accuracy_score` works fine with encoded labels too). However, the generated code's approach of decoding both makes `y_test` and `y_pred` consistent with the ""original"" names for direct comparison if needed elsewhere. This is more of a stylistic difference than a functional flaw.

The code is very close to perfect and demonstrates a strong understanding of the task and best practices in scikit-learn.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd
import numpy as np # Importing numpy is not strictly necessary for pandas corr, but good practice for numerical ops

# Define the data for the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 1...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the Pearson correlation coefficients as requested, accurately prints them, and correctly determines and prints which feature has a stronger absolute correlation with ""PerformanceScore"". The variable names are clear, and the use of f-strings for formatted output is good practice. The comments correctly note that `numpy` is not strictly necessary but good for general numerical operations, which aligns with common data science practices. The code fully addresses all requirements of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import proportional_hazards_test
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, train_test_sp...",7,"The generated code attempts to address all requirements of the prompt, showing a good understanding of the advanced statistical modeling techniques.

**Strengths:**
*   **Comprehensive Imports:** All necessary libraries are imported.
*   **Kaplan-Meier & CoxPH Implementation:** Correctly implements K-M curves and CoxPH models using `lifelines`. It also correctly handles the ""time-to-event"" aspect by setting `duration=1` (as the prompt implies a single event point).
*   **Propensity Score Matching:** Correctly sets up PSM for Pclass 1 vs. Pclass 3 and implements a basic nearest-neighbor matching algorithm.
*   **Bootstrap Resampling:** Demonstrates general bootstrap resampling for Cox model coefficients.
*   **Bayesian Analysis:** Implements a Bayesian logistic regression for survival prediction using PyMC3, with appropriate priors and sampling.
*   **Model Validation:** Includes a train-test split for Cox model evaluation (C-index) which is a crucial aspect of model validation.
*   **Data Preprocessing:** Handles missing values and performs feature transformations (scaling, one-hot encoding) appropriately.

**Areas for Improvement to achieve a higher score:**

1.  **""Voyage as Time-To-Event"" Interpretation:** The prompt states ""voyage as time-to-event data (with survival as event indicator)"". Both the ground truth and generated solution interpret this as a fixed time `T=1` and `E=death`. While this is a plausible interpretation given the dataset's limitations, it effectively reduces the Cox model to a logistic regression. The prompt could have been clearer, but if the intention was for the time to vary, then a more complex approach (e.g., creating synthetic time data, or explaining why this interpretation is used) would be needed. The current approach is *technically* correct given the data, but it limits the ""survival analysis"" aspect to binary classification. A key difference from the ground truth is that the ground truth explicitly states and justifies this interpretation: ""Since no specific time of death is given, we assume a fixed time T=1 for all, and the event E is death (Survived=0). This effectively treats the Cox model as a logistic regression for binary outcomes."" The generated code doesn't explicitly justify this design choice, which can lead to misinterpretation of what the Cox model is doing.

2.  **Advanced Statistical Tests (Multivariate, Interaction Effects, Model Diagnostics):**
    *   **Multivariate Analysis:** Partially covered by fitting the CoxPH model with multiple covariates.
    *   **Interaction Effects:** The generated code *attempts* an interaction effect (`Sex_Age_Interaction`) but then passes `numerical_features_int` to the `StandardScaler` which means `Sex_Age_Interaction` gets scaled *after creation*, which is correct. However, it then re-runs `ColumnTransformer` *including* `Sex_Age_Interaction` in `numerical_features_int` which might be problematic if Sex or Age are also in there. This is slightly less clean than the ground truth's approach of creating the interaction *after* initial preprocessing on transformed features or before general transformations. More concerning is that `preprocessing.get_feature_names_out()` might give ambiguous names when dealing with interaction terms if not handled carefully, and the code would need to specifically interpret the `coef` of `num__Sex_Age_Interaction`.
    *   **Model Diagnostics:** The code correctly calls `proportional_hazards_test`, but the warning about its interpretation with constant duration should be included, as it was in the ground truth.

3.  **Propensity Score Matching Refinement:**
    *   The matching algorithm is custom-implemented with a `for` loop and a caliper, which is commendable. However, dedicated libraries (`PsmPy`, `Matching`, `DoWhy` for causal inference) or `NearestNeighbors` from `sklearn.neighbors` (as used in the ground truth) would be more robust and idiomatic for large datasets.
    *   The ground truth also includes a crucial ""balance check"" after PSM (e.g., SMD), which is essential to confirm the matching was successful and produced balanced groups. This is missing in the generated code.

4.  **Bayesian Analysis Integration & Interpretation:**
    *   The PyMC3 model is generally correct for a basic logistic regression.
    *   However, compared to the ground truth, the generated code *hides* the standard output and plots of the PyMC3 model. While the prompt asks for *only* Python code, an expert code evaluator would expect these lines to be present but commented out, demonstrating awareness of the necessary diagnostic steps for Bayesian models. The ground truth provides detailed `az.summary`, `az.plot_trace`, and `az.plot_posterior` calls.

5.  **Sensitivity Analysis:** The generated code provides a *conceptual* outline for sensitivity analysis, similar to the ground truth, but doesn't implement a concrete example of changing imputation strategy for age and comparing results, as done in the ground truth. The prompt requested implementation, not just conceptual discussion.

6.  **Code Comments and Explanations:** The request asks for ""only the Python code without explanations,"" but the generated code removed even matplotlib `plt.show()` and `print()` statements for various sections, which makes it harder to assess execution and results without modifying it. The ground truth includes these outputs which is helpful for evaluation.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# 1. Create the pandas DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
        'Final_Sc...",9,"The generated code is largely correct and addresses almost all requirements of the task.

**Strong Points:**
- **DataFrame Creation:** Correctly creates the DataFrame as specified.
- **Correlation Calculation:** Correctly calculates and prints the Pearson correlation coefficient. It offers two valid methods (`.corr()` and `pearsonr`), which is good.
- **Scatter Plot:**
    - Correctly uses `seaborn.scatterplot` for coloring by 'Subject'.
    - Includes correct title and axis labels.
    - Correctly places the legend outside the plot using `bbox_to_anchor`.
    - `tight_layout()` is used for better plot presentation.
- **Grade Assignment:** The initial `pd.cut` attempt was a bit convoluted and had a slight logical error for the boundaries as stated in the comments (`bins=[df['Final_Score'].min() - 1, 69.99, 79.99, 89.99, df['Final_Score'].max() + 1]`). However, the subsequent use of an `apply` function with a custom `assign_grade` function is robust and perfectly implements the specified grading logic. This is a very good and clear solution to the grading requirement.
- **Grade Distribution:** Correctly calculates and prints the count of students in each grade category. `sort_index(ascending=False)` is a nice touch for presenting grades A, B, C, D in order.

**Areas for Minor Improvement (for a perfect 10):**
- **Unused Import:** `numpy` is imported in the ground truth for `np.select` (an efficient way to assign grades), but it's not used in the generated code's final grade assignment method (`apply`). While not a critical error, removing unused imports is good practice.
- **Redundant Code Block:** The commented-out `pd.cut` section for grade assignment, while showing thought process, could be removed in a final production-ready script for conciseness unless it's explicitly for exploration.
- **Output Formatting:** While the ground truth code used more explicit ""--- Section Name ---"" and newlines for clarity between different output sections, the generated code uses `""\n"" + ""=""*50 + ""\n""`. This is a minor stylistic difference and doesn't affect correctness but readability for a benchmark might slightly prefer the ground truth's approach.
- **Lack of Print Statements for Intermediate Steps:** The ground truth added print statements like ""DataFrame created successfully:"" and ""Scatter plot displayed successfully."" which makes the output flow clearer for a user. The generated code mostly only prints the final results.

Overall, the code is highly functional and meets the core requirements effectively. The minor points are mostly about presentation and code tidiness rather than functional correctness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Create the pandas DataFrame for 'TECHCO' stock prices
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', 
                            '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
    'Ticker': ['TECHCO']*10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 15...",10,"The generated code is excellent. It correctly implements all the requested functionalities: DataFrame creation with 'Date' as index, calculation of daily percentage return, 3-day SMA, and 5-day SMA. The plotting part is well-done with proper labels, title, and legend, and the plot is displayed. Crucially, the identification of bullish crossover dates is accurate and uses the correct logic (`shift(1)`) to detect the crossing point rather than just a condition. The output for crossovers is also handled correctly, distinguishing between cases where crossovers are found or not. The code quality is high with logical structure and necessary imports.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Create a pandas DataFrame with the provided data
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['...",10,"The generated code is excellent and correctly addresses all aspects of the task.

**Strong Points:**
- **Correctness:** All calculations (revenue per product, average order value, identifying repeat customers) are accurate. The DataFrame creation and manipulation are perfectly handled.
- **Completeness:** Every requirement outlined in the prompt (DataFrame creation, product revenue calculation, horizontal bar chart, average order value, repeat customer identification) is fully met.
- **Code Quality:**
    - **Imports:** Necessary imports (`pandas`, `matplotlib.pyplot`) are included at the beginning.
    - **Structure:** The code is well-structured with clear comments indicating each task section, making it easy to follow.
    - **Readability:** Variable names are descriptive.
    - **Plotting:** The horizontal bar chart is correctly generated with appropriate title, labels, and grid, and `plt.tight_layout()` is used for good presentation. The use of `figsize` is also good practice.
    - **Output Formatting:** The print statements for average order value and repeat customers are clear and nicely formatted, making the output comprehensible.
    - **Edge Case Handling (Implicit):** While not explicitly `if num_unique_orders > 0:`, the task data guarantees `num_unique_orders` will be > 0, so the average order value calculation won't cause a zero division error here. The handling of no multi-purchase customers is also good.

**Minor Observations (Not impacting score, just stylistic choices):**
- The `product_revenue` sort order is `ascending=True` in the generated code, while the ground truth used `ascending=False`. Both are valid ways to sort and display the data; the prompt didn't specify the sort order for the plot explicitly, so either is acceptable. For revenue, descending usually makes more sense to see top performers easily, but ascending isn't incorrect.
- The `print` statements include separators `(""=""*50 + ""\n"")` which enhance readability in the console output.

Overall, the solution is robust, accurate, and well-presented.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

# Define the initial DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Create a new numerical feature ""DeviceType_Numeric""
# Map ""Desktop"" to 0...",9,"The generated code is very good and completes almost all aspects of the task correctly.

Strong points:
- The initial DataFrame is correctly defined.
- Creation of `DeviceType_Numeric` is accurate and uses `map` efficiently.
- Calculation of `PagesPerMinute` is correct.
- The handling of division by zero (NaNs and Infs) is robust, although the ground truth provides a slightly more concise way of achieving the same for `np.inf` values. The key is that both NaNs and Infs are converted to 0, which is the desired outcome.
- The selection of the final feature set `X` is correct.
- The head of the feature DataFrame is printed as requested.
- Imports are correct and the code structure is clear and readable.

Areas for improvement (minor):
- In the handling of `PagesPerMinute`, the generated code uses two separate lines for `replace([np.inf, -np.inf], 0)` and `fillna(0)`. While functionally correct, the ground truth's approach of `replace([np.inf, -np.inf], np.nan, inplace=True)` followed by `fillna(0, inplace=True)` is slightly more conventional and potentially cleaner for handling NaNs *and* Infs by first converting Infs to NaNs, then filling all NaNs. However, the generated code's logic is sound and achieves the same result for this specific problem (replace all problem values with 0).
- The print statement for the head of the DataFrame is slightly different but conveys the same information.

Overall, it's a highly competent solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the mean and standard deviation of ""TemperatureCelsius"". It then precisely identifies outliers using the 1.5 standard deviation rule, considering both upper and lower bounds. Finally, it iterates through the identified outliers and prints the specified details (SensorID, Timestamp, and TemperatureCelsius). The code is well-structured, easy to read, and includes informative print statements for intermediate steps, which aids in understanding the process. The use of `outliers_mask` and then filtering the DataFrame is a clean and efficient way to handle the outlier identification. The conditional printing for no outliers found is also a good touch for completeness. It fully meets all the requirements of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [1...",10,"The generated code is excellent and perfectly solves the task. It correctly defines the DataFrame, calculates the `EngagementScore` as specified, and accurately identifies the `PostType` with the highest average `EngagementScore` for each `Platform`. The method used for identifying the best `PostType` per platform (`groupby(level='Platform').idxmax()`) is a very idiomatic and efficient pandas approach. The printing format is clear and well-organized, neatly presenting all the required information.

Strong points:
- **Correctness:** All parts of the task are addressed correctly.
- **Code Quality:** The code is well-structured, uses appropriate pandas functions (`groupby`, `mean`, `idxmax`, `loc`), and variable names are descriptive. Comments are added where necessary to explain the logic.
- **Completeness:** Every requirement of the prompt is met.
- **Pandas Idiomatic Usage:** The `.idxmax()` strategy on a grouped series to find the index of the maximum value within each group is a very clean and efficient way to achieve the desired result, showcasing good understanding of pandas functionalities.
- **Clear Output:** The final print statement is well-formatted and easy to read.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# 1. Define the pandas DataFrame representing patient admissions
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagn...",10,"The generated code is highly correct, complete, and demonstrates good code quality.

**Correctness:**
- The DataFrame is correctly defined with all specified columns and data types.
- The ""LengthOfStay"" column is accurately calculated by taking the difference between ""DischargeDate"" and ""AdmissionDate"" and extracting days.
- The grouping by ""Diagnosis"" and calculation of average ""LengthOfStay"" and total ""Patient Count"" (using `size`) is correct.
- The identification of the diagnosis with the longest average ""LengthOfStay"" is also correct using `idxmax()`.

**Code Quality:**
- Imports are handled correctly (`import pandas as pd`).
- The code is well-structured, following the logical steps outlined in the task.
- Variable names are clear and descriptive (`diagnosis_summary`, `longest_stay_diagnosis`).
- Print statements are descriptive and format the output reasonably well for readability.
- Comments are helpful, clearly indicating which task requirement each section addresses.

**Completeness:**
- All four requirements of the task have been met:
    1.  Calculation of ""LengthOfStay"".
    2.  Calculation of average ""LengthOfStay"" and total ""Patient Count"" per unique ""Diagnosis"".
    3.  Printing this information.
    4.  Identifying and printing the ""Diagnosis"" with the longest average ""LengthOfStay"".

The generated code successfully fulfills all aspects of the prompt without any significant errors or omissions. While the ground truth code includes a function wrapper and more granular print statements demonstrating steps, the generated code directly addresses the core computational and output requirements effectively and concisely, which is often desirable in a direct code generation scenario. The use of `size` for `Patient_Count` is perfectly valid and idiomatic Pandas for counting rows in each group.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8]...",10,"The generated code is excellent.
1.  **Correctness**: It correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified using the formula (`(""Rating"" * ""Votes"") / (sum of all ""Votes"")`), adds it as a new column, and correctly identifies and prints the title and weighted rating of the movie with the highest ""WeightedRating"". The formatting of the weighted rating for printing is also appropriate.
2.  **Code Quality**: The code is well-structured, uses clear variable names, and is easy to read and understand. It correctly imports pandas.
3.  **Completeness**: All requirements of the task are fully met. The code directly addresses all three parts of the post-DataFrame creation instructions.

The generated code is concise, efficient, and fully functional for the given task. It mirrors the ground truth's logic very closely in terms of correctness and even includes similar formatting for the output. The only minor difference is the lack of explicit handling for `total_votes == 0`, which is a good practice but not strictly necessary for this specific dataset where `total_votes` will never be zero. However, given the context of the problem and the provided data, this is not a practical flaw.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000,...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, precisely calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"" using `groupby().agg()`, and then presents this information in a new DataFrame. The use of named aggregations (`AveragePriceUSD` and `AverageMileage`) makes the output columns clear and readable, and the inclusion of `.reset_index()` ensures that 'Brand' remains a column in the final output, which is a common and often desired format. The code is well-structured and adheres to all requirements of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, ...",10,"The generated code is correct, complete, and follows good coding practices. It accurately defines the DataFrame, adds the ""TotalStats"" column, identifies the Pokemon with the highest ""TotalStats"", and calculates the average ""Speed"" for each ""Type1"". The code is clear and directly addresses all parts of the prompt. While the ground truth added print statements for clarity, the generated code was only asked to *identify* and *print* certain values, which it did correctly. The commented-out print statement for the full DataFrame is a good practice for debugging.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_boul...",7,"The generated code provides a solid attempt at addressing the comprehensive task, demonstrating an understanding of many requirements. However, it falls short of the ""expert"" level required and has several areas for improvement or missing elements compared to the ground truth.

**Strengths:**
*   **Good Feature Engineering:** Includes many advanced features as requested, such as `Family_Size`, `Is_Alone`, `Has_Cabin`, `Cabin_Deck`, `Title`, `Name_Length`, `Ticket_Group_Size`, `Ticket_Is_Group`, `Fare_Per_Person`. The `Family_ID` is an interesting addition.
*   **Multiple Clustering Algorithms:** Implements K-Means, Hierarchical, DBSCAN, and GMM.
*   **Multiple Validation Metrics:** Uses Silhouette, Calinski-Harabasz, Davies-Bouldin, and BIC/AIC for GMM, along with the Elbow method.
*   **Dimensionality Reduction:** Incorporates PCA, t-SNE, and UMAP for visualization.
*   **Comprehensive Visualizations:** Provides count plots for cluster distribution, scatter plots for reduced dimensions, bar plots for survival rates, and plots for features per cluster.
*   **External Validation:** Performs a chi-squared test for survival outcomes per cluster, which is a good addition.
*   **Code Structure:** Generally well-organized with clear sections.
*   **Error Handling (Implicit):** Manages `np.inf` and `np.nan` for `Fare_Per_Person` calculation.

**Areas for Improvement / Missing Features (Compared to Ground Truth):**

1.  **Feature Engineering Completeness:**
    *   **Ticket Prefixing:** The ground truth extracts meaningful prefixes from ticket numbers (e.g., 'PC', 'A/5', 'STON/O') which can be strong categorical features. The generated code only uses `Ticket_Group_Size` and `Ticket_Is_Group`.
    *   **Interaction Features:** The ground truth includes `Age_Class`, `Fare_Class`, `FarePerPerson_Class` which are good interaction features. The generated code does not explicitly create these.
    *   **Social Network Analysis (Advanced):** While `Family_ID` is present, the ground truth's `TicketGroupSize` combined with `FarePerPerson` more directly addresses sharing tickets and can indicate social groups beyond just family.

2.  **Cluster Stability Analysis:**
    *   The generated code's stability analysis for K-Means is simplistic (only checks Silhouette score variation which is not robust for stability) and conceptually lacks the ""consensus clustering"" aspect mentioned in the prompt. The ground truth uses Adjusted Rand Index (ARI) comparisons across multiple runs with different seeds, which is a much more appropriate and robust method for stability analysis. The prompt specifically asked for ""cluster stability analysis and consensus clustering"".

3.  **DBSCAN Parameter Tuning:**
    *   The generated code uses arbitrary `eps=2` and `min_samples=5`. The ground truth code correctly identifies that DBSCAN ""requires careful parameter tuning"" and suggests how to estimate `eps`. Without proper tuning (e.g., using a k-distance plot), DBSCAN results are often suboptimal or yield too few/too many clusters.
    *   The generated code's DBSCAN analysis also doesn't handle noise points (-1) in the plotting or metrics as cleanly as the ground truth.

4.  **Optimal Cluster Selection Logic:**
    *   While it calculates metrics, the selection of ""optimal K"" is based solely on max Silhouette for K-Means and Hierarchical, and min BIC for GMM. The ground truth visually presents all metrics and allows for manual interpretation (or implies a more nuanced selection) rather than a single hard-coded metric. The ground truth's approach to optimal k selection is slightly more nuanced, aligning with the subjective nature of cluster validation.

5.  **Code Output/Clarity:**
    *   The generated code prints BIC/AIC scores but doesn't explicitly visualize the other score types (Silhouette, CH, DB) for all algorithms as the ground truth does. This makes it harder to compare algorithms and choose an optimal k comprehensively.
    *   The final summaries of clusters (e.g., ""Distinctive Numerical Features (avg vs overall avg)"") are excellent additions for interpretation, but the plotting of categorical features (stacked bar) might be less intuitive for direct comparison than separate bars or violin plots for some features.

6.  **Minor Code Quality/Robustness:**
    *   The explicit `umap.umap_ as umap` import is a good detail present in the ground truth for older `umap-learn` versions. The generated code still uses `import umap.umap_ as umap` which is good form.
    *   The ground truth has robust handling for missing `titanic.csv` with a `try-except` block.

Overall, the generated code performs well for a general clustering task but misses some of the nuances and ""expert"" level details for comprehensive analysis and robust validation compared to the ground truth, particularly in advanced feature engineering and the rigor of cluster stability analysis.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dyst...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the book count and average price per author, and identifies the author with the highest total stock. The code is well-structured, uses appropriate pandas functions (`groupby`, `agg`, `idxmax`, `max`), and presents the output clearly. The use of `to_string(index=False)` for printing the `author_stats` DataFrame is a neat and effective way to format the output. All requirements of the task are fully met.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import warnings
from scipy import stats
from sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve, validation_curve, train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import IterativeImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.ensemble import RandomForestClassifier, Gra...",8,"The generated code demonstrates a strong understanding of the requirements, covering almost all advanced aspects of the task.

**Strong Points:**

*   **Comprehensive Preprocessing:** Correctly implements `IterativeImputer` for multiple imputation, a custom `OutlierCapper` for outlier treatment (IQR-based, which is a good choice for this dataset), `StandardScaler`, and `PolynomialFeatures`. The order of operations in the `CustomPreprocessingTransformer` is logical.
*   **Modular Design:** The code is well-structured with clear functions for base model optimization, ensemble building, evaluation, plotting, and statistical testing. This enhances readability and maintainability.
*   **Multiple Imputation:** Uses `IterativeImputer` as requested.
*   **Outlier Detection and Treatment:** Implements capping based on IQR, which is a valid strategy.
*   **Advanced Feature Engineering:** Includes `PolynomialFeatures` for both polynomial and interaction terms.
*   **Multiple Base Models and Optimization:** Covers Random Forest, Gradient Boosting, XGBoost, Neural Network, and SVM, all optimized with `GridSearchCV` and appropriate parameter grids.
*   **Ensemble Methods:** Implements Voting, Stacking, and a custom Blending approach. The blending implementation, especially how it handles base model training and meta-feature generation, is well-thought-out.
*   **Advanced Evaluation:**
    *   **Stratified K-Fold:** Used consistently for `GridSearchCV`, learning curves, validation curves, and the final evaluation loop.
    *   **Learning Curves:** Correctly implemented and plotted.
    *   **Validation Curves:** Correctly implemented and plotted for an example hyperparameter.
    *   **Bootstrap Sampling:** Implements bootstrap sampling for confidence intervals on the test set, including a good helper function (`get_predictions`) to handle different model types.
*   **Model Interpretability (SHAP):** Uses SHAP and correctly attempts to distinguish between `TreeExplainer` and `KernelExplainer` based on model type. Provides both summary plots.
*   **Statistical Significance Testing:** Implements paired t-tests on the cross-validation fold scores, which is crucial for comparing model performance robustly.
*   **Model Calibration:** Correctly uses `CalibratedClassifierCV` with `isotonic` method and plots calibration curves, including Brier score calculation.
*   **Comprehensive Dashboard:** Presents performance metrics in a table and heatmap, and attempts to integrate SHAP and confidence intervals into the dashboard concept.
*   **Proper Imports and Reproducibility:** All necessary libraries are imported, and `RANDOM_STATE` is used consistently.
*   **Error Handling (File Not Found):** Includes a try-except block for loading the CSV.

**Areas for Improvement (Why not a 9 or 10):**

1.  **Outlier Capper Implementation (Slight Deviation from Ground Truth):** The ground truth provides a `TransformerMixin`-based `OutlierCapper` that fits lower/upper bounds during `fit` and applies them during `transform`. The generated code's `cap_outliers_iqr` function is a standalone function that gets called *within* the `CustomPreprocessingTransformer`'s `fit` and `transform` methods directly. While functional, this means the IQR bounds are re-calculated on the *transformed* data (after imputation) or are *re-calculated* on the test set during `transform` (which is data leakage). A proper `TransformerMixin` for outlier capping would calculate the bounds only on training data during `fit` and then apply those *fixed* bounds to both training and test data during `transform`. The current implementation of `cap_outliers_iqr` applies the capping based on the *current* DataFrame it receives, which means it will recalculate the IQR for the test set, violating data leakage principles in some scenarios.
2.  **SHAP for KernelExplainer:** The `KernelExplainer` requires a `predict_proba` function as its first argument, and `X_data` (or a background dataset) as its second. The generated code attempts to use `model.predict_proba` but then passes `X_data` directly for the `shap_values` calculation (`explainer.shap_values(X_data)`). While `KernelExplainer` expects `X` for prediction, it's typically a single instance or small few instances for **explanation**, not the entire `X_data` again. The `shap.summary_plot` also expects a background dataset for `KernelExplainer` when plotting. The current approach with `KernelExplainer` might be computationally very expensive for large `X_data` or might not produce the most representative SHAP values if run on the entire `X_data`. A common practice is to sample from `X_data` for `KernelExplainer`'s background dataset and then explain a subset of `X_data`.
3.  **Bootstrap for Blending Model:** The `bootstrap_confidence_intervals` function handles the blending model correctly by reconstructing the prediction logic. However, it assumes the base models are directly available (e.g., `model_obj['base_models']`). For best practice, during bootstrap, the base models should also be *re-trained* on the bootstrapped training data if you want truly robust confidence intervals reflecting the full model building process. The current implementation trains base models for blending *once* on `X_train_processed` and then uses those fitted models for test set bootstrapping. This is acceptable for many cases, but conceptually, if you sample the test set, ideally the *entire* modeling pipeline should be applied to fully reflect variance. However, given the complexity, this might be an acceptable trade-off.
4.  **Ensemble Model Training for Evaluation:** In `run_stratified_kfold_evaluation_with_raw`, the base models are correctly cloned and retrained for each fold. For ensemble models (Voting, Stacking), they are also cloned and refitted. However, for Blending, the `meta_learner` is used directly from `ensemble_fitted_models`, while its base models are re-trained inside the fold loop. This is correct for the `meta_learner` that takes base model predictions as input, but it's part of the subtle complexities of ensemble evaluation.
5.  **Output Clarity:** The `print` statements are helpful, but the final text output for the dashboard could be more structured (e.g., separating the confidence intervals more clearly from the main metric table).

Despite these minor points, the generated code is highly functional and meets almost all requirements impressively. The attention to detail in areas like data leakage prevention (training/test split before preprocessing), stratified CV, and handling different model types for `predict_proba` is commendable.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.multivariate.manova import MANOVA
from sklearn.experimental import enable_iterative_imputer # Required for IterativeImputer
from sklearn.impute import KNNImputer, IterativeImputer
from sklearn.linear_model import LogisticRegression, HuberRegressor
from sklearn.model_selection import train_test_split, GridSearc...",9,"The generated code is highly comprehensive and addresses almost all aspects of the complex task. It demonstrates a strong understanding of advanced statistical modeling, anomaly detection, and data analysis best practices.

**Strong Points:**
*   **Comprehensive Imputation:** Successfully implements and compares KNN and Iterative Imputer (MICE-like), including a basic comparison of descriptive statistics.
*   **Advanced Models:** Implements Logistic Regression with L1/L2 regularization (with GridSearchCV for hyperparameter tuning, which is a good addition), PyGAM, and Bayesian Logistic Regression using PyMC3. The PyMC3 implementation is well-structured.
*   **Advanced Hypothesis Testing:** Correctly implements MANOVA, Permutation Tests, and Bootstrap Confidence Intervals with clear explanations and visualizations.
*   **Anomaly Detection:** Implements Isolation Forest, LOF, and One-Class SVM, and provides a visualization of their detected outliers using PCA.
*   **Dimensionality Reduction:** Performs both PCA and Factor Analysis, including explained variance visualization for PCA.
*   **Robust Methods:** Discusses and demonstrates `HuberRegressor` for robust regression and highlights Spearman correlation as another robust method.
*   **Model Diagnostics:** Conducts detailed diagnostics for Logistic Regression using `statsmodels.api.Logit`, including residual analysis (deviance residuals), influence measures (Cook's distance, Leverage), and goodness-of-fit (pseudo R-squared). It also visualizes these.
*   **Advanced Visualizations:** Creates Q-Q plots and a comprehensive ""Diagnostic Dashboard"" with multiple relevant plots for EDA, model performance, and data structure.
*   **Robustness:** Includes `try-except` blocks for file loading and checks for PyGAM/PyMC3 availability, which is good practice. Uses `warnings.filterwarnings` effectively.
*   **Code Quality:** Imports are well-organized, variable names are clear, and comments explain sections and key steps.

**Areas for Minor Improvement/Consideration (to reach a perfect 10):**
*   **MICE Implementation:** While Iterative Imputer is MICE-like, a direct implementation of MICE (e.g., using `fancyimpute.MICE` or a custom loop of regression models) would have been slightly more explicit in comparing ""MICE"" as stated in the prompt, or at least a clearer note on *why* `IterativeImputer` is considered MICE-like. The ground truth explicitly states `enable_iterative_imputer` for `MICE-like` and then proceeds.
*   **GAM Partial Dependence Plot:** The GAM partial dependence plotting is attempted if `pygam` is available, but the plotting code in the dashboard is complex for a multi-subplot setting. The ground truth has a more straightforward, dedicated plot for GAM partial dependencies for *all* features, which might be more illustrative of GAM's strength. The current dashboard plot only shows one if GAM is available. A single dedicated plot for all feature partial dependencies would be helpful.
*   **MANOVA Output Clarity:** The `manova_results['x0']['stat']` output from `statsmodels` MANOVA can be cryptic. While the ground truth also just prints `mc.mv_test()`, the generated code provides a general error message which is less helpful than the ground truth’s specific advice about singular matrices. A more user-friendly parsing or interpretation of `manova_results` would be beneficial, even if it's just printing the relevant p-values directly.
*   **Influence Plot:** `statsmodels` has a dedicated `influence_plot` function (as used in the ground truth) that provides a standard visualization of Cook's distance vs. Leverage, often more intuitive than separate stem plots. This could have been utilized.
*   **Robust Statistical Methods (Cont.):** While HuberRegressor and Spearman are good examples, the prompt mentioned handling ""non-normal distributions"" and ""outliers"" more broadly. While this is covered somewhat, a more explicit link between the *demonstration* and the *application* to the `water_potability` dataset (e.g., demonstrating how HuberRegressor *could* be used if 'Potability' was a continuous variable, or explicitly using robust scaling like `RobustScaler` before some models) would enhance this section. However, the current demonstration is adequate.
*   **Clarity on `pymc3` dependencies:** While it checks `pymc3` availability, `arviz` is also a critical dependency for plotting PyMC3 results. It's good that it checks for both.

Overall, this is an excellent submission, demonstrating a very strong grasp of the requirements and delivering high-quality, comprehensive code. The minor points are mostly about enhancing clarity or adding slightly more idiomatic statistical visualizations, rather than critical errors or missing functionalities.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA, FactorAnalysis
from sklearn.manifold import TSNE
from sklearn.cluster import (KMeans, AgglomerativeClustering, DBSCAN, 
                             SpectralClustering)
from sklearn.mixture import GaussianMixture
from sklearn.metrics import (silhouette_s...",8,"The generated code provides a very comprehensive solution to the complex task, covering almost all specified requirements with good structure and modularity.

**Strong Points:**
- **Comprehensive Feature Engineering:** The code goes beyond basic ratios and includes sophisticated features like Ideal Water Quality Index (WQI) and detailed Contamination Risk Scores, defined with domain-specific thought (e.g., pH deviation from ideal, specific thresholds for contaminants). This is a significant strength and directly addresses a key requirement.
- **Multiple Clustering Algorithms:** Implements K-Means, Hierarchical (Agglomerative), DBSCAN, GMM, and Spectral Clustering, as requested.
- **Robust Optimal Cluster Determination:** Calculates multiple metrics (Elbow, Silhouette, Davies-Bouldin, Calinski-Harabasz, AIC/BIC for GMM) and provides visualizations for selecting the optimal 'k'. This is crucial for proper clustering analysis.
- **Multiple Dimensionality Reduction Techniques:** Applies PCA, t-SNE, UMAP, and Factor Analysis for both 2D and 3D visualization purposes.
- **Cluster Stability Analysis:** Implements a simplified but effective bootstrap resampling approach with Adjusted Rand Index calculation, along with plotting the distribution of scores, which is a good effort towards this advanced requirement.
- **Detailed Cluster Profiling:** Clearly analyzes cluster profiles based on mean values of both original and engineered features, including potability rates, offering valuable insights.
- **Comprehensive Visualization Dashboard:** Utilizes Plotly for interactive 2D/3D scatter plots of DR results and radar charts for cluster profiles, and bar charts for potability rates. This fulfills a major visualization requirement.
- **Domain-Specific Validation:** Includes a dedicated section for external validation and contextualizing results with WHO/EPA water quality standards, demonstrating a strong understanding of the problem domain.
- **Cluster-Based Anomaly Detection:** Implements anomaly detection based on distance from cluster centroid for K-Means and directly points out DBSCAN's noise points as anomalies, addressing this advanced requirement.
- **Code Quality and Structure:** The code is well-organized into logical functions, which improves readability, maintainability, and reusability. Proper imports are in place. Use of `tqdm` for progress bars is a nice touch.

**Areas for Improvement (to reach a 9 or 10):**
- **DBSCAN eps Automation:** While the code attempts to estimate `eps` for DBSCAN, it still relies on a somewhat arbitrary percentile (`np.percentile(distances, 70)`). A more robust or visually guided approach (e.g., analyzing the K-distance graph's ""elbow"" programmatically, or suggesting manual inspection with clear plot display) would make it more automated and less prone to suboptimal parameter choice. The ground truth explicitly plots the K-distance graph and suggests visual inspection.
- **Bootstrap Resampling for Stability:** The current implementation of bootstrap stability is a bit simplified. While it calculates ARI between the bootstrap-trained model's predictions on the original data and the original clustering, a more rigorous ""consensus clustering"" involves building a co-occurrence matrix from multiple runs or re-sampling and then clustering that matrix. The ground truth provides a good example of this more rigorous consensus clustering by building a co-occurrence matrix and then performing hierarchical clustering on it.
- **Factor Analysis Error Handling:** The current code has a `try-except` block for Factor Analysis but doesn't handle the case where `n_components` might be greater than `n_features` if the task involved an arbitrary `n_components` input, leading to a potential ValueError. The condition `if n_components <= df_scaled.shape[1]:` addresses this, but more explicit error messages for larger-scale usage could be useful.
- **`n_init` for K-Means:** The ground truth consistently uses `n_init=10` or higher, which is good practice for K-Means to mitigate the impact of random initialization. The generated code also sets `n_init=10` for K-Means and GMM models, which is good.
- **Missing `warnings.filterwarnings('ignore')` at the top:** While it appears later, having it at the very top is standard practice to suppress any warnings right from the start.

Overall, this is an excellent solution that demonstrates a deep understanding of data science principles and the specific requirements of the prompt. The few areas for improvement are minor enhancements for robustness and best practices rather than critical flaws.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Make sure the CSV file is in the current directory."")
    exit()

# 1. Determine which features have the strongest positive and negative correlations with 'quality'
# Calculate correlations with 'quality'
quality_correlations = df.corr...",9,"The generated code is very good and accurately addresses almost all aspects of the task.

**Strengths:**
- **Correctness:** All calculations (correlations, top positive/negative) are correct. Heatmap and scatter plots are generated properly.
- **Completeness:** All four main requirements are met:
    1.  Determine strongest positive/negative correlations.
    2.  List top 3 positive/negative.
    3.  Create correlation heatmap.
    4.  Create scatter plots with regression lines for most positive/negative features.
- **Code Quality:**
    -   Proper use of pandas for data loading and correlation.
    -   Effective use of `seaborn` and `matplotlib` for plotting.
    -   Correct application of `scipy.stats.linregress` for linear regression.
    -   Handles `FileNotFoundError` gracefully.
    -   Clear and well-formatted output for top correlations.
    -   Uses subplots for scatter plots, which is a good presentation choice.

**Areas for Minor Improvement (to reach a perfect 10):**
-   **R-squared vs. R:** In the scatter plot legends, the generated code uses `R={r_value_pos:.2f}`. Conventionally, `R^2` (coefficient of determination) is a more intuitive measure for explaining variance in a regression line. `r_value_pos**2` would be more appropriate for `R^2`. The ground truth code correctly uses `R²={r_value_pos**2:.2f}`.
-   **X-axis for regression line:** When plotting the regression line, the generated code uses `df[most_pos_feat]` directly as the x-values for `plt.plot`. While this often works visually because the scatter plot also uses these values, it's more robust and mathematically correct to plot over a range of x-values (e.g., `np.linspace(df[most_pos_feat].min(), df[most_pos_feat].max(), 100)`) to ensure the line spans the entire data range smoothly, or at least use the min/max values to create a line segment as done in the ground truth (`x_pos = np.array([df[most_positive_feature].min(), df[most_positive_feature].max()])`). This is a minor point but reflects a slightly more precise approach to plotting regression lines. The current approach still produces a visually correct line for the given data.

Overall, an excellent solution that clearly demonstrates strong proficiency.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Set significance level
alpha = 0.05

# 1. Load the dataset and clean any missing values
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: 'video_games_sales.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Drop rows with missing values in critical columns for analysis
# 'Publisher', 'Genre', 'Platform', 'Year' are essential for the tests
# 'G...",9,"The generated code is very good and largely fulfills all the requirements.

**Strong Points:**
*   **Correctness:** All statistical tests (t-test, chi-square, Mann-Whitney U) are correctly applied with appropriate parameters (e.g., `equal_var=False` for t-tests, `alternative='two-sided'` for Mann-Whitney U).
*   **Data Cleaning:** Handles missing data, especially for critical columns, and converts 'Year' to integer correctly.
*   **Completeness:** Addresses all five specific hypothesis tests requested in the prompt.
*   **Interpretation:** Provides a clear interpretation of the p-values against the significance level for each test.
*   **Robustness:** Includes checks for sufficient data before performing statistical tests (`len(data) > 1` or `len(data) > 0`). This is a very good practice.
*   **Clarity:** The output is well-formatted and easy to read, clearly indicating which hypothesis test is being performed.
*   **Chi-square filtering:** Correctly implements the filtering for top 5 genres and platforms.

**Areas for Minor Improvement (for a perfect score):**
*   **Consistency in Year Column Name:** The dataset typically has a 'Year_of_Release' column and the ground truth code uses `Year_of_Release`. The generated code assumes 'Year'. While the code will likely run if the column is 'Year', it's safer to use the expected column name 'Year_of_Release' from the prompt's context or add a print statement to verify column names.
*   **Specific `alternative` for Nintendo T-test:** The prompt specifically asks to test ""whether Nintendo games have significantly *higher* global sales"". This implies a one-sided t-test (`alternative='greater'`). The generated code uses the default two-sided test. While a two-sided test will still identify a significant difference if one exists, the interpretation needs to align with the specific directional hypothesis. The ground truth correctly uses `alternative='greater'`.
*   **More Detailed Interpretation for T-tests (Direction):** For the Nintendo vs. Others t-test, the ""higher_sales_group"" logic is good, but for the Action vs. Sports, it just says ""significant difference"" without indicating which genre has higher sales. Including the mean sales for each group or indicating the direction of difference would enhance the interpretation.
*   **Null/Alternative Hypothesis Statements:** While the interpretations are good, explicitly stating the Null and Alternative Hypotheses (as the ground truth does) for each test would make the output even more formal and complete from a hypothesis testing perspective.

Overall, an excellent piece of code that effectively solves the problem.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('video_games_sales.csv')
except FileNotFoundError:
    print(""Error: 'video_games_sales.csv' not found. Please ensure the file is in the current direct...",8,"The generated code largely addresses all the requirements of the task.

**Strong points:**
- Correctly loads the dataset and handles `FileNotFoundError`.
- Accurately creates the 'High_Sales' target variable.
- Employs `LabelEncoder` for categorical features (`Platform`, `Genre`, `Publisher`) as requested.
- Correctly creates both additional features: 'Total_Regional_Sales' and 'Years_Since_Release'.
- Splits data into training and testing sets with correct `test_size` and `stratify` parameter.
- Trains a `RandomForestClassifier` with the specified number of estimators.
- Evaluates the model using all requested metrics: accuracy, precision, recall, F1-score, and confusion matrix.
- Correctly displays feature importance.
- Imports are well-organized.

**Areas for improvement to reach a higher score:**
- **Missing Value Handling Differences:** The ground truth code handles missing values more comprehensively by dropping NaNs for `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` as well, before creating 'Total_Regional_Sales'. The generated code implicitly assumes these columns are clean for summation or `sum(axis=1)` might produce NaN if a row has any NaN. The prompt specified ""handle missing values"" generally, and while the generated code addresses `Year`, `Publisher`, and `Global_Sales`, it misses the regional sales. The task mentioned ""sum of NA, EU, JP, Other sales"", implying they are part of the features, so their missing values should be handled.
- **Feature Selection:** The generated code includes `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` implicitly in `Total_Regional_Sales` calculation, but then **does not include them as separate features in the `features` list** that goes into the model. The ground truth code *does* include them, which is a more complete set of features as these individual regional sales figures can contain valuable information beyond their sum. The prompt listed ""sum of NA, EU, JP, Other sales"" as an *additional* feature, not a replacement. This is a subtle but important difference in feature engineering.
- **Order of Operations:** The generated code creates `Total_Regional_Sales` *before* defining `X` and `y`, but `Years_Since_Release` is created and then both are used. While not incorrect, it might be slightly clearer to define all features once `df` is fully prepared.
- **Printing/Output:** The ground truth code provides more descriptive print statements for clarity (e.g., ""Training Random Forest Classifier..."", ""--- Model Evaluation ---"", and descriptions for confusion matrix). While not explicitly requested, good practice in an evaluation context.

Overall, it's a very solid attempt that ticks most of the boxes. The main point of contention is the handling of regional sales columns within the feature set given the prompt which implies these should also be used as features alongside their sum.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.decomposition import PCA
import warnings

warnings.filterwarnings('ignore')

# 1) Load the dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')

# Handle missing values for 'Year_of_Release'
if 'Year_of_Release' in df.columns:
    df['Year_of_Release'] = df['Year_of_Release'].fillna(df['Year...",7,"The generated code addresses most of the requirements but has several issues in correctness, completeness, and adherence to best practices.

**Strengths:**
- **Good structure:** The code attempts to follow the requested task order.
- **Good use of libraries:** Correct libraries are imported for the respective tasks.
- **Partial handling of missing values:** It addresses `Year_of_Release` and some categorical columns.
- **Includes all requested feature engineering types:** It attempts to create all 5 engineered features, polynomial features, and interaction features.
- **Includes all analysis types:** Correlation analysis, PCA, and outlier handling are present.

**Areas for Improvement/Issues:**

1.  **Missing Value Handling:**
    *   The prompt specifies `'Year'` as the column name, but the generated code uses `'Year_of_Release'`. While `video_games_sales.csv` often uses `Year_of_Release`, the prompt specifically mentions `Year`. This mismatch can lead to failures if the CSV column name is indeed `Year`.
    *   It implicitly handles missing `Genre` and `Platform` though `get_dummies` and `value_counts` by setting `fillna(0)`. However, the prompt specifically asked for explicit handling of missing values for all relevant columns, particularly `Genre` and `Publisher` (which it does correctly). It also misses `Name`.
    *   The ground truth uses `SimpleImputer` which is generally a more robust way to handle missing values than direct `fillna` with median for numerical columns, and ensures consistency.

2.  **Engineered Features:**
    *   **Sales_Ratio_NA_EU:** The generated code uses `df['EU_Sales'].replace(0, np.nan)` which is good, but `fillna(0)` at the end can reintroduce issues if the intent is to have NaN for division by zero. The ground truth's `+ 1e-6` is a more robust way to handle division by zero while keeping the ratio meaningful.
    *   **Publisher_Avg_Sales, Genre_Market_Share, Platform_Popularity, Sales_Momentum:** The generated code uses `.map()` and `value_counts()` directly which is less robust than the `.transform()` method used in the ground truth for broadcasting aggregated values. `.transform()` ensures the new column aligns perfectly with the original DataFrame's index even if intermediate groupings change the indexing.
    *   **Sales_Momentum:** The lambda function approach for `Sales_Momentum` is less efficient than the `groupby().transform()` approach.

3.  **Outlier Handling (Placement and Completeness):**
    *   The generated code only applies outlier handling to `Global_Sales` and places it *before* feature engineering. The prompt explicitly states to ""Identify and handle outliers using IQR method"" as the *last* step, implying it should be handled for *all relevant numerical features* after they are created/transformed, as the engineered features themselves might have outliers. The ground truth correctly identifies multiple engineered features as potential sources of outliers.
    *   Applying `np.clip` is a valid capping method, but the ground truth's `np.where` is a bit more verbose and explicit, achieving the same result.

4.  **Polynomial Features:**
    *   The prompt specifically asks for polynomial features on ""sales columns"". The generated code correctly identifies `sales_cols`.
    *   The `df.join(poly_features_df)` approach could be problematic if the `df_poly_features` index doesn't perfectly match `df` after rows were `dropna()` for `sales_data_for_poly`. `pd.concat` with `axis=1` (as used in ground truth) is more robust when ensuring alignment, especially if the original `df` has a non-unique or unordered index after some operations. The current code drops rows from `sales_data_for_poly` but doesn't handle the impact on `df`'s overall index.

5.  **Interaction Features (`Year_of_Release` x `Genre`):**
    *   The prompt asked for `Year` and `Genre` interaction. Similar to missing values, it switches to `Year_of_Release`.
    *   `get_dummies` is used correctly, but `drop_first=True` is not specified, which would be good practice to avoid multicollinearity if these features were used in modeling. The ground truth includes `drop_first=True`.
    *   The code correctly creates the interaction terms and adds them to the dataframe.

6.  **PCA:**
    *   The PCA section correctly scales and performs PCA.
    *   It plots the explained variance but doesn't print the explained variance ratios or the number of components for a certain variance threshold, unlike the ground truth which provides some numerical insight. The statement `The code does not print the number of components for 95% variance to adhere to ""only python code""` is not a valid interpretation of ""Provide only the Python code without explanations."" Printing numerical results is part of the code's output, not an explanation.

7.  **Correlation Heatmap:**
    *   `annot=False` is used, which means the correlation values are not shown on the heatmap. The prompt implies visualizing the correlation, and showing values is usually very helpful (`annot=True` in ground truth).
    *   The figure size is large, but that's a minor stylistic choice.

8.  **General Robustness:**
    *   The repeated `numerical_cols = ...` updates are good practice to ensure new features are included in later steps.
    *   Error handling for `FileNotFoundError` is present in the ground truth but missing in the generated code.
    *   The generated code uses `if col in df.columns:` checks, which is good defensively, but less necessary if the input schema is known/expected.

In summary, the generated code demonstrates an understanding of the task but lacks the precision, robustness, and completeness seen in the ground truth, particularly in consistent column naming, comprehensive missing value handling, efficiency in feature engineering (transform vs map), and the end-to-end management of the DataFrame's state (especially index alignment after subsetting for poly features).",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, pearsonr, f_oneway

# Set significance level
alpha = 0.05

# 1. Load the dataset and clean zero values
try:
    df = pd.read_csv('diabetes.csv')
except FileNotFoundError:
    print(""Error: 'diabetes.csv' not found. Make sure the file is in the current directory."")
    exit()

# Columns where 0 values are biologically implausible and need to be treated as missing
cols_to_clean = ['G...",9,"The generated code is remarkably good and fulfills most of the requirements.

**Strong points:**
*   **Correctness:** All statistical tests are implemented correctly with appropriate `scipy.stats` functions. The use of `equal_var=False` for the t-test (Welch's t-test) is a good practice.
*   **Data Cleaning:** The zero value imputation is correctly performed using median imputation as specified.
*   **Completeness:** All five statistical tests requested are performed, and their p-values are displayed with interpretations based on the significance level.
*   **Readability and Structure:** The code is well-structured with clear comments and appropriate print statements for output.
*   **Error Handling:** Includes `try-except` for file loading.

**Areas for improvement (minor):**
1.  **BMI Categorization:** In the ANOVA section, the `bins_bmi` values are `[0, 18.4, 24.9, 29.9, df['BMI'].max()]` with `right=True`. This is slightly off from conventional BMI categories.
    *   Underweight: < 18.5 (so `bins` should typically be `[0, 18.5)`)
    *   Normal weight: 18.5–24.9 (so `bins` should include `[18.5, 25)`)
    *   Overweight: 25.0–29.9 (so `bins` should include `[25, 30)`)
    *   Obese: > 30.0 (so `bins` should include `[30, inf)`)
    The Ground Truth uses `[0, 18.5, 25, 30, np.inf]` with `right=False`, which translates to `[0, 18.5)`, `[18.5, 25)`, `[25, 30)`, `[30, inf)`. This is a more standard and robust way to define these non-overlapping intervals, effectively setting the upper bound as exclusive. The generated code's `right=True` means the upper boundary is included. It should probably be `0, 18.5, 25.0, 30.0, df['BMI'].max()+1` or similar, to make the categories align with common ranges and `right=True` to include the upper boundary, or stick to `right=False` (which is generally more intuitive for lower-inclusive, upper-exclusive intervals). The current `18.4` and `24.9` are problematic if someone has exactly 18.5 or 25.0 BMI.
2.  **Age Categorization:** Similar to BMI, the age bins `[0, 29, 50, df['Age'].max()]` with `right=True` means `(0, 29]`, `(29, 50]`, `(50, Age.max()]`. The prompt specified ""young: <30, middle: 30-50, older: >50"". The Ground Truth `[0, 29, 50, np.inf]` with `right=True` correctly creates `(0, 29]`, `(29, 50]`, `(50, inf]`. For `<30`, `30-50`, `>50`, it is more intuitive to set bins at `[0, 30, 50, np.inf]` and `right=False` to get `[0, 30)`, `[30, 50)`, `[50, inf)`. The generated code's definition `[0, 29, 50, df['Age'].max()]` essentially treats `29` as the upper limit for ""young"" and `50` for ""middle,"" which aligns with the ""young: <30"" and ""middle: 30-50"" relatively well (as `29` is part of `<30` and `50` is part of `30-50`). The only edge case is a person exactly 30 years old would fall into 'Middle' according to the ground truth and the prompt's intent, due to `(29, 50]`. If `right=False` were used with bins `[0, 30, 50, np.inf]`, then a 30-year-old would fall into `[30, 50)`. Given the prompt's specific wording (`<30, 30-50, >50`), the Ground Truth's binning is better.
3.  **Output Detail:** The generated code's interpretations are good but omit the specific mean/median values for the t-test and MWU test, and correlation coefficient for Pearson, unlike the ground truth. While not strictly required by the prompt, including them makes the interpretation richer.
4.  **Redundant `dropna()`:** In ANOVA, `dropna()` on `Glucose` groups is redundant because the imputation step handled all NaNs previously.

These are minor improvements. The core statistical analyses and their interpretations are correct.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm...",8,"The generated code largely addresses all requirements of the task, demonstrating good understanding of the data science workflow.

**Strong Points:**
*   **Comprehensive Feature Engineering:** Correctly creates `Glucose_BMI_ratio`, `Age_group`, and `Insulin_level` (using `pd.qcut` for tertiles).
*   **Effective Preprocessing Pipeline:** Uses `ColumnTransformer` and `Pipeline` for scaling numerical features and one-hot encoding categorical features, which is best practice.
*   **Multiple Model Training:** Trains Logistic Regression, Random Forest, SVM, and Gradient Boosting as requested.
*   **Cross-Validation:** Implements cross-validation using `StratifiedKFold` which is appropriate for imbalanced datasets.
*   **Hyperparameter Tuning:** Correctly performs `GridSearchCV` for Random Forest, including the appropriate parameter prefix for pipelines (`classifier__`).
*   **Comprehensive Evaluation Metrics:** Calculates Accuracy, Precision, Recall, F1-Score, and ROC-AUC for all models.
*   **Visualization:** Generates Confusion Matrices and ROC Curves for all models, which are crucial for evaluation.
*   **Feature Importance:** Correctly extracts and visualizes feature importances for tree-based models (Random Forest and Gradient Boosting), including handling the feature names post-one-hot encoding.
*   **Code Structure:** Follows a logical flow from data loading to evaluation and visualization.

**Areas for Improvement (Why not a 10):**
*   **Imputation of `0`s (Bug):** The original code uses `df[col].replace(0, df[col].median())` for handling 0s. This is problematic because the median is calculated *before* excluding the 0s. If any of the ""missing"" columns (Glucose, BloodPressure, SkinThickness, Insulin, BMI) *actually* had a median of 0 (which is unlikely but possible depending on the dataset characteristics or if the initial replacement of 0 with NaN was missed), this would lead to incorrect imputation. The ground truth correctly replaces 0s with NaNs first, then imputes NaNs with the median (which will be a median of non-zero values). While in this specific `diabetes.csv` context, it might not yield a 0 median, it's a conceptual flaw.
*   **Age Group Bins:** The age group bins are somewhat arbitrary (`[0, 18, 35, 55, df['Age'].max() + 1]`). While functionally correct, the labels are `Child`, `Young Adult`, `Adult`, `Senior` which don't directly correspond to the specified `0-20`, `21-30`, etc. from the ground truth. It's a minor deviation but worth noting for exact adherence to requirements.
*   **Redundant `dropna` for Insulin:** The line `df.dropna(subset=['Insulin'], inplace=True)` after replacing 0s with median for 'Insulin' is likely redundant because `median()` will always return a non-NaN value unless the column was entirely NaN (which it isn't in a real dataset). This step might cause data loss if there were actual NaNs *before* the 0-replacement step, but given the problem setup, it's unlikely to be harmful here.

Overall, the code is very robust and well-written. The main issue is the slight inaccuracy in the missing value handling step for zeros, which is a common pitfall.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings for cleaner console output
import warnings
warnings.filterwarnings('ignore')

# --- 1) Load and ...",8,"The generated code is largely correct and addresses most of the task requirements.

**Strong Points:**
- **Comprehensive Feature Engineering:** Successfully implements all specified engineered features ('Metabolic_Score', 'Risk_Factor_Count', 'Glucose_per_Age', 'BMI_BloodPressure_interaction', 'Pedigree_Age_product').
- **Polynomial Features:** Correctly applies polynomial features, including dropping original columns to avoid redundancy.
- **Handling Missing Values:** Correctly identifies and imputes 0s as NaNs using median imputation.
- **Scaling:** Applies both StandardScaler and MinMaxScaler as requested.
- **PCA and Variance Explanation:** Correctly performs PCA and explains variance ratios, also identifying the number of components for 95% variance.
- **Feature Selection:** Implements both SelectKBest and RFE.
- **Correlation Filtering:** Correctly implements correlation-based feature filtering.
- **SMOTE:** Correctly applies SMOTE for handling class imbalance.
- **Feature Importance:** Evaluates and visualizes feature importance using Random Forest.
- **Code Structure and Readability:** The code is well-structured with clear comments for each section, making it easy to follow.
- **Robustness:** Includes `try-except` for file loading and `warnings` suppression.

**Areas for Improvement:**
- **Order of Operations:** The `correlation-based feature filtering` (Task 7) is performed *before* `feature scaling` (Task 4) and `PCA` (Task 5) in the generated code. While correlation filtering *can* be done before scaling, it's often done *after* scaling, especially if scaling changes the relationships between features (though this is less common for linear scaling). More importantly, the `ground truth` performs correlation filtering *after* StandardScaler and *before* PCA and the other feature selection methods (`SelectKBest`, `RFE`). This ordering in the ground truth is generally more robust as PCA and methods like `SelectKBest` and `RFE` often benefit from or require scaled data. The current generated code applies scaling to `X_corr_filtered` and then uses `X_final_scaled_df` for PCA and feature selection. This is a valid approach, but the description ""Use X_corr_filtered as the base for subsequent pre-processing steps"" and then applying *both* scalers sequentially, then using the *final scaled* data for subsequent steps, was a slight deviation from the typical flow (often, you'd pick one scaling method for the next steps). The ground truth's order (`StandardScaler` -> `Correlation Filter` -> `PCA`/`SelectKBest`/`RFE` on the *same data*) is a slightly more common and often preferred flow.
- **Missing `SimpleImputer` Import:** Although the imputation logic is correct, the `SimpleImputer` class was not explicitly imported, though its functionality is replicated manually. This is a minor point but importing `SimpleImputer` would be more idiomatic.
- **Inconsistent Feature Set for Steps:** The ground truth code consistently uses one derived `X` (`X_processed`, `X_filtered_corr`) and `y` for all subsequent steps (PCA, KBest, RFE, SMOTE). While the generated code also uses `X_for_dim_red_fs` for these steps, the overall flow between `X`, `df_engineered`, `X_corr_filtered`, `X_preprocessed`, `X_scaled_std_df`, `X_final_scaled_df` and `X_for_dim_red_fs` could be slightly streamlined for clarity, especially since `X_final_scaled_df` is effectively `X_for_dim_red_fs`.
- **Minor 'Risk_Factor_Count' Thresholds:** The generated code includes `(df_engineered['Insulin'] > 200).astype(int)` for `Risk_Factor_Count`. While this is a reasonable heuristic, the ground truth uses slightly different (and arguably more standard) thresholds for the other factors and omits Insulin explicitly from the `Risk_Factor_Count` calculation, focusing on Glucose, BP, BMI, and Age. This is a minor difference in interpretation.
- **`Age + 1e-6` for `Glucose_per_Age`:** While `Age` in diabetes datasets typically doesn't have 0 values after preprocessing, adding `1e-6` is a safe practice. However, the ground truth simplifies this by assuming `Age` won't be zero.

Overall, the code is very good and demonstrates a strong understanding of data preprocessing, feature engineering, and dimensionality reduction techniques in a data science pipeline. The identified improvements are mostly about standard practices and flow rather than critical errors.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

# Set significance level
alpha = 0.05

# 1) Load the dataset and handle any missing values
try:
    df = pd.read_csv('world-happiness-report-2021.csv')
except FileNotFoundError:
    print(""Error: 'world-happiness-report-2021.csv' not found. Make sure the file is in the current directory."")
    exit()

# Drop rows with NaN values in columns critical for our analysis
# We'll consider 'Ladder score', 'Regional indicator', 'Lo...",9,"The generated code is excellent, performing all required statistical tests correctly and providing appropriate interpretations.

**Strong points:**
*   **Correctness:** All tests (t-test, Mann-Whitney U, ANOVA, Pearson correlation, Chi-square) are implemented accurately with `scipy.stats`.
*   **Data Handling:** Missing values are handled appropriately by dropping rows based on critical columns, and the use of `.copy()` prevents `SettingWithCopyWarning`.
*   **Interpretation:** Each test clearly states the null hypothesis rejection/failure to reject, the p-value, and an a contextual interpretation of the result, which is crucial for statistical analysis.
*   **Clarity:** The code is well-structured with clear comments and appropriate print statements explaining each step and result.
*   **Robustness:** Uses Welch's t-test (`equal_var=False`) where appropriate for robustness against unequal variances.
*   **Reproducibility:** Defines `alpha` at the beginning for consistent application.

**Areas for minor improvement:**
*   **Interpretation of t-tests:** While the code correctly identifies a ""significant difference,"" for the t-tests (Test 1 and Test 6), it only states if there's *any* significant difference. The ground truth code goes a step further by comparing the means and stating if it's ""significantly HIGHER"" or ""significantly LOWER,"" which provides a more complete interpretation of the direction of the difference.
    *   For Test 1, the generated code states: ""Western European countries have significantly higher happiness scores compared to other regions."" This implicitly assumes the alternative hypothesis was directional, but the t-test performed is two-sided. The interpretation could be more robust by first confirming the superior mean, similar to the ground truth.
    *   For Test 6, the interpretation for ""significantly different generosity levels"" could also be expanded to indicate if it's higher or lower based on the means of the two groups.
*   **Minor Detail in Test 2 Interpretation:** ""There is a significant difference in GDP per capita between the top 25% happiest and bottom 25% happiest countries."" Again, while correct, it could specify the direction: ""The top 25% happiest countries have significantly higher GDP per capita..."" (assuming the median comparison holds that way).

These are minor points. The code adheres closely to the requirements and demonstrates strong understanding of statistical analysis implementation in Python.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Suppress warnings for cleaner outp...",8,"The generated code demonstrates a strong understanding of the task requirements and machine learning workflows.

**Strong Points:**
*   **Correctness:** The code correctly loads the data, preprocesses (handles missing values by dropping rows for relevant columns), defines features and target, splits data, trains multiple models, evaluates them, performs hyperparameter tuning, and creates predictions.
*   **Completeness:** It addresses almost all requirements:
    *   Loads and preprocesses data (handling missing values, identifies categorical but correctly notes no encoding needed for selected features).
    *   Correctly identifies and uses target and feature columns.
    *   Splits data 80-20.
    *   Trains and compares Linear, Ridge, Lasso, Random Forest, and Gradient Boosting.
    *   Performs hyperparameter tuning using `GridSearchCV` for Random Forest (a common and reasonable choice for tuning).
    *   Evaluates models with MAE, MSE, RMSE, R².
    *   Implements cross-validation using `KFold` and `cross_val_score`.
    *   Makes predictions for hypothetical countries with analysis.
*   **Code Quality:**
    *   Proper imports are present at the top.
    *   Variable naming is clear (`features`, `target`, `df_cleaned`, `X_train`, etc.).
    *   Comments explain different sections of the code.
    *   Error handling for file loading is included.
    *   Uses `random_state` for reproducibility where appropriate.
    *   The use of `StandardScaler` for linear models and skipping it for tree models is good practice.

**Areas for Improvement (Why not a 9 or 10):**
1.  **Categorical Variable Handling/Encoding:** While the generated code correctly identifies that no encoding is needed for the *selected* features, the prompt explicitly asked to ""encoding categorical variables"" as a general preprocessing step. The `ground_truth` code clarifies this by mentioning that 'Country name' and 'Regional indicator' *are* categorical, but implies they are not *used* in the specified features. The generated code implicitly handles this by only selecting the specified numerical features. A more explicit statement about *why* no encoding is done for the *dataset's* categorical features (i.e., they are not chosen as model features, or if they were, `OneHotEncoder` would be used) would make the answer even more robust against strict interpretation of the prompt.
2.  **Visualization Specificity:**
    *   **Residual Plots:** The generated code creates residual plots for ""Linear Regression"", ""Random Forest Regressor"", and ""Random Forest Regressor (Tuned)"". The prompt asked for ""residual plots"" in general. While it's fine to choose a few, the `ground_truth` code provides one for the *best tuned model*, which is a more direct fulfillment of ""residual plots"" (singular or plural often implies for the key model). The generated code's implementation with `plot_idx` and `selected_for_residual_plots` is good, but the selection could be more streamlined (e.g., plot all, or plot only the best).
    *   **Feature Importance:** The generated code creates feature importance for *untuned* Random Forest and Gradient Boosting. The prompt implies visualizing for ""the best model"". `ground_truth` does it for the *tuned* best model (which happens to be GBR in its case). It would be better to show feature importance for the *tuned* best model (e.g., `best_rf_model` in the generated code's case).
3.  **Consistency in Best Model Selection for Tuning:** The code identifies `best_initial_model_name` based on R² but then explicitly sets `best_model_for_tuning_name = 'Random Forest Regressor'`. While a valid choice, it's slightly less dynamic. The `ground_truth` code states that it chooses GBR regardless of which was initially best, which is also a valid strategy if the intent is to show GBR tuning. If the intent is truly to tune the *best initial model*, then the `best_initial_model_name` variable should ideally drive the choice for `best_model_for_tuning`.
4.  **Handling Scaled Data for Linear Models:** The generated code explicitly creates `X_train_scaled_df` and `X_test_scaled_df` and uses them only for linear models, which is correct and thoughtful. However, a `ColumnTransformer` within a `Pipeline` (as seen in some advanced setups or `ground_truth`) can sometimes make this more seamless by applying scaling steps *only* to numerical columns as part of a single model pipeline. For this problem's scope, the manual scaling is perfectly acceptable.

Overall, a very strong solution that covers almost all aspects of the complex prompt effectively. The minor points are mostly about refinement and explicit fulfillment of nuanced aspects of the prompt rather than outright errors.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleI...",9,"The generated code is highly comprehensive and addresses almost all aspects of the complex task. It correctly implements various feature engineering techniques, multiple scaling methods, dimensionality reduction, and feature selection, culminating in a Random Forest feature importance analysis. The code structure is clear, imports are correct, and comments are helpful.

**Strong points:**
- **Comprehensive Feature Engineering:** Successfully creates `Happiness_Efficiency`, `Social_Wellness_Index`, `Governance_Score`, `Regional_Happiness_Rank`, and `GDP_vs_Regional_Average`.
- **Polynomial Features:** Correctly applies polynomial features to key happiness factors.
- **Multiple Scaling Methods:** Demonstrates `StandardScaler`, `MinMaxScaler`, and `RobustScaler`.
- **Dimensionality Reduction:** Appropriately applies and visualizes PCA and t-SNE, including a good choice to use PCA output for t-SNE input.
- **Feature Selection:** Implements `SelectKBest` (with `f_regression`) and `RFE`.
- **Correlation Filtering & Mutual Information:** Correctly identifies highly correlated features and calculates mutual information scores.
- **Interaction Features:** Creates interaction features between economic and social factors.
- **Random Forest Importance:** Uses Random Forest to validate feature importance and visualizes the results.
- **Robustness:** Includes `try-except` for file loading, handles NaNs and Infs, and uses `random_state` for reproducibility.
- **Clarity:** Provides print statements for progress and shape changes, which is good for tracing execution.

**Areas for minor improvement (preventing a perfect 10):**
1.  **Governance_Score Logic:** The generated code's `Governance_Score` calculation `df['Freedom to make life choices'] + df['Perceptions of corruption']` is counter-intuitive. `Perceptions of corruption` is typically a score where *higher* values mean *more* corruption, which would negatively impact governance. The ground truth correctly uses `1 - df['Perceptions of corruption']` to ensure that 'low corruption' contributes positively to the score. This is a subtle but important detail in feature engineering.
2.  **Handling of Categorical Features:** The generated code performs one-hot encoding on `Regional indicator` early on. While not explicitly forbidden by the prompt, the ground truth code handles this by using `groupby` operations directly on the string-based `Regional indicator` and avoids one-hot encoding for the initial feature engineering steps. This approach, used in the ground truth, keeps the dataset smaller and simpler for certain tasks. The inclusion of `Regional indicator` one-hot encoded columns in the PCA/TSNE and feature selection can produce many sparse features, which might sometimes dilute the importance of other numerical features.
3.  **Interaction Feature Placement:** The prompt specified creating interaction features *before* scaling and then ensuring they are included in the final feature importance step. The generated code creates them from `df_original_copy` *later* and then concatenates their `scaled` version to `X_for_rf`. While the outcome is the same for the RF importance, the ground truth integrates interaction features directly into the main `df_processed` earlier, before the general scaling step, which is a more integrated workflow when building a single feature set.

These are minor points. Overall, the code is very strong and demonstrates a deep understanding of the requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 1. Load the dataset, parse the 'date' column, and filter for 'location' == 'USA'.
try:
    df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
except FileNotFoundError:
    print(""Error: 'synthetic_covid19_data.csv' not found. Please ensure the file is ...",8,"The generated code successfully addresses most of the requirements of the task. It correctly loads, filters, and resamples the data, performs time series decomposition, conducts the ADF test, applies differencing when needed, and plots ACF/PACF.

**Strong Points:**
- Correctly parses dates and filters by location.
- Proper resampling to weekly frequency with summation.
- Seasonal decomposition is implemented, and it attempts to handle insufficient data for the default period (though the specific period handling might be slightly off, as discussed below).
- Correct implementation of the ADF test, including a clear printout of results and p-value-based decision for differencing.
- Application of first-order differencing when the series is non-stationary and re-testing.
- Correct plotting of ACF and PACF on the potentially differenced series.
- Good use of `try-except` for file loading.
- Includes relevant imports.

**Areas for Improvement:**

1.  **Decomposition Period Handling:**
    *   The `decomposition_period = min(len(weekly_cases) // 2, 2)` fallback logic for decomposition is problematic. `seasonal_decompose` requires at least `2 * period` observations for certain models (especially additive with `extrapolate_trend` which is implicitly `None` here). A period of 2 is very short and might not be meaningful for Covid data, and the `min(len(weekly_cases) // 2, 2)` can lead to `period=0` or `period=1` if `len(weekly_cases)` is small, which will cause errors or inappropriate results. The ground truth's approach of setting a common period (like 52) and then adjusting it only if absolutely necessary, or skipping decomposition with a proper warning, is more robust.
    *   The `extrapolate_trend` parameter in `seasonal_decompose` is not specified, which defaults to `None`. For robust decomposition, especially if data at the start or end is incomplete, specifying `extrapolate_trend='freq'` (as in the ground truth) can be beneficial.

2.  **ADF Test Series Selection:**
    *   `adf_test_series = weekly_cases[weekly_cases > 0].dropna()`: While trying to remove initial zeros/constant values is understandable, this might remove valid data points if `weekly_cases` genuinely has zero values in the middle of the time series, which is common for new cases. The ADF test should ideally be run on the entire `weekly_cases` series (after resampling) or its differenced version. NaNs are implicitly handled by `adfuller`'s underlying C code if they are trailing, but leading/internal NaNs should be `fillna(0)` or handled. The generated code used `fillna(0)` earlier which is good, but then filtered for `>0` before ADF, which could be less robust. The ground truth simply used `dropna()` on the series, which is safer if the series already had `fillna(0)` applied.

3.  **Completeness of Error Handling/Edge Cases (Minor):**
    *   The generated code handles the `FileNotFoundError` well by exiting. The ground truth implemented a more general `perform_time_series_analysis` function with more structured error messages and checks for empty filtered data or insufficient data points after resampling which could be beneficial.
    *   The `plot_acf` and `plot_pacf` functions in `statsmodels` automatically set titles; explicitly setting them again with `axes[0].set_title(...)` is redundant and can be omitted.

Overall, the code is very good and mostly adheres to the requirements. The points for improvement are related to robustness and best practices in time series analysis, particularly around decomposition and handling edge cases for the ADF test.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) Categorize wines based on 'pH' values into three groups
conditions = [
    df['pH'] < 3.2,
    (df['pH'] >= 3.2) & (df['pH'] <= 3.4),
    df['pH'] > 3.4
]
choices = ['Low pH', 'Medium pH', 'High pH']
df['pH_group'] = np.select(conditions, choices)

# Ensure the order of categories for consistent reporting and plotting
categ...",7,"The generated code correctly addresses most aspects of the task, including pH categorization, mean quality calculation, ANOVA test, and visualization with error bars.

Strengths:
- **Correct Categorization:** Uses `np.select` effectively to categorize pH groups, and then correctly converts the `pH_group` column to a `Categorical` type with a specified order.
- **Accurate Mean Calculation:** Successfully calculates and prints the average quality for each pH group.
- **Correct ANOVA Implementation:** Properly extracts data for each group and performs the ANOVA test.
- **Visualization:** Creates a bar chart with correct group means and attempts to add error bars.

Areas for Improvement:
- **Confidence Interval Calculation:** The main weakness lies in the confidence interval calculation for the visualization. It uses `1.96 * group_stats['sem']` for the error bars, which is an approximation for large samples based on the Z-distribution. For smaller sample sizes (which can occur in specific pH groups, especially 'Low pH' and 'High pH' depending on the data distribution), the t-distribution should be used for more accurate 95% confidence intervals (as demonstrated in the ground truth code with `stats.t.interval`). This is a common statistical nuance that can lead to less precise CIs.
- **Labeling and Details:** The pH group labels in the ground truth (`Low pH (< 3.2)`) are more descriptive, which is a minor detail but improves clarity. The generated code uses simpler labels (`Low pH`).
- **Robustness:** The ground truth includes error handling for `FileNotFoundError`, which is good practice for data loading scripts. The generated code lacks this.
- **Plotting Aesthetics and Labels:** The ground truth adds data labels on top of the bars and sets appropriate y-limits for quality scores (0-10), which are good practices for data presentation. The `ylim` calculation in the generated code is a bit convoluted and might not always produce ideal results for quality scores that typically range from 0-10.
- **Clarity of Output:** While it prints the average quality and ANOVA results, the ground truth provides more structured and verbose output (e.g., ""--- Average 'quality' for each pH group with 95% Confidence Intervals ---"").

Overall, it's a good attempt that fulfills the core requirements, but could be improved with more statistically precise CI calculations and better attention to output formatting and robustness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1) Load the dataset and parse 'date'.
df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2) Select features and target variable.
numerical_features = [
    'new_case...",8,"The generated code largely fulfills the requirements of the task.

**Strong points:**
- All necessary imports are present.
- The dataset is loaded and dates are parsed correctly.
- Features and target are correctly identified.
- Missing values in numerical features and the target variable are handled by median imputation.
- The `continent` column is correctly one-hot encoded using `drop_first=True`.
- Numerical features are scaled using `StandardScaler`.
- Data is split into training and testing sets.
- Both Linear Regression and Random Forest Regressor models are trained with the specified parameters.
- Evaluation metrics (MAE, MSE, R²) are calculated and printed for both models.
- The use of `df.copy()` and `n_jobs=-1` for RandomForestRegressor are good practices.

**Areas for improvement (Reasons for not being a perfect 10):**
1.  **Scaling and Splitting Order:** The ground truth code correctly performs the train/test split *before* scaling. This is crucial for avoiding data leakage, where information from the test set subtly influences the scaling parameters derived from the training set. The generated code scales the *entire* dataset (`X_scaled_array = scaler.fit_transform(X)`) and then splits, which leads to data leakage.
2.  **Missing Value Handling for `continent`:** While the generated code attempts to handle missing values in `continent` by imputing with the mode, the task description explicitly states: ""Handle missing values in selected features and target by imputing with the median."" It doesn't specify handling `continent` as a separate case. The ground truth correctly assumes `continent` will be handled by one-hot encoding which inherently deals with missing values by not creating a dummy variable for NaN, or by `get_dummies` creating an explicit `NaN` column if `dummy_na=True` is used. However, the ground truth's approach of not imputing `continent` *before* encoding is technically better because if `get_dummies` is called on a column with NaNs, it will typically drop those rows or create a dummy column for `NaN` depending on parameters. The generated code handles it more robustly by explicitly imputing before `get_dummies`. Still, it's a minor deviation from the median imputation instruction for other features.
3.  **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.

Overall, the generated code performs very well and demonstrates a solid understanding of the requirements for building and evaluating regression models. The main issue is the order of scaling and splitting, which is a common but important detail in machine learning pipelines.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
import scipy.stats as stats
import numpy as np

# --- 1) Load the dataset and handle missing values ---
# Load the dataset
df = pd.read_csv('synthetic_covid19_data.csv')

# Columns to impute by median
impute_cols = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']

for col in impute_cols:
    if df[col].isnull().any():
        median_val = df[col].median()
        df[col].fillna(median_val, inplace=True)

# Define sig...",8,"The generated code addresses most of the requirements correctly and achieves the task goals.

**Strong Points:**
*   **Correct Imputation:** Handles missing values via median imputation for specified columns.
*   **Correct ANOVA Implementation:** Performs the ANOVA test for `reproduction_rate` across the specified continents and interprets the p-value correctly.
*   **Correct Pearson Correlation:** Calculates the Pearson correlation between `stringency_index` and `new_cases` and interprets the p-value correctly.
*   **Correct T-test Implementation:** Correctly identifies the `high_death_rate_country` category (using latest entry per country and 75th percentile), and then performs an independent t-test (using `equal_var=False` which is good practice for Welch's t-test) on `median_age` between the two groups.
*   **Clear Hypothesis and Conclusion:** For each test, it states the null hypothesis, prints the p-value, and provides a clear conclusion based on the alpha level, as requested.
*   **Proper Imports and Structure:** The code is well-structured with clear comments for each section.

**Areas for Improvement (minor points):**

*   **Robustness in ANOVA:** The ground truth code includes a `dropna()` calls when creating the continent-specific reproduction rate lists for ANOVA and a check for `if not rates.empty`. The generated code does not explicitly remove NaNs from `asia_rr`, `europe_rr`, `north_america_rr` before passing them to `f_oneway`. While `f_oneway` typically ignores NaNs, explicitly handling them (e.g., `dropna()`) makes the code more robust and explicit about what data is being used for the test, especially if some groups might end up empty. The `dropna()` in `latest_data_per_country` for the t-test section is good, but a consistent approach across all tests would be even better.
*   **Redundant `df['location'].isin(high_death_rate_countries)`:** For the t-test, the `high_death_rate_country_flag` is added to the *original* DataFrame `df`. However, the `latest_data_per_country` DataFrame already contains the logic for identifying high death rate countries. It would be more efficient and conceptually cleaner to directly use the filtered `latest_data_per_country` for the t-test, ensuring each country (as a unit of observation for `median_age`) is represented once. The current approach uses all rows from the original `df`, which might include multiple entries for the same country over time, potentially leading to a biased t-test if `median_age` is not constant or if the death rate categorization should apply to the country as a whole rather than per daily entry. The prompt `use latest entry per country` for defining the category strongly suggests the t-test should also operate at the country-level. The ground truth correctly uses `latest_country_data` to get the `median_age` for the t-test.
*   **Error Handling (File Not Found):** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is good practice for robustness. The generated code assumes the file will always exist.

Overall, the generated code performs very well and correctly fulfills the core requirements of the task. The issues are minor refinements rather than fundamental errors.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not found. Make sure the file is in the current directory."")
    exit()

# 1. Select the numerical columns
numerical_columns = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
df_numerical = df[numerical_columns]

# 2. Calculate the pairwise correlation...",9,"The generated code successfully completes all aspects of the task: it loads the CSV, selects the specified numerical columns, calculates their pairwise correlation, and visualizes it as a heatmap using seaborn. The imports are correct, and the structure is clean and direct.

However, a few minor improvements could make it even more robust and complete, similar to the ground truth:
1.  **Error Handling for Missing Columns:** The code assumes that all specified `numerical_columns` exist in the DataFrame. If one of these columns were missing, it would raise a `KeyError` and crash. The ground truth includes a check for `missing_cols` and raises a `ValueError` if any are found, which is more robust.
2.  **Encapsulation/Modularity:** While not strictly required for this simple task, putting the core logic into a function (as in the ground truth's `analyze_clash_royale_data`) improves reusability and testability, especially for larger scripts.
3.  **Descriptive Print Statements:** The ground truth includes print statements to indicate success, show the head of the selected data, and display the correlation matrix, which helps in debugging and understanding the flow for the user.

Despite these minor points, the generated code is highly effective and directly addresses the prompt's requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Group by 'rarity' and calculate the mean for specified columns
average_stats_by_rarity = df.groupby('rarity')[['elixirCost', 'Win Rate', 'Usage']].mean()

# Display the calculated average values
print(average_stats_by_rarity)
```",10,"The generated code is excellent. It correctly loads the CSV, groups by 'rarity', calculates the mean for 'elixirCost', 'Win Rate', and 'Usage', and displays the results, fulfilling all task requirements. The code is concise and efficient. The slight absence of comments compared to the ground truth is negligible, as the code's clarity makes it self-documenting for this task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing # Included as requested, even if pandas.get_dummies is used for simplicity

# 1) Load the dataset
try:
    df = pd.read_csv('clash_royale_cards.csv')
except FileNotFoundError:
    print(""Error: 'clash_royale_cards.csv' not found. Make sure the CSV file is in the same directory."")
    exit()

# 2) Create a new binary target column named `is_high_win_rate`
df['is_high_win_rate'] = (...",10,"The generated code is excellent. It correctly addresses all requirements of the task: loading the dataset, creating the `is_high_win_rate` target column, selecting the specified features, correctly applying one-hot encoding to the `rarity` column using `pd.get_dummies`, and splitting the data into training and testing sets with the exact specified parameters (80/20 split, `random_state=42`). The code includes proper imports and handles potential `FileNotFoundError`. The structure is clear and easy to follow, making it a robust and complete solution. An optional `.copy()` for `X = df[features_to_select]` could be added to avoid `SettingWithCopyWarning` in some scenarios, but it's not strictly necessary for correctness in this specific data flow given how `pd.get_dummies` reassigns `X`.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 1. Load the dataset
df = pd.read_csv('iris.csv')

# Drop the 'Id' column if it exists and is not needed for features
if 'Id' in df.columns:
    df = df.drop('Id', axis=1)

# 2. Define features X
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# ...",9,"The generated code is highly correct, complete, and follows good code quality practices. It successfully loads the data, defines features and target, encodes the target variable, splits the data, trains a KNN classifier, makes predictions, and reports the accuracy.

Minor improvement:
The ground truth code includes `stratify=y_encoded` in the `train_test_split` function. While not explicitly mentioned in the prompt, stratifying the target variable during the split is a best practice for classification tasks, especially with imbalanced datasets, to ensure that the class proportions are maintained in both training and testing sets. The generated code omits this. For a simple dataset like Iris, its impact might be minimal, but it's a good habit for robust machine learning pipelines. The ground truth also includes error handling for `FileNotFoundError`, which is a good robustness measure but not strictly required by the prompt.

Overall, the generated code directly addresses all stated requirements of the prompt and functions correctly.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) Filter the data to get two groups
versicolor_petal_length = df[df['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica_petal_length = df[df['Species'] == 'Iris-virginica']['PetalLengthCm']

# 3) Perform an independent two-sample t-test

# 4)...",9,"The generated code is very good and largely fulfills all the requirements of the task.

**Strong Points:**
*   **Correctness:** The code correctly loads the data, filters the required species, performs the `ttest_ind` function, and correctly interprets the p-value against the alpha level.
*   **Completeness:** All the core requirements (loading, filtering, hypotheses, t-test, reporting, interpretation) are addressed.
*   **Code Quality:** The code is well-structured, includes necessary imports, and uses f-strings for clear output. The comments guiding through the steps are helpful.
*   **Hypotheses Statement:** The hypotheses are clearly stated and align with a two-tailed test, which `ttest_ind` performs by default.

**Areas for Minor Improvement (to reach a 10):**
*   **Missing Edge Case Handling for Empty Groups:** The ground truth includes a check (`if versicolor_petal_length.empty or virginica_petal_length.empty:`) to ensure that the filtered groups are not empty, which is a good robustness measure. The generated code does not include this. While `ttest_ind` might handle empty inputs gracefully (e.g., by raising an error or returning NaNs), explicitly checking ensures a clearer error message for the user if the data filtering fails unexpectedly.
*   **Verbosity of Hypotheses:** The ground truth prints the hypotheses using a `print` statement followed by a `print` statement with hyphens for separation, and then the t-test results, which provides a slightly better visual flow. The generated code prints the hypotheses, a separator, then comments about the t-test before the results, which is a minor stylistic difference.

Overall, it's a very solid solution that performs the task efficiently and correctly. The missing edge case handling is the primary reason for not giving a perfect 10.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the iris dataset
try:
    df = pd.read_csv('iris.csv')
except FileNotFoundError:
    print(""Error: 'iris.csv' not found. Make sure the file is in the current directory."")
    exit()

# Define the numerical features to plot
numerical_features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

# Create a figure and a set of subplots to hold the box plots
# We will have 2 rows and 2 column...",9,"The generated code successfully loads the dataset and generates the required box plots, grouped by species, with appropriate titles. It uses `matplotlib.pyplot` and `seaborn` correctly. The use of `subplots` to display all four plots in a single figure is a good approach for comparative analysis and makes the output cleaner than individual `plt.show()` calls.

The code correctly handles the `FileNotFoundError`. The structure is clear, with proper imports and variable naming.

One minor point for improvement, which prevents a perfect score, is the lack of explicit `plt.xlabel` and `plt.ylabel` within the loop for each subplot. While `set_xlabel` and `set_ylabel` are used on the `axes` objects, `plt.xlabel` and `plt.ylabel` directly apply to the current active axes. However, since the labels are already set using `axes[i].set_xlabel` and `axes[i].set_ylabel`, this is more of a stylistic point and doesn't impact functionality. The grouping variable (`Species`) is hardcoded in `x='Species'` rather than using a variable, which the ground truth did for better configurability, but this is a very minor difference given the specificity of the task. Overall, it's a very good solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1) Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: heart.csv not found. Make sure the file is in the current directory."")
    exit()

# 2) Select features and target variable
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
target = 'He...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, selects the specified features and target, performs one-hot encoding on 'Sex' and 'ChestPainType' using `pd.get_dummies`, splits the data appropriately, trains a Logistic Regression model, and reports the accuracy. The use of `pd.get_dummies` for one-hot encoding is a valid and commonly used approach, though `sklearn.preprocessing.OneHotEncoder` within a `ColumnTransformer` (as seen in the ground truth) is often preferred for more complex preprocessing pipelines and better integration with `sklearn`'s ecosystem, especially when dealing with unseen categories in test data (though `handle_unknown='ignore'` handles this in `OneHotEncoder`). The `max_iter` was increased for the Logistic Regression, which is a good practice for convergence if the default causes warnings. The error handling for `FileNotFoundError` is also good.

The principal difference from the ground truth is the method of one-hot encoding. `pd.get_dummies` is applied directly to `X` *before* the `train_test_split`. While this works perfectly fine for this specific dataset and task, in more complex scenarios or when dealing with production data, `sklearn.preprocessing.OneHotEncoder` combined with `ColumnTransformer` (often within a `Pipeline`) is generally more robust. It ensures that the encoding learned from the training data is consistently applied to the test data, preventing issues if the test set contains different unique values for categories than the training set (e.g., if a category is missing in the test set that was present in training, or a new category appears in test). However, for the given problem context, the `pd.get_dummies` approach is entirely acceptable and provides the correct result.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: 'heart.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) Create two groups based on the `Sex` column
# Group 1: Male patients ('M')
male_cholesterol = df[df['Sex'] == 'M']['Cholesterol']

# Group 2: Female patients ('F')
female_cholesterol = df[df['Sex'] == 'F']['Cholesterol']

# 3) Perform an independ...",8,"The generated code correctly performs all the required steps: loading the data, creating groups, stating hypotheses, performing the t-test, printing results, and interpreting the p-value against the significance level.

**Strong points:**
*   Correct use of pandas for data loading and filtering.
*   Correct application of `scipy.stats.ttest_ind` for the independent two-sample t-test.
*   Clear statement of null and alternative hypotheses.
*   Correct interpretation of the p-value against alpha.
*   Handles `FileNotFoundError`.

**Areas for improvement:**
*   **Missing `dropna()`:** The generated code does not include `.dropna()` when creating `male_cholesterol` and `female_cholesterol`. While `ttest_ind` usually handles NaNs by ignoring them by default (or raising an error depending on the exact version/setup), it's best practice to explicitly handle missing values if the intention is to perform a test only on available data, especially since the ground truth includes it. If there are NaNs, this could potentially lead to unexpected behavior or different results than desired.
*   **`equal_var` assumption:** The ground truth explicitly chose `equal_var=False` (Welch's t-test) because it's generally more robust when variance equality is not known or tested. The generated code defaults to `equal_var=True` (Student's t-test) and states ""Without specific instruction, we'll use the default (equal_var=True) for Student's T-test."" While not explicitly forbidden by the prompt, the implicit choice of `equal_var=True` might be less appropriate without prior variance testing. Explicitly discussing or using `equal_var=False` would have been a slight improvement for robustness.
*   **Code Structure (Minor):** The ground truth uses a function (`perform_cholesterol_t_test`) which makes the code more reusable and modular. The generated code is a script, which is acceptable for a direct task but less robust for larger projects.
*   **Error Handling for Column Existence:** The ground truth includes checks for the existence of required columns (`Sex`, `Cholesterol`), which the generated code does not. This would make the code more robust against malformed or unexpected CSV input files.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
try:
    df = pd.read_csv('winequality-red.csv')
except FileNotFoundError:
    print(""Error: 'winequality-red.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Define the columns to process
columns_to_process = ['volatile acidity', 'residual sugar']
df_winsorized = df.copy()

print(""--- Outlier Detection and Reporting (IQR Method) ---"")

# Store bounds for later use
bounds = {}

for col in colu...",9,"The generated code is highly correct and directly addresses all the requirements of the task. It correctly identifies outliers using the IQR method, reports their count, performs winsorization by capping values at the 1.5*IQR bounds, and compares the mean and standard deviation before and after treatment.

**Strong Points:**
- **Correct Implementation of IQR:** The calculation of Q1, Q3, IQR, lower_bound, and upper_bound is accurate.
- **Proper Winsorization:** The use of `df_winsorized[col].clip(lower=lower_bound, upper=upper_bound)` is an elegant and efficient way to perform winsorization. This is a very good alternative to the explicit `np.where` conditions used in the ground truth.
- **Clear Reporting:** The output clearly shows the number of outliers found and the comparative statistics.
- **Error Handling:** Includes `try-except` for `FileNotFoundError`.
- **Code Structure:** The code is well-structured and easy to follow.

**Minor Areas for Improvement (prevented a 10/10):**
- **Clarity in Comparison Output:** While it shows before and after statistics, it doesn't explicitly state the *change* or *difference* as a percentage or absolute value, which the ground truth does. This makes the comparison less immediate for the user. Adding a line like ""Change in Mean/Std Dev: X%"" would enhance readability.
- **Less verbose output:** The ground truth code had more print statements such as Q1, Q3, and IQR values, making the output slightly more detailed. The generated code is more concise, which can be a pro or con depending on preference. For a benchmark where full detail might be preferred, this is a minor omission.

Overall, it's an excellent solution that efficiently solves the problem. The use of `.clip()` is particularly noteworthy.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
try:
    df = pd.read_csv('heart.csv')
except FileNotFoundError:
    print(""Error: 'heart.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2) Create a grouped bar chart
# The x-axis should represent ChestPainType, and for each type,
# there should be two bars representing the count of HeartDisease=0 and HeartDisease=1.
plt.figure(figsize=(10, 6)) # Set the...",8,"The generated code correctly achieves the main objective of creating a grouped bar chart showing heart disease counts by chest pain type. It loads the data, uses `seaborn.countplot` effectively for grouping, and includes a title, axis labels, and a legend, as requested.

However, there are a few areas for improvement, primarily regarding robustness and clarity:

1.  **Hardcoded File Path/Functionality:** The ground truth wraps the logic in a function and includes a `try-except` block for `FileNotFoundError` as well as a general `Exception`. The generated code has a `try-except` for `FileNotFoundError` but doesn't wrap the core logic in a function, making it less reusable.
2.  **Legend Clarity for HeartDisease:** While the generated code correctly displays the `HeartDisease` legend as 0 and 1, the ground truth code explicitly maps these to 'No Heart Disease' and 'Heart Disease'. This makes the legend much more intuitive and user-friendly, fulfilling the spirit of clear labeling. The generated code comments on this but doesn't implement it.
3.  **Missing `ChestPainType` Ordering:** The ground truth explicitly orders the `ChestPainType` categories (`TA`, `ATA`, `NAP`, `ASY`). While `seaborn` often sorts alphabetically by default if the column isn't categorical, explicitly defining the order ensures consistency and correctness, especially if the default sort doesn't match the desired order or data description. This was not addressed in the generated code.

Overall, it's a very good attempt that functionally meets most requirements but could be improved with better error handling, a more user-friendly legend, and explicit handling of categorical order.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: 'spotify_2023.csv' not found. Make sure the file is in the current directory."")
    exit()

# 1) Clean the 'in_shazam_charts' column
# Remove commas
df['in_shazam_charts'] = df['in_shazam_charts'].astype(str).str.replace(',', '')
# Convert to numeric, coercing errors to NaN
original_nan_count = df['in_shazam_charts'].isna().sum()
df['in_shazam_charts'] = pd....",9,"The generated code correctly performs all the required tasks: loading the CSV, cleaning and converting the `in_shazam_charts` column (including reporting NaNs), grouping by `key` and `mode`, and calculating the average `danceability_%` and `valence_%`.

**Strong Points:**
- Correctly uses `pd.read_csv`.
- Effectively cleans the `in_shazam_charts` column by removing commas and converting to numeric with `errors='coerce'`.
- Accurately reports NaNs created during the conversion, although the phrasing is slightly different from the ground truth (""Number of NaNs created"" vs ""Count of NaNs in ... after cleaning and coercion""). Both are valid interpretations of ""report count of NaNs"".
- Correctly groups by `key` and `mode` and calculates the mean for the specified columns using `agg`, which is a clean way to rename output columns.

**Minor Improvement/Difference:**
- The prompt asked for ""report count of NaNs"", and the ground truth reported the total count of NaNs in the column after the operation. The generated code reports the *number of new NaNs created* during the `pd.to_numeric` step. While this is useful information, and technically a ""count of NaNs"", it's slightly different from the total count after the process. Both interpretations are reasonable, but the ground truth's approach of reporting the final total NaN count is perhaps a more direct answer to ""report count of NaNs"" in the column. This is a minor point and doesn't detract significantly from the functionality.

Overall, the code is very high quality and effectively solves the problem.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 1. Load the dataset
try:
    df = pd.read_csv('spotify_2023.csv')
except FileNotFoundError:
    print(""Error: spotify_2023.csv not found. Make sure the file is in the current directory."")
    exit()

# 2. Ensure the `streams` column is numeric (coerce errors to NaN, then drop rows with NaN)
# The 'streams'...",9,"The generated code is very good and largely fulfills all the requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly loads the dataset, handles the `streams` column conversion and NaN values, selects the correct features and target, splits the data, trains a Linear Regression model, and evaluates it using R2 and MSE.
- **Completeness:** All steps outlined in the prompt are addressed.
- **Code Quality:** Proper imports are used. The variable naming is clear (`features`, `target`, `df_model_ready`). The `try-except` block for file loading is good practice. The use of `astype(str)uture`.str.replace(r'[^\d.]', '', regex=True)` for `streams` is a good proactive step to handle potential non-digit characters, although the prompt only asked for `coerce` errors to NaN.

**Minor Improvements:**
- The prompt explicitly stated to ""drop rows with NaN in `streams` or predictor columns"". The generated code correctly uses `df.dropna(subset=[target] + features)`. This is a slight deviation from the ground truth which defines `model_columns` and then uses `dropna` on that. Both approaches achieve the same functional outcome of dropping NaNs in the relevant columns for the model. The generated code's approach is arguably more direct for the specified task since it immediately drops NaNs based on the columns actually used for the model.
- The prompt implicitly assumes that `in_spotify_playlists` and `energy_%` are already clean or that any NaNs in them should be handled by the same `dropna` call. The generated code handles this correctly by including them in the `subset` for `dropna`.
- The ground truth includes `print` statements for better user feedback during execution. While not explicitly asked for, adding these can improve user experience, especially in a benchmark context. For example, printing the R2 and MSE at the end is a requirement, which is done.

Overall, an excellent submission. The added `str.replace` for `streams` is a thoughtful addition to handle more complex data cleaning scenarios, even if not strictly required by the prompt's explicit phrasing.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# 1) Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 2) Combine released_year, released_month, and released_day into a single datetime column
# named release_date. Handle potential errors during conversion by setting invalid dates to NaT.
date_string_series = df['released_year'].astype(str) + '-' + \
                     df['released_month'].astype(str) + '-' + \
                     df['released_day'].astype(str)

df['release_date'] = pd.to_datetime(date_...",9,"The generated code is highly effective and largely correct in addressing the task requirements.

**Strong points:**
*   **Correctness:** It correctly loads the data, combines date components into a `release_date` column using `pd.to_datetime` with `errors='coerce'` (crucial for handling invalid dates), accurately calculates `days_since_release` using a fixed reference date, and displays the required columns for the first 5 rows.
*   **Error Handling (Implicit):** The `errors='coerce'` in `pd.to_datetime` correctly handles invalid dates by converting them to `NaT`, which then propagates to `days_since_release` as `NaN`.
*   **Integer Type for `days_since_release`:** The use of `astype('Int64')` correctly assigns a nullable integer type, fulfilling the requirement for `days_since_release` to be an integer while still accommodating `NaN` values that arise from `NaT` in `release_date`.
*   **Conciseness:** The code is quite concise and readable.

**Areas for minor improvement (why it's not a 10):**
*   **Robustness of Date String Creation:** The `date_string_series` creation (`df['released_month'].astype(str) + '-' + df['released_day'].astype(str)`) is functional but doesn't include zero-padding for month/day (e.g., '1' vs '01'). While `pd.to_datetime` is often smart enough to parse these, explicit zero-padding (`.str.zfill(2)`) as shown in the ground truth makes the date string format more robust and consistent, especially if a less flexible parser were used or if the string were inspected directly. For this specific task, `pd.to_datetime` with `errors='coerce'` handles it well enough, so it's a minor point.
*   **No File Not Found Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While the prompt only asked for ""proper imports (pandas)"" and assumed the file is in the current directory, robust code for a data science benchmark typically includes this. This is a very minor omission given the specific constraints of the prompt, but it's good practice.

Overall, the code performs very well against the requirements.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: youtube_trendings.csv not found. Please ensure the file is in the current directory."")
    exit()

# 1) Process 'Video views' and 'Likes' columns
for col in ['Video views', 'Likes']:
    if col in df.columns:
        # Remove commas
        df[col] = df[col].astype(str).str.replace(',', '', regex=False)
        # Convert to numeric, coercing errors to N...",9,"The generated code correctly implements all specified requirements: loading the CSV, cleaning 'Video views' and 'Likes' columns by removing commas, converting to numeric, and filling NaNs with 0. It also correctly fills missing 'Category' values with 'Unknown'. The final grouping and summation by category for views and likes is also accurate. The code includes proper imports and handles `FileNotFoundError`.

Minor areas for improvement:
1. The handling for missing 'Category' column: While it correctly prints a warning and creates a dummy 'Unknown' column, this might lead to unexpected behavior if the original data truly lacked a column meant for categorization. The ground truth's approach of checking for the column before grouping and aggregating is slightly safer as it reports an error if a crucial column for the task is missing.
2. Error/Warning messages: The printed warnings for missing columns are good, but the ground truth handles the aggregation step more robustly by checking for the existence of both 'Video views'/'Likes' and 'Category' columns before attempting `groupby` and `sum()`, providing more specific error messages if either is missing. The generated code's final `else` statement for missing columns is less specific.

Overall, the code is very good and functionally achieves the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

# 1. Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: 'youtube_trendings.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2. For the Likes column: remove any commas, convert to numeric, handle failed conversions, fill NaNs
# Remove commas and convert to numeric, coercing errors to NaN
df['Likes'] = df['Likes'].astype(str).str.replace(',', '', regex=...",9,"The generated code is very good and accurately addresses almost all aspects of the task.

**Strong points:**
*   **Correctness:** Loads data, correctly cleans the 'Likes' column (removes commas, converts to numeric, handles errors with `coerce`, fills NaNs with 0), fills missing 'Category' values, correctly splits data into 'Music' and 'Sports' groups, and performs the `ttest_ind` with `equal_var=False`.
*   **Hypotheses:** Clearly states the null and alternative hypotheses.
*   **Results & Interpretation:** Correctly prints the t-statistic and p-value, and provides a correct interpretation based on the given alpha level.
*   **Error Handling:** Includes `FileNotFoundError` handling and checks for empty groups before performing the t-test.
*   **Imports:** Uses the correct imports (`pandas`, `scipy.stats`).

**Areas for minor improvement:**
*   **Clarity in `Likes` Cleaning:** The ground truth code used `df['Likes_cleaned'] = ...` to create a *new* column for the cleaned `Likes` values. While the generated code correctly operates *in-place* on the original `Likes` column (which is fine), creating a new column can sometimes be clearer for debugging and ensuring original data integrity is maintained, especially if the original column format might be useful for other tasks. However, for this specific task, modifying in-place is acceptable.
*   **Numbered Comments:** The generated code directly incorporates the requirements as numbered comments (2. for Likes cleaning, 3. for Category, 4. for groups, etc.), which is good. However, the output formatting slightly deviates from the ground truth's verbose printing around each step. This is a minor aesthetic difference and doesn't impact correctness.
*   **Equal Variance Assumption Comment:** While `equal_var=False` is correctly used, the comment about ""Walch's t-test"" is a slight misspelling (it's Welch's t-test).

Overall, the solution is highly competent and provides a complete and correct implementation of the requirements. The minor points are mostly stylistic or very subtle, not critical correctness issues.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd

# 1. Load the dataset
try:
    df = pd.read_csv('youtube_trendings.csv')
except FileNotFoundError:
    print(""Error: 'youtube_trendings.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2. For the Likes and Dislikes columns:
#    - remove any commas
#    - convert them to numeric types (e.g., float)
#    - if conversion fails for any value, treat that value as NaN
#    - then fill all resulting NaNs in these columns with 0

# Clean 'Li...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It loads the dataset, cleans the 'Likes' and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0. It then correctly calculates 'TotalEngagement' and 'LikeDislikeRatio' as specified. Finally, it displays the required columns.

The use of `fillna(0)` and `pd.to_numeric(errors='coerce')` is appropriate for handling missing and non-numeric values. The addition of `+ 1` in the 'LikeDislikeRatio' calculation correctly prevents division by zero.

The only minor area for improvement is that the generated code creates new columns `Likes_cleaned` and `Dislikes_cleaned` instead of overwriting the original 'Likes' and 'Dislikes' columns as the ground truth does. While functionally correct, the task implies modifying the original columns or creating new ones with the same explicit name, not necessarily distinct `_cleaned` suffixes. However, this is a style choice more than a functional error. The ground truth code also includes more robust error handling for file loading, which is good practice but not strictly required by the prompt instructions regarding feature engineering.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 1. Clean 'Launched Price (USA)' column
# Remove 'USD ' and commas, then convert to numeric, coercing errors to NaN
df['Launched Price (USA)'] = df['Launched Price (USA)'].astype(str).str.replace('USD ', '', regex=False).str.replace(',', '', regex=False)
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

# 2. Clean 'Screen Size' column
# Remove ' inches' and convert to n...",9,"The generated code successfully completes all aspects of the task, including loading the CSV, cleaning both specified columns, handling conversion errors to NaN, grouping by the correct columns, and calculating the specified averages. The code is concise and efficient.

One minor point for improvement: The generated code overwrites the original `Launched Price (USA)` and `Screen Size` columns with their cleaned, numeric versions. While this is functionally correct for this task, best practice often involves creating new columns for cleaned data (e.g., `Cleaned Price (USD)`) to preserve the original raw data, especially in more complex data pipelines where the original string values might be needed later or for verification. The ground truth code follows this practice. However, given the specific prompt, overwriting is acceptable here as the task doesn't explicitly forbid it or require preserving original columns. It also lacks error handling for file loading, but the prompt implicitly assumes the file exists. Overall, it's a very good solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def clean_camera_mp(camera_spec):
    """"""
    Cleans a camera specification string to extract the first sequence of digits as MP value.
    Returns 0 if no digits are found or input is not a string.
    """"""
    if pd.isna(camera_spec):
        return 0
    spec_str = str(camera_spec)
    match = re.search(r'(\d+)', spec_str)
    if match:
        return int(match.group(1))
    return 0

# 1) Load the dataset
try:
    df = pd.read_csv('mobiles_2025.csv')
e...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, defines and applies the cleaning function to extract MP values, handles missing values gracefully, creates the 'Total Camera MP' feature, and displays the specified columns for the first 10 rows. The `clean_camera_mp` function correctly uses regex to extract the first sequence of digits.

The only minor point of improvement is that the regex `r'(\d+)'` uses a capturing group `(\d+)` but then refers to `match.group(1)`. While this works, `match.group(0)` would also work and is slightly more direct as it refers to the entire matched string. However, this is a very minor stylistic point and does not affect correctness. The ground truth code includes more extensive print statements for user feedback during execution, which is good practice for more complex scripts, but not strictly required by the prompt here. Its absence does not detract from the core functionality or correctness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1) Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean specified columns and handle conversion errors
columns_to_process = ['RAM', 'Battery Capacity', 'Screen Size', 'Launched Price (USA)']

# Clean RAM
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '', regex=False)
df['RAM'] = pd.to_nu...",9,"The generated code correctly addresses all requirements of the task: it loads the dataset, cleans the specified columns (handling conversion errors and dropping NaNs as requested), defines features and target, splits the data, trains a Linear Regression model, and reports the R2 and MAE scores. The imports are correct, and the code structure is logical and easy to follow.

Minor points for improvement (not critical but good practice):
1.  **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While not explicitly asked for, it's good practice for robust code. The generated code assumes the file will always be present, which could lead to a crash if it's not.
2.  **Redundancy in Cleaning `Launched Price (USA)`:** The `astype(str)` call is repeated before the second `replace` for 'Launched Price (USA)'. While harmless, it's not strictly necessary.
3.  **Output Formatting:** The output `print` statements could be a bit more verbose (e.g., ""Model Evaluation Results:"") as in the ground truth, but this is a stylistic choice.

Overall, it's a very good solution that perfectly accomplishes the core task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency, ttest_ind, f_oneway

# Load the dataset
df = pd.read_csv('titanic.csv')

# Set alpha level for hypothesis testing
alpha = 0.05

print(""--- Statistical Analysis and Hypothesis Testing on Titanic Dataset ---"")
print(f""Alpha level (α): {alpha}\n"")

# 1. Chi-square test: Association between Pclass and Survival
print(""1. Chi-square test: Association between Pclass and Survival"")
# Create a contingency table
con...",8,"The generated code correctly performs all the requested statistical tests (chi-square, t-test, ANOVA) and calculates their respective effect sizes (Cramer's V, Cohen's d, Eta-squared). It also interprets the results based on the p-value and the specified alpha level.

Strengths:
- Correct application of `chi2_contingency`, `ttest_ind`, and `f_oneway`.
- Correct calculation of Cramer's V, Cohen's d, and Eta-squared.
- Clear conclusion statements based on p-value and alpha.
- Handles missing 'Age' values appropriately for the t-test.
- Proper imports and variable naming.

Areas for Improvement (to reach a higher score):
- **Robustness in Chi-square's Cramer's V calculation:** The ground truth includes a check `if (n * min_dim) > 0 else 0` to prevent division by zero in `cramers_v` calculation, which is a good practice. While unlikely for this dataset, it's good to be robust.
- **T-test equal_var assumption:** The generated code defaults to `equal_var=False` (Welch's t-test), which is generally a safer and often recommended approach when you can't be certain about variance equality. However, the ground truth explicitly performs Levene's test to *decide* whether to assume equal variances or not, which demonstrates a deeper understanding of the assumptions of the t-test and how to conditionally apply `equal_var`. This is a more complete and statistically rigorous approach.
- **Missing File Handling:** The generated code doesn't include a `try-except` block for `pd.read_csv`, which means it will crash if `titanic.csv` is not found, unlike the ground truth.
- **Readability/Structure:** While generally good, the ground truth uses more extensive comments and print statements to clearly delineate each section, explain hypotheses, and justify certain steps (e.g., Levene's test), enhancing its educational value and clarity for someone reading the code.
- **Handling of edge cases/warnings**: For ANOVA, if Levene's test indicates unequal variances, it's good practice to print a warning as it suggests that the assumptions for ANOVA might be violated and results should be interpreted with caution, potentially requiring alternative methods. The ground truth includes this.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: 'imdb_top_250.csv' not found. Please ensure the file is in the current directory."")
    exit()

# Group by Year and calculate average Rating and Runtime
yearly_avg = df.groupby('Year').agg(
    Avg_Rating=('Rating', 'mean'),
    Avg_Runtime=('Runtime', 'mean')
)

# Create a line plot for average Rating over the years
plt.figur...",6,"The generated code correctly addresses the core requirements: it loads the CSV, groups by `Year`, calculates average `Rating` and `Runtime`, and plots the average `Rating` over `Year` with a title and labels. The plotting setup (fig size, grid, xticks rotation) is also good.

However, it is missing crucial data cleaning steps that are often necessary when dealing with real-world datasets, especially for the `Year` and `Runtime` columns. The ground truth includes robust cleaning for these columns to ensure numerical conversion and handle potential non-standard formats (e.g., `'Year'` might contain `(2000)` or `'Runtime'` might contain `120 min`). Without this, the code is brittle and likely to fail or produce incorrect results if the CSV is not perfectly clean. This significantly impacts its robustness and generalizability. While the prompt doesn't explicitly state ""clean data,"" a robust solution for a data science task generally anticipates common data quality issues.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# 1. Load the dataset
try:
    df = pd.read_csv('imdb_top_250.csv')
except FileNotFoundError:
    print(""Error: imdb_top_250.csv not found. Please ensure the file is in the current directory."")
    exit()

# Ensure 'Year' and 'Rating' columns exist and are of appropriate types
if 'Year' not in df.columns or 'Rating' not in df.columns:
    print(""Error: 'Year' or 'Rating' column missing in the dataset."")
    exit()

# Convert 'Year' to numeri...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading and Handling:** It correctly loads the CSV, and crucially, handles `FileNotFoundError` and converts the `Year` column to numeric, including handling `coerce` errors which is important for robustness. It also correctly drops rows where `Year` or `Rating` become NaN.
- **Correct Grouping:** The creation of `group_before_1990` and `group_in_or_after_1990` based on the `Year` column is accurate.
- **Correct T-test Application:** It correctly uses `scipy.stats.ttest_ind` and captures the t-statistic and p-value.
- **Hypotheses Statement:** The null and alternative hypotheses are clearly and correctly stated.
- **Interpretation:** The interpretation of the p-value against the alpha level is correct, providing a clear conclusion based on statistical significance.
- **Robustness Checks:** The code includes checks for `Year` or `Rating` columns missing and, very importantly, checks if groups have sufficient data (at least 2 samples) for a meaningful t-test, which is excellent for robustness.
- **Readability:** The code is well-structured with clear comments indicating each step as per the task requirements.

**Areas for Minor Improvement (Why not a 10):**
- **Implicit `Rating` Conversion:** The ground truth explicitly converts `df['Rating']` to numeric, which is good practice. The generated code relies on `df.dropna(subset=['Year', 'Rating'], inplace=True)` to handle potential non-numeric `Rating` values in conjunction with the `Year` conversion. While it *might* work if `Rating` is already numeric or `dropna` catches conversion issues, explicitly converting `Rating` to numeric as `pd.to_numeric(df['Rating'], errors='coerce')` provides more control and clarity.
- **`equal_var` Parameter:** The generated code uses `equal_var=False` (Welch's t-test) and states ""We assume unequal variances (Welch's t-test) as it's often more robust."" While this is a valid and often preferred assumption in practice, the prompt did not specify which variance assumption to make. The ground truth explicitly used `equal_var=True` and mentioned the implications. For a benchmark where exact replication might be implicitly expected or where the prompt is silent on optional parameters, mimicking the ground truth's assumption (or explicitly discussing both) would be slightly better for a perfect score, although the choice of `equal_var=False` is statistically defensible and robust.
- **Order of Operations:** The generated code prints hypotheses before creating the groups, which is a minor stylistic difference from the ground truth and doesn't affect correctness but might be slightly less intuitive logically (i.e., you define the groups first, then pose hypotheses *about* those groups).

Overall, this is an excellent solution that demonstrates a strong understanding of the task and robust coding practices.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create the 'Decade' column
df['Decade'] = ((df['Year'] // 10) * 10).astype(str) + 's'

# Display the 'Title', 'Year', and 'Decade' for the first 10 movies
print(df[['Title', 'Year', 'Decade']].head(10))
```",9,"The generated code correctly loads the dataset, performs the feature engineering to create the `Decade` column as specified, and displays the required columns for the first 10 rows. All primary requirements of the task are met. The code is concise and directly addresses the problem.

Strong points:
- Correctly loads the CSV.
- Successfully implements the `Decade` calculation using integer division and string concatenation.
- Correctly displays the requested columns (`Title`, `Year`, `Decade`) for the first 10 entries using `head(10)`.

Minor points for improvement (not critical for the task but good practice):
- The `Year` column is assumed to be numeric and clean. In a real-world scenario, it's good practice to handle potential non-numeric values in the 'Year' column (e.g., using `pd.to_numeric` with `errors='coerce'`) before performing arithmetic operations, as shown in the ground truth. However, for a benchmark assuming clean data, this is acceptable.
- No error handling for `FileNotFoundError`.
- No function encapsulation, which while not explicitly asked for, generally leads to more modular and reusable code (as seen in the ground truth).
- The `print` output format is default pandas `head()` output, which includes the index. The ground truth explicitly removes the index and prints with `to_string(index=False)` for a cleaner output, which is a very minor aesthetic point.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Please make sure the file is in the current directory."")
    exit()

# 2) For numerical columns, impute missing values with the mean
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_cols:
    if df[col].isnull().any():
        mean_val = df[col].mean()
        df[col].fil...",9,"The generated code correctly addresses all parts of the task: loading the dataset, imputing missing numerical values with the mean, imputing missing `sex` values with the mode, grouping by `species` and imputed `sex`, and calculating the average `body_mass_g`. The code structure is clear and follows the prompt's steps.

A minor detail for improvement is the conditional check for `sex` column mode calculation:
`if not df['sex'].dropna().empty:`
This additional check is robust but might be slightly overcomplicated for a general case where the `mode()` method itself handles an empty series if the column is entirely NaN (though it would then return an empty Series, and `[0]` would indeed error). However, the ground truth's simpler `df[CATEGORICAL_COL_MODE].mode()[0]` relies on the assumption that there will always be at least one non-NA value to derive a mode, which is reasonable for standard datasets. The generated code also includes a fallback to 'unknown' if the sex column is entirely NaN, which is a good defensive programming practice, although not explicitly required by the prompt's expectation for this specific problem (which implies valid data for mode calculation).

Overall, it's a very good solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# 1. Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Please ensure the file is in the current directory."")
    exit()

# 2. Filter the data to include only records where species is 'Adelie' or 'Gentoo'
filtered_df = df[df['species'].isin(['Adelie', 'Gentoo'])].copy()

# 3. Within this filtered dataset, remove any rows where flipper_length_mm or species is missin...",9,"The generated code is very good and mostly addresses all requirements correctly.

**Strong points:**
*   **Correctness:** Loads the data, filters correctly, handles NAs, performs the t-test, states hypotheses, prints results, and interprets them based on the given alpha.
*   **Completeness:** Addresses most of the specific steps outlined in the prompt (loading, filtering, NA removal, t-test, hypotheses, results, interpretation).
*   **Code Quality:** Imports are correct, variable names are clear, and the code follows a logical flow. The use of `.copy()` when filtering is also good practice to avoid `SettingWithCopyWarning`.
*   **Error Handling:** Includes a basic `FileNotFoundError` check, which is a good minimal effort for robustness.

**Areas for improvement (minor points):**
*   **`ttest_ind` parameter:** The `ttest_ind` function by default assumes equal variances (`equal_var=True`). The prompt does not explicitly state which type of t-test (assuming equal or unequal variances) should be performed. The ground truth code specifically opted for `equal_var=False` (Welch's t-test) for robustness, which is generally a safer default in real-world scenarios unless variance equality is confirmed or assumed for specific reasons. While the generated code's default is still a valid independent t-test, explicitly setting `equal_var=False` would align with a more robust statistical approach often preferred when the assumption of equal variances might not hold. This is a subtle statistical point, not a direct code error, but worth noting in the context of expert evaluation.
*   **Hypothesis Detail:** The ground truth includes the mathematical representation of the hypotheses (e.g., ""μ_Adelie = μ_Gentoo""), which adds clarity and precision. The generated code's statements are correct, but could be enhanced with this detail.
*   **Minor formatting/output:** The ground truth added more verbose print statements about dataset shapes and NA removal counts, which can be useful for debugging and understanding data transformations. This was not strictly required by the prompt but adds to the overall utility of the script as an analytical tool. The generated code's hyphens (`-` * 70) are fine for separation but less informative than descriptive print statements.

Overall, the generated code is a very solid solution that fulfills the core requirements effectively. The points for improvement are minor enhancements rather than critical errors.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Define features X and target y
feature_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
target_column = 'species'

# Combine all relevant columns for missing value check
column...",9,"The generated code successfully completes all the requirements of the task. It correctly loads the data, selects the specified features and target, handles missing values by dropping rows with NAs in the relevant columns, encodes the target variable, splits the data into training and testing sets with the correct proportion and `random_state`, trains a Logistic Regression model with `max_iter=1000`, and reports the accuracy score. The imports are proper, and the code structure is clear and follows the requested steps.

The only minor area for improvement (and why it didn't get a perfect 10) is the lack of basic error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth. While not explicitly asked for in the prompt, it's good practice for robust code. However, given the prompt's focus on data preparation and model training, its omission doesn't detract from the core task fulfillment.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Make sure it's in the current directory."")
    exit()

# 2) Drop rows where `body_mass_g` or `species` is NA
# Using .copy() to avoid SettingWithCopyWarning if subsequent operations modified the DataFrame
df_cleaned = df.dropna(subset=['body_mass_g', 'species']).copy()

# 3) For eac...",10,"The generated code successfully completes all aspects of the task. It correctly loads the dataset, handles NA values in the specified columns by dropping rows, accurately calculates the average body mass per species, and generates a clear bar plot with a title and labeled axes. The use of `try-except` for file loading is good practice, and `dropna(subset=...)` specifically targets the required columns. The plot is well-formatted and uses Seaborn effectively. The code quality is high, with clear variable names and logical flow.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: 'penguins.csv' not found. Make sure the file is in the current directory."")
    exit()

# 2. Drop rows where specified columns are NA
columns_to_check_na = [
    'culmen_length_mm', 
    'culmen_depth_mm', 
    'flipper_length_mm', 
    'body_mass_g'
]
df_cleaned = df.dropna(subset=columns_to_check_na).cop...",10,"The generated code successfully addresses all requirements of the task. It correctly loads the dataset, handles NA values in the specified columns, creates the `culmen_area` feature, calculates the Pearson correlation matrix for the required columns, and visualizes it as an annotated heatmap with the specified title. Error handling for file not found and missing correlation columns is also included. The code is well-structured, uses appropriate libraries, and includes comments explaining each step. The use of `.copy()` to avoid `SettingWithCopyWarning` is good practice, and the `matplotlib` adjustments for better readability (figure size, rotation, tight layout) are also well-implemented.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
try:
    df = pd.read_csv('penguins.csv')
except FileNotFoundError:
    print(""Error: penguins.csv not found. Please ensure the file is in the current directory."")
    exit()

# 2) Select the features and target variable
# Using .copy() to ensure we're working on a separate DataFrame and avoid SettingWithCopyWarning
features_to_select = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target_variable...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
*   Correctly loads the dataset and handles `FileNotFoundError`.
*   Correctly selects the specified features and separates the target variable.
*   Correctly imputes numerical NA values with the mean for the specified columns. It uses `X.loc[:, col] = X[col].fillna(mean_val)` which is a robust way to avoid `SettingWithCopyWarning`.
*   Correctly imputes 'sex' NA values with 'Unknown'.
*   Correctly maps 'MALE', 'FEMALE', and 'Unknown' (including '.' as a bonus for robustness) to the specified numerical values.
*   Correctly applies one-hot encoding to the 'island' column.
*   Prints the shape and the head of the processed feature matrix as requested.

**Areas for Improvement / Minor Issues:**
*   **Target column processing:** The task explicitly stated ""The code should...select the features...The target variable is `species`"". While it separates `y = df[target_variable].copy()`, the prompt then asks ""Print the shape of the resulting *feature matrix* and the first 5 rows of the *processed features*."" This implies `y` should be dropped from the `X` ultimately presented. The generated code selects `X = df[features_to_select].copy()` which correctly includes only features, but it doesn't then show the final `X` *after* all transformations (imputation, encoding) that would lead to a common `X` that typically contains `island` and `sex` processed. It shows `X` after processing, but the initial selection of `X` is already only the features. The ground truth code `df_processed.drop(columns=[target_variable])` is a clearer way to achieve a final feature matrix *after* all processing steps have been applied to the combined dataframe. However, given `X = df[features_to_select].copy()` and processing `X` directly, the output is indeed the processed features. This is more of a style difference than a functional error for the output request.
*   **Clarity on `sex` mapping:** The comment `Note: Some datasets might contain '.' as a gender value, which is not NA. We'll include '.' in the mapping to -1 as it represents an unspecified sex.` is good, but the task only specified 'MALE', 'FEMALE', and 'Unknown'. Including '.' is an acceptable enhancement for robustness, but it wasn't explicitly asked for, so it's a minor deviation (not an error).
*   **Drop First for One-Hot Encoding**: While not explicitly mandated by the prompt, it's common practice in classification tasks to use `drop_first=True` with `pd.get_dummies` to avoid multicollinearity. The prompt didn't specify, so `drop_first=False` is not incorrect for the given instructions.

Overall, the code is very good and functionally solves the problem with only minor stylistic and best-practice considerations.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# 1. Create the pandas DataFrame
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002',...",9,"The generated code correctly creates the DataFrame, calculates total spending per customer, identifies the top 3 customers by spending, and calculates total quantity sold per category. All imports are correct, and the code structure is logical and easy to follow.

The only minor area for improvement is in the printing format for the ""Top 3 Customers by Total Spending"". While `print(top_3_customers)` is correct, the ground truth code used a loop (`for customer_id, total_spending in top_3_customers.items(): print(f""CustomerID: {customer_id}, Total Spending: ${total_spending:,.2f}"")`) which provides a slightly more readable and formatted output, indicating each customer ID and their spending clearly. This is a very minor stylistic difference and does not impact correctness or functionality. However, the problem statement did specify ""Identify and print the 'CustomerID' and their total spending..."", which the ground truth's loop fulfills more explicitly.

Overall, the generated code is excellent and fully addresses the core requirements of the task.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, ConfusionMatrixDispl...",8,"The generated code successfully addresses most of the requirements.

**Strong Points:**
- **Correct Data Preprocessing:** Handles missing values for 'Age' and 'Embarked' appropriately, drops irrelevant columns, and uses `ColumnTransformer` with `StandardScaler` and `OneHotEncoder` correctly within a `Pipeline`.
- **Proper Model Training and Evaluation:** Uses a `Pipeline` for end-to-end processing and modeling, making the code clean and robust. Evaluates the model using all requested metrics (accuracy, precision, recall, F1, ROC-AUC).
- **Comprehensive Visualizations:** Successfully generates both a Confusion Matrix and an ROC Curve using `ConfusionMatrixDisplay` and `RocCurveDisplay`.
- **Feature Coefficients Interpretation:** Attempts to extract and display coefficients, and provides a good interpretation for them.

**Areas for Improvement:**
- **Feature Engineering Not Explicitly Done:** The ground truth code includes feature engineering (`FamilySize`) as a step before defining X and y and the preprocessor. The generated code skips this step, which is a subtle but important difference in preprocessing completeness. The `numerical_features` and `categorical_features` are inferred based on dtypes, which might include `SibSp` and `Parch` if not dropped or engineered. The ground truth explicitly drops them after adding `FamilySize`.
- **Feature Name Retrieval for Coefficients:** The way `all_feature_names` is constructed for coefficient display is incomplete. `model_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)` only gets the one-hot encoded names, but `numerical_features` are just the original column names. After passing through `StandardScaler` in the `ColumnTransformer`, the `numerical_features` technically retain their names, but combining them directly with the OHE names in `all_feature_names = numerical_features + ohe_feature_names.tolist()` might lead to an incorrect order or mismatch if the column order from the `ColumnTransformer` isn't precisely maintained for numerical features. The ground truth's `get_feature_names_out()` directly from the *fitted* preprocessor is more robust for obtaining all final feature names, including transformed numerical ones and one-hot encoded ones, in the correct order. This is a common pitfall when extracting coefficients from pipelines.
- **Cross-Validation Scope:** The generated code only performs `cross_val_score` for accuracy. The ground truth uses `cross_validate` to get multiple metrics from cross-validation (accuracy, precision, recall, f1, roc_auc), which is more thorough for assessing model stability across different metrics.
- **Minor Interpretation Detail:** The interpretation of coefficients could explicitly mention the concept of log-odds, as in the ground truth.

Overall, the generated code is very good and covers most of the major requirements. The subtle points around feature engineering, precise feature name extraction for coefficients, and more comprehensive cross-validation are areas where it could be improved to perfectly match the ground truth's robustness and completeness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# 1. Create the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
    'Ticker': ['TECHCO']*8,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0,...",10,"The generated code is excellent. It perfectly meets all the requirements of the task:
1.  **Correctness**: The DataFrame is created correctly with 'Date' as the index. The 3-day SMA for 'Close' price is calculated accurately, and the `rolling(window=3).mean()` method correctly produces `NaN` values for the initial days, as specified. The daily percentage change for 'Volume' is also correctly calculated using `.pct_change()`, which inherently places `NaN` for the first day.
2.  **Code Quality**: The code is clean, well-structured, and includes the necessary `pandas` import. Variable names are appropriate, and the steps are logically ordered.
3.  **Completeness**: All parts of the prompt are addressed: DataFrame creation, setting the index, calculating both the SMA and volume percentage change, and printing the final DataFrame with the new columns. The handling of `NaN` values for initial days/first day is also spot on.

No improvements are needed for this solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# 1. Create the pandas DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, ...",9,"The generated code correctly creates the DataFrame, calculates the 'Score_Improvement' column, calculates the average improvement per subject, and identifies the students meeting the specified criteria. The imports are correct, and the code structure is logical.

The only minor area for improvement is in the printing of the student IDs for the last requirement. While converting to a list and printing is functionally correct, directly printing the series or iterating through it (as in the ground truth) can sometimes be slightly cleaner or more robust if no students meet the criteria (though `tolist()` still works for an empty series). However, this is a very minor stylistic point and does not detract from the correctness or completeness of the solution. The ground truth includes additional print statements for clarity and separation, which the generated code mostly omits, making its output slightly less verbose but still perfectly understandable.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

# Create the pandas DataFrame
df = pd.DataFra...",9,"The generated code correctly defines the DataFrame, filters it accurately based on the specified conditions (""Department"" == ""Tech"" and ""PerformanceScore"" > 3), and then correctly calculates and prints the average ""ProjectsCompleted"" for this filtered group. The code is clean, well-structured, and directly addresses all parts of the prompt.

One minor improvement, though not critical for correctness in this specific case, would be to add a check for an empty `filtered_employees` DataFrame before attempting to calculate the mean. If no employees met the criteria, `mean()` would return `NaN`, which might not always be the desired output (e.g., an explicit message like ""No employees found matching the criteria"" might be more informative). However, for the given data, there are matching entries, so this edge case doesn't cause an issue here.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# 1) Calculate the conversion rate for users on ""Desktop""
de...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the conversion rates for Desktop and Mobile users separately. It adheres to all requirements of the task.

**Strong points:**
*   Correctly initializes the Pandas DataFrame based on the provided data.
*   Accurately filters the DataFrame for 'Desktop' and 'Mobile' users.
*   Correctly calculates the conversion rate using `sum()` for converted users and `len()` for total users, then performing the division and multiplication by 100. This is an entirely valid, though less concise, alternative to `.mean()`.
*   Includes robust handling for edge cases where `desktop_total_users` or `mobile_total_users` might be zero, preventing division by zero errors.
*   Prints the results clearly labeled and formatted to two decimal places, as requested.
*   Properly uses `f-strings` for output.

**Minor areas for improvement (not critical but good practice):**
*   The ground truth code uses `df['Converted'].mean() * 100` which is a more concise and idiomatic pandas way to calculate the percentage of `True` values in a boolean series. While the generated code's approach is correct, using `.mean()` can be slightly more efficient for very large datasets and is generally preferred for its expressiveness in pandas.

Overall, this is a very good solution that fully addresses the problem.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

# 1. Define the pandas DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Convert 'Timestamp' to datetime objects (opt...",10,"The generated code is excellent. It correctly defines the DataFrame as specified in the task using the provided data. It then accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"" using the `groupby()` and `mean()` methods, which is the standard and most efficient pandas way to achieve this. The output is clear, well-formatted, and explicitly states which average belongs to which sensor, fulfilling all requirements.

One minor addition, though not strictly required by the prompt, is the conversion of the 'Timestamp' column to datetime objects. While not essential for THIS specific task (only `groupby` and `mean` on `AirQualityIndex`), it demonstrates good practice for handling time series data, which is a common use case with sensor readings. This doesn't detract from its score for the given task and actually shows a slightly more thoughtful approach to data preparation.

The comments are clear and helpful, aligning with good coding practices. Overall, it's a perfect solution to the prompt.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# 1) Create a new feature ""EngagementScore""
df[""EngagementScore...",9,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"", filters for Instagram posts, and accurately computes and prints the average engagement score for those posts. It addresses all requirements of the task. The code quality is good, with proper imports and clear variable names.

One minor improvement would be to include a check for an empty `instagram_posts_df` before attempting to calculate the mean, as done in the ground truth code. While in this specific dataset `instagram_posts_df` will never be empty, in a more general scenario, an empty DataFrame would raise an error when calculating the mean. This makes the code more robust. The additional print statements providing the full DataFrame and the filtered DataFrame are helpful for debugging but not strictly required by the prompt's final output, though they don't detract from correctness.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",10,"The generated code correctly defines the DataFrame, calculates the ""LengthOfStay"" column, filters the DataFrame for ""Flu"" patients, and accurately calculates and prints their average ""LengthOfStay"". The code is well-structured, uses appropriate pandas functionalities, and addresses all aspects of the task. The comments are helpful, although the ground truth provides a bit more detail in its comments. The use of f-strings for printing the output is good practice. The `.copy()` in the ground truth is good for avoiding `SettingWithCopyWarning`, but the generated code's approach is acceptable as it doesn't modify the filtered DataFrame in place afterward. Overall, it's an excellent solution.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}

# Create the pan...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating, fulfilling all task requirements. The code structure is clear, and it follows good practices for DataFrame manipulation. The only minor improvement would be to include handling for the edge case where no Sci-Fi movies might be found, although for this specific dataset, that's not an issue. The ground truth includes this robustness, which is why the generated code isn't a perfect 10.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

# Create the pandas DataFrame
df = pd.DataFrame(data)

# ...",9,"The generated code correctly defines the DataFrame, filters for Toyota cars, and accurately calculates their average mileage. It adheres to all the requirements outlined in the task. The code is clean, readable, and follows good practices (e.g., f-strings for printing).

**Strong points:**
*   Correctly defines the DataFrame as specified.
*   Accurately filters the DataFrame for ""Toyota"" brand cars using boolean indexing.
*   Correctly calculates the mean of the ""Mileage"" column for the filtered DataFrame.
*   Uses a clear f-string for printing the result, including formatting to two decimal places, as implicitly suggested by the ground truth.
*   The overall structure is logical and easy to follow.

**Areas for minor improvement (preventing a perfect 10):**
*   The generated code includes additional print statements for the full DataFrame and the filtered Toyota DataFrame that were not explicitly requested by the prompt (although the ground truth had these commented out). While these are useful for debugging/verification, they deviate slightly from the minimal output requested for just the average mileage. The prompt specifically asked to ""Calculate and print the average 'Mileage'. Provide only Python code."" This implies the final output should only be the average mileage.
*   The ground truth included an `if not toyota_cars_df.empty:` check, which is a good practice for robustness in a general solution, even if not strictly necessary for this specific dataset. The generated code skips this small robustness check.

Overall, the generated code is very good and effectively solves the problem. The minor issues are more about exact adherence to output constraints and minor robustness considerations rather than correctness or functionality.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defen...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. It correctly completes all the required tasks. The added print statements for the full DataFrame and the filtered DataFrame are helpful for debugging and understanding the output, although not explicitly requested, they don't detract from the solution.

One minor point of improvement could be handling the edge case where `fire_pokemon_df` might be empty (though not an issue with the given data). The ground truth code includes a check for `fire_pokemon_df.empty`, which makes it more robust. However, for this specific task and dataset, the generated code performs perfectly. The output format for printing the average stat is also slightly different from the ground truth (no f-string formatting to two decimal places), but functionally correct.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from matplotlib.gridspec import GridSpec

# Set aesthetic style for Seaborn plots
sns.set_style(""whitegrid"")
sns.set_palette(""viridis"") # A nice default palette

# --- Data Loading and Preprocessing ---
try:
    df = pd.read_csv('titanic.csv')
except FileNotFoundError:
    print(""Error: titanic.csv not found. Please ensure the file is in the current di...",8,"The generated code correctly addresses most of the task requirements and produces sophisticated visualizations. It demonstrates good understanding of data visualization libraries (matplotlib, seaborn, plotly) and data preprocessing.

**Strong Points:**
- **Correctness:** All five visualization types are implemented, and they generally convey the intended information.
- **Data Preprocessing:** Handles missing values and creates `Age_Group` and `Survived_Status` columns, which are useful for plotting.
- **Multi-panel figures:** Successfully creates separate multi-panel plots for survival rates and a dashboard.
- **Specific Visualizations:** Heatmap, violin plots, and interactive scatter plot are all implemented correctly.
- **Interactive Plot (Plotly):** Uses `px.scatter` effectively with `color`, `symbol`, `size`, and hover data.
- **Dashboard:** Attempts a comprehensive dashboard using `GridSpec` and incorporates additional plots and text annotations for insights.

**Areas for Improvement:**
- **Dashboard Annotations:** While text annotations are present, they are placed awkwardly using `fig.text` on the side of the plots. The ground truth integrates them more cleanly within a dedicated subplot, making it part of the visual flow rather than floating text.
- **Dashboard Plot Choices:** The dashboard deviates slightly from the prompt's implied comprehensive view by adding a donut plot (Overall Survival Rate) and overall age/fare distributions. While these are not wrong, they replace potential opportunities for more complex analyses that fit a ""dashboard"" such as the combination of class and gender data the ground truth included, or survival by embarked location, which could have been more relevant to the Titanic context. Also, the row for basic survival rates is a bit redundant given the first multi-panel plot.
- **Clarity in Plot 1:** The `errorbar=None` in plot 1 removes insight into the variability of survival rates, which can be useful when dealing with proportions.
- **Missing Categorical Conversions for Pclass:** In the `multi-panel figure`, `Pclass` is plotted as a numerical category (1, 2, 3) rather than converting it to '1st Class', '2nd Class', '3rd Class' as done in the ground truth for better readability. This is a minor point but impacts presentation quality.
- **Redundant Dropping of Columns:** The `df.drop(columns=['Cabin', 'Name', 'Ticket'], inplace=True)` line is unnecessary as these columns are not used in the analysis anyway and `Cabin` typically has too many NaNs, so it's a good decision not to use it, but explicitly dropping it isn't strictly needed for the task as stated. The `hover_name=""Sex""` in the interactive plot is also a bit odd given the column was dropped, but it just means the `Sex` column will show in hover which is fine.
- **`plt.suptitle` placement:** For the dashboard, the `y` parameter in `plt.suptitle` could be adjusted slightly for better positioning to avoid being too close to the edge of the figure, although `constrained_layout=True` usually handles this well.

Overall, the code is very strong and provides a solid solution to the problem. The improvements suggested are mostly about minor polish and adherence to best practices in data storytelling rather than critical correctness issues.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price""...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, accurately filters for the ""Dystopian"" genre, and correctly calculates and prints the average stock for that genre. The code is concise and directly addresses all parts of the task. The use of an f-string for printing the result with 2 decimal places (`.2f`) is also a good practice for presenting numerical output. It fully meets the requirements and demonstrates good understanding of pandas operations. While the ground truth includes optional print statements and an `if not empty` check, these were not explicitly required by the prompt, and the generated code correctly solves the core problem without them, assuming valid input data (which is given in this case).",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Set significance level
alpha = 0.05

# Load the dataset
try:
    df = pd.read_csv('water_potability.csv')
except FileNotFoundError:
    print(""water_potability.csv not found. Please ensure the file is in the current directory."")
    exit()

# Handle missing values: Impute with median
for col in ['ph', 'Sulfate', 'Trihalomethanes']:
    if col in df.columns:
        df[col].fillna(df[col].median(), inplace=True)

# Ensure...",7,"The generated code correctly performs most requested statistical tests (t-tests, chi-square, Shapiro-Wilk) and calculates Cohen's d. It handles file loading and separates data for analysis. However, it has some significant shortcomings and inaccuracies compared to the ground truth:

1.  **Missing Value Handling:** The generated code only imputes a *subset* of columns (`'ph', 'Sulfate', 'Trihalomethanes'`) with median. The ground truth imputes *all* numerical columns with their mean, which is a more comprehensive and often preferred approach for continuous data in statistical contexts. This partial imputation means other columns with potential missing values will still have NaNs, which might affect subsequent tests if not explicitly handled (though `dropna()` in tests partially addresses it).
2.  **Cohen's d Calculation:** The generated code calculates Cohen's d using the *pooled standard deviation* approach, which assumes equal variances. The t-test used (`equal_var=False`) is Welch's t-test, which does *not* assume equal variances. For consistency and best practice when using Welch's t-test, Cohen's d should ideally be calculated using a standard deviation based on the sample sizes and standard errors (e.g., as described by G.V. Glass, or using Hedges' g which is similar but applies a correction). While Cohen's d is broadly applied, the ground truth's calculation of pooled standard deviation is more robust and includes specific edge case handling (e.g., `s1==0 and s2==0`), making it more reliable. The generated code only checks `pooled_std != 0` the ground truth includes more comprehensive checks for zero standard deviation.
3.  **Interpretation of Cohen's d:** The generated code calculates Cohen's d but does not provide any interpretation (e.g., small, medium, large effect), which was a specific requirement in the prompt and is valuable for understanding practical significance. The ground truth provides detailed categories for interpretation.
4.  **Chi-square Test Expectation:** While assuming a 50/50 distribution for the chi-square goodness-of-fit is a reasonable default if no specific expected distribution is given, the ground truth provides a clearer explanation for this assumption. The generated code also includes a minor bug for chi2 test invalidity check, printing ""Skipping Chi-square test: Expected counts are too low (less than 5)."" when referring to `min(expected_counts)` instead of checking if any observed count is less than 5.
5.  **Robustness and Error Handling:** The ground truth includes more robust error handling (e.g., for file not found, printing specific imputed column names, handling extremely small sample sizes for t-tests, explicit checks for standard deviation in Cohen's d).
6.  **Code Structure and Readability:** The ground truth has better print statements, clear section headings, and explicit `ALPHA` variable, which makes the output and logic easier to follow and more professional. The generated code's print statements are more compact and less descriptive.

To improve, the generated code should:
*   Implement a more comprehensive missing value imputation for all numerical columns.
*   Refine the Cohen's d calculation to align with the non-equal variance assumption of Welch's t-test, or at least use the more robust pooled standard deviation calculation from the ground truth and add interpretation of the effect size.
*   Add descriptive print statements for all analyses, including interpretation of Cohen's d.
*   Improve error handling and robustness checks, especially for statistical tests (e.g., minimum sample sizes, zero standard deviation for Cohen's d).",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, RocCurveDi...",8,"The generated code is well-structured and addresses most of the requirements of the task.

**Strong Points:**
*   **Comprehensive Imports:** All necessary libraries are imported.
*   **Data Loading and Initial Handling:** Correctly handles `FileNotFoundError`.
*   **Data Preprocessing (Partial):** Performs mean imputation and StandardScaler for feature scaling.
*   **Train-Test Split:** Correctly uses 80/20 split with `stratify=y` for class balance.
*   **Multiple Models:** Implements Logistic Regression, Random Forest, and SVM.
*   **Evaluation Metrics:** Calculates accuracy, precision, recall, F1-score, and ROC-AUC.
*   **Visualizations:** Generates confusion matrices and ROC curves for each model.
*   **K-Fold Cross-Validation:** Implements 5-fold cross-validation using `StratifiedKFold` and `cross_val_score` for all models and metrics.
*   **Model Comparison:** Compares models and selects the best one based on F1-score (from the test set, although the ground truth uses CV F1-score, which is more robust).
*   **Feature Importance:** Correctly displays feature importance for Random Forest and Logistic Regression.

**Areas for Improvement:**
*   **Missing Value Handling in Preprocessing:** The generated code performs `df.fillna(df.mean(), inplace=True)` *before* the train-test split. This is a form of data leakage, as information from the test set is used to impute training data (and vice-versa, though less critically). The ground truth correctly handles this by allowing imputation and scaling to be fitted only on the training data and then transforming both, or by using a `Pipeline` for cross-validation.
*   **Cross-Validation Preprocessing:** For k-fold cross-validation, the generated code scales the *entire* dataset (`X_scaled = scaler.fit_transform(X)`), and then passes this `X_scaled` to `cross_val_score`. While `cross_val_score` does new splits, the scaling step (`fit_transform`) still happens on the *entire* dataset outside the folds, leading to data leakage in feature scaling during cross-validation. The ground truth uses a `Pipeline` within the `cross_val_score` call, which is the correct and robust way to handle preprocessing steps inside cross-validation.
*   **Best Model Selection Criterion:** The code selects the best model based on the F1-score from the *test set*. While this is a valid approach, using the F1-score from the *cross-validation* results (as done in the ground truth) is generally more robust as it provides a less biased estimate of performance.
*   **ROC Curve Plotting:** The ROC curves are plotted individually. The ground truth code combines all ROC curves into a single plot for easier comparison, which is a minor but good enhancement.
*   **Confusion Matrix Display:** The `ConfusionMatrixDisplay` from sklearn.metrics could be used for a slightly more integrated display, though the current `sns.heatmap` approach is also perfectly acceptable.

Overall, it's a very good attempt that only misses some crucial data preprocessing and cross-validation best practices to prevent data leakage.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer # For handling missing values

# Set plot style
sns.set_style(""whitegrid"")

# Load the dataset
try:
    df = pd.read_csv('water_potability.csv')
except FileNotFoundError...",9,"The generated code is very good and largely fulfills all requirements of the task.

**Strong Points:**
*   **Correctness:**
    *   Successfully loads data and handles `FileNotFoundError`.
    *   Correctly imputes missing values using `SimpleImputer` before feature engineering.
    *   Generates a good set of ratio and interaction features.
    *   Attempts polynomial features, though manually (`x**2`) which is a reasonable approach given the potential for dimensionality explosion with `PolynomialFeatures` on all columns.
    *   Correctly calculates correlation matrices.
    *   Identifies top 5 correlated features.
    *   Applies `SelectKBest` and `RFE` with `RandomForestClassifier` correctly and scales data prior to selection.
    *   Generates comprehensive visualizations, including a correlation heatmap, feature importance plot, and pairwise scatter plots.
    *   Compares feature sets effectively using set operations.
*   **Code Quality:**
    *   Proper imports are present.
    *   Code is well-structured with clear comments for different sections.
    *   Uses `sns.set_style(""whitegrid"")` for better aesthetics.
    *   Handles potential `inf` values after ratio calculations, which is a good robustness check.
    *   Uses `n_jobs=-1` for `RandomForestClassifier`, which is good for performance.
*   **Completeness:**
    *   Addresses all 6 requirements specified in the prompt.
    *   The narrative about feature selection comparison is a nice touch for analysis.

**Areas for Improvement (minor, leading to a score of 9 instead of 10):**

1.  **Polynomial Features Implementation:** While the manual polynomial features (`X_eng['pH_SQ'] = X_eng['ph']**2`) prevent feature explosion, the prompt specifically asked for `PolynomialFeatures` from `sklearn.preprocessing`. The ground truth demonstrates a more robust way to use `PolynomialFeatures` on a selected subset of columns which is generally preferred for consistency and scalability than manually creating individual squared terms. The generated code's approach for polynomial features is less comprehensive than intended by the prompt's mention of `PolynomialFeatures`, as it only creates squared terms for a few features and doesn't generate interaction terms (e.g., pH*Hardness) via `PolynomialFeatures` as the ground truth does.
2.  **Top Correlated Features - Absolute Value:** The prompt implies ""most correlated"" which typically means highest absolute correlation. The generated code calculates `potability_correlations = correlation_matrix['Potability'].sort_values(ascending=False).drop('Potability')` and then uses `potability_correlations.head(5).index.tolist()`. This selects the top 5 *positively* correlated features. The print statement `print(f""Top 5 features most correlated with Potability (absolute correlation):\n{potability_correlations.abs().head(5)}\n"")` correctly shows the absolute values, but the `top_5_correlated_features` list itself is based on non-absolute sorting. The ground truth correctly sorts by `abs()` to get the true ""top"" correlations regardless of sign.
3.  **Visualization of Correlation Heatmap:** The generated code plots the heatmap only for `top_corr_features_for_heatmap` (top 20 features). While this can improve clarity for very large feature sets, the prompt implied (and ground truth shows) a heatmap for ""all features including engineered ones"" for a comprehensive overview. Showing the full heatmap, perhaps with `annot=False` or a smaller `annot_kws` could still achieve this without being too cluttered, or splitting it into two plots (one full, one for top selected).
4.  **Redundant `SimpleImputer`**: While `SimpleImputer` is correct, the `water_potability.csv` dataset only has NaNs in 'ph', 'Sulfate', and 'Trihalomethanes'. A simple `fillna(df[col].mean())` loop (as in ground truth) is equally effective and perhaps slightly more direct for this small dataset, though `SimpleImputer` is more robust for general use. This is a minor point.

Overall, the code is highly functional, well-written, and addresses almost all aspects of the task effectively. The few minor deviations from the ideal interpretation of the prompt's requirements (especially regarding `PolynomialFeatures` and absolute correlation sorting) prevent a perfect score.",gemini_2.5_flash_thinking_5_20_thinking,LLM_judge_config
