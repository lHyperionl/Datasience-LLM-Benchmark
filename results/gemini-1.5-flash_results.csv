task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

df = pd.read_csv(""winequality-red.csv"", sep="";"")

print(""Column Names and Data Types:"")
print(df.dtypes)

print(""\nDescriptive Statistics for Numerical Columns:"")
print(df.describe())

print(""\nTotal Number of Rows and Columns:"")
print(df.shape)

categorical_cols = df.select_dtypes(include=['object']).columns
print(""\nCategorical or Ordinal Columns:"")
print(categorical_cols)

```",7,"The generated code correctly loads the dataset and provides descriptive statistics and dimensions. However, it fails to identify data types accurately as requested. The ground truth correctly identifies potential categorical/ordinal columns even if they are stored as numerical types (like 'quality'), which the generated code misses by strictly relying on `object` dtype. The generated code also makes an incorrect assumption about the separator `;`, whereas the default `,` works for `winequality-red.csv`. If the `sep=;` was removed, the code might still get a good score for the given dataset. The ground truth also explicitly highlights how `quality` might be treated as an ordinal variable despite being numerical, which is valuable domain knowledge. The error handling for file not found is also missing.

Specific areas for improvement:
- **Data Type Identification:** The task specifically asked to ""identify the data type of each column (numerical, categorical, etc.)"". The generated code just prints `df.dtypes`, which shows pandas dtypes but doesn't interpret them as ""numerical"" or ""categorical"". It also misses the nuance of numerical columns that might be treated as categorical/ordinal.
- **Categorical Column Identification:** `df.select_dtypes(include=['object'])` will not identify columns like 'quality' which are numerical but often treated as ordinal. The ground truth's discussion on this point is superior.
- **Robustness:** No error handling for `FileNotFoundError`.
- **Clarity of Output:** The output for data types and column types could be more explicitly labeled as numerical/categorical/ordinal as per the prompt.
- **Incorrect `sep` parameter**: The `winequality-red.csv` file uses a comma (`,`) as a separator, not a semicolon (`;`). While it might work for some datasets, it's incorrect for this specific file, which could lead to loading errors depending on the file's structure.",gemini-flash-1.5,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

df = pd.read_csv(""video_games_sales.csv"")

print(df.head(10))
print(df.shape)
print(df.dtypes)
print(df.sort_values('Global_Sales', ascending=False).head(10))
print(df['Platform'].unique())
print(df['Genre'].unique())
print(df[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']].describe())
print(df.isnull().sum())

```",7,"The generated code largely addresses the requirements of the task. It correctly loads the dataset, displays the head, shape, column data types, top 10 games by global sales, unique values for Platform and Genre, summary statistics for sales columns, and missing values.

However, it lacks the detailed output formatting and descriptive print statements present in the ground truth, which makes the output less readable and informative for an EDA task. Specifically:
- **No descriptive print statements:** The generated code just prints the output of various pandas functions without any labels or explanations, making it hard to discern which output corresponds to which task requirement.
- **Error handling:** There is no error handling for `FileNotFoundError` when loading the CSV, nor for missing columns (e.g., 'Global_Sales', 'Platform', 'Genre', or sales columns) which could cause the script to crash if the dataset or its structure deviates from expectations.
- **Completeness:** While it performs the actions, it doesn't explicitly state the column names and their data types (it uses `df.dtypes` instead of `df.info()`, which is less comprehensive for EDA as it doesn't include non-null counts). Also, for missing values, it just prints the series, whereas the ground truth explicitly states if there are no missing values.
- **Displaying all column names and their data types:** It uses `df.dtypes` which is a valid way to get data types but `df.info()` as used in the ground truth provides more comprehensive information, including non-null counts, which is valuable for EDA.

To achieve a higher score, the generated code should incorporate more descriptive print statements, robust error handling, and consider using `df.info()` for a more complete display of column information.",gemini-flash-1.5,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('video_games_sales.csv')

median_year = df['Year'].median()
df['Year'].fillna(median_year, inplace=True)

df.dropna(subset=['Publisher'], inplace=True)

df['Years_Since_Release'] = 2023 - df['Year']

df.drop_duplicates(subset=['Name', 'Platform'], inplace=True)

df = df[df['Global_Sales'] >= 0.1]

print(df.shape)
print(df.head())
```",7,"The generated code correctly implements most of the required steps, including loading the data, handling missing 'Year' by median, removing missing 'Publisher', creating 'Years_Since_Release', dropping duplicates, and filtering by 'Global_Sales'. It also correctly displays the shape and head.

However, it lacks robustness and completeness in a few areas:
1.  **Error Handling**: It doesn't include any `try-except` blocks for file not found errors or other potential issues during data loading and processing, which is present in the ground truth.
2.  **Data Type Coercion**: The ground truth explicitly converts 'Year' and 'Global_Sales' columns to numeric types with `errors='coerce'`, which is crucial for handling potential non-numeric entries in these columns before calculations or filtering. The generated code assumes these columns are already numeric or will be handled implicitly.
3.  **Years_Since_Release Negative Values**: The ground truth includes a step to ensure 'Years_Since_Release' is non-negative, which is a good practice if future years are present in the 'Year' column. The generated code does not account for this.
4.  **Implicit Assumptions**: It implicitly assumes `Year` and `Global_Sales` are clean numeric columns without needing `to_numeric` or `dropna` after conversion, which might not always be true in real-world datasets.
5.  **Output Clarity**: While it prints shape and head, the ground truth provides more detailed print statements at each step, showing the changes in shape, which is helpful for debugging and understanding the pipeline.

Overall, it's a good attempt, but the missing data type robustness and error handling prevent it from being a fully robust and production-ready solution.",gemini-flash-1.5,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('video_games_sales.csv')

# Bar chart of top 10 platforms
top_platforms = df.groupby('Platform')['Global_Sales'].sum().nlargest(10)
plt.figure(figsize=(10, 6))
top_platforms.plot(kind='bar')
plt.title('Top 10 Gaming Platforms by Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('top_platforms.png')
plt.show()


# Pie chart of genre distributi...",6,"The generated code correctly attempts to create all the required visualizations and includes proper imports and basic labels, demonstrating an understanding of the core task. However, it lacks robustness, proper data preprocessing for key columns, and refinement in various plots to match the quality and reliability of the ground truth.

Specific areas for improvement:
1.  **Missing Data Preprocessing:** The most critical issue is the complete lack of data preprocessing, especially for 'Year_of_Release' (named 'Year' in the generated code) and sales columns. The ground truth explicitly handles `errors='coerce'` and `dropna` for numeric conversion, which is crucial for real-world datasets that might have non-numeric entries. The generated code also attempts to group by 'Year' (which isn't explicitly defined or cleaned in the generated code itself), while the ground truth correctly uses 'Year_of_Release' and converts it to int. This will likely lead to errors or incorrect plots if the 'Year' column in the CSV is not perfectly clean.
2.  **Robustness in File Loading:** The generated code directly `pd.read_csv`, which will throw an error if the file is not found. The ground truth includes a robust `try-except` block for file loading.
3.  **Visualization Refinements:**
    *   **Bar Chart:** The ground truth adds `color='skyblue'`, `grid`, and uses `nlargest(10)` which is good. Generated is fine, but less aesthetically pleasing.
    *   **Pie Chart:** The ground truth handles sparse genres by grouping them into ""Other"" to prevent an overly cluttered pie chart, which is a good practice for readability. The generated code plots all genres, which could be unreadable with many small categories.
    *   **Histogram:** The ground truth sets a `range` for the histogram to focus on the majority of sales (`quantile(0.99)`), which is often useful for skewed distributions. The generated histogram just uses `bins=30`, which might not provide the most informative view.
    *   **Scatter Plot:** The ground truth uses `alpha=0.5` and `s=10` for better visualization of potentially overlapping points, which the generated code lacks.
    *   **Line Plot:** The ground truth specifically filters years (`df['Year_of_Release'] = df['Year_of_Release'].astype(int)`) and then filters relevant years for the plot (1980-2020), which demonstrates better data handling for time-series. The generated code groups by `df['Year']` which is not the correct column name from the dataset ('Year_of_Release') and doesn't handle its data type or potential out-of-range values.
4.  **Redundant `plt.savefig()` and `plt.show()` calls within a loop (implicit):** While not explicitly in a loop here, the code calls `plt.savefig()` and `plt.show()` for each plot. Often, for multiple plots, you might want to save them all or display them in a more controlled manner, though for this specific task, it's not a major flaw. It's just a less efficient way to manage outputs compared to perhaps saving all figures at the end.
5.  **Lack of `numpy` import:** While not explicitly used in the generated code, `numpy` is often a good general import for data science tasks, used implicitly by pandas operations. The ground truth includes it.
6.  **Minor Label/Title differences:** Titles and labels are mostly fine, but the ground truth often provides slightly more descriptive labels (e.g., ""Total Global Sales (Millions)"").

In summary, the generated code produces the requested charts, but its lack of data robustness and refinement in plotting choices makes it less professional and reliable compared to the ground truth.",gemini-flash-1.5,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('diabetes.csv')
print(df.head(10))
print(df.shape)
print(df.dtypes)
print(df['Outcome'].value_counts(normalize=True) * 100)
print(df.isnull().sum())
print(df.describe())
print((df == 0).sum())

```",6,"The generated code addresses most of the requirements but lacks robustness, clarity, and some specific details requested in the prompt.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly calculates the shape, column data types, and outcome percentages.
*   Correctly identifies missing values and provides summary statistics.
*   Attempts to identify zero values.

**Areas for Improvement:**
*   **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice. The generated code assumes the file exists.
*   **Output Clarity:** The prompt asks for clear labeling of each output section (e.g., ""--- 1. First 10 rows...""). The generated code simply prints the outputs without descriptive headers, making it harder to understand which output corresponds to which task requirement.
*   **Specific Zero Value Check:** The prompt specifically asks to ""Identify any columns with zero values *that might represent missing data*."" The generated code uses `(df == 0).sum()` which checks *all* columns for zeros, including 'Pregnancies' and 'Outcome' where zero can be a legitimate value, not necessarily missing data. The ground truth correctly defines `COLUMNS_TO_CHECK_FOR_ZERO_AS_MISSING` and provides percentage counts which is more informative.
*   **`df.info()` vs `df.dtypes`:** While `df.dtypes` shows data types, `df.info()` (used in the ground truth) also provides non-null counts, which is more comprehensive for understanding the dataset structure and potential missing values. The prompt specifically asked for ""List all column names and their data types"" where `df.info()` is often preferred.
*   **Readability:** The ground truth code is much more readable due to comments, clear variable names, and structured output formatting. The generated code is very concise but less user-friendly for analysis.",gemini-flash-1.5,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

df = pd.read_csv('diabetes.csv')

cols_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
df[cols_with_zeros] = df[cols_with_zeros].replace(0, pd.NA)

imputer = SimpleImputer(strategy='median')
df[cols_with_zeros] = imputer.fit_transform(df[cols_with_zeros])

df.drop_duplicates(inplace=True)

def bmi_category(bmi):
    if bmi < 18.5:
        return 'U...",7,"The generated code addresses most of the requirements but has some areas for improvement in robustness, completeness, and adherence to best practices.

**Strengths:**
*   Correctly identifies and replaces zero values with `pd.NA` (or `np.nan`/`None`).
*   Uses `SimpleImputer` for handling missing values with the median strategy, which is efficient.
*   Correctly removes duplicate rows.
*   Creates the `BMI_Category` column accurately using a function and `apply`.
*   Applies `StandardScaler` to numerical features.
*   Prints the shape and head of the cleaned dataframe as requested.

**Areas for Improvement:**

1.  **Robustness and Error Handling:**
    *   **Loading CSV:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice. The generated code assumes the file exists.
    *   **Column Existence Checks:** When replacing zeros or imputing, the generated code directly assigns to `df[cols_with_zeros]`. While this often works, a more robust solution (like the ground truth) would check if each column actually exists in the DataFrame before attempting operations on it, especially when dealing with potentially dynamic datasets or user input.
    *   **BMI Column Check for Category:** Similarly, the ground truth explicitly checks if the 'BMI' column exists before trying to create 'BMI\_Category'.

2.  **Imputation Scope:**
    *   The `SimpleImputer` is applied only to `cols_with_zeros`. While this fulfills the requirement of handling specific columns, the ground truth correctly imputes only on the columns that *actually had zeros replaced*, ensuring that other unrelated `NaN` values (if any existed initially or were introduced by other means) are not accidentally imputed using this specific strategy. The generated code also reassigns the entire `df[cols_with_zeros]` block, which is fine, but `fit_transform` can be applied more granularly.

3.  **Numerical Feature Scaling:**
    *   The list `numerical_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Pregnancies', 'Age']` is hardcoded. This is acceptable for this specific task given the dataset details, but a more general and robust approach (like the ground truth) would dynamically select numerical columns using `df.select_dtypes(include=np.number).columns.tolist()`. This handles datasets with varying columns more gracefully.
    *   The ground truth also correctly excludes the 'Outcome' column from scaling, assuming it's the target variable, which is standard practice. The generated code does not explicitly exclude it.

4.  **Clarity and Readability (Minor):**
    *   The `bmi_category` function is clear, but `pd.cut` (as used in the ground truth) is often a more idiomatic and often more performant way to categorize numerical data into bins in pandas.

5.  **Output Details:**
    *   The ground truth provides more detailed output messages about the original shape, number of duplicates removed, and columns scaled, which aids in understanding the data cleaning process. The generated code only prints the final shape and head.",gemini-flash-1.5,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('diabetes.csv')

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

df.hist(figsize=(12, 10))
plt.suptitle('Histograms of Numerical Features', fontsize=16)
plt.show()

fig, axes = plt.subplots(nrows=len(df.columns)-1, ncols=1, figsize=(8, 20))
for i, col in enumerate(df.columns[:-1]):
    sns.boxplot(...",8,"The generated code successfully addresses all the visualization requirements of the task. It includes correct implementations for the heatmap, histograms, box plots, pie chart, scatter plots, and pair plot. The code also uses proper imports and good structure.

**Strong points:**
*   **Correctness:** All plots are generated as requested.
*   **Completeness:** All 7 visualization types are included.
*   **Readability:** The code is well-structured and easy to understand.
*   **Efficiency:** Uses `df.hist()` for quick histogram generation and `df.columns[:-1]` for numerical columns in box plots.

**Areas for improvement (minor points):**
*   **Numerical column identification:** The ground truth code explicitly identifies numerical columns by dropping 'Outcome'. While `df.corr()` and `df.hist()` implicitly handle numerical columns, the box plot loop `df.columns[:-1]` assumes 'Outcome' is the last column and all others are numerical, which might be less robust if column order changes or if there are other non-numerical columns.
*   **Box plot aspect ratio:** For the box plots, the generated code uses `nrows=len(df.columns)-1, ncols=1`, which can lead to very tall plots, especially if there are many features. The ground truth code uses a more flexible `n_rows` and `n_cols` calculation for better layout.
*   **Scatter plot legends and labels:** The generated scatter plots have generic titles (""BMI vs Glucose"", ""Age vs Pregnancies"") and default legends. The ground truth code adds more descriptive titles and modifies legend labels for clarity.
*   **Pair plot title positioning:** The `plt.suptitle` for the pair plot in the generated code might overlap with the plot, whereas the ground truth uses `y=1.02` to adjust it.
*   **General error handling and initial data info:** The ground truth includes error handling for `FileNotFoundError` and prints basic `df.head()`, `df.info()`, and `df.describe()`, which are good practices for data analysis scripts but not strictly required by the prompt ""provide only the Python code without explanations"".",gemini-flash-1.5,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

df = pd.read_csv(""world-happiness-report-2021.csv"")

print(df.head(10))
print(df.shape)
print(df.dtypes)
print(df['Regional indicator'].value_counts())
print(df.nlargest(10, 'Ladder score'))
print(df[['Ladder score', 'Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices']].describe())
print(df.isnull().sum())

```",6,"The generated code correctly performs most of the required data analysis steps. It successfully loads the dataset, displays the first 10 rows, shows the shape, identifies column data types, counts unique regional indicators, finds the top 10 happiest countries, calculates summary statistics for key factors, and identifies missing values.

However, the code lacks structure, readability, and robustness compared to the ground truth. Specifically:
- **No clear separation of tasks:** The output is a continuous stream without clear headings or separators for each task, making it hard to follow the output.
- **Missing explicit print statements for task descriptions:** The generated code only prints the data, not what each print statement represents (e.g., ""1. First 10 rows of the dataset:"").
- **Error handling:** There is no `try-except` block for `FileNotFoundError` when loading the CSV, which is crucial for robust code.
- **Column validation:** The code directly accesses columns like `'Regional indicator'`, `'Ladder score'`, etc., without checking if they exist in the DataFrame. If a column is missing, it will raise a `KeyError`. The ground truth includes checks for column existence.
- **Specific output for ""Top 10 happiest countries"":** The generated code prints the entire row for the top 10 countries, whereas the prompt implies only relevant columns (`Country name`, `Ladder score`) should be shown. The ground truth correctly filters these columns.
- **`df.dtypes` vs `df.info()`:** While `df.dtypes` shows data types, `df.info()` (used in the ground truth) provides a more comprehensive summary including non-null counts and memory usage, which is generally preferred for ""listing all column names and their data types"" in EDA.

To improve, the generated code should incorporate task-specific print statements, add error handling for file loading, implement checks for column existence, and refine the output format for clarity and user-friendliness.",gemini-flash-1.5,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np

df = pd.read_csv('world-happiness-report-2021.csv')

for col in df.columns:
    if df[col].dtype == 'number':
        df[col].fillna(df[col].mean(), inplace=True)
    else:
        df[col].fillna(df[col].mode()[0], inplace=True)

df['Happiness_Category'] = pd.cut(df['Ladder score'], bins=[-np.inf, 4, 6.5, np.inf], labels=['Low', 'Medium', 'High'], right=False)
df['GDP_Category'] = pd....",8,"The generated code successfully addresses most of the task requirements, demonstrating a good understanding of data preprocessing techniques.

**Strong Points:**
- **Loading Data:** Correctly loads the CSV file.
- **Missing Values:** Handles missing values for both numerical and categorical columns effectively, though the `dtype == 'number'` check is less robust than `np.number` or `select_dtypes`.
- **Derived Features:**
    - `Happiness_Category`: Uses `pd.cut` correctly to create the categories.
    - `GDP_Category`: Correctly uses `pd.qcut` for quartile-based categorization.
    - `Well_being_Score`: Correctly calculates the average.
- **Outlier Removal:** Implements the IQR method correctly for 'Ladder score'.
- **Standardization:** Correctly uses `StandardScaler` on numerical features.
- **Encoding:** Correctly uses `LabelEncoder` for 'Regional indicator'.
- **Output:** Displays the shape and head of the cleaned dataset as requested.

**Areas for Improvement:**
- **Error Handling:** The ground truth includes `try-except` for file loading, which is a good practice for robustness. The generated code lacks this.
- **Handling Missing Values (Type Check):** Using `df[col].dtype == 'number'` is less precise than `df.select_dtypes(include=np.number)` or `df.select_dtypes(include=['int64', 'float64'])`. While it might work in most cases, `np.number` is more comprehensive.
- **Readability/Structure:** The ground truth code breaks down the tasks into clearly commented sections, which significantly improves readability and maintainability, especially for complex preprocessing pipelines. The generated code is more monolithic.
- **Feature Selection for Standardization:** While the generated code scales all numerical columns, the ground truth explicitly considers what columns to scale, which can be important if some numerical columns are identifiers or non-scalable. It's a minor point here but good practice.
- **Implicit Requirements (Printing during process):** The ground truth includes print statements for initial shape, number of outliers removed, etc., which helps in debugging and understanding the pipeline's progress. The generated code only prints the final shape and head. The prompt didn't explicitly ask for these intermediate prints, but they are a sign of good development practice in data science pipelines.

Overall, the code is functional and produces the expected output. The noted improvements are primarily about robustness, readability, and adherence to best practices in a production or collaborative environment.",gemini-flash-1.5,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('world-happiness-report-2021.csv')

fig = px.choropleth(df, locations=""Country name"", locationmode='country names', 
                    color='Ladder score', hover_name=""Country name"", 
                    title='World Happiness Scores 2021')
fig.show()

region_avg = df.groupby('Regional indicator')['Ladder score'].mean().reset_index...",7,"The generated code creates most of the requested visualizations successfully and uses the specified libraries (matplotlib, seaborn, plotly). However, there are several areas for improvement:

**Strengths:**
*   **Correctness and Completeness (for most parts):** Successfully creates a world map, horizontal bar chart, correlation heatmap, two scatter plots, and box plots, fulfilling the core requirements.
*   **Proper Imports:** Includes all necessary libraries.
*   **Clear Plot Titles:** Most plots have appropriate titles.

**Areas for Improvement:**
*   **Column Renaming:** The ground truth renames columns for better readability and consistency (`Ladder score` to `Happiness Score`, `Logged GDP per capita` to `GDP per Capita`, etc.). The generated code uses the original column names, which can be less intuitive for labeling and harder to manage for plots.
*   **Scatter Plot Issues:**
    *   The task explicitly asks for two separate scatter plots: ""GDP per capita vs Ladder score AND Social support vs Ladder score"". The generated code creates these as separate figures, which is less efficient and typically presented side-by-side in one figure as shown in the ground truth for better comparison.
    *   The generated scatter plots lack features like `hue` for regional indicators, which would add valuable context and visual information, as included in the ground truth. Legends are also missing.
*   **Correlation Heatmap:** The generated code directly computes `df.corr()`. This includes non-numerical columns or irrelevant numerical identifiers from the original dataset if not explicitly handled (though for this specific dataset, `Ladder score` and the factors are mostly numerical, it's good practice to select relevant columns as done in the ground truth). The ground truth carefully selects only the relevant happiness factors for the heatmap.
*   **Radar Chart Significant Issue:**
    *   **Normalization:** The most critical flaw in the radar chart is the lack of normalization. The `Ladder score` is on a much larger scale (e.g., 2-8) compared to other factors like `Generosity` or `Perceptions of Corruption` (e.g., 0-1). Plotting these on the same radar chart without normalization makes the visualization misleading and difficult to interpret. The ground truth correctly uses `MinMaxScaler` to normalize the radar features, which is essential for a meaningful radar chart comparison.
    *   **Categories:** The generated code includes `Ladder score` itself as a category in the radar chart, which isn't typically how radar charts for ""profiles"" are structured. A happiness profile usually compares *contributing factors*, not the overall score itself, especially when the overall score is what determines the ""top 5"". The ground truth correctly uses only the factors contributing to happiness as categories.
    *   **Manual Trace Addition:** Adding each trace manually (`fig.add_trace(...)` five times) is not scalable and less elegant than looping through the top-N countries as the ground truth demonstrates.
    *   **`range` parameter:** The `polar=dict(radialaxis=dict(visible=True, range=[0, 10]))` for the radar chart is an arbitrary range that works somewhat by chance but doesn't reflect the actual scales of the *normalized* features. This further highlights the normalization issue.
*   **General Formatting/Aesthetics:**
    *   The ground truth includes `sns.set_style(""whitegrid"")` and `plt.rcParams` for consistent and improved aesthetics across all plots. This is a good practice for comprehensive reports.
    *   Usage of `plt.tight_layout()` is crucial for preventing labels and titles from overlapping, which the ground truth consistently applies but the generated code misses for some plots.
    *   More informative `fmt` for `heatmap` (e.g., `fmt="".2f""`) is good practice.

While the code generates most requested plots, the significant flaw in the radar chart (lack of normalization and inappropriate categories) and minor issues in scatter plot details and overall aesthetic consistency prevent a higher score.",gemini-flash-1.5,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

print(df.head(10))
print(df.shape)
print(df.dtypes)

print(df.groupby('continent')[['total_cases', 'total_deaths']].sum())

latest_date = df.groupby('location')['date'].max()
top_5_cases = df[df.date.isin(latest_date)].groupby('location')['total_cases'].max().nlargest(5)
print(top_5_cases)


print(df[['new_cases', 'new_deaths']].agg(['mean', 'median', 'min', 'max', 'std']))

print(df.isnull().sum...",7,"The generated code addresses most of the requirements but has some notable shortcomings in terms of robustness, clarity, and full adherence to all instructions.

**Strengths:**
*   Correctly loads the CSV and parses dates.
*   Displays the first 10 rows (though the output header is missing).
*   Correctly shows the shape and dtypes.
*   Correctly calculates continental sums.
*   Identifies top 5 locations for total cases, although the logic for `latest_date` is slightly off and might not always select the *latest* date's 'total\_cases' if multiple entries exist for the same 'location' on different dates. The ground truth handles this more robustly by selecting the *latest* record for each location before sorting.
*   Correctly calculates summary statistics for `new_cases` and `new_deaths`.
*   Correctly counts missing values.

**Areas for Improvement:**

1.  **Output Readability/Clarity:** The generated code lacks print statements and descriptive headers for each section of the output, making it harder to interpret the results compared to the ground truth. For example, just `print(df.head(10))` doesn't tell the user what they are looking at without context.
2.  **Robustness/Error Handling:** The ground truth includes `try-except` blocks for file loading, which is crucial for real-world applications. The generated code does not.
3.  **Specific Task Adherence (4. Column Names and Data Types):** The prompt requested ""List all column names and their data types."" The generated code uses `df.dtypes`, which provides the data types, but `df.info()` (used in the ground truth) is generally preferred as it also shows non-null counts, which is very useful for initial EDA.
4.  **Top 5 Locations (Logic Flaw):**
    *   `latest_date = df.groupby('location')['date'].max()` correctly finds the latest date for each location.
    *   `top_5_cases = df[df.date.isin(latest_date)].groupby('location')['total_cases'].max().nlargest(5)` has a subtle issue. `df.date.isin(latest_date)` will filter the DataFrame to *all* rows where the date is *one of the latest dates for *any* location*. This isn't guaranteed to give you the *latest* entry for *each* location. For instance, if Location A has a latest date of 2023-01-01 and Location B has a latest date of 2023-01-01, rows from Location C on 2023-01-01 would also be included, *even if 2023-01-01 isn't Location C's latest date*. The ground truth's `sort_values` followed by `drop_duplicates(subset=['location'], keep='last')` is a more robust way to ensure you're getting the single, most recent record for each location before extracting total cases and sorting.
5.  **Handling Non-Numeric Data for Statistics:** The generated code directly calls `agg` on `df[['new_cases', 'new_deaths']]`. If these columns contain non-numeric data (e.g., strings), this could lead to errors. The ground truth explicitly uses `pd.to_numeric` with `errors='coerce'` and `dropna()` to make it more resilient to data quality issues. This is good practice for EDA.
6.  **Missing Imports:** While the core functionality is there, an `import pandas as pd` is included, as expected.

The generated code provides a functional solution for most parts but lacks the polish, robustness, and output clarity of the ground truth, which are important aspects of a complete EDA script.",gemini-flash-1.5,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('winequality-red.csv')
average_quality = df['quality'].mean()
print(f""Average Quality: {average_quality}"")

quality_alcohol = df.groupby('quality')['alcohol'].mean()
print(f""\nAverage Alcohol per Quality:\n{quality_alcohol}"")

plt.figure(figsize=(10, 6))
sns.barplot(x=quality_alcohol.index, y=quality_alcohol.values)
plt.xlabel('Quality')
plt.ylabel('Average Alcohol Content')
plt.title('Average A...",9,"The generated code correctly addresses all parts of the task: calculating the mean quality, grouping by quality to get average alcohol, and creating a bar chart. It uses proper imports (pandas, seaborn, matplotlib).

Strong points:
- Correctly calculates the mean quality.
- Correctly groups by 'quality' and calculates the mean 'alcohol'.
- Generates a bar chart visualizing the relationship, with appropriate labels and title.
- Concise and readable.

Areas for minor improvement (not critical for task completion but good practice):
- The `print` statements could be slightly more descriptive or match the ground truth's detailed output for better user understanding.
- The bar chart could benefit from additional styling (e.g., `palette`, `fontsize` for labels/title), data labels, and potentially `plt.tight_layout()` for better presentation, as seen in the ground truth.
- Adding error handling for `FileNotFoundError` when reading the CSV would make the code more robust.",gemini-flash-1.5,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

df['hosp_patients'].fillna(0, inplace=True)
df['icu_patients'].fillna(0, inplace=True)
for col in ['positive_rate', 'reproduction_rate']:
    df[col].fillna(df[col].mean(), inplace=True)

df['mortality_rate'] = (df['total_deaths'] / df['total_cases']) * 100
df['mortality_rate'].fillna(0, inplace=True)
df['mortality_rate'].replace([np.inf, -np.inf], 0, inplace=True)

df['vaccina...",8,"The generated code correctly performs most of the required steps: loading data and parsing the date column, handling missing values for `hosp_patients` and `icu_patients` with 0, creating `mortality_rate` and `vaccination_rate_per_capita` columns with correct division by zero handling, and filtering for the year 2021. It also displays the shape and head.

However, there are a few minor issues and omissions:
- **Mean imputation for `positive_rate` and `reproduction_rate`:** While the code attempts to fill NaNs with the mean, it doesn't explicitly convert these columns to numeric types first (`errors='coerce'`). If these columns contain non-numeric data, `mean()` might fail or produce incorrect results. The ground truth explicitly converts them.
- **Display format:** The ground truth code includes descriptive print statements for the shape and head, making the output more user-friendly. The generated code simply prints the shape and head without labels.
- **File Not Found Handling:** The generated code lacks a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness.
- **`SettingWithCopyWarning`:** While not a critical error, directly filtering `df = df[df['date'].dt.year == 2021]` might sometimes lead to `SettingWithCopyWarning` if subsequent operations modify the filtered DataFrame. Using `.copy()` as in the ground truth is a safer practice.

Overall, it's a very solid attempt that accomplishes the core requirements. The mentioned points are minor improvements.",gemini-flash-1.5,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

india_data = df[df['location'] == 'India']
plt.figure(figsize=(12, 6))
plt.plot(india_data['date'], india_data['new_cases'])
plt.xlabel('Date')
plt.ylabel('New Cases')
plt.title('Time Series of New Cases in India')
plt.show()

continent_deaths = df.groupby('continent')['total_deaths'].sum()
plt.figure(figsize=(12, 6))
sns.bar...",7,"The generated code correctly addresses most of the requirements for visualization types and general data loading.

**Strong Points:**
*   Correctly loads data and parses the 'date' column.
*   Successfully generates a time series plot, bar chart, scatter plot, histogram, and box plot as requested.
*   Uses `matplotlib.pyplot` and `seaborn` as specified.
*   Includes basic titles and labels for all charts.

**Areas for Improvement (Why it isn't higher):**

1.  **Handling of `total_deaths` for Bar Chart:** The prompt for task 3 states ""sum of 'total_deaths' for each 'continent'"". The generated code simply sums `df.groupby('continent')['total_deaths'].sum()`. This is problematic because `total_deaths` is a cumulative metric. Summing it directly across all entries for a continent will lead to inflated numbers, especially for longer time series. The correct approach (as in the ground truth) is to take the *latest* `total_deaths` value *per location* and then sum *those* by continent. This ensures each country's most up-to-date death count contributes once to its continent's total. This is a significant data handling error for this specific task.
2.  **Scatter Plot `latest_data` Subsetting:** While it correctly identifies the `idxmax()` logic for `latest_data`, it doesn't handle `NaN` values for `gdp_per_capita` or `total_vaccinations`. These should ideally be dropped before plotting to ensure a clean visual.
3.  **Visual Enhancements and Robustness:**
    *   No error handling for file loading (`try-except`).
    *   Lack of `tight_layout()` for plots, which often leads to labels overlapping.
    *   Does not set a professional seaborn theme (`sns.set_theme`).
    *   Missing specific customizations like `grid(True)` or specific `palette` choices.
    *   The scatter plot does not include `hue` for continent or `size` for population, which were present in the ground truth and add significant insight when available. Also, log scales for `gdp_per_capita` and `total_vaccinations` would often be beneficial for highly skewed data, as seen in the ground truth.
    *   Does not explicitly handle `NaN` values before plotting for histogram and box plot, though `sns.histplot` and `sns.boxplot` are generally robust to them.

The most critical issue is the incorrect calculation for `total_deaths` by continent, which affects the data interpretation significantly. Other points are mostly about robustness and presentation quality.",gemini-flash-1.5,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

cr_data = pd.read_csv('clash_royale_cards.csv')

print(cr_data.dtypes)

print(cr_data[['elixirCost', 'Win Rate', 'Usage']].describe())

print(len(cr_data))
```",8,"The generated code correctly addresses all core requirements of the task: loading the CSV, identifying column data types, calculating descriptive statistics for the specified columns, and reporting the total number of rows. It uses appropriate pandas functions (`dtypes`, `describe`, `len`) efficiently.

However, there are a few areas for improvement to match the robustness and clarity of the ground truth:

1.  **Error Handling:** The generated code lacks error handling for `FileNotFoundError` during CSV loading. This makes it less robust than the ground truth, which includes a `try-except` block.
2.  **Output Clarity:** While the `describe()` method provides the requested statistics, its output also includes statistics for count, 25%, 50%, and 75% percentiles, which were not explicitly requested. The ground truth specifically calculates `mean`, `median`, `std`, `min`, `max` using `agg()`, leading to a more focused output as per the prompt's request.
3.  **Column Names Listing:** The prompt asked to ""List all column names and identify the data type of each column."" The generated code only prints `cr_data.dtypes`, which effectively shows data types and implicitly lists column names, but doesn't explicitly print the column names distinctly as desired (e.g., in a separate list or loop). The ground truth's `for col in df.columns: print(f""- {col}"")` provides a clearer separation of this requirement.
4.  **Readability/Comments:** The ground truth includes more descriptive print statements and comments, which enhances readability and understanding, especially for someone unfamiliar with the code.

Overall, it's a very good attempt that fulfills the functional requirements, but could be improved in terms of robustness, output precision, and presentation.",gemini-flash-1.5,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv('clash_royale_cards.csv')
filtered_df = df[df['elixirCost'] <= 3]
result_df = filtered_df[['Card', 'Win Rate']].sort_values('Win Rate', ascending=False)
print(result_df)

```",9,"The generated code correctly solves the task: it loads the CSV, filters by elixirCost, selects the specified columns, and sorts the result as required. The code is concise and idiomatic pandas, which is a strength. It includes the necessary import.

The only minor area for improvement, preventing a perfect score, is the lack of error handling for the `FileNotFoundError` or other potential issues during CSV loading, which the ground truth code addresses. While not explicitly asked for in the prompt's requirements (which focused on the core data manipulation), robust code typically includes such handling. However, given the prompt's scope, the core logic is perfectly executed.",gemini-flash-1.5,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('clash_royale_cards.csv')
plt.figure(figsize=(10, 6))
sns.histplot(df['elixirCost'], kde=False)
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.show()

```",7,"The generated code correctly loads the CSV, generates a histogram for `elixirCost`, and includes a title and labels. It uses appropriate libraries (`pandas`, `matplotlib`, `seaborn`).

Areas for improvement:
1.  **Robustness**: The ground truth includes error handling for `FileNotFoundError` and checks if the `elixirCost` column exists. The generated code does not include these checks, making it less robust if the file is missing or the column name is incorrect.
2.  **Clarity of bins**: While `histplot` auto-generates bins, explicitly defining them (`bins=range(1, 12)`) as in the ground truth for `elixirCost` (which is discrete, typically 1 to 10) would make the histogram more accurate and readable for this specific data, ensuring each integer cost has its own bar.
3.  **Y-axis Label**: ""Frequency"" is acceptable, but ""Number of Cards"" as in the ground truth is more semantically precise for this dataset.
4.  **Aesthetic Enhancements**: The ground truth adds `edgecolor='black'`, `color='skyblue'`, `xticks` and `grid` for better visual presentation and readability. While not strictly required by the prompt, they enhance the quality of the plot.",gemini-flash-1.5,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

iris = pd.read_csv('iris.csv')

print(iris.dtypes)

numerical_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
print(iris[numerical_cols].describe())

print(iris['Species'].value_counts())
```",7,"The generated code successfully completes the core tasks: loading the CSV, displaying data types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts. However, it lacks robustness and user-friendliness compared to the ground truth.

**Strengths:**
*   Correctly loads the CSV file.
*   Uses `df.dtypes` to list column data types.
*   Correctly uses `df[numerical_cols].describe()` to get descriptive statistics (mean, std, min, max, and also median and quantiles, which are suitable).
*   Correctly uses `df['Species'].value_counts()` to report unique species and their counts.
*   Properly imports pandas.

**Areas for Improvement (Why not a higher score):**
*   **Error Handling:** The generated code does not include any error handling for `FileNotFoundError` (if `iris.csv` isn't present) or other potential issues during file loading. The ground truth includes a `try-except` block for robustness.
*   **Missing Specific Output:** While `describe()` provides mean, std, min, and max, it also provides other statistics (25%, 50%, 75% percentiles, and count). The task asked for specific ones (mean, median, standard deviation, min, max). While `describe()` is often acceptable, the ground truth's use of `.agg(['mean', 'median', 'std', 'min', 'max'])` is more precise to the requested output.
*   **Clarity and Readability of Output:** The output from the generated code is concise but less user-friendly than the ground truth. The ground truth adds print statements with headings (e.g., ""--- Column Names and Data Types ---"") which significantly improve readability and understanding for the user. The generated code just prints the dataframes directly without context.
*   **Completeness of requests:** The prompt explicitly asked to ""List all column names and identify the data type of each column"". The generated code only provides `df.dtypes` which implies the column names but doesn't explicitly list them separately as `df.columns.tolist()`.
*   **Robustness in Column Selection:** The generated code assumes the `numerical_cols` and `Species` column exist. The ground truth includes checks to ensure these columns are actually present in the DataFrame before attempting operations, which prevents potential `KeyError` exceptions.",gemini-flash-1.5,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

iris = pd.read_csv('iris.csv')
setosa = iris[iris['Species'] == 'Iris-setosa']
print(setosa[['SepalLengthCm', 'SepalWidthCm']])

```",8,"The generated code correctly performs all the required steps: loading the CSV, filtering by 'Iris-setosa' species, and selecting the 'SepalLengthCm' and 'SepalWidthCm' columns, then printing the result. It uses proper imports and is concise.

However, it lacks robustness compared to the ground truth. It does not include error handling for `FileNotFoundError` or `KeyError`, which could occur if the `iris.csv` file is missing or if the column names are incorrect in the dataset. While the prompt asked for ""only the Python code without explanations"" and did not explicitly state error handling was required, it's a significant aspect of production-ready code for data loading tasks. The ground truth demonstrates best practices by incorporating these safeguards.",gemini-flash-1.5,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

iris = pd.read_csv('iris.csv')
plt.figure(figsize=(8, 6))
plt.scatter(iris['SepalLengthCm'], iris['SepalWidthCm'], c=iris['Species'].astype('category').cat.codes)
plt.title('Sepal Length vs. Sepal Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.show()

```",6,"The generated code correctly loads the data and creates a scatter plot with the specified axes. It also includes a title and axis labels. However, it fails on the crucial requirement to color the points by the `Species` column in a readable and distinguishable way. Using `astype('category').cat.codes` assigns numerical codes to species, which `matplotlib` then interprets as a continuous colormap. This makes it impossible to distinguish between discrete species, as intended by the task, and does not provide a legend for species differentiation. The ground truth code iterates through species and plots them individually, which is a common and effective way to achieve discrete coloring and enable a legend. The generated code also lacks error handling for file not found or missing columns, which is present in the ground truth for robustness.",gemini-flash-1.5,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

df = pd.read_csv('heart.csv')

print(df.dtypes)

print(df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR']].describe())

print(df['HeartDisease'].value_counts())
```",7,"The generated code correctly performs the core tasks: loading the CSV, displaying data types, calculating descriptive statistics for the specified columns, and reporting value counts for `HeartDisease`.

However, it lacks robustness and completeness compared to the ground truth.
Specifically:
1.  **Column Names Listing:** The task explicitly asked to ""List all column names and identify the data type of each column."" The generated code only prints `df.dtypes`, which identifies data types but doesn't explicitly list the column names separately in the way the ground truth does (e.g., iterating and printing each name).
2.  **Specific Descriptive Statistics:** The task asked for ""mean, median, standard deviation, min, max"". While `describe()` outputs these, it also outputs `count`, `25%`, and `75%`. The ground truth specifically selects and renames for exact matching the requirement. This is a minor point but impacts exact adherence.
3.  **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and general exceptions during CSV loading, which is crucial for real-world applications. The generated code lacks this.
4.  **Clarity and Formatting:** The ground truth provides clear print statements and headings for each section, making the output much more readable and user-friendly. The generated code's output is terse.
5.  **Robustness for Column Existence:** The ground truth checks if numerical columns and the `HeartDisease` column actually exist before attempting operations, providing informative warnings/errors if they don't. The generated code would crash if a specified column were missing.

Overall, it's a functional solution for the core logic but misses important aspects of robustness, clarity, and specific output formatting from the prompt.",gemini-flash-1.5,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

df = pd.read_csv('heart.csv')
filtered_df = df[(df['Age'] > 50) & (df['HeartDisease'] == 1)]
print(filtered_df[['Age', 'Sex', 'ChestPainType', 'Cholesterol']])

```",9,"The generated code correctly addresses all functional requirements: it loads the CSV, filters by 'Age' > 50 and 'HeartDisease' == 1, and selects/displays the specified columns. Imports are correct.

Areas for minor improvement (not critical for the core task, hence a 9):
- The ground truth includes a function wrapper and error handling (`try-except` for `FileNotFoundError`, `KeyError`), which makes the code more robust and reusable. The generated code is a direct script. While not explicitly asked for, a robust solution often includes these.
- The ground truth includes comments and print statements for clarity, which are good practices.

Overall, for the bare-bones task asked, the generated code is highly accurate and efficient.",gemini-flash-1.5,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

df = pd.read_csv('winequality-red.csv')
filtered_df = df[(df['alcohol'] > 12) & (df['pH'] < 3.3)]
highest_quality_wine = filtered_df.loc[filtered_df['quality'].idxmax()]
print(highest_quality_wine)

```",8,"The generated code correctly addresses all parts of the task: it applies the compound filtering condition, finds the wine with the highest quality in the filtered subset, and displays all its features. The use of pandas for filtering and `idxmax()` is efficient and correct.

Strengths:
- Correctly implements all three core requirements.
- Uses appropriate pandas functionalities (`read_csv`, boolean indexing, `idxmax`, `loc`).
- Concise and to the point.

Areas for Improvement (to achieve a perfect score):
- Lack of error handling (e.g., `FileNotFoundError` if the CSV doesn't exist, or if the filtered DataFrame is empty).
- No clear function encapsulation, which makes the code less reusable and testable compared to the ground truth.
- Does not explicitly define the dataset path as a variable, making it slightly less flexible.
- No print statements to indicate progress or the number of filtered wines, which can be helpful for debugging and user understanding.
- Does not handle the edge case where `filtered_df` might be empty, which would cause an error when trying to call `idxmax()` on an empty Series.
- The output for `print(highest_quality_wine)` might be truncated by default in some environments; the ground truth's use of `.to_string()` ensures all features are displayed.",gemini-flash-1.5,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('heart.csv')

plt.figure(figsize=(8, 6))
plt.hist(df['Age'], bins=20)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(x='Sex', data=df)
plt.title('Distribution of Sex')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()

```",8,"The generated code correctly loads the dataset and generates both the histogram for 'Age' and the bar chart for 'Sex', satisfying the core requirements of the task. It uses appropriate libraries (pandas, matplotlib, seaborn) and includes titles and labeled axes for both plots.

Strong points:
- Correctly uses `pd.read_csv` to load the data.
- Successfully generates a histogram for 'Age' using `plt.hist`.
- Successfully generates a bar chart for 'Sex' using `sns.countplot`.
- Includes titles and axis labels for both plots as requested.
- Proper use of `plt.show()` to display plots.

Areas for improvement to achieve a higher score (9 or 10):
- **Sex labels:** The `Sex` column in `heart.csv` typically uses 0 and 1. The ground truth explicitly maps these to 'Female' and 'Male' for better readability on the bar chart. The generated code does not do this, leaving numerical labels on the 'Sex' axis, which is less user-friendly. This is a common practice in data visualization benchmarks for clarity.
- **Error handling:** The ground truth includes robust error handling for `FileNotFoundError` and `KeyError`. While not explicitly requested in the prompt, it's good practice for production-ready code.
- **Plot aesthetics/details:** The ground truth adds `kde=True` to the histogram for a more advanced visualization, uses `bins='auto'` for optimal binning, and includes `plt.grid` and `plt.tight_layout` for better presentation. These are minor but contribute to higher quality plots.
- **Figure sizing:** While `plt.figure(figsize=(8, 6))` is present, the specific aesthetics and optimal settings demonstrate a deeper understanding when comparing to the ground truth.",gemini-flash-1.5,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')

print(df.dtypes)

df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
print(df['streams'].isnull().sum())

numerical_cols = ['streams', 'bpm', 'danceability_%', 'valence_%', 'energy_%']
print(df[numerical_cols].describe())

print(df['artist(s)_name'].nunique())

```",7,"The generated code correctly performs the core data manipulation and reporting tasks. It loads the CSV, converts the `streams` column to numeric (handling errors), calculates descriptive statistics for the specified columns, and counts unique artists.

However, it lacks some robustness and completeness compared to the ground truth:
- **Error Handling:** It doesn't include a `try-except` block for `FileNotFoundError` when loading the CSV, which is crucial for a real-world scenario.
- **Output Clarity:** The output for ""Column Names and Data Types"" and ""Descriptive Statistics"" is less explicitly stated. `df.info()` from the ground truth provides a much clearer and more comprehensive overview of column types, non-null counts, and memory usage. The generated code only prints `df.dtypes` without any descriptive header.
- **Reporting NaNs:** While it correctly calculates and prints the sum of NaNs in `streams`, it doesn't provide any descriptive text (e.g., ""Number of NaN values in 'streams':"") nor does it show sample rows with NaNs, which is helpful for debugging or understanding data quality issues.
- **Descriptive Statistics Detail:** It prints the full `describe()` output, which includes counts and 25%/75% quartiles. The ground truth specifically focuses on mean, median, std, min, and max, and clearly labels them.
- **Verbosity/User-Friendliness:** The ground truth includes more print statements to clearly label each section of the output, making the results much easier to interpret for a user.

Overall, it's a functional solution that hits the key requirements but could be improved in terms of error handling, output clarity, and user experience.",gemini-flash-1.5,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')
df = df[df['released_year'] == 2023]
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
result = df[['track_name', 'artist(s)_name', 'streams']].sort_values('streams', ascending=False)
print(result)

```",9,"The generated code successfully completes all specified tasks: loading the CSV, filtering by `released_year` 2023, selecting the required columns, converting `streams` to numeric, and sorting. The code is concise and efficient.

One minor point is the lack of error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. While not explicitly asked for in the prompt's requirements for the *generated* code, robust code usually includes it. However, given the prompt asked for ""only the Python code without explanations"" and focused on the data manipulation steps, the generated code directly addresses the prompt's core requirements very well.

Also, the ground truth explicitly drops rows with `NaN` in streams before sorting, stating ""If NaNs should be kept, this line can be omitted. For a clean top-streams list, dropping them is good."" The generated code doesn't explicitly drop NaNs, which means rows with NaN streams would appear at the end after sorting. This is a subtle difference and the prompt didn't strictly require dropping NaNs, so it's a minor point. Because of this, the generated code doesn't produce an *identical* output if NaNs are present, but it still correctly sorts according to the prompt's requirements.",gemini-flash-1.5,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('spotify_2023.csv')

plt.figure(figsize=(10, 5))
plt.hist(df['bpm'], bins=20)
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 5))
plt.scatter(df['danceability_%'], df['energy_%'])
plt.title('Danceability vs. Energy')
plt.xlabel('Danceability (%)')
plt.ylabel('Energy (%)')
plt.show()

```",9,"The generated code successfully loads the dataset, generates both the histogram for `bpm`, and the scatter plot for `danceability_%` vs `energy_%`. It correctly includes titles and axis labels for both plots, and uses the correct libraries.

**Strong points:**
*   Correctly loads the CSV using pandas.
*   Successfully generates a histogram for `bpm` with appropriate labels and title.
*   Successfully generates a scatter plot for `danceability_%` vs `energy_%` with appropriate labels and title.
*   Uses `matplotlib.pyplot.show()` to display the plots, which is good practice.

**Areas for minor improvement:**
*   The ground truth demonstrates more robust error handling (e.g., `try-except` blocks for `FileNotFoundError` and `KeyError` for missing columns). While not strictly requested for the primary task, it's good practice for production-ready code.
*   The ground truth also uses `plt.tight_layout()` to prevent labels from overlapping, which can be useful when plots become more complex.
*   The ground truth uses more descriptive titles and axis labels (e.g., including 'percentage' in the scatter plot labels and 'beats per minute' in the histogram x-axis, and referencing the dataset name in the histogram title). This enhances clarity.
*   The ground truth opens new figures explicitly (`plt.figure()`) for each plot before defining them, which is a slightly clearer way to manage multiple plots. While the generated code implicitly does this with successive `plt.plot()` calls followed by `plt.show()`, explicit creation makes the intent clearer.

Overall, the generated code is very good and fulfills the core requirements of the prompt effectively. The points for improvement are mostly about robustness and minor enhancements to clarity and presentation, rather than fundamental correctness.",gemini-flash-1.5,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

print(df.dtypes)

for col in ['Video views', 'Likes', 'Dislikes']:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '', regex=False), errors='coerce').fillna(0)

print(df[['Video views', 'Likes', 'Dislikes']].describe())

df['Category'] = df['Category'].fillna('Unknown')

print(df['Category'].nunique())

```",8,"The generated code correctly addresses most of the task requirements.

**Strong points:**
- **Correct cleaning and conversion:** It correctly handles comma removal, converts to numeric, and fills NaNs with 0 for the specified numeric columns in a concise manner.
- **Descriptive stats:** It correctly calculates and prints the descriptive statistics using `.describe()`.
- **Category NaN fill:** It correctly fills missing `Category` values with 'Unknown'.
- **Unique category count:** It correctly reports the unique count for the `Category` column.
- **Proper imports:** It includes the necessary `pandas` import.

**Areas for improvement:**
- **Clarity in output for column types:** While `print(df.dtypes)` provides the data types, the request was ""List all column names and their inferred data types."" `df.info()` from the ground truth is generally preferred for this as it also shows non-null counts, which is more comprehensive and often more useful for data inspection. The current output is just the series of dtypes, which is less readable for a full report.
- **Explicit logging/reporting for each step:** The ground truth code includes more verbose print statements for each step, indicating what is being done and confirming success (e.g., ""Successfully loaded..."", ""Processing column..."", ""initial NaN count...""). While not strictly required by the prompt's final output, it makes the code more understandable and user-friendly often desired in benchmark tasks.
- **Error handling for file loading:** The ground truth includes a `try-except` block for `FileNotFoundError` which is good practice. The generated code assumes the file exists.
- **Formatting of descriptive statistics:** `df.describe()` is great, but the ground truth explicitly formatted the stats for each column with labels, which could be seen as more aligned with ""Calculate and report basic descriptive statistics (mean, median, std, min, max)"". `describe()` gives more than just these, and in a different format.

Overall, it's a very functional and concise solution that fulfills the core requirements. The minor points are mostly about output presentation and robustness, not correctness of the data manipulation itself.",gemini-flash-1.5,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')
df['Video views'] = pd.to_numeric(df['Video views'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
filtered_df = df[df['Video views'] > 50000000]
result_df = filtered_df[['Video', 'Video views']].sort_values('Video views', ascending=False)
print(result_df)

```",9,"The generated code correctly implements all specified requirements: loading the dataset, cleaning the 'Video views' column (removing commas, converting to numeric, filling NaNs with 0), filtering by views > 50,000,000, selecting the correct columns, and sorting in descending order. The code is concise and efficient.

One minor improvement, though not strictly required by the prompt, would be to include error handling for `FileNotFoundError` as seen in the ground truth, making the code more robust. However, given the prompt's focus on the data manipulation logic and the assumption of the file's presence, its omission doesn't detract significantly from its correctness in solving the core task.",gemini-flash-1.5,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')

for col in ['Likes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

plt.figure(figsize=(10, 6))
plt.scatter(df['Video views'], df['Likes'])
plt.title('Video Views vs. Likes')
plt.xlabel('Video Views (cleaned)')
plt.ylabel('Likes (cleaned)')
plt.show()
```",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans the 'Likes' and 'Video views' columns by removing commas, converting to numeric, and filling NaNs with 0. It also generates the scatter plot with the specified title and axis labels.

Strong points:
- **Correctness:** The core logic for data cleaning (remove commas, convert to numeric with `errors='coerce'`, fill NaNs with 0) is perfectly implemented and correctly applied to both specified columns.
- **Plotting:** The scatter plot is correctly generated with the specified title and axis labels.
- **Conciseness:** The code is quite concise while still being readable and effective.

Areas for very minor improvement (which are not critical and don't significantly reduce the score for this particular task level):
- **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice in real-world scenarios but wasn't explicitly requested in the prompt as a hard requirement beyond ""provide only the Python code.""
- **Plot Readability:** Could add `alpha` or `s` parameters to `plt.scatter` for better visualization of dense plots (though not strictly required by the prompt).
- **Column Existence Check:** The ground truth checks if columns exist before cleaning and plotting, which adds robustness. However, for a benchmark where column names are known and expected, this might be considered overkill.

Overall, the generated code is highly effective and meets all the specified requirements.",gemini-flash-1.5,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', '').str.strip(), errors='coerce')
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', '').str.strip(), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', '').str.strip(), errors='coerce')
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').str.st...",9,"The generated code correctly addresses all parts of the task. It loads the data, correctly cleans all specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing correct substrings, converting to numeric, and handling errors by coercing to NaN. It then prints the data types, calculates descriptive statistics for the required columns, and reports the count of unique values in the `Company Name` column.

The code is concise and efficient for the specified cleaning tasks. The use of `.str.replace()` followed by `pd.to_numeric(errors='coerce')` is the correct approach.

Potential minor improvements (not critical for score deduction given the task's brevity):
- The `describe()` output for the specified columns includes more than just mean, median (50%), std, min, and max. The ground truth specifically filters for these, but the prompt asks for ""basic descriptive statistics (mean, median, std, min, max)"". `describe()` does provide these, albeit with additional stats like count and 25%/75% percentiles. For a strict interpretation, one might filter the `describe()` output.
- No error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. However, the prompt didn't explicitly ask for robust error handling beyond the `errors='coerce'` for numeric conversion.
- The output format for the unique company count is just the number; the ground truth provides a more descriptive string output. Again, this wasn't explicitly requested in the prompt beyond ""Report the count"".",gemini-flash-1.5,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (India)'] = df['Launched Price (India)'].astype(str).str.replace('INR ', '', regex=False).str.replace(',', '', regex=False).str.replace('₹', '', regex=False).astype(float, errors='coerce')
df['RAM'] = df['RAM'].astype(str).str.replace('GB', '', regex=False).astype(float, errors='coerce')

filtered_df = df[(df['Company Name'] == 'Apple') & (df['Launched Price (India)'] < 100000)]

result_df = filtered_df[['Mod...",9,"The generated code correctly performs all the required steps: loading the data, cleaning both 'Launched Price (India)' and 'RAM' columns (including handling errors with `coerce`), filtering by company and price, selecting the specified columns, and sorting. The chaining of `str.replace` and `astype(float, errors='coerce')` is concise and effective for cleaning.

Minor potential improvements for robustness or clarity:
1.  **Chained `astype` and `errors='coerce'`**: While `astype(float, errors='coerce')` works well for the final conversion, the `str.replace` operations should conceptually precede the type conversion. For example, `df['Launched Price (India)'].astype(str)...astype(float, errors='coerce')` is correct, but directly using `pd.to_numeric` can sometimes be more explicit for complex string to numeric conversions, especially when intermediate NaNs might be desired before the final numeric conversion. However, for this task, the current chaining is perfectly valid.
2.  **Explicit `ascending=True`**: Although `sort_values` defaults to `ascending=True`, explicitly stating it can improve readability and prevent misinterpretation.
3.  **Error Handling for File Loading**: The ground truth includes `try-except` for `pd.read_csv`, which is good practice for robust applications. The generated code assumes the file exists, which is acceptable given the task's context (assuming the file is in the current directory), but less robust.
4.  **Redundant `regex=False`**: `str.replace` defaults to `regex=False` for string patterns, so it's not strictly necessary.

Overall, it's a very good solution that fulfills all the requirements efficiently.",gemini-flash-1.5,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('mobiles_2025.csv')

df['RAM'] = df['RAM'].astype(str).str.replace('GB', '').str.strip()
df['RAM'] = pd.to_numeric(df['RAM'], errors='coerce')

df['Battery Capacity'] = df['Battery Capacity'].astype(str).str.replace('mAh', '').str.strip()
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'], errors='coerce')

plt.figure(figsize=(10, 6))
plt.scatter(df['RAM'], df['Battery Capacity'], c=pd.factorize(df['Compan...",7,"The generated code successfully completes most of the task requirements. It correctly loads the CSV, cleans both the `RAM` and `Battery Capacity` columns by removing units and converting them to numeric, handling errors by coercing to NaN. It also generates a scatter plot with a title and axis labels.

However, there are a few areas for improvement:
1.  **Coloring based on Company Name**: While `c=pd.factorize(df['Company Name'])[0]` does assign different colors to different company names, it uses an arbitrary numeric mapping which often results in a generic colormap that doesn't effectively distinguish categories without a colorbar or legend. The prompt specifically asked to ""Color the points in the scatter plot according to the `Company Name` column"", implying clear visual differentiation and typically a legend, which `plt.scatter` with `c=factorize` doesn't provide automatically in an intuitive way. Using `seaborn.scatterplot` with `hue='Company Name'` (as in the ground truth) would be a much better approach as it automatically handles categorical coloring and generates a legend, fulfilling the intent more cleanly.
2.  **Lack of Legend**: Directly related to the point above, because `plt.scatter` with `factorize` is used, there's no legend indicating which color corresponds to which company. This makes the ""color by Company Name"" requirement less effective for interpretation.
3.  **In-place cleaning**: The generated code cleans the `RAM` and `Battery Capacity` columns in-place, overwriting the original columns. While this works, it's generally safer and often clearer to create new cleaned columns (e.g., `RAM_cleaned`, `Battery Capacity_cleaned`) as done in the ground truth. This preserves the original data for potential future use or debugging.

Overall, it's a solid attempt that performs the core data manipulation and plotting, but could be enhanced for better visualization and data handling practices.",gemini-flash-1.5,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

df = pd.read_csv('titanic.csv')
print(df.head())
print(df.shape)
print(df.dtypes)
survival_rate = (df['Survived'].sum() / len(df)) * 100
print(f""Survival Rate: {survival_rate:.2f}%"")
print(df.isnull().sum())

```",8,"The generated code successfully performs all required tasks: loading data, displaying head, showing shape, calculating survival rate, and counting missing values. It uses correct pandas functions for these operations.

Areas for improvement to achieve a higher score:
1.  **Readability/Clarity**: The output lacks clear labels for each section of the analysis (e.g., ""1. First 5 rows:"", ""2. Shape of the dataset:""). The ground truth code uses `print` statements to label each output, which significantly improves readability for a user.
2.  **Robustness**: It does not include error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth and is a good practice for file operations.
3.  **Column Data Types**: The request specifically asked for ""list all column names and their data types"". While `df.dtypes` gets the job done, `df.info()` as used in the ground truth provides a more comprehensive summary, including non-null counts and memory usage, which is often preferred for EDA.
4.  **Instructions Adherence**: The prompt specifically asked for ""list all column names and their data types"" as a distinct point (3). The generated code provides `df.dtypes`, which is correct for types, but `df.info()` is often preferred as it also includes non-null counts which are very useful in EDA. The ground truth explicitly used `df.info()` for this point.",gemini-flash-1.5,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv(""imdb_top_250.csv"", encoding='latin-1')

print(""Column Names and Data Types:"")
print(df.dtypes)

print(""\nDescriptive Statistics:"")
print(df[['Year', 'Rating', 'Runtime']].describe())

print(""\nTotal Number of Movies:"", len(df))
```",6,"The generated code addresses most of the requirements but lacks robustness and precision in its implementation.

**Correctness:**
*   It correctly loads the CSV (assuming `latin-1` encoding is appropriate, which might not always be the case for such a dataset but isn't explicitly wrong without more context).
*   It lists column names and data types using `df.dtypes`.
*   It calculates descriptive statistics for 'Year', 'Rating', and 'Runtime' using `.describe()`.
*   It reports the total number of movies using `len(df)`.

**Code Quality:**
*   It includes the necessary `pandas` import.
*   The code is concise.

**Completeness:**
*   **Missing Robustness:** There's no error handling for `FileNotFoundError`, which is crucial for real-world scripts.
*   **Runtime Column Issue:** The `Runtime` column in the `imdb_top_250.csv` dataset is typically in a string format (e.g., ""142 min""). The generated code attempts to call `.describe()` directly on it without converting it to a numeric type. This will likely cause an error or produce `NaN` values for the 'Runtime' statistics, making the 'Descriptive Statistics' calculation incomplete or incorrect for this specific column. The ground truth explicitly handles this conversion.
*   **Specific Statistics:** The task asked for ""mean, median, standard deviation, min, max"". While `describe()` outputs these, the prompt specifically listed them, implying a more direct extraction or presentation might be preferred if some other statistics were to be omitted by `describe()`. The ground truth explicitly selects these.
*   **Output Formatting:** The output for descriptive statistics is the full `describe()` table. While it contains the requested metrics, the ground truth explicitly formatted it to only show `mean`, `median`, `std`, `min`, `max` and renamed `50%` to median, which is a bit more refined and aligned with the ""report basic descriptive statistics"" requirement.

To improve, the code needs to:
1.  Add `try-except` for `FileNotFoundError`.
2.  Implement logic to convert the 'Runtime' column from string to numeric before calculating statistics.
3.  Consider explicitly selecting only the required statistics (mean, median, std, min, max) from `describe()` and possibly renaming '50%' to 'median' for clarity if the output needs to be very specific.",gemini-flash-1.5,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv(""imdb_top_250.csv"")
filtered_df = df[df['Rating'] >= 9.0]
result_df = filtered_df[['Title', 'Rating']].sort_values('Rating', ascending=False)
print(result_df)

```",9,"The generated code correctly addresses all the core requirements of the task: loading the CSV, filtering by rating, selecting specified columns, and sorting. It's concise and effectively uses pandas functionalities. The only minor point for improvement, which is common in benchmark evaluation where robust production-ready code is often desired, is the lack of error handling (e.g., `FileNotFoundError`, `KeyError`) that was present in the ground truth. While not explicitly requested in the prompt, robust code typically includes such handling. However, given the prompt's focus on the data manipulation steps, the generated code performs exceptionally well.",gemini-flash-1.5,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('imdb_top_250.csv')

plt.figure(figsize=(8, 6))
plt.hist(df['Rating'], bins=20)
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x='Runtime', y='Rating', data=df)
plt.title('Rating vs. Runtime')
plt.xlabel('Runtime')
plt.ylabel('Rating')
plt.show()

```",7,"The generated code correctly addresses the main plotting requirements: loading the CSV, generating a histogram for 'Rating', and a scatter plot for 'Rating' vs. 'Runtime'. It also includes appropriate titles and axis labels. However, it lacks robustness in handling potential issues like `FileNotFoundError` or non-numeric `Runtime` data, which is common in real-world datasets. The ground truth code significantly improves on this by including robust error handling and explicit data preprocessing for the `Runtime` column (e.g., extracting numeric values from '120 min' strings and handling NaNs), which was implicitly expected for a complete solution. The generated code assumes `Runtime` is directly numeric, which might not always be true, leading to potential errors or incorrect plots. The binning for the histogram is adequate (20) but could be slightly improved (e.g., using `kde=True` from seaborn as in the ground truth for a smoother distribution visualization).",gemini-flash-1.5,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

penguins = pd.read_csv('penguins.csv')

print(penguins.isnull().sum())

numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
penguins_no_na = penguins.dropna(subset=numerical_cols)

print(penguins_no_na[numerical_cols].agg(['mean', 'median', 'std', 'min', 'max']))

print(penguins['species'].nunique())
print(penguins['island'].nunique())

```",7,"The generated code correctly performs the core data manipulation tasks: loading the dataset, reporting NA counts, dropping NAs for specific columns and calculating descriptive statistics, and counting unique values for specified columns. However, it lacks robustness and user-friendliness compared to the ground truth.

**Strengths:**
*   Correctly loads the CSV.
*   Correctly uses `isnull().sum()` to report NA counts per column.
*   Correctly uses `dropna(subset=...)` to filter rows based on NAs in the specified numerical columns.
*   Correctly calculates descriptive statistics using `.agg()`.
*   Correctly uses `nunique()` for the 'species' and 'island' columns.
*   Includes the necessary `pandas` import.

**Areas for Improvement:**
*   **Lack of Readability/Clarity:** The output is very terse. There are no print statements explaining what each output block represents, making it difficult for a user to understand the results without knowing the exact order of operations. The ground truth provides clear headers and explanations for each section.
*   **Missing Error Handling:** It doesn't handle `FileNotFoundError` when loading the CSV, which is crucial for robust code. If `penguins.csv` isn't present, the script will crash.
*   **Partial NA Report:** While `penguins.isnull().sum()` outputs all columns' NA counts, it doesn't explicitly filter to show *only* columns with missing values or state whether there are no missing values at all, as the ground truth does.
*   **No Report on Filtered Rows:** It doesn't report how many rows were dropped after filtering, which is very useful for understanding the impact of the `dropna` operation.
*   **No Column Existence Check:** It assumes 'species' and 'island' columns exist without checking, which could lead to `KeyError` if the dataset structure changes.
*   **No Output for Empty Filtered Data:** If `penguins_no_na` were to become empty after dropping NAs, the `agg` call would still run, but it would be good practice to add a check for an empty DataFrame before computing statistics (though less critical for this specific dataset).

To achieve a higher score, the generated code needs to incorporate more robust error handling, clearer output descriptions, and more comprehensive reporting (e.g., number of rows before/after filtering).",gemini-flash-1.5,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

df = pd.read_csv('penguins.csv')
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]
selected_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]
cleaned_df = selected_df.dropna()
print(cleaned_df.head())

```",9,"The generated code correctly performs all requested operations: loading the data, filtering by species and island, selecting the specified columns, dropping rows with missing values, and displaying the first 5 rows. The code is concise and efficient.

Areas for minor improvement:
1.  **Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice for robust code. The generated code lacks this.
2.  **Explicit `subset` in `dropna()`:** While `dropna()` without `subset` works for a DataFrame with only the target columns (as is the case here), explicitly stating `subset=['culmen_length_mm', 'culmen_depth_mm']` makes the intent clearer and the code more robust if `selected_df` somehow contained other columns.
3.  **Explicit `head()` argument**: `head()` defaults to 5, but `head(5)` is more explicit.
4.  **Print statement clarity**: The ground truth adds a descriptive print message before displaying the head, which improves readability for the user.",gemini-flash-1.5,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

penguins = pd.read_csv(""penguins.csv"")
penguins_clean = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])
plt.figure(figsize=(8, 6))
plt.scatter(penguins_clean['culmen_length_mm'], penguins_clean['culmen_depth_mm'], c=penguins_clean['species'].astype('category').cat.codes)
plt.title('Culmen Length vs. Culmen Depth of Penguins')
plt.xlabel('Culmen Length (mm)')
plt.ylabel('Culmen Depth (mm)')
plt.show()

```",7,"The generated code correctly loads the data, filters missing values for the specified columns, and creates a scatter plot with the correct axes and labels. It also includes a title. However, it uses `matplotlib.pyplot.scatter` for coloring by species, which requires converting the categorical `species` column to numerical codes. While functional, this approach doesn't provide a direct color mapping (like a legend) that visually links each species to its specific color on the plot, unlike `seaborn.scatterplot` which is designed for this and was implicitly hinted at by one of the evaluation criteria (""Use matplotlib or seaborn for plotting.""). Using seaborn would have resulted in a clearer and more intuitive plot with a legend automatically generated. The prompt specifically asked to ""Color the points in the scatter plot according to the species column"" which implies a clear, interpretable distinction. The current implementation creates a distinction, but without a legend, it's not immediately interpretable.",gemini-flash-1.5,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# 1. Read the dataset
try:
    df = pd.read_csv(""penguins.csv"")
except FileNotFoundError:
    print(""Error: penguins.csv not found in the current directory."")
    exit()


# 2. Fill NA values with the mean for numerical columns
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm']
for col in numerical_cols:
    mean_val = df[col].mean()
    df[col].fillna(mean_val, inplace=True)


# 3. Calculate and print descriptive statistics
for col in numerical_cols:
    mea...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, fills NA values in the specified numerical columns with their respective means, calculates and prints the mean, median, and standard deviation for these cleaned columns, and finally prints the total number of rows and columns.

**Strong points:**
*   **Correctness:** All tasks are performed accurately.
*   **Completeness:** All specified requirements are met.
*   **Code Quality:** The code is well-structured, uses clear variable names, and handles file not found errors gracefully. The use of a loop for NA imputation and statistics calculation for multiple columns is efficient.
*   **Conciseness:** The code is quite concise while remaining readable.

**Minor areas for improvement (not penalizing score significantly, but good practice):**
*   **Error Handling for Columns:** The code assumes that `culmen_length_mm` and `culmen_depth_mm` columns definitely exist. While this is likely true for `penguins.csv`, in a more general scenario, it would be robust to check if `col in df.columns` before attempting operations on `df[col]`, similar to how the ground truth does. This would prevent `KeyError` if a column name was misspelled or missing.
*   **Output Formatting:** The output for statistics is good, but adding a newline or more clear separators between the statistics of different columns (e.g., `print(""\n"" + ""-"" * 20 + ""\n"")` instead of just `print(""-"" * 20)`) could improve readability slightly.
*   **Constants:** Using a constant for the filename (`DATASET_FILE = ""penguins.csv""`) and the list of numerical columns (`NUMERICAL_COLUMNS = ['culmen_length_mm', 'culmen_depth_mm']`) as in the ground truth can improve maintainability and readability, especially in larger scripts.",gemini-flash-1.5,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Drop rows with NA in flipper_length_mm
penguins = penguins.dropna(subset=['flipper_length_mm'])

# Create a histogram of flipper_length_mm
plt.figure(figsize=(8, 6))
sns.histplot(data=penguins, x='flipper_length_mm')
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.show()

# Create a bar cha...",8,"The generated code correctly addresses all requirements of the task. It successfully loads the data, handles NA values in `flipper_length_mm`, and generates both a histogram and a bar chart with appropriate titles and labels.

Strong points:
- Correctly uses `pd.read_csv` to load the data.
- Correctly uses `dropna(subset=['flipper_length_mm'])` to handle NAs.
- Uses `sns.histplot` for the histogram and `sns.countplot` for the bar chart, which are appropriate and efficient for the given visualizations.
- Applies correct titles and x-axis labels as specified in the prompt for both plots.

Areas for minor improvement (to reach a 9 or 10):
- The y-axis label for the histogram is `Frequency`. While `Count` is also common for histograms, `Frequency` or `Density` (if `kde=True` or `stat='density'` were used) is more precise for a histogram. The ground truth used `Count`, so it's consistent with that interpretation.
- It lacks error handling for `FileNotFoundError` when loading the CSV, which is good practice in production-ready code.
- Could include `plt.tight_layout()` for better plot presentation, especially when saving plots.
- The ground truth includes `sns.set_theme` and more detailed aesthetic settings (like font sizes, grid lines), which enhance readability and professionalism but are not strictly required by the prompt.",gemini-flash-1.5,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Filter data for Torgersen island
torgersen_penguins = penguins[penguins[""island""] == ""Torgersen""]

# Count penguins by species
species_counts = torgersen_penguins[""species""].value_counts()

# Print the counts
print(species_counts)
```",9,"The generated code correctly addresses all the core requirements of the task. It loads the dataset, filters it for the 'Torgersen' island, and counts the occurrences of each species, printing the result as requested. The code is concise and clear.

**Strong points:**
*   Correctly uses `pandas.read_csv` to load the data.
*   Accurately filters the DataFrame using boolean indexing for the 'Torgersen' island.
*   Correctly uses `value_counts()` to count the species within the filtered subset.
*   Prints the final result clearly.

**Areas for improvement (minor):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and checks for column existence. The generated code lacks this, which could lead to crashes if the file is missing or columns are misnamed. While not strictly required by the prompt's output format, it's a good practice for production-ready code.
*   **Print Statements:** The ground truth provides more verbose and informative print statements about the steps being performed and the success/failure of operations. The generated code only prints the final `species_counts`. For a benchmark, the current print statements are perfectly valid, but for a real-world script, more context would be beneficial.

Overall, it's a very good solution that directly solves the problem presented. The missing error handling is a common omission in concise benchmark solutions but is an important consideration in real-world applications.",gemini-flash-1.5,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

print(""Shape of the DataFrame:"")
print(df.shape)

print(...",10,"The generated code is excellent. It correctly defines the DataFrame, prints its shape, and lists the data types of all columns, fulfilling all requirements of the task. The imports are correct, the DataFrame creation is accurate, and the subsequent operations (`.shape` and `.dtypes`) are applied and printed as expected. The code is clean, concise, and directly addresses the prompt. Both the generated code and the ground truth code produce identical and correct outputs for the given task. The only minor difference is the lack of extensive comments and separator lines in the generated code, which is perfectly acceptable for a functional solution to this specific task and does not detract from its correctness or completeness.",gemini-flash-1.5,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(""titanic.csv"")

#1. Survivors vs Non-survivors
plt.figure(figsize=(8,6))
df['Survived'].value_counts().plot(kind='bar')
plt.title('Survival Count')
plt.xlabel('Survived (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

#2. Passenger Class Distribution
plt.figure(figsize=(8,6))
df['Pclass'].value_counts().plot(kind='bar')
plt.title('Passenger Class Distribution')
plt.xlabel('Passenger Class')
...",8,"The generated code successfully creates all four required visualizations with proper titles and labels, demonstrating a good understanding of the task. It correctly handles missing age values by filling them with the mean, which is an acceptable approach.

**Strengths:**
- All four specified plots are generated.
- Each plot has a title and axis labels.
- Missing 'Age' values are handled (`fillna(df['Age'].mean())`).
- Uses `pandas` for data manipulation and `matplotlib.pyplot` for plotting, as requested.
- `xticks(rotation=0)` is used effectively for bar charts.

**Areas for Improvement:**
- **Clarity of Axis Labels (Visualization 1):** For the ""Survivors vs Non-survivors"" chart, the x-axis label `Survived (0 = No, 1 = Yes)` is functional but less user-friendly than mapping `0` and `1` to descriptive labels like 'Non-survivor' and 'Survivor' as done in the ground truth. This improves readability significantly.
- **Plot Customization/Aesthetics:** The generated plots are basic. The ground truth code includes more aesthetic enhancements like `color`, `grid`, and `tight_layout()`, which improve the overall visual quality and professional appearance of the plots. While not strictly required by the prompt, these are good practices for visualization.
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` when loading the CSV, which is robust programming practice. The generated code assumes the file will always be present, which is less ideal for real-world scenarios.
- **Missing `numpy` Import:** Although not explicitly used in the generated code, `numpy` is often imported alongside pandas for data science tasks. The ground truth includes it, but it's not a critical omission here.

Overall, the generated code directly addresses the core requirements of the task efficiently. Improvements are mainly in robustness and presentation quality, not in meeting the functional requirements.",gemini-flash-1.5,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]}

df = pd.DataFrame(data)

avg_midterm = df['Midterm_Score'].mean()
print(f""Average Mi...",9,"The generated code is highly correct and addresses all the requirements of the task. It correctly creates the DataFrame, calculates the average midterm and final scores, and identifies the subject with the highest average final score. The code structure is clean, and necessary imports are present.

One minor suggestion for improvement would be to match the output format of the ground truth code more closely, specifically by adding a descriptive prefix to the printed average scores (e.g., ""Average Midterm Score for all students:"") and formatting the numerical outputs to two decimal places (e.g., `{avg_midterm:.2f}`). While not strictly required by the prompt's core logic, consistent and user-friendly output is a good practice in data analysis scripts. However, this is a minor presentation detail and doesn't affect the correctness or completeness of the solution.",gemini-flash-1.5,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']), 
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'], 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5], 
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0], 
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0], 
        'Volume': [100000...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame with the 'Date' column as the index, calculates the 'Daily_Range', finds the date with the highest 'Close' price, and prints the updated DataFrame. The code is clean and uses appropriate pandas functions.

The only minor point for improvement, which keeps it from a perfect 10, is the formatting of the printed date for the highest close price. The ground truth explicitly formats it to 'YYYY-MM-DD', which can be slightly clearer than the default timestamp string. However, since the task didn't explicitly ask for a specific date format, this is a very minor detail.

Strong points:
- Correct DataFrame creation and index setting.
- Accurate calculation of 'Daily_Range'.
- Correct use of `idxmax()` to find the date with the highest 'Close' price.
- All required print statements are present.
- Proper use of pandas library.",gemini-flash-1.5,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', ...",10,"The generated code is correct, complete, and of good quality. It successfully addresses all requirements of the task:
1. It correctly creates the pandas DataFrame with the specified data.
2. It accurately calculates the 'Total_Revenue' for each transaction and adds it as a new column.
3. It correctly identifies the product sold the most in terms of quantity using `groupby()` and `idxmax()`.
4. It correctly prints the first 5 rows of the DataFrame, including the newly added 'Total_Revenue' column (since `df.head()` defaults to 5 rows).

The code is concise, uses appropriate pandas functions, and follows standard Python practices. There are no missing imports or logical errors.",gemini-flash-1.5,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDurationMinutes""].mean()
print...",9,"The generated code correctly defines the DataFrame, calculates the average session duration, and counts users by device type, fulfilling all requirements of the task. The code is clean, concise, and uses appropriate pandas methods (`.mean()`, `.value_counts()`). The output format is also good.

Areas for minor improvement (not critical but good practice):
1.  The ground truth includes a descriptive print statement for the average session duration, indicating it's the ""1. Average Session Duration:"", which aids readability when multiple outputs are present. The generated code's print statement is good, but could be slightly more explicit if needed.
2.  The ground truth includes a separator `print(""-"" * 50)` between the two distinct tasks, which marginally improves output clarity.

These are minor stylistic preferences and do not detract from the functional correctness or quality of the generated code. Therefore, it achieves a high score.",gemini-flash-1.5,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

max_temp = df[""TemperatureCelsius""].max()
min_humidity = df[""HumidityPercent""].m...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the maximum temperature and minimum humidity. It perfectly fulfills all the requirements of the task. The code is concise and efficient.

One very minor point for improvement, which keeps it from a perfect 10, is the formatting of the print statements. The ground truth code included units (°C and %) in its print statements, which is good practice for readability and understanding of numerical output. While not explicitly asked for in the prompt's output format, it's a small detail that enhances user experience with data. However, for core correctness and functionality, the generated code is excellent.",gemini-flash-1.5,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts.
total_likes = df[""Likes""].sum()
print(f""Total Likes: {total_likes}"")

# ...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the total number of likes, and precisely identifies the PostType with the highest average shares. The code is clean, well-structured, and directly addresses all the requirements of the task. The use of f-strings for printing the results is good practice.",gemini-flash-1.5,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It correctly defines the DataFrame, calculates the average age, and counts patients by gender. The code quality is good, with proper imports and a logical structure.

The only minor difference from the ground truth is the formatting of the print statements.
- For average age, the ground truth uses `f""Average Age of all patients: {average_age:.2f} years""` which includes more descriptive text and formats the age to two decimal places. The generated code uses `f""Average age of patients: {average_age}""` which is slightly less descriptive and lacks the formatting.
- For gender counts, the ground truth uses `print(""\nNumber of patients by Gender:"")` while the generated code uses `print(""\nNumber of patients per gender:"")`.

These are minor stylistic differences and do not affect the correctness or functionality of the code. The core logic and output are identical.",gemini-flash-1.5,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1. Title of the movie with the highest rating
highest_rated_movie = df.loc[df[""...",10,"The generated code is excellent.
1. Correctness: It correctly defines the DataFrame, identifies the title of the movie with the highest rating using `idxmax()` (which is a very efficient and idiomatic pandas way to do it), and calculates the average year. All outputs are as expected.
2. Code Quality: The code is clean, concise, and uses proper pandas functions for the tasks. Imports are correct. Variable names are descriptive.
3. Completeness: It addresses all requirements of the task: DataFrame creation, finding the highest-rated movie's title, and calculating the average year.

The use of `df.loc[df[""Rating""].idxmax()]` for finding the row with the maximum rating is particularly good, as it's more direct than filtering based on `max()` and then picking the first element, as shown in the ground truth. This is a common and efficient pandas pattern. The output phrasing is slightly different from the ground truth but perfectly acceptable and clear.",gemini-flash-1.5,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

print(""Total number of cars:"", len(df))
print(""Average PriceUSD:"", df[...",10,"The generated code accurately and efficiently solves the task. It correctly defines the DataFrame, imports `pandas`, and performs both required calculations (total number of cars and average PriceUSD). The use of `len(df)` for total cars and `df[""PriceUSD""].mean()` for average price is perfectly idiomatic pandas and achieves the desired results. The code is concise and directly addresses all parts of the prompt. While the output formatting for the average price isn't explicitly fixed to two decimal places, the task didn't strictly require it, and the numerical value is correct. Overall, an excellent solution.",gemini-flash-1.5,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}
df = pd.DataFrame(data)

highest_hp_pokemon = df.loc[df[""HP""].idxmax(), ""Name""]
l...",10,"The generated code is correct, complete, and of good quality. It precisely defines the DataFrame as specified, and accurately uses `idxmax()` and `idxmin()` along with `.loc[]` to find and print the names of the Pokemon with the highest HP and lowest Defense, respectively. The code is concise and efficient, mirroring the core logic of the ground truth code.",gemini-flash-1.5,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv('titanic.csv')

missing_cols = df.columns[df.isnull().any()]

df['Age'].fillna(df['Age'].median(), inplace=True)

df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

df.drop('Cabin', axis=1, inplace=True)

df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

print(df.shape)
print(df.head())

```",8,"The generated code correctly performs all the core data cleaning steps as requested by the task.

**Strengths:**
*   **Correctness:** All 6 requirements (load, fill Age, fill Embarked, drop Cabin, create Title, display shape/head) are met correctly.
*   **Conciseness:** The code is quite concise, especially for the title extraction using `str.extract`, which is a good pandas idiom.
*   **Imports:** Includes necessary imports (`pandas`).

**Areas for Improvement (to reach a 9 or 10):**
*   **Robustness (`FileNotFoundError`):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the script more robust. The generated code lacks this.
*   **Clarity on Missing Values (Step 1):** The task explicitly asks to ""Identify columns with missing values."" While the code `missing_cols = df.columns[df.isnull().any()]` does this, it doesn't *display* them to the user as the ground truth does (even if commented out, the intention was there). To fully meet ""identify columns with missing values,"" a print statement would be beneficial.
*   **Title Standardization:** The ground truth includes an important step of standardizing less common titles (e.g., 'Mlle' to 'Miss', 'Dr' to 'Rare'). While the prompt didn't *strictly* require this sophisticated level of feature engineering, it's a common and valuable step in real-world data cleaning for title extraction and makes the 'Title' column much more useful. Its absence makes the output less complete in a practical sense.
*   **Explicit Requirement for `re` (minor):** Although `str.extract` internally uses regex, `re` isn't explicitly imported or used as a direct module, which isn't an issue, but if the task implied direct `re` usage for title extraction (as the ground truth showed with `get_title`), then it would be a minor point. However, `str.extract` is a more idiomatic pandas approach.

Overall, it's a very good attempt that addresses the core requirements effectively. The missing error handling and title standardization are the main points preventing a perfect score.",gemini-flash-1.5,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}
df = pd.DataFrame(data)

# 1. Print the tit...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies the title of the book with the highest price, and precisely calculates the total number of books in stock. The code is well-structured, uses appropriate pandas functions (`idxmax()`, `loc[]`, `sum()`), and provides clear output for both tasks. It fully meets all requirements of the prompt.",gemini-flash-1.5,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

df = pd.read_csv('water_potability.csv')
print(df.head(10))
print(df.shape)
print(df.dtypes)
print(df['Potability'].value_counts(normalize=True) * 100)
print(df.isnull().sum())
print(df.describe())

```",6,"The generated code correctly performs most of the requested EDA steps, but it lacks proper labeling and formatting, making the output difficult to understand. It also doesn't explicitly display all column names and their data types as requested, instead using `df.dtypes` which is less informative than `df.info()`. The error handling for file loading is also missing.

Specific areas for improvement:
- **Output Clarity:** The ground truth includes print statements with clear headers for each section (e.g., ""--- 1. First 10 rows of the dataset ---""). The generated code just prints the output without any context, making it hard to interpret what each block of output represents.
- **Data Types/Column Names:** While `df.dtypes` shows data types, `df.info()` is explicitly mentioned in the ground truth and provides a much more comprehensive summary, including non-null counts, which is valuable for EDA. The prompt asked to ""List all column names and their data types"" explicitly, `df.dtypes` achieves this but `df.info()` is a better choice for completeness.
- **Missing Values:** The generated code correctly displays missing values, but the ground truth explicitly filters to show only columns with missing values and adds a message if no missing values are found, which is a nice touch for clarity.
- **Robustness:** The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file will always be present.
- **Completeness:** The prompt asked to ""Include proper imports"". While `import pandas as pd` is there, the ground truth also includes `DATASET_FILE` as a constant which is good practice for maintainability.",gemini-flash-1.5,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv(""water_potability.csv"")

missing_cols = df.columns[df.isnull().any()]

for col in missing_cols:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col].fillna(df[col].mean(), inplace=True)

df.drop_duplicates(inplace=True)

df = df[
    (df['ph'] >= 0) & (df['ph'] <= 14) &
    (df['Sulfate'] >= 0) &
    (df['Trihalomethanes'] >= 0) &
    (df['Turbidity'] >= 0)
]


summary = pd.DataFrame({
    'Before Cleaning': [df.isnull(...",4,"The generated code attempts to address the requirements but falls short in several key areas, leading to incorrect or incomplete execution of the task.

**Correctness Issues:**
*   **Summary `Before Cleaning` is Incorrect:** The `summary` DataFrame is created with `df.isnull().sum().sum()` and `df.shape` *after* the cleaning operations (missing value imputation, duplicate removal, and range filtering) have already been applied to `df`. This means the ""Before Cleaning"" statistics shown in the `summary` DataFrame are actually the ""After Cleaning"" statistics, and the code fails to capture the true initial state of the dataset.
*   **Partial Range Validation:** The range validation for numerical columns is incomplete. It only checks `ph`, `Sulfate`, `Trihalomethanes`, and `Turbidity` for non-negativity (and `ph` for upper bound). Many other numerical columns (`Hardness`, `Solids`, `Chloramines`, `Conductivity`, `Organic_carbon`) are not checked for non-negativity, which is a crucial part of data range validation for water quality parameters.
*   **No Verification of Missing Values After Cleaning:** While it tries to fill missing values, there's no explicit verification or printout to confirm that *all* missing values have been handled, as requested by requirement 5 (though the incorrect summary table attempts to show this).

**Completeness Issues:**
*   **Lack of Detailed Output:** The prompt explicitly asked for a summary showing *before and after* statistics for *missing values count* and *dataset shape*. The generated code's summary table is conceptually flawed and doesn't clearly present these two distinct states.
*   **No Identification of Missing Columns Before:** The prompt asked to ""Identify columns with missing values"" (before filling). The generated code identifies them internally for filling but doesn't print this information to the console.
*   **No Verification of Remaining Missing Values:** The prompt asked to ""Verify that all values are within reasonable ranges"" and then to verify ""no missing values remain."" The code doesn't explicitly print confirmation of these.
*   **Error Handling for File Loading:** The ground truth includes robust error handling for `FileNotFoundError`. The generated code does not, which is a good practice for real-world scripts.
*   **Readability/User Feedback:** The ground truth provides detailed print statements to guide the user through each cleaning step (e.g., ""Filling missing values in 'Col' with its mean,"" ""Removed X duplicate rows,"" ""Removed Y rows due to out-of-reasonable-range values""). The generated code lacks this level of user feedback, making it harder to understand what cleaning steps were performed and their impact.

The code correctly identifies and fills missing numerical values with the mean and correctly removes duplicates. However, the evaluation for ""before and after"" is fundamentally flawed, and the range validation is incomplete. Thus, it doesn't fully satisfy the prompt's requirements.",gemini-flash-1.5,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('water_potability.csv')

plt.figure(figsize=(8, 6))
sns.countplot(x='Potability', data=df)
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potability (0: Not Potable, 1: Potable)')
plt.ylabel('Count')
plt.show()

numerical_cols = ['pH', 'Hardness', 'Chloramines']
plt.figure(figsize=(15, 5))
for i, col in enumerate(numerical_cols):
    plt.subplot(1, 3, i+1)
    sns....",8,"The generated code correctly creates all four requested visualizations (bar chart, histograms, boxplot, heatmap). It uses appropriate libraries (pandas, matplotlib, seaborn) and sets reasonable titles and labels for plots.

Areas for Improvement:
1.  **Readability/Robustness:** The ground truth includes `sns.set_style(""whitegrid"")` for a consistent aesthetic and robust error handling for `FileNotFoundError` when reading the CSV. The generated code lacks these.
2.  **Detail in Plot Labels:** The ground truth provides more descriptive axis labels and titles (e.g., 'Potability (0: Not Potable, 1: Potable)' in more places, specific units for pH, Hardness, Chloramines). The generated code's labels are more generic.
3.  **Handling Missing Values for Histograms:** While Seaborn's `histplot` can handle NaNs by default, explicitly using `.dropna()` as in the ground truth for individual columns is good practice to prevent unexpected behavior with other plot types or if `kde=True` interacts differently with NaNs in different versions. For the heatmap, `df.corr()` handles NaNs pairwise, which is generally acceptable, but `numerical_df = df.select_dtypes(include=['number'])` in the generated code is a good approach to ensure only numerical features are used for correlation, which the ground truth implied but didn't explicitly select.
4.  **Plot Aesthetics/Refinements:** The ground truth includes `plt.tight_layout()` for all plots, which helps prevent labels/titles from overlapping, and `plt.grid()` for better readability. It also uses specific `palette` and `color` arguments for better visual distinction, which the generated code mostly omits (except for default seaborn color cycles). `plt.xticks(ticks=[0, 1], labels=[...])` also provides more explicit control over x-axis labels.",gemini-flash-1.5,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures

df = pd.read_csv('winequality-red.csv')

# Feature Engineering
df['volatile_acidity_ratio'] = df['volatile acidity'] / df['fixed acidity']
df['total_sulfur_dioxide_ratio'] = df['total sulfur dioxide'] / df['free sulfur dioxi...",6,"The generated code attempts to address the task requirements but falls short in several areas, particularly regarding the completeness and clarity of the feature engineering and selection comparison.

**Correctness:**
- **Feature Engineering:** It creates two ratio features and uses `PolynomialFeatures` for interactions. However, the use of `PolynomialFeatures` without careful selection or understanding can lead to an explosion of features, making subsequent steps less meaningful. Also, one ratio feature has a potential division by zero (`free_sulfur_dioxide` can be 0).
- **Feature Selection:** RFE and SelectKBest are applied correctly to the training data.
- **Output:** The output format for comparison is minimal and doesn't fully meet the requirement of ""Compare and contrast the feature sets selected by each method"" or ""Display the final selected features with their importance scores or rankings."" The RFE ranks are for *all* features, not just the selected ones, and the `kbest_scores` need to be paired with their respective features for clarity.

**Code Quality:**
- **Imports:** Necessary imports are present.
- **Structure:** The code is structured logically with distinct sections for loading, feature engineering, and feature selection.
- **Readability:** Generally readable.
- **Error Handling:** Lacks error handling for file loading, which is good practice.
- **Scaling:** Does not scale the data, which is generally important for many machine learning algorithms, though less critical for tree-based models like Random Forest. However, SelectKBest (f_regression) might benefit from it.

**Completeness:**
- **Feature Engineering:** Meets the ""at least three new meaningful features"" partially. The polynomial features approach is broad and generates many features, making it harder to discern specific ""meaningful"" ones without further processing or selection. The two ratio features are meaningful. It doesn't handle NaNs/Infs that might arise from division by zero.
- **Feature Selection Methods:** Applies both RFE and SelectKBest.
- **Top 5-7 features:** RFE selects 7, SelectKBest selects 6. This is within the requested range.
- **Comparison:** The comparison is very minimal. It just prints the selected features from each method. It does not explicitly ""compare and contrast"" in a narrative or structured way, such as identifying common features, unique features, or discussing why differences might arise (as done in the ground truth).
- **Display Final Selected Features with Importance/Rankings:** For RFE, it prints `rfe_ranks` which is the ranking for all features. It doesn't extract the importance scores for the *selected* RFE features (which requires re-fitting or inspecting the estimator). For SelectKBest, it prints the scores, but they are not clearly paired with the corresponding features in the output in a very readable format. It doesn't show rankings for SelectKBest features.

**Specific Areas for Improvement:**
1.  **Robust Feature Engineering:**
    *   Add `EPSILON` to denominators to prevent division by zero in ratio features.
    *   Handle `NaN` or `inf` values that might result from feature engineering.
    *   Be more selective with `PolynomialFeatures` (e.g., specific interactions) or explain why a full `degree=2` approach is taken and how the resulting many features are handled in selection.
2.  **Meaningful Comparison:**
    *   Explicitly list common and unique features selected by each method.
    *   Add a section explaining the differences in selection principles between RFE (wrapper, considers interactions) and SelectKBest (filter, univariate).
3.  **Clear Output for Scores/Rankings:**
    *   For RFE, show the feature importances of the underlying estimator *for the selected features*.
    *   For SelectKBest, present the selected features and their scores in a more readable format, perhaps a pandas DataFrame sorted by score.
4.  **Scaling:** Consider adding `StandardScaler` or `MinMaxScaler` for the features, as it's good practice for many ML pipelines, even if not strictly required for the specific models selected.
5.  **Data Loading Robustness:** Add `try-except` block for `pd.read_csv` as shown in the ground truth.",gemini-flash-1.5,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns

d...",4,"The generated code attempts to address many aspects of the task but falls short on several critical requirements and demonstrates significant issues in data preprocessing and model evaluation methodology.

**Primary Reasons for Failure/Critical Issues:**

1.  **Incorrect Data Preprocessing:**
    *   **Dropping `Name` column and then `LabelEncoding` it**: The task explicitly asks to *drop irrelevant columns* like 'Name' and 'Rank'. The generated code drops it first for X and y, but then tries to LabelEncode it, which is contradictory and likely to cause errors or simply not work as intended. The ground truth correctly drops 'Name' and 'Rank' *before* defining X and y.
    *   **Handling of `User_Score`**: The `User_Score` column often contains non-numeric values (like 'tbd'). The generated code simply `dropna()` on the entire DataFrame, which would remove a significant portion of the data prematurely without properly attempting to convert `User_Score` to numeric or handling 'tbd' specifically. The ground truth correctly converts `User_Score` to numeric with `errors='coerce'` and then imputes.
    *   **Categorical Encoding (LabelEncoder vs. OneHotEncoder)**: Using `LabelEncoder` for categorical features like 'Platform', 'Genre', 'Publisher', 'Developer' is inappropriate for regression models as it imposes an arbitrary ordinal relationship that doesn't exist. `OneHotEncoder` is required, as used in the ground truth.
    *   **Missing Values (Missing Strategy)**: The code uses `dropna()` on the entire dataset early on. This can lead to significant data loss and is generally not the best approach for missing values. The ground truth uses `SimpleImputer` with appropriate strategies (`median` for numerical, `most_frequent` for categorical).
    *   **`StandardScaler` Usage**: Applying `StandardScaler` to `X_train` and then `X_test` *before* the KFold cross-validation loop for individual models is incorrect. The scaling should be part of a pipeline *inside* the cross-validation folds to prevent data leakage from the validation/test sets, as correctly done in the ground truth using `ColumnTransformer` and `Pipeline`.

2.  **Incorrect Cross-Validation for Individual Models:**
    *   The cross-validation loop is performed on `X_train` and `y_train` only. The task explicitly states ""Use cross-validation with 5 folds to evaluate *each model*"". This usually implies evaluating on the entire dataset (or at least a separate validation set if data splitting is performed pre-CV). The ground truth code performs CV on the full `X` and `y` dataset, ensuring a robust evaluation without pre-splitting into train/test before CV. While it's valid to split data first and then do CV on the training set, the code then *stops* evaluating individual models after the CV loop, only evaluating the ""best model"" later on `X_test`. This makes the ""scores"" array in `results` only reflect internal training set performance, not generalization.

3.  **Incomplete/Incorrect Voting and Stacking Ensemble Implementation:**
    *   **Voting Ensemble:** The generated code implements a very basic voting ensemble by simply taking the mean of predictions from the *fitted* models. This is conceptually wrong for a `VotingRegressor` as it should combine the *unfitted* base estimators within a `VotingRegressor` object and then fit that ensemble model. The generated code also doesn't perform cross-validation for the voting ensemble.
    *   **Stacking Ensemble:** The meta-learner is trained on `meta_train` (concatenated predictions from base models on `X_train`) and `y_test`. This is a critical error: `y_test` should not be used to train the meta-learner, and `meta_train` should be the predictions *from the training set* or more accurately, out-of-fold predictions using a proper stacking setup (which the ground truth correctly implements using `StackingRegressor`'s `cv` argument). The `meta_test` is also incorrectly generated using `X_test` directly, not out-of-fold predictions. `StackingRegressor` from scikit-learn is designed to handle this correctly, but the generated code tries to implement it manually and incorrectly.

4.  **Hyperparameter Tuning:**
    *   The generated code only tunes `GradientBoostingRegressor`, not automatically the ""best individual model"" as requested.
    *   The range of `param_grid` is very limited.

5.  **Feature Importance Plot:**
    *   The generated code attempts to plot `feature_importances_` for the `best_model` (which is `GradientBoostingRegressor`). However, due to the incorrect scaling (`StandardScaler` applied to `X_train` pre-CV), the feature names `X.columns` won't align correctly with the scaled features, leading to a misleading plot. The ground truth carefully aligns `all_feature_names` after preprocessing and one-hot encoding.
    *   The task explicitly asks for feature importance plots for *ensemble models*. Only one individual model's importances are shown, and it's not even universally applicable if other models were 'best'.

6.  **Overall Structure and Readability:**
    *   The code is a single block, lacking clear separation of concerns with comments and print statements to delineate task sections, unlike the ground truth.
    *   Error handling for file not found is missing.

**What was done correctly (though often with underlying issues):**

*   Basic loading of CSV.
*   Defining regression target (`Global_Sales`).
*   Inclusion of required ensemble models (Random Forest, Gradient Boosting, XGBoost, AdaBoost).
*   Attempted basic metrics (MAE, MSE, RMSE, R²).
*   Attempted a basic feature importance plot.
*   Attempted residual plot (though for an incorrectly implemented stacking model).",gemini-flash-1.5,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
import statsmodels.api as sm
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import pymc as pm
import ruptures as rpt
import lifelines

df = pd.read_cs...",3,"The generated code attempts to address all requirements but suffers from significant issues in correctness and completeness across most of the tasks.

**Primary Reasons for Failure:**

1.  **Fundamental Data Loading and Cleaning:** The `df['Year'] = pd.to_datetime(df['Year'], format='%Y').dt.year` line assumes a 'Year' column which doesn't exist in the typical 'video_games_sales.csv' (it's 'Year_of_Release'). It also doesn't handle NaNs in `Year_of_Release` or `Global_Sales` as a critical first step. The ground truth appropriately renames and converts and handles NaNs.
2.  **Multivariate Regression (Incorrect Implementation):**
    *   `df_model['Year'] = pd.to_categorical(df_model['Year'])` is not a valid pandas function; it should be `pd.get_dummies`.
    *   Using `OneHotEncoder` on 'Year' will treat it as a categorical feature, which is incorrect for a numerical year. It should be used as a continuous predictor.
    *   The `remainder='passthrough'` in `ColumnTransformer` (which should be used for `Year`, not `OneHotEncoder`) isn't properly used as 'Year' (should be `Year_of_Release`) is passed to `OneHotEncoder`.
    *   No interaction terms are implemented, which was a specific requirement.
    *   The OLS model is built using `X_train` directly without handling the transformed data correctly (e.g., column names from `ColumnTransformer` are not directly accessible for `sm.add_constant`).
    *   No confidence or prediction intervals are calculated for this model.
3.  **Polynomial Regression with Regularization (Incorrect Implementation):**
    *   `X_poly = np.array(df_model['Year']).reshape(-1,1)` uses `Year` as the sole feature, not `Year_of_Release`, and should be transformed using `PolynomialFeatures` first.
    *   `y_scaled = scaler.fit_transform(np.array(y).reshape(-1,1))` scales the target variable, which is generally not done for models like Ridge/Lasso unless it's for specific types of models or to compare coefficients across different scales, and if scaling is done, the model's predictions should be inverse-transformed back. The ground truth scales only the features and applies `PolynomialFeatures` correctly.
    *   No confidence or prediction intervals are calculated.
4.  **Bayesian Regression (PyMC vs PyMC3, Incomplete):**
    *   It imports `pymc` which is `PyMC version 5+`, while the ground truth and prompt's hint implied `PyMC3`. While `pymc` is the current, `PyMC3` (or older versions) have significant differences in syntax. The ground truth uses `PyMC3` which aligns with common data science practices for this specific task where `PyMC3` was widely used for statistical modeling. Using the newer `pymc` requires more careful porting of the code.
    *   The model (intercept, normal, half-cauchy) is very basic and doesn't account for categorical features like Genre/Platform (which were specified in the prompt). The `shape` argument is wrong for `beta` if `X_poly_scaled` only has one feature.
    *   No prediction intervals or uncertainty quantification are calculated or even attempted beyond sampling the trace.
5.  **Seasonal Decomposition (Incomplete):** While the line exists, there's no attempt to visualize or interpret the decomposition, which is a common and expected output for this step. No consideration for yearly data (where seasonality is unlikely).
6.  **ARIMA (Incomplete and Lacks Validation):**
    *   It uses `statsmodels.tsa.arima.model.ARIMA`, which is okay, but doesn't use `pmdarima.auto_arima` to find optimal p,d,q values, which was implied by ""forecast future global sales trends"" and is standard practice.
    *   No confidence intervals are calculated or used from `arima_forecast`.
7.  **Change Point Detection (Basic, Lacks Context):** `result = algo.predict(pen=10)` uses an arbitrary penalty. The ground truth intelligently uses `2 * np.log(n_samples_signal)`. No visualization is generated.
8.  **Survival Analysis (Incorrect Definition, Lacks Covariates):**
    *   `df_survival['Game_Longevity'] = df_survival['Global_Sales'] > 0` is an incorrect and overly simplistic definition of game longevity. It implies a game ""survives"" as long as it has any sales, ignoring the concept of duration. The ground truth's definition based on `Year_of_Release` and `Year_of_Release_last_observed` is more appropriate.
    *   `kmf.fit(df_survival['Year'], event_observed=df_survival['Game_Longevity'])` feeds `Year` as the duration, not a derived game longevity duration.
    *   No Cox Proportional Hazards model is implemented to include covariates (Genre, Platform, Publisher, etc.), which was a specific requirement for ""model uncertainty in predictions"".
    *   No confidence/prediction intervals are calculated for survival.
9.  **Confidence and Prediction Intervals (Largely Missing):** This was a specific and important requirement mentioned multiple times in the prompt. The code only generates an ARIMA forecast object but doesn't explicitly extract/use the intervals or mention how they are calculated for other models. The ground truth shows explicit calculation and printing of CIs/PIs for OLS, ARIMA, and even approximate PIs for polynomial regression.
10. **Time Series Cross-Validation (Incomplete and Partially Correct):** Only ARIMA is partially implemented, and even that is basic. The requirement was to validate *all* models using time series cross-validation. The ground truth applies TS-CV to ARIMA and all regression models.
11. **Code Quality:**
    *   Missing imports for `matplotlib.pyplot`, `seaborn`, `PolynomialFeatures`, `LinearRegression`, `mean_squared_error`, `r2_score`, `statsmodels.formula.api`, `arviz`, `KaplanMeierFitter`, `CoxPHFitter`, `pmdarima`, `warnings`.
    *   No error handling for file not found.
    *   Lack of comments and print statements for clarity on what each section does or its results.
    *   Variables like `df_model['Year']` being assigned a boolean categorical then reassigned, leading to inconsistent state.
    *   Inconsistent variable naming (`Year` vs `Year_of_Release`).

In summary, while the code attempts some basic calls for each required model type, the specific implementation details often miss critical aspects of the prompt (interaction terms, proper variable handling, CI/PIs, robust survival definition, proper cross-validation across all models, optimal parameter selection for ARIMA) and contain several fundamental errors in data handling and model setup.",gemini-flash-1.5,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.manifold import TSNE, UMAP
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from gap_statistic import GapStatistic
from scipy.cluster.hierarchy import dendrogram, linkage
from statsmodels.stats.multicomp import pairwise_...",4,"The generated code attempts to address many aspects of the prompt but fails significantly in correctness, completeness, and code quality related to actual implementation details.

**Correctness & Completeness Issues:**

1.  **Preprocessing:**
    *   `df.dropna(inplace=True)` at the beginning is too aggressive. It removes rows essential for features like `Regional_Sales_Distribution` (if sales are 0, they are not NaN) and prematurely drops `Year_of_Release`, `Genre`, `Publisher`, `Platform` etc., which might have some NaNs but are crucial. The ground truth handles this much more robustly.
    *   `df['Year'] = pd.to_numeric(df['Year'], errors='coerce')` fails because the column is `Year_of_Release`, not `Year`. This likely causes most of the data to be dropped.
    *   `df = df[df['Year'] >= 2000]` is an arbitrary filtering not requested by the prompt and might severely reduce the dataset size.
    *   `LabelEncoder` for 'Genre' and 'Publisher' is problematic for numerical clustering because it imposes an arbitrary ordinal relationship, which is incorrect for nominal categorical data. One-Hot Encoding (as in ground truth) is the correct approach.
    *   `df['Regional_Sales_Distribution'] = df[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].values.tolist()`: This creates a list of lists in a column, which is not a numerical feature suitable for direct scaling and clustering. The prompt asked for *proportions* of sales, which the ground truth correctly calculates.
    *   `df = df.sort_values(by=['Genre', 'Total_Sales'], ascending=False).groupby('Genre').head(100)`: This line drastically samples the data, keeping only the top 100 per genre. This was not requested and significantly alters the analysis.
    *   `df_num` only selects a limited set of numerical features, missing the crucial *distribution* features requested.

2.  **Clustering Implementation:**
    *   **Optimal K:** The code uses `gap_statistic.optimal_k()`, which is good for finding optimal K, but then hardcodes `n_clusters=3` for KMeans, Hierarchical, and GMM, completely ignoring the calculated optimal K. DBSCAN parameters are also hardcoded without context.
    *   **DBSCAN:** `eps=0.5` and `min_samples=5` are arbitrary and rarely work effectively without tuning. The ground truth attempts a heuristic for `eps` or at least mentions the criticality of tuning.
    *   **Hierarchical Clustering:** `n_clusters=3` is hardcoded.
    *   **GMM:** `n_components=3` is hardcoded.

3.  **Dimensionality Reduction:**
    *   UMAP import is `from umap import UMAP` which is commonly `import umap.umap_ as umap` in newer versions, the generated code might fail based on environment setup. (Minor, but worth noting).
    *   The results of TSNE and UMAP are calculated but **not visualized or used for cluster visualization**, which is a key part of the requirement.

4.  **Consensus Clustering:**
    *   ""Simplified - majority voting"" is a very rudimentary form of consensus clustering and doesn't align with the common understanding or implementation of consensus clustering (e.g., co-occurrence matrix clustering as in ground truth). It also just takes the mode of the labels, which isn't robust.

5.  **Cluster Profiling & Statistical Testing:**
    *   It only performs `pairwise_tukeyhsd` on `Total_Sales` vs. `Cluster`. This is a very limited scope. The prompt requested ""detailed cluster profiles with statistical significance testing"" across *multiple* features (numerical and categorical), which the ground truth correctly implements using ANOVA and Chi-squared tests.

6.  **Anomaly Detection:**
    *   `pyod.models.iforest` is used, which is fine, but the original request implies `sklearn.ensemble.IsolationForest` given the `sklearn` heavy context. Both are valid. However, it doesn't show any *details* about the anomalies or how to interpret them, unlike the ground truth which prints top anomalous games.

7.  **Association Rule Mining:**
    *   `frequent_itemsets = apriori(df[['Genre', 'Platform']], min_support=0.05, use_colnames=True)`: `df[['Genre', 'Platform']]` are no longer categorical strings; they are label-encoded integers. Apriori requires one-hot encoded boolean data. This line will break or produce meaningless results. The ground truth correctly uses `OneHotEncoder`.

8.  **Recommendation System:**
    *   The `recommend_games` function directly accesses `df['Cluster']` but `df['Name']` might have duplicates. The logic can be problematic if multiple games share the same name, or if the original `df` indices diverge from the `df_scaled` indices after various filtering/dropping steps. The `df_original.assign` approach in the ground truth is safer.

9.  **Validation:**
    *   Calculates silhouette, calinski, and davies scores, which is good. But no output or interpretation of these scores is provided.

**Code Quality:**

*   **Imports:** Some redundant imports (e.g., `dendrogram`, `linkage` if not plotted). Missing key imports from `sklearn.compose` and `sklearn.pipeline` which are crucial for robust preprocessing.
*   **Structure:** Lacks clear print statements to indicate progress or section completion, making it hard to follow the execution flow.
*   **Robustness:** No `try-except` blocks for file loading or other critical operations. Many hardcoded values appear without explanation.
*   **Comments:** Sparse comments, making it difficult to understand the intent behind certain choices (e.g., why `df[df['Year'] >= 2000]`).

Overall, the generated code has several critical flaws in data handling, algorithm application, and feature engineering which would prevent it from producing meaningful results or even running correctly through all steps as intended by the task.",gemini-flash-1.5,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm impo...",3,"The generated code attempts to address many parts of the prompt but largely fails to implement them correctly or completely, often providing only placeholder or incorrect logic.

Here's a breakdown of the failures:

1.  **Comprehensive Feature Engineering**: The ground truth uses `SimpleImputer` for zeros and `PolynomialFeatures` to create interactions, then adds custom interactions. The generated code only adds two very basic polynomial features and doesn't handle the 0s as NaNs/missing values, which is crucial for this dataset. This results in poor preprocessing.
2.  **Advanced Ensemble Methods**:
    *   **Voting Classifier**: The generated code defines `voting_hard` and `voting_soft` but *never trains or uses them*. They are merely initialized.
    *   **Bagging**: It initializes `bagging_models` but again, never trains or uses them. It also incorrectly uses the *values* of the `models` dictionary as `base_estimator` directly, which won't work correctly as they are untrained model instances that will be fit multiple times. `base_estimator` should be a type, not an instance.
    *   **Boosting**: The boosting models are defined in the `models` dictionary but are only *potentially* trained if Optuna picks them, not as standalone ensemble methods that are properly evaluated.
3.  **Stacking Ensemble**: `base_learners` and `meta_learner` are defined but `StackingClassifier` is *never initialized or used*. This is a critical omission.
4.  **Bayesian Optimization (Optuna)**:
    *   The `objective` function only includes a placeholder `if clf_name == ""Logistic Regression"":` for hyperparameter tuning. It explicitly states ""add hyperparameter search space for other models here..."", which means it's incomplete and generic. The ground truth provides detailed hyperparameter tuning for XGBoost, which is a complex and common use case.
    *   It uses `cross_val_score(model, X_train,y_train, cv=5, scoring='accuracy').mean()` to get the score. While technically correct for some models, the ground truth explicitly uses a train-validation split and early stopping for XGBoost, which is more appropriate for tree-based models where `n_estimators` is a tuned parameter.
5.  **Nested Cross-Validation**:
    *   The implementation of nested CV is fundamentally flawed. It has an outer loop that iterates through folds, but the `param_grid` for the `GridSearchCV` inside the inner loop is `{}`, meaning *no hyperparameter tuning happens*. It only runs `GridSearchCV` with default parameters for `LogisticRegression`, not for a dynamically selected or best model.
    *   It only checks `if score > best_score:` to select a `best_model`, which means it will only store one model and its score from the entire nested CV process, not the aggregate performance metrics (mean and std dev) across all folds for *all* models/ensembles as per the ground truth.
    *   It doesn't calculate or store any of the required metrics (AUC, Accuracy, Precision, Recall, F1) for any of the models/ensembles across the folds.
    *   It uses `KFold` instead of `StratifiedKFold`, which is important for imbalanced datasets like diabetes.
6.  **Custom Ensemble Methods with Dynamic Weight Assignment**: This requirement is completely missed. The generated code has no implementation of custom ensembles or dynamic weight assignment.
7.  **CatBoost for Handling Categorical Features**: While CatBoost is imported and added to the `models` dictionary, the data preprocessing explicitly states `categorical_features = []`. The ground truth correctly identifies that after comprehensive preprocessing, there aren't explicit categorical features that CatBoost would handle, but the prompt asks for its *application*. The generated code doesn't demonstrate its use beyond inclusion in a dictionary, and doesn't explicitly show how it handles categorical features if they were present.
8.  **Model Interpretation using SHAP values**: SHAP is initialized for a `best_model` selected by the flawed nested CV process. Since the nested CV only evaluates `LogisticRegression` and selects only one, the SHAP interpretation isn't comprehensive for the full range of models or the best ensemble. There are also no SHAP plots like bar plots or dependence plots, only `shap.plots.beeswarm` is called.
9.  **Model Calibration and Reliability Analysis**: `CalibratedClassifierCV` is initialized and fitted. However, no reliability analysis (e.g., Brier score comparison, calibration curve plots) is performed or presented.
10. **Ensemble Diversity Analysis and Model Combination Strategies**:
    *   ""Diversity Analysis (Example - needs more advanced metrics)"" is listed but has no code.
    *   ""Model Combination Strategies"" is also listed but no code exists. The ground truth includes correlation matrices of OOF predictions and a meta-learner trained on OOF predictions.

**Code Quality**:
*   Imports are okay, but some are unused (e.g., `GridSearchCV`, `RandomizedSearchCV`, `OneHotEncoder`, `Trials`, `fmin`, `tpe`, `hp` from `hyperopt` are imported but Optuna is used, and `GridSearchCV` isn't used effectively).
*   Structure is largely functional but lacks the modularity and separation of concerns found in the ground truth (e.g., dedicated functions for preprocessing, evaluation).
*   Lack of comments explaining design choices or complex parts.
*   The `X_processed = preprocessor.fit_transform(X)` line is problematic because `X` contains the original column names, but `X_processed` returns a numpy array, losing feature names. This impacts subsequent steps like SHAP, which often expects feature names.
*   Repeated boiler-plate code for model definition, rather than leveraging loops.

In summary, the code provides a high-level outline of the requested features but lacks actual robust and complete implementations for almost all of them. Many sections are either placeholders or contain logical errors that prevent them from fulfilling the task requirements.",gemini-flash-1.5,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import scipy.stats as stats
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from lifelines import CoxPHFitter
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.impute import KNNImputer
import arviz as az

# 1. Load and preprocess
data = pd.read_csv(""diabetes.csv"")
imputer = KNNImputer(n_neighbors=5)
...",5,"The generated code attempts to address all requirements but falls significantly short in implementation details, resulting in critical correctness and completeness issues.

**Correctness Issues:**
*   **Hierarchical Model (2-3):** The implementation of the hierarchical model is incorrect. It tries to create group-level effects (`a`) with `shape=len(X_train)`, but `X_train` is the entire feature matrix for all individuals, not a grouping variable. This does not establish groups or mixed effects as required. The `mu = a + pm.math.dot(X_train,b)` line adds a per-sample intercept, which is not a hierarchical model but rather just an overparameterized linear model. A proper hierarchical model requires a distinct grouping variable (like `Age_Group` in the ground truth) to model variation *across* groups.
*   **Bayesian Logistic Regression (4):** The prior for `beta` (coefficients) is simple but acceptable. However, the model definition `mu = a + pm.math.dot(X_train,b)` with `pm.Normal` likelihood is for a linear regression, not a *logistic* regression. The task explicitly asks for Bayesian *logistic* regression. The `pm.math.sigmoid` is introduced the *next* time it is run, but it should be part of a `Bernoulli` likelihood for classification, which is missing.
*   **Survival Analysis (6):** The simulation of `time_to_event` as `np.random.exponential` is not linked to the `Outcome` variable. The ground truth correctly simulates this by linking `Outcome` (diabetes status) to the simulated time, distinguishing between observed events and censored observations. The generated code also directly uses `data_scaled` for `cph.fit`, implying it expects all columns as features, which might be incorrect; it should explicitly define features.
*   **Gaussian Processes (7):** Using `sklearn.gaussian_process.GaussianProcessRegressor` is not a *Bayesian* implementation using PyMC3/PyMC4 as specified. The task asks for Bayesian models specifically (PyMC3/PyMC4). PyMC3 has a dedicated GP module.
*   **Causal Inference (8):** The code explicitly states that PSM and IV methods are ""not included in this dataset"" or ""needs an instrumental variable, not included,"" effectively side-stepping the requirement rather than demonstrating a simulated or conceptual implementation as done in the ground truth. The task implicitly expects a creative, defensible simulation if data doesn't provide direct columns.
*   **Model Comparison (9):** While `az.waic` and `az.loo` are correctly called, they are applied to models that are either incorrectly defined (hierarchical) or mislabeled (the pseudo-hierarchical model becomes the hierarchical one for comparison, and the logistic model isn't truly logistic).
*   **Posterior Predictive Checks (10):** The `plot_ppc` calls are made, but their utility is limited by the incorrect model definitions. Specifically, plotting PPC for a `pm.Normal` outcome (even if `y_train` is 0/1 binary) will not show a binary distribution, which should be the case for a logistic regression.

**Completeness Issues:**
*   **Comprehensive Missing Value Handling:** While `KNNImputer` is used, the ground truth explicitly handles zeros as NaNs for specific columns common in this dataset, which is a more comprehensive approach for this specific problem (e.g., BloodPressure = 0 is not valid).
*   **Age_Group and Group Variable:** The crucial step of creating a `Age_Group` (or similar) variable for hierarchical modeling is completely missed. This makes the hierarchical model implementation fundamentally flawed.
*   **Informative Priors:** While the Bayesian logistic regression mentions ""Informative Priors,"" the actual priors (`mu=0, sigma=1`) are generic and not ""informative based on medical knowledge"" as requested.
*   **Warning Suppression & Expert Comments:** The detailed warning suppression and extensive comments and explanations present in the ground truth for crucial steps are absent. This hinders readability and understanding, especially for a complex task.
*   **Missing Imports:** `matplotlib.pyplot` and `seaborn` are used in the ground truth for plotting and are good practices generally, but not imported in the generated code.

**Code Quality:**
*   Variable naming is acceptable.
*   Code structure is okay for a sequential script.
*   Lack of comments and clear sectioning (beyond numbered points) makes the code harder to follow.
*   The use of `pm.sample` missing `return_inferencedata=True` and `target_accept` from newer PyMC3/ArviZ good practices.

In summary, the generated code understands *what* needs to be done but fails significantly in *how* to implement the more complex requirements (hierarchical models, Bayesian GP, proper causal inference setup, and accurate Bayesian logistic regression likelihood) within the PyMC3 framework as requested. Many implementations are either off-the-shelf scikit-learn models (GP) or fundamentally misrepresent the statistical models.",gemini-flash-1.5,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture, SpectralClustering, OPTICS
from sklearn.manifold import TSNE, UMAP
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from gap_statistic import GapStatistic
import matplotlib.pyplot as plt
from scipy.stats import f_oneway, chi2_contingen...",4,"The generated code attempts to address many aspects of the prompt but falls significantly short in implementation details, correctness, and completeness compared to the ground truth.

**Critical Issues and Missing Functionality:**

1.  **Dataset Loading and Preprocessing:**
    *   The generated code assumes `Weight`, `Height`, `Waist`, `Hips` columns exist for feature engineering, which are NOT in `diabetes.csv`. This leads to immediate `KeyError` at `df['BMI'] = df['Weight'] / (df['Height']**2)`. The ground truth correctly identifies and handles missing values (0s) in the `diabetes.csv` specifically.
    *   Metabolic Syndrome indicators are conceptually correct in the prompt but the implementation fails due to non-existent columns. The ground truth correctly engineers these features from available data.
    *   The `df.dropna()` is a very aggressive and usually unacceptable approach for `diabetes.csv` where 0s represent missing values; the ground truth uses `SimpleImputer` for more robust handling.

2.  **Dimensionality Reduction:**
    *   The autoencoder part is commented out and incomplete (`#from tensorflow.keras.layers import Input, Dense`, etc.). The prompt explicitly asked for autoencoder-based embeddings. The generated code also struggles to provide the actual `X_ae` for visualization as done in the ground truth.
    *   `UMAP` is imported but the package is typically `umap-learn`, not just `umap`. The ground truth includes the correct import (`import umap.umap_ as umap`).

3.  **Optimal Clustering Parameter Determination:**
    *   Only `gap_statistic` is mentioned for KMeans. The prompt explicitly asked for ""silhouette analysis, gap statistic, and elbow method with statistical validation"" for determining optimal parameters for *multiple* clustering algorithms. The generated code uses fixed `n_clusters=3` for many algorithms without proper optimization loops.
    *   For DBSCAN/OPTICS, `eps` and `min_samples` are hardcoded without any attempt at estimation (like the k-distance graph in the ground truth).
    *   No statistical validation is implemented for parameter selection.

4.  **Consensus Clustering:**
    *   The `consensus_labels = np.array([results[algo] for algo in results]).mean(axis=0).round()` is a highly simplistic and incorrect way to perform consensus clustering. It does not implement the co-occurrence matrix approach described in the ground truth, nor does it identify stable subgroups.

5.  **Patient Phenotype Profiles:**
    *   The print statements only show mean/std for numeric columns and value counts for others, which is basic. The prompt asked for ""detailed patient phenotype profiles for each cluster with statistical significance testing"". The ground truth performs Kruskal-Wallis and Chi-squared tests for significance.
    *   No visualizations of cluster profiles are generated.

6.  **Anomaly Detection:**
    *   Uses `pyod.models.iforest.IForest` instead of `sklearn.ensemble.IsolationForest`. While `pyod` is a valid library, `IsolationForest` from scikit-learn is more standard for this task.
    *   Only Isolation Forest is used; the prompt specifically asked for ""identify unusual patient profiles or outliers"" implying multiple methods as typically done in the ground truth (Isolation Forest, OneClassSVM, LOF).
    *   No visualization of anomalies on reduced dimensions is provided.

7.  **Time-Series Clustering:**
    *   The provided `diabetes.csv` is cross-sectional. The ground truth correctly identifies this and provides a conceptual explanation. The generated code has a commented-out section `if 'Time' in df.columns:` indicating it's not adapted for the given dataset and the `TimeSeriesKMeans` usage within is incorrect (`.reshape(1,-1)`).

8.  **Association Rule Mining:**
    *   The `X_binary = (X >= X.mean()).astype(int)` is a very naive and potentially problematic binarization for numerical features. The ground truth uses `pd.qcut` for more robust discretization.
    *   Missing `mlxtend` imports outside the main code block (though included in the file imports).

9.  **Patient Risk Stratification:**
    *   ""Build a model (e.g., Logistic Regression) using cluster labels as predictors of outcome."" is a placeholder comment and does not implement a risk stratification *system* or provide clinical interpretation based on clustering results. The ground truth provides detailed clinical interpretation and assigns risk levels based on cluster characteristics.

**Code Quality:**
*   Lack of proper error handling (e.g., `try-except` for file loading).
*   Scattered comments rather than organized functions or classes, making it hard to follow.
*   Many parts are commented out or incomplete.
*   Doesn't assume CSV file is in the current directory and handles `FileNotFoundError`.

Overall, while the generated code imports many relevant libraries and attempts to touch upon the required concepts, its implementation is largely superficial, incomplete, and fundamentally incorrect or leads to immediate errors given the specified `diabetes.csv` dataset.",gemini-flash-1.5,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score, RepeatedStratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catbo...",4,"The generated code attempts to address many requirements but falls short in implementation details, correctness, and completeness for a task of this complexity.

**Critical Issues and Missing/Incorrect Implementations:**

1.  **Data Preprocessing and Feature Engineering:**
    *   `df = df.dropna()` is a very aggressive approach and can lead to significant data loss, especially if missing values are concentrated. The ground truth uses `SimpleImputer`.
    *   `df['Regional indicator'] = LabelEncoder().fit_transform(df['Regional indicator'])` is incorrect for one-hot encoding or proper categorical handling in a regression context, especially when it's a ""regional indicator"" as a nominal variable. `OneHotEncoder` should be used which the ground truth does. The `preprocessor` then attempts to use `OneHotEncoder` on `'Ladder score in Dystopia'` which is likely a numerical column (from the task description and common sense, a ""score"" implies numeric), and `StandardScaler` on `'Regional indicator'` which after `LabelEncoder` is ordinal, but should be one-hot encoded for proper handling. This entire `ColumnTransformer` setup is likely flawed.
    *   Crucially, the `preprocessor` is *defined* but never *used* when training the models. `model.fit(X_train, y_train)` directly uses raw `X_train`, bypassing all defined preprocessing. This is a fundamental error. The ground truth correctly integrates the preprocessor into pipelines.
    *   Feature engineering is minimal to none; it's mostly basic preprocessing.

2.  **Ensemble Methods Implementation:**
    *   **Voting Regressor:** The `VotingRegressor_Hard` and `VotingRegressor_Soft` are defined in the `models` dictionary, but they are not dynamically used with Optuna or within the nested CV setup. `study.optimize(lambda trial: objective(trial, model, X_train, y_train), n_trials=100)` iterates through a fixed `model` (e.g., `GradientBoostingRegressor` after the loop), not through `models.items()`.
    *   **Bagging:** Similar to voting, they are defined but not properly incorporated into the training and evaluation loop.
    *   **Boosting:** While `XGBoost`, `LightGBM`, `CatBoost` are present, their specific training parameters are not consistently handled across the general `objective` function for Optuna.
    *   **Stacking:** The stacking implementation `stacking_model = VotingRegressor(estimators=list(zip(['lr','gb','xgb'],base_learners)))` is conceptually incorrect both in terms of what `StackingRegressor` is (it builds new features for a meta-learner) and also uses `VotingRegressor` incorrectly for this purpose (it should be `StackingRegressor`). It's also not integrated into evaluation.

3.  **Bayesian Optimization (Optuna/Hyperopt):**
    *   **Optuna:** The `objective` function for Optuna only tunes `n_estimators`, `max_depth`, and `learning_rate`. This is a very limited set of hyperparameters compared to the rich set available for models like XGBoost/LightGBM. The ground truth includes more parameters.
    *   The loop `for name, model in models.items():` followed by `study.optimize(...)` then `best_params = study.best_params` is incorrect. `best_params` will only store the parameters of the *last* model in the `models` dictionary that Optuna ran for. It doesn't save best parameters for *each* model. The ground truth correctly stores and uses best parameters for a specific model (XGBoost in its example).
    *   **Hyperopt:** The Hyperopt part `hyperopt_objective`, `fmin` etc. is defined but never called or used anywhere in the code. It's dead code.

4.  **Nested Cross-Validation:**
    *   `RepeatedStratifiedKFold` is for *classification*, but the task is regression. `KFold` or `RepeatedKFold` should be used. This is a critical error.
    *   The `outer_cv.split(X,y)` uses the *full* `X` and `y` (unpreprocessed) for splitting when it should be using preprocessed data within the splits, or preprocessing should be part of the pipeline inside the loop.
    *   The model fit inside the nested CV loop `model= GradientBoostingRegressor(**study.best_params)` re-initializes a GBM model. `study.best_params` only contains the parameters from the *last* Optuna run (as per the earlier bug). This means only one set of hyperparameters is ever used, and they might not be optimal for that specific model or outer fold. A true nested CV should *contain* the hyperparameter search within the inner loop of *each* outer fold. The ground truth explicitly states this and demonstrates it by using the already tuned `best_xgb_model`.
    *   The preprocessing steps are completely absent from the nested CV loop, meaning models are trained on raw data.

5.  **Custom Ensemble:**
    *   The `custom_ensemble` function is provided but there's no actual implementation or evaluation of a custom ensemble with dynamic weight assignment based on performance. The ground truth directly implements this using OOF predictions and an optimization algorithm. The provided custom ensemble is a simple weighted average, not dynamic weight assignment.

6.  **Advanced Techniques (Blending & Multi-level Stacking):**
    *   These are listed in the task but completely absent from the generated code.

7.  **Model Interpretation (SHAP & LIME):**
    *   SHAP and LIME are *called*, but the models are trained on raw `X_train` and `X_test` (as preprocessing is never applied), which means the interpretability will be for the raw features, not the preprocessed ones (like one-hot encoded categories or scaled numerical features). This can lead to misleading interpretations.
    *   The `predict_fn` for LIME uses `model.predict` which is likely a raw `GradientBoostingRegressor`, not a pipeline. It won't handle preprocessing for the LIME explainer correctly.

8.  **Model Calibration and Uncertainty Quantification:**
    *   `CalibratedClassifierCV` is for *classification* calibration, not regression. For regression, it's about checking residual distribution and creating prediction intervals (e.g., via quantile regression or bootstrapping), which the ground truth does. The provided code does not implement this.

9.  **Ensemble Diversity Analysis and Model Combination Optimization:**
    *   The code explicitly states `more advanced diversity metrics and optimization strategies` are required, indicating this was not implemented beyond a ""simplified example"" which is not even present. The ground truth fully implements this.

**Overall Code Quality:**
*   Imports are present but many (like `RepeatedStratifiedKFold`, `CalibratedClassifierCV`) are incorrect for the task's regression nature.
*   Structure is disjointed; components are defined but not properly integrated into a cohesive workflow (e.g., preprocessor, different ensemble models, Optuna results).
*   Lack of comments in complex sections makes it hard to follow the intent.
*   Error handling (e.g., `FileNotFoundError` for CSV) is missing.

The generated code demonstrates familiarity with some library functions but fails to correctly integrate them into a robust machine learning pipeline, particularly for the advanced requirements like nested CV, dynamic ensembles, and complex interpretation that heavily rely on correct data flow (preprocessing) and evaluation methodology.",gemini-flash-1.5,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import statsmodels.formula.api as smf
import patsy
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from causalinference import CausalModel
from scipy.stats import gaussian_kde
import arviz as az

data = pd.read_csv(""world-happiness-report-2021.csv"")

# 1. Data Preprocessing
data = data.dropna(subset=['Ladder score'])
data = data.fillna(data.median())
data = pd.get_dummies(d...",3,"The generated code attempts to address several parts of the prompt but fails significantly in correctness, completeness, and code quality across multiple requirements.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

1.  **PyMC3 vs. PyMC:** The prompt explicitly stated ""PyMC3 or PyMC4"". The generated code uses `pymc` which defaults to PyMC V5+. This is a breaking change from PyMC3, leading to different function signatures (e.g., `Normal` parameter names like `sigma` vs `sd`) and overall model syntax. This makes many of the PyMC models incorrect and unexecutable without modification for the specified version.
2.  **Hierarchical Bayesian Model (Incorrect Implementation):**
    *   The `alpha` parameter is shaped by `data['Regional indicator_East Asia'].nunique()`. This assumes that `Regional indicator_East Asia` is the *only* one-hot encoded regional column and that `nunique()` on this single column will give the total number of regions. This is fundamentally incorrect for regional random effects. It should be based on the number of unique `Regional_indicator` categories.
    *   The `beta` parameter is poorly defined (`data.shape[1] - data['Regional indicator_East Asia'].nunique() -1`). It attempts to subtract one-hot encoded columns but gets it wrong, and it's unclear which specific columns it means to model.
    *   The `mu` calculation is `mu_alpha + pm.math.dot(data.drop(columns=['Ladder score', 'Regional indicator']), beta)` which is highly problematic. It incorrectly assumes `mu_alpha` applies globally without regional indexing and attempts to use the *entire dropped dataframe* with a `beta` that is not shaped correctly for individual coefficients of specific predictors.
    *   No proper data preparation for PyMC3/PyMC (e.g., converting pandas columns to numpy arrays within the model for better performance/clarity).
3.  **Structural Equation Modeling (SEM) (Gross Simplification):** The prompt asked for SEM to model causal pathways. The generated code provides a single `statsmodels.formula.api.ols` linear regression, which is *not* SEM. SEM involves latent variables, multiple observed indicators for constructs, and a system of regressions describing causal relationships between these constructs. This is a complete failure to meet the SEM requirement.
4.  **Causal Inference (IV and PSM - Severely Lacking):**
    *   **Instrumental Variables:** The code states ""requires an instrument, example not provided here."" The prompt *required* implementation of IV and specifically mentioned simulating a time component for DiD. It was expected that a simulation or a plausible assumption would be made for IV to demonstrate the setup, similar to how the ground truth simulated an instrument.
    *   **Propensity Score Matching:** The PSM implementation is incorrect. `LogisticRegression` is used to predict propensity scores, but then no matching occurs (`NearestNeighbors` or similar) and no ATE is calculated on a matched or weighted sample. The subsequent lines `X = data[['Logged GDP per capita']]` and `y = data['Ladder score']` are entirely wrong for propensity score modeling, as propensity scores predict the *treatment* (e.g., high GDP), not the outcome (happiness). The prompt specifically asked for the causal effect of GDP on happiness, implying GDP should be the treatment.
5.  **Gaussian Processes (Missing):** The code explicitly states ""# Requires more complex setup not included here"". This is a direct failure to implement a core requirement.
6.  **Mediation Analysis (Missing):** The code states ""# Requires additional steps for proper mediation analysis, not shown here"". This is another direct failure to implement a core requirement. The ground truth performed a basic mediation analysis using the outputs of the SEM-like sub-models.
7.  **Difference-in-Differences (Incorrect Simulation and Implementation):**
    *   The simulation of `time` is simply a random assignment of 0 or 1, not creating `pre` and `post` periods consistently for each country or simulating a panel structure.
    *   There's no simulation of a 'treated' group or a 'policy' impact within that group. The `Logged GDP per capita * time` interaction term is the correct component, but the data setup does not support a meaningful DiD analysis.
8.  **Bayesian Model Comparison (Incomplete):** While it calls `az.waic` and `az.loo`, it only does so for the hierarchical model, not for comparison against *another* model as requested (e.g., non-hierarchical). It correctly notes that Bayes factors are complex but fails to provide a full comparison using the requested metrics.
9.  **Credible Intervals, PPC, Sensitivity Analysis (Incomplete):** `az.plot_posterior` and `az.plot_ppc` are called, which is a start, but sensitivity analysis is explicitly stated as ""# Sensitivity analysis requires additional steps, not shown here"", which is a failure to meet the requirement. The ground truth showed sensitivity to prior choice.
10. **Data Preprocessing:** While attempting missing value handling and one-hot encoding, it's far less comprehensive than required (e.g., `data.fillna(data.median())` is very basic and might not be appropriate for all columns, and scaling/standardization is entirely missing for numeric predictors, which is crucial for interpreting coefficients in PyMC models).

In summary, the code provides minimal, often incorrect, and incomplete implementations for most of the advanced requirements, failing to capture the nuances and complexity requested. The lack of adherence to the specified PyMC3 version and the fundamental errors in model definitions (especially the hierarchical model) make it largely unusable as a comprehensive solution.",gemini-flash-1.5,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, GaussianMixture
from sklearn.manifold import TSNE, MDS
import umap
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.model_selection import GridSearchCV
import hdbscan
from sklearn.neighbors import kneighbors_...",6,"The generated code attempts to address several aspects of the prompt but falls short in completeness, accuracy, and advanced implementation of several key requirements.

**Strengths:**
*   **Basic Structure:** Includes imports, data loading, and attempts at key sections like clustering, dimensionality reduction, network analysis, and association rules.
*   **Some Algorithm Selection:** Correctly identifies and uses `scikit-learn` for basic clustering algorithms (K-Means, DBSCAN, Agglomerative, GMM) and `hdbscan`.
*   **Partial Dimensionality Reduction:** Includes t-SNE, UMAP, and PCA, and correctly sets up a basic autoencoder structure.
*   **Association Rule Mining:** Uses `mlxtend` correctly for association rules.
*   **Attempt at Anomaly Detection:** Uses `IsolationForest`.

**Weaknesses and Areas for Improvement:**

1.  **Completeness & Depth of Implementation:**
    *   **Advanced Feature Engineering:** The feature engineering is rudimentary (`Regional_GDP`, `Happiness_Profile`). The ground truth included more sophisticated interaction terms and proper handling of categorical features before scaling.
    *   **Multiple Clustering Algorithms:** While listed, their implementation is very basic. There's no proper handling of optimal parameter selection for DBSCAN/HDBSCAN (eps, min\_samples, min\_cluster\_size), which are crucial for their performance. Spectral Clustering is missing.
    *   **Optimal Clustering Parameters:** The code only demonstrates `GridSearchCV` for KMeans (`n_clusters`). It completely misses silhouette analysis, gap statistic, elbow method (though the `GridSearchCV` for silhouette is a good start), and stability analysis for *all* relevant algorithms, which was a core requirement. The ground truth demonstrated how to integrate silhouette and elbow for multiple algorithms.
    *   **Consensus Clustering:** The proposed consensus clustering is extremely simplified (`np.bincount(row).argmax()`). This is not a robust consensus method. True consensus clustering involves building a co-association matrix and then clustering that matrix (e.g., using hierarchical clustering), as demonstrated in the ground truth.
    *   **Detailed Country Profiles:** The code only creates a `country_profiles` DataFrame and suggests ""Perform t-tests, ANOVA, etc., here,"" without any actual implementation or effect size analysis. The ground truth provided concrete examples of ANOVA and Eta-squared calculation.
    *   **Anomaly Detection:** Only `IsolationForest` is implemented. LOF and One-Class SVM were requested.
    *   **Network Analysis:** The network analysis is attempted but uses `np.corrcoef(X_scaled.T)` which computes correlations *between features*, not *between countries* (which is what `cosine_similarity(X)` does). This is a critical error in interpreting the network. The `community_louvain` part is also missing proper visualization and interpretation of communities in the context of country relationships.
    *   **Country Recommendation System:** It explicitly states ""Simplified Example"" and offers no concrete implementation or logic beyond a comment.
    *   **Policy Insights & Geopolitical Validation:** These sections are merely comments, not code. The ground truth provided a detailed qualitative interpretation linked to cluster profiles and suggested how geopolitical validation would be performed with concrete examples.

2.  **Code Quality & Correctness:**
    *   **Error Handling/Robustness:** Missing `try-except` blocks around clustering algorithms, which can fail if `n_clusters` is ill-defined or for other reasons.
    *   **Missing Value Imputation:** It imputes `X.mean()` globally, which is okay for simplicity but the ground truth was more explicit about handling numerical vs. categorical features (median for numerical, mode for categorical).
    *   **Autoencoder:** The autoencoder model is incorrectly defined as `autoencoder.encoder.predict(X_scaled)`. The `encoder` part is not a direct attribute of the `Model` instance. It should be separated into a `Model` that takes the `input_layer` and outputs the `encoded` layer.
    *   **Consistency:** The feature scaling and feature selection is not consistent across all steps. For example, the `X` dropped 'Ladder Score', which might be needed for profiling later.
    *   **Visualization:** No plotting for clustering, dimensionality reduction, or anomalies, except implied by commented sections or external libraries. The ground truth included detailed visualizations.
    *   **Dependencies:** The generated code imports `MDS` for dimensionality reduction but never uses it.

3.  **Adherence to Specific Requirements:**
    *   The prompt specifically asked for Spectral Clustering and HDBSCAN (which it partially gets with `hdbscan`) but also `Isomap` (Manifold Learning) and `autoencoders` for dimensionality reduction. `Isomap` is missing and autoencoder is incorrectly implemented.
    *   Specific metrics for optimal parameter selection (silhouette, gap statistic, elbow, stability analysis) are largely skipped.
    *   Statistical significance testing and effect size analysis in country profiling is missing.
    *   The network analysis is fundamentally flawed due to `np.corrcoef(X_scaled.T)`.
    *   Missing detailed country profiles with statistical significance testing and effect size analysis.

The generated code provides a bare-bones skeleton, often stating what *could* be done without implementing it, or implementing it incorrectly or simplistically, which is not suitable for an ""expert code evaluator"" benchmark.",gemini-flash-1.5,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import numpy as np

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')

# Preprocessing
data['date'] = pd.to_datetime(data['date'])
germany_data = data[data['location'] == 'Germany'].set_index('date')
germany_data = germany_data[['new_cases', 'stringency_index', 'people_fully_vaccinated']]
ger...",8,"The generated code correctly implements the core requirements of the task: data loading, filtering, SARIMA model training with exogenous variables, forecasting, and RMSE evaluation. It also includes plotting.

Strong points:
- Correctly uses `auto_arima` to find SARIMA orders and applies them to the `SARIMAX` model.
- Handles exogenous variables in both training and forecasting.
- Calculates and prints RMSE.
- Generates a plot of actual vs. forecasted values.
- Addresses the specific location ('Germany') and target variable ('new_cases').

Areas for improvement to reach a higher score:
- **Missing Value Handling:** The generated code only uses `ffill` for missing values. The ground truth provides a more robust approach with `ffill`, `bfill`, and optional interpolation, which is crucial for time series data where leading `NaN`s (which `ffill` won't catch) or gaps can occur. The prompt specifically asked to ""Handle missing values in target and exogenous variables (e.g., forward fill or interpolation)"", suggesting a more comprehensive approach.
- **Error Handling:** The generated code lacks error handling for file not found, insufficient data, or potential issues during model fitting, which is present in the ground truth and makes the code more robust.
- **Plotting Detail:** While a plot is included, it only plots the test data and forecast. The ground truth's plot is more comprehensive, including the training data and a more descriptive title with RMSE.
- **Clarity and Comments:** The ground truth code is well-commented and explicitly breaks down each step according to the prompt's requirements, which improves readability and maintainability. The generated code is more concise but could benefit from more descriptive variable names (e.g., `data` vs. `df`, `germany_data` vs. `germany_df`) and comments.
- **`inplace=True` usage:** While functionally acceptable, using `inplace=True` can sometimes lead to `SettingWithCopyWarning` in more complex chained operations, as seen with `germany_data.fillna(method='ffill', inplace=True)`. It's generally safer and sometimes more readable to assign the result of the operation back to the DataFrame (e.g., `germany_data = germany_data.fillna(method='ffill')`).",gemini-flash-1.5,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy.stats import kruskal
from scipy.stats import shapiro

df = pd.read_csv('winequality-red.csv')

df['quality_group'] = pd.cut(df['quality'], bins=[0, 4, 6, 10], labels=['low', 'medium', 'high'], right=False)

features = df.columns[:-2] 
results = {}

for feature in features:
    group...",5,"The generated code attempts to address the requirements but has several critical issues that make it incomplete and in some cases, incorrect.

**Correctness Issues:**
1.  **Missing `statsmodels` import alias:** `sm.stats.anova_lm` is called but `import statsmodels.api as sm` is missing. This will cause a `NameError`.
2.  **Incorrect Normality Test Logic:** The `shapiro` test is only applied to the first group (`group_data[0]`) for an initial check, and then a loop checks `all([shapiro(g)[1] > 0.05 for g in group_data])`. While this technically checks all groups, the subsequent ANOVA is performed based on the initial `stat, p = shapiro(group_data[0])` check, not on the result of the `all()` check. More importantly, it only checks normality, not homogeneity of variances, which is a key assumption for ANOVA.
3.  **Incomplete ANOVA Implementation:** The code uses `statsmodels.formula.api.ols` to fit a linear model and then `sm.stats.anova_lm` for ANOVA. This is a valid approach, but the decision to use ANOVA should also consider the homogeneity of variances (Levene's test), which is completely absent.
4.  **Incomplete Post-hoc Analysis:**
    *   For ANOVA, it correctly calls `pairwise_tukeyhsd`.
    *   For Kruskal-Wallis, it simply states `'Dunn\'s test needed'`, which means the required post-hoc analysis for non-parametric tests is *not* performed. This is a major omission as the task explicitly asked for it.
5.  **Hardcoded Feature Selection:** `features = df.columns[:-2]` assumes the last two columns are `quality` and `quality_group`. While `quality_group` is new, `quality` is before it and might not always be the second to last. It's better to explicitly exclude known non-feature columns.
6.  **Box Plot Subplots:** The plotting loop has a fixed `plt.subplot(2, 3, i + 1)`, meaning it expects 6 significant features (2 rows, 3 columns). If there are more or fewer, the plots will be arranged incorrectly or an error might occur if `i+1` goes beyond 6. It doesn't dynamically adjust for the number of significant features.

**Code Quality Issues:**
1.  **Lack of Comments:** The code is very sparse on comments, making it harder to understand the logic, especially for the statistical decisions.
2.  **Repetitive Shapiro calls:** The `shapiro` function is called twice for `group_data[0]`, once explicitly and once within the `all()` comprehension.
3.  **Missing Error Handling:** No `try-except` block for `pd.read_csv`, which is crucial for robustness.

**Completeness Issues:**
1.  **Missing Levene's Test:** As mentioned, homogeneity of variance check (Levene's test) is completely overlooked before deciding on parametric ANOVA.
2.  **No Dunn's Test Implementation:** The code identifies that Dunn's test is needed but does not implement it, which is a direct failure to meet the ""perform post-hoc tests (Tukey's HSD or Dunn's test)"" requirement.
3.  **Limited Summary:** The summary is just a print statement of significant features. It doesn't include the statistical results or post-hoc details as requested in ""Summarize which features significantly differ between groups.""
4.  **Fixed Quality Grouping:** `pd.cut` with fixed bins `[0, 4, 6, 10]` is correct for the specified task, but the previous code identified this requirement and explicitly used `assign_quality_tier` and `Categorical` ordering, which is slightly more robust for defining categories.

Overall, the code demonstrates some understanding of the task but falls short on implementing key statistical steps (Levene's test, Dunn's test) and robust code practices.",gemini-flash-1.5,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, f1_score
import shap
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('synthetic_covid19_data.csv')
df['date'] = pd.to_datetime(df['date'])

# Cre...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being production-ready or fully correct for a time-series task.

**Correctness:**
*   **Target Variable Definition:** The target variable definition is flawed. The prompt explicitly states: ""A 'high_daily_death_increase' is defined as a day where 'new_deaths' for a 'location' is in the top 10% of its own historical 'new_deaths' (excluding days with zero new deaths)."" The generated code uses `np.percentile(new_deaths, 90)` which correctly gets the 90th percentile of *non-zero* new deaths. However, `group['high_daily_death_increase'] = (group['new_deaths'] >= threshold).astype(int)` will mark *any* day with `new_deaths >= threshold` as positive, even if that day's new deaths are 0, or if it's not truly in the top 10%. The ground truth correctly adds `(df['new_deaths'] > 0)` to ensure only actual increases are considered.
*   **Feature Engineering `fillna`:** The `fillna(method='bfill')` approach for rolling and lagged features is incorrect for time-series data. `bfill` (backward fill) uses future information, leading to data leakage. For time-series, `ffill` (forward fill) followed by `fillna(0)` or similar is appropriate, or `min_periods` for rolling functions. The prompt specifically suggested ""fill with 0 or backfill"" for *missing values from feature engineering*, implying handling NaNs *after* calculation, not directly in the rolling/lag calculation itself. The choice of `bfill` is risky.
*   **Time-based Split:** While the code attempts a time-based split using `df[:train_size]` and `df[train_size:]`, it's not explicitly sorting the DataFrame by `date` *before* the split, which is crucial for a chronological split across all locations. If the DataFrame is not sorted by date, this split becomes effectively random or based on the original data order, not time. The ground truth correctly sorts `df_sorted_for_split = df.sort_values(by='date')` before splitting.
*   **Missing Value Handling (General):** The `X.fillna(0, inplace=True)` for all features including numerical and categorical columns before preprocessing is dangerous. `fillna(0)` for `people_fully_vaccinated` or `population` might be acceptable, but for `stringency_index` or `gdp_per_capita` it implies a value of zero, which might not be appropriate (e.g., mean imputation might be better). The prompt specified handling for engineered features (fill with 0 or backfill), and implicitly for other features, but universal 0 is a strong assumption. Also, `inplace=True` with `SettingWithCopyWarning` is often an issue.
*   **SHAP Implementation:** The `shap_values[1]` for `plot_type=""bar""` is probably not what's intended for the overall magnitude of feature importance. `shap.summary_plot(explainer.shap_values(X_test_processed), X_test_processed, plot_type=""bar"")` correctly plots absolute SHAP values. Also, the `explainer` should ideally be fit on a representative background dataset (like `X_train_processed`) not just `X_test`.

**Code Quality:**
*   Lack of comments and print statements: The ground truth provides helpful print statements at each step, making the code much easier to follow and debug. The generated code lacks this.
*   `SettingWithCopyWarning`: Using `X.fillna(0, inplace=True)` on a slice of a DataFrame `X = df[features]` will likely trigger a `SettingWithCopyWarning`, which can lead to unexpected behavior. It's generally safer to work on copies or use specific assignments.
*   Redundant `reset_index(drop=True)` after `groupby().apply()`: `reset_index(drop=True)` after every `groupby().apply()` can be inefficient, especially if the subsequent operations require sorting again. This is more of a style point, but worth noting.

**Completeness:**
*   **Target Definition (Zero Deaths):** The generated code does not explicitly exclude days with zero new deaths *from the comparison* when defining the target, which is subtle but important. If a location's 90th percentile is calculated correctly from non-zero deaths, then `new_deaths >= threshold` would correctly exclude zero deaths on its own. However, the ground truth's `(df['new_deaths'] > 0)` condition explicitly handles cases where `new_deaths` could be 0, but the threshold happens to be 0 or less (e.g., unlikely but possible if the 90th percentile of positive numbers is tiny).
*   **Handling `people_fully_vaccinated` / `population` ratio:** The generated code includes `people_fully_vaccinated` and `population` as separate features but does not explicitly calculate the ratio as requested. The prompt asked for `'people_fully_vaccinated' / 'population'`.
*   **SHAP top N features:** While it calls `max_display=10`, the `shap.summary_plot` is correctly called without `label=1` or `[1]` for a bar plot, which means the plots would show global feature importance.
*   **No imports for `warnings` or `classification_report`:** The generated code is missing an import for `warnings` (which isn't strictly necessary but is good practice to suppress warnings related to `use_label_encoder`) and `classification_report`, which is a useful part of evaluation.

Overall, the generated code outlines the correct steps but has significant issues in the implementation details, particularly around time-series data handling (target, feature engineering `fillna`, time split) and the robustness of missing value imputation.",gemini-flash-1.5,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from patsy import dmatrices
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.interpolate import interp1d

# Load data
data = pd.read_csv(""synthetic_covid19_data.csv"")

# 1. Data preprocessing
data['date'] = pd.to_datetime(data['date'])
data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']

# 2. Missing value handling
for col in ['reproduction_rate', 'stringency_index', 'vaccination_perce...",6,"The generated code attempts to address all parts of the task but falls short on several critical aspects, primarily correctness in model implementation and missing value handling, and completeness in interpretation.

**Correctness Issues:**

1.  **Missing Value Handling:**
    *   The `interp1d` usage is problematic. It tries to interpolate `missing_indices` based on `date` values, which might not be contiguous or sorted for interpolation. The `transform` method with `interpolate` (as used in the ground truth) is the idiomatic and robust way to perform group-wise interpolation in pandas.
    *   The manual loop with `interp1d` and `loc_data.loc` assignments is inefficient and can lead to `SettingWithCopyWarning` if not handled carefully.
    *   The `fillna(x.median())` as a fallback is a good idea, but the initial interpolation method is flawed. More importantly, it doesn't consider `population` and `people_fully_vaccinated` missing values in the same comprehensive way as the ground truth, which is crucial for `vaccination_percentage`.
    *   It does not handle `population` being zero or NaN leading to `vaccination_percentage` issues.

2.  **Mixed-Effects Model Specification (Critical Flaw):**
    *   The core mixed-effects model setup is incorrect for `statsmodels.MixedLM`. The formula `(1|continent/location)` is `patsy` syntax for nested random effects *design matrices*, but `statsmodels.MixedLM` itself doesn't directly parse this syntax in the `formula` argument for defining groups.
    *   Instead, for random intercepts nested within groups, you need to provide a single `groups` series that uniquely identifies each *combination* of `continent` and `location`. The ground truth correctly creates `continent_location_id` and uses that as `groups`. The generated code uses `groups=data['location']`, which only accounts for random effects by location, not nested within continent, and completely ignores the `re_formula='1'` which should be sufficient for random intercepts if the `groups` are correctly specified.
    *   The use of `dmatrices` is generally helpful for formula-based design matrices, but it's not correctly integrated with how `MixedLM` expects its `groups` argument. `MixedLM` needs *the specific column* that defines the high-level grouping for random effects, not just an indicator from the formula.
    *   The `re_formula='1'` in `MixedLM` is correct for a random intercept, but without the correct `groups` variable representing the continent-location nesting, it doesn't model the problem as specified.

3.  **Reporting of Random Effects:**
    *   `results.random_effects` gives the BLUPs (Best Linear Unbiased Predictors) of the random effects for each group, not their variance components directly. One needs to look at `results.cov_re` (as shown in the ground truth) to get the variance component. This is a subtle but important distinction.

**Completeness Issues:**

1.  **Interpretation:** While it correctly identifies where interpretation would go, it doesn't provide the detailed, conditional interpretation based on significance and coefficient direction, as done in the ground truth.
2.  **Initial Data Preprocessing:** It doesn't sort the data by `location` and `date`, which is crucial for time-series operations like interpolation to be meaningful.
3.  **Robustness:** No `try-except` block for file loading, no warnings configuration, and inadequate handling of edge cases for `people_fully_vaccinated` and `population` (e.g., population being zero or leading to vaccination % > 100).

**Code Quality:**

1.  The manual looping for interpolation is less Pythonic and less efficient than pandas' built-in `groupby().transform()` methods.
2.  Imports are mostly correct, though `interp1d` was used inefficiently.
3.  The overall structure is okay, but the execution of model setup steps has critical errors.

Overall, the failure to correctly implement the mixed-effects model and the inefficient/incorrect missing value handling are major drawbacks. The task explicitly asked for ""random intercepts for 'location' nested within 'continent'"", which was not achieved.",gemini-flash-1.5,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

df = pd.read_csv('clash_royale_cards.csv')
X = df[['elixirCost', 'Win Rate', 'Usage']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

wcss = []
for i in range(2, 8):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(range(2, 8), wc...",7,"The generated code correctly addresses most of the requirements but lacks some robustness and completeness compared to the ground truth.

**Strong Points:**
*   Correctly loads the dataset and selects the specified features.
*   Correctly standardizes the features using `StandardScaler`.
*   Correctly implements the WCSS calculation for the elbow method and plots it.
*   Applies K-Means clustering with a chosen K (implicitly K=3 from the elbow method).
*   Correctly performs PCA to reduce dimensions to 2 components.
*   Creates a scatter plot of the PCA-reduced features, colored by cluster labels.
*   Includes all necessary imports.

**Areas for Improvement:**
*   **K-Means `n_init` parameter:** The `KMeans` initialization in the generated code `kmeans = KMeans(n_clusters=i, random_state=42)` relies on the default `n_init` behavior, which can trigger a warning in newer scikit-learn versions. The ground truth explicitly sets `n_init=10` to suppress this warning and ensure multiple initializations for better results. This makes the code more robust and up-to-date with best practices.
*   **Error Handling for File Not Found:** The generated code does not include any error handling for `FileNotFoundError` when attempting to load the CSV, which could cause the script to crash. The ground truth includes a `try-except` block for this.
*   **Elbow Method Presentation:** While the elbow plot is shown, the generated code doesn't explicitly state or confirm the chosen K based on the plot within the code comments or print statements, as the ground truth does. This makes the thought process clearer.
*   **Plot Labels/Titles Detail:** The plot titles and axis labels are minimal (""Elbow Method"", ""Clusters after PCA""). The ground truth provides more descriptive titles and labels, including explained variance for PCA components, which improves the interpretability of the plots.
*   **Completeness of output:** The ground truth includes print statements to confirm data loading, feature selection, standardization, and the number of chosen clusters, making the code's execution flow more transparent and debuggable. The generated code is more concise but sacrifices some verbosity that aids understanding.
*   **Clarity of variable names:** In the final plot, `principal_components[:, 0]` and `principal_components[:, 1]` are used directly, which is functional but less readable than assigning them to named columns in a DataFrame (`pca_df['PC1']`, `pca_df['PC2']`) as done in the ground truth.",gemini-flash-1.5,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('clash_royale_cards.csv')
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['Win Rate']
encoder = OneHotEncoder(handle_unknown='ignore')
encoded_rarity = encoder.fit_transform(X[['rarity']]).toarray()
encoded_df = pd.DataFrame(encoded_rarity, c...",8,"The generated code successfully completes the task and addresses most of the requirements.

**Strong points:**
*   Correctly loads the dataset and selects the specified features and target.
*   Performs one-hot encoding for the `rarity` column using `OneHotEncoder` as suggested (though `pd.get_dummies` is often simpler for `DataFrame` columns).
*   Correctly splits the data into training and testing sets with the specified `test_size` and `random_state`.
*   Trains a `RandomForestRegressor` model with `random_state` for reproducibility.
*   Evaluates the model using both R-squared and Mean Squared Error.
*   Includes all necessary imports.

**Areas for improvement:**
*   **Robustness in `OneHotEncoder`:** When `OneHotEncoder` is used, the `pd.concat` operation to combine the encoded features with the original numerical features can sometimes lead to issues if the index of `X` is not contiguous or if rows are dropped in `X` before encoding. The `pd.get_dummies` approach on the initial DataFrame is often more robust as it handles the alignment automatically, or ensuring the index is reset after any droppings is crucial when using `OneHotEncoder` and then `pd.concat`.
*   **Missing `drop_first=True` for OneHotEncoder:** While `OneHotEncoder` is used, it doesn't specify `drop_first=True` (which is standard for `get_dummies`), potentially creating a collinearity issue (though often minor for tree-based models). For categorical features, `drop_first=True` is generally good practice to avoid multicollinearity.
*   **Error Handling/Missing Columns:** The ground truth includes checks for missing files and missing columns, which makes the code more robust. The generated code lacks these checks.
*   **Missing Value Handling:** The generated code does not explicitly handle missing values in the selected features or target, which can cause errors or unexpected behavior if the CSV contains NaNs. The ground truth includes a basic `.dropna()`.
*   **Output Formatting:** The output `print` statements are functional, but the ground truth provides clearer, more informative print statements.",gemini-flash-1.5,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy import stats

data = pd.read_csv('clash_royale_cards.csv')
rarities = data['rarity'].unique()
win_rates = [data[data['rarity'] == r]['Win Rate'].dropna() for r in rarities]
fvalue, pvalue = stats.f_oneway(*win_rates)

print(""Null Hypothesis: There is no significant difference in mean Win Rate across different card rarity categories."")
print(""Alternative Hypothesis: There is a significant difference in mean Win Rate across at least one pair of card rarity ...",7,"The generated code correctly performs the ANOVA test and outputs the F-statistic and p-value. It also correctly states the hypotheses and performs the comparison against the alpha value.

However, it lacks robustness and completeness in several areas compared to the ground truth:
1.  **Error Handling:** It doesn't include any error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is crucial for real-world applications.
2.  **Data Cleaning/Preparation:** It implicitly handles NaN values in 'Win Rate' via `.dropna()` but doesn't explicitly ensure 'Win Rate' is numeric using `pd.to_numeric` with `errors='coerce'`, which can be important if the column contains non-numeric strings. It also doesn't check for NaN values in the 'rarity' column.
3.  **Edge Cases/Validation:** It doesn't validate if there are enough unique rarity categories (at least 2) or if each category has sufficient data points (at least 2 observations) for ANOVA to be meaningful. The ground truth handles this by warning/exiting if groups are too small.
4.  **Clarity of Interpretation:** While it states `Reject null hypothesis: {pvalue < alpha}`, it doesn't provide the more detailed, human-readable conclusion statement (e.g., ""Since the p-value is less than alpha, we reject H0. Conclusion: There is a statistically significant difference...""). This is a minor point but affects the completeness of the interpretation step.

Overall, it's a functional solution for the core statistical task, but it could be significantly improved for production readiness and robustness.",gemini-flash-1.5,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
...",9,"The generated code is highly correct and complete, addressing almost all requirements of the task.

**Strong points:**
*   **Correct Imports:** All necessary libraries are correctly imported.
*   **Data Loading and Feature/Target Definition:** The dataset is loaded, and X and y are correctly defined as specified.
*   **Target Encoding:** `LabelEncoder` is used appropriately to encode the `Species` column.
*   **Data Splitting:** Data is split into training and testing sets with the correct `test_size` and `random_state`.
*   **Model Training and Evaluation:** All three specified classifiers (Logistic Regression, SVC with linear kernel, Random Forest with `n_estimators=100`) are instantiated and trained.
*   **Metric Calculation:** Accuracy, precision (macro), recall (macro), and F1-score (macro) are calculated and printed for each model.
*   **Best Model Identification:** The code correctly identifies and prints the best model based on the highest F1-score (macro average).
*   **Clarity:** The code is well-structured and easy to understand.

**Areas for minor improvement (why it's not a 10):**
*   **Logistic Regression `max_iter`:** The ground truth code explicitly sets `max_iter=200` for `LogisticRegression` to avoid potential convergence warnings, which is a good practice for robustness in a benchmark. The generated code uses the default, which might (though unlikely for iris) lead to convergence issues or warnings in some environments. While not strictly required by the prompt, it's a minor detail that enhances robustness.
*   **`stratify` in `train_test_split`:** The ground truth includes `stratify=y_encoded` (or `stratify=y` if `y` is transformed in place), which is best practice for classification tasks, especially with potentially imbalanced datasets, to ensure the class distribution is maintained across splits. The generated code omits `stratify`, which for the Iris dataset might not have a significant impact due to its balanced nature, but it's a best practice omission.
*   **Printing format:** While the output format is clear, the ground truth uses slightly more verbose and descriptive print statements for clarity during execution (e.g., ""--- Model Performance Comparison ---""). This is a stylistic preference and doesn't impact correctness.

Overall, the generated code is excellent and fully satisfies the core requirements of the task. The minor points are more about robustness and best practices rather than direct errors or omissions of explicit requirements.",gemini-flash-1.5,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

df = pd.read_csv('iris.csv')
features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
X = df[features]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
kmeans.fit(X_scaled)
labels = kmeans.labels_
pca = PCA(n_components=...",7,"The generated code correctly performs most of the required steps: loading data, selecting features, standardizing, applying K-Means (K=3, random_state=42, n_init='auto' is implicitly handled correctly for the fit method with default n_init), and PCA. It also generates two scatter plots.

However, there are a few areas for improvement:
1.  **Missing `LabelEncoder` for true species:** The code hardcodes `species_mapping`. While this works for the Iris dataset, using `LabelEncoder` (as in the ground truth) is more robust and generalizable for unknown or larger sets of categorical labels. This was implicitly requested by ""coloring the points by the true `Species` labels (after encoding `Species` to numerical values for coloring)"".
2.  **Plotting quality/completeness:**
    *   The titles are very basic (""K-Means Clustering"" and ""True Species Labels"") and could be more descriptive as requested (""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"" and ""PCA of Iris Dataset: Points Colored by True Species Labels"").
    *   Missing axis labels.
    *   Missing colorbars, which are crucial for interpreting the scatter plots, especially for K-Means clusters and true species.
    *   The `iris.csv` file not found error handling was missing (though not explicitly requested, it's good practice shown in GT).

Overall, it's a solid attempt that gets the core logic right but falls short on the plotting details and generalizability of the species encoding.",gemini-flash-1.5,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
df = pd.read_csv('iris.csv')

# Create new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Pe...",10,"The generated code is excellent and correctly addresses all requirements of the task. It loads the dataset, correctly creates the `PetalArea` and `SepalArea` features, defines `X` including original and new features, encodes the target `y`, trains the Gradient Boosting Classifier with the specified parameters, performs 5-fold cross-validation, and reports the mean accuracy. The imports are correct and the code structure is clear and concise. It fully matches the expectations.",gemini-flash-1.5,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

df = pd.read_csv('heart.csv')

X = df[['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope']]
y = df['HeartDisease']

c...",7,"The generated code largely fulfills the requirements of the task. It correctly loads the data, identifies features and target, performs one-hot encoding for categorical features, scales numerical features, splits the data, trains a RandomForestClassifier, and evaluates it using the specified metrics.

However, there are a few areas for improvement:
1.  **Data Leakage in Preprocessing:** The code applies `fit_transform` to the entire `X` dataframe for both `OneHotEncoder` and `StandardScaler` *before* splitting the data into training and testing sets. This can lead to data leakage, as the scaling and encoding parameters are learned from the entire dataset, including the test set. The ground truth correctly addresses this by using `ColumnTransformer` and `Pipeline` to ensure preprocessing is fit *only* on the training data.
2.  **`ColumnTransformer` and `Pipeline` Usage:** The prompt implies ""comprehensive preprocessing,"" and while the generated code performs the steps, it does so manually. The ground truth demonstrates best practices using `ColumnTransformer` and `Pipeline`, which are more robust, prevent data leakage, and make the code cleaner and more reproducible for complex preprocessing steps. The generated code concatenates `X_numerical` and `X_categorical` directly, which requires careful handling of column names/order if they were to be used for inspection, though for model training it often works.
3.  **Variable Name Anomaly:** There's a typo/inconsistency: `rfrf = RandomForestClassifier(...)` is defined, but then `rf.fit(...)` is called. This would lead to an error.

To improve the score, the primary focus should be on integrating preprocessing within a `Pipeline` to prevent data leakage and follow best practices.",gemini-flash-1.5,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

df = pd.read_csv('heart.csv')
X = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

wcss = []
silhouette_scores = []
for k in range(2, 6):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=...",7,"The generated code correctly performs most of the required steps. It successfully loads the data, selects and standardizes the features, calculates WCSS and silhouette scores for different K values, and applies K-Means with PCA for visualization.

However, there are several areas for improvement:

1.  **Justification of K:** The task explicitly asks to ""Determine an appropriate number of clusters (K) for K-Means, trying values for K from 2 to 5. Use the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K."" The generated code simply sets `k = 2` without any clear justification or explanation of *why* `k=2` was chosen based on the plots it generated. A good solution would either programmatically select K (e.g., finding the knee point) or add a comment explaining the manual reasoning for the chosen K.
2.  **Plot Quality/Completeness:**
    *   The WCSS and Silhouette score plots are generated but lack titles. The ground truth code combined these into a single figure with clear titles and labels for comparison.
    *   The final PCA plot lacks a legend to distinguish the clusters by color, which is crucial for interpreting the clustering results.
    *   The plot titles are simple (e.g., 'Elbow Method', 'K-Means Clustering with PCA') and could be more descriptive as in the ground truth.
3.  **Robustness:** The ground truth includes a `try-except` block for loading the CSV, which makes the code more robust against `FileNotFoundError`. The generated code does not include this.
4.  **Assumptions/Clarity:** The chosen K value (`k=2`) is hardcoded without linking it to the generated plots or the problem statement (which implies a selection process).

Overall, the code is functional and addresses the core technical requirements, but the lack of explicit justification for K, simplified plots, and missing robustness detract from its completeness and adherence to all aspects of the prompt.",gemini-flash-1.5,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np

data = pd.read_csv('winequality-red.csv')
data['good'] = np.where(data['quality'] >= 7, 1, 0)
data = data.drop('quality', axis=1)
X = data.drop('good', axis=1)
y = data['good']
scaler = StandardScaler()
X_scaled = scaler...",8,"The generated code correctly implements the core requirements of the task: creating a binary target, preprocessing, splitting data, training a logistic regression model, and evaluating performance. It also correctly extracts and presents feature importance.

Areas for improvement to reach a higher score:
1.  **Handling missing values:** The generated code does not explicitly check for or handle potential missing values. While this specific dataset (`winequality-red.csv`) might not have missing values, a robust solution should include this step as specified in the prompt ""handle missing values if any"". The ground truth includes a check and a basic imputation strategy.
2.  **Stratified sampling:** The `train_test_split` function is called without `stratify=y`. For classification tasks, especially with imbalanced classes (which `winequality-red` with quality >=7 vs <7 often is), stratified sampling ensures that the train and test sets have a similar proportion of the target classes. This is a common best practice.
3.  **Reproducibility (random_state):** While `random_state=42` is used for `train_test_split`, it's not used when initializing `LogisticRegression`. This can lead to slightly different model results each time the code is run, though for many standard solvers, the effect on coefficients might be minimal. However, it's good practice for full reproducibility.
4.  **Clarity of Feature Importance:** While the feature importance is correctly extracted and sorted, labeling the sorting key as `key=abs` makes it a bit more explicit that the sorting is based on the absolute value of coefficients, which is important for understanding true importance. The ground truth explicitly creates an 'Abs_Coefficient' column, which can be clearer.
5.  **Robust File Handling:** The ground truth includes a `try-except` block for `pd.read_csv`, which makes the script more robust to missing files. This wasn't explicitly asked for but is good practice.
6.  **Code Structure and Comments:** The ground truth uses comments to clearly delineate the different stages of the task (e.g., ""1. Load the dataset"", ""2. Create binary target variable""). This improves readability and maintainability significantly. The generated code is functional but lacks this structural clarity.
7.  **Variable Names:** The generated code uses `data['good']` for the target variable, which is acceptable. The ground truth uses `df['is_good_wine']`, which can be slightly more descriptive.",gemini-flash-1.5,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

df = pd.read_csv('heart.csv')

plt.figure(figsize=(8, 6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.show()

model = LogisticRegression()
model.fit(df[['Oldpeak']], df['HeartDisease'])
print(f""Oldpeak coefficient: {model.coef_[0][0]}"")

df['Oldpeak_bin'] = pd.qcut(df['Oldpeak'], q=3, labels=['low', 'medium', 'high'])

proportions...",7,"The generated code correctly addresses the core requirements for tasks 1 (loading data), 2 (box plot), 3 (logistic regression coefficient), 4 (discretization), and 5 (proportions).

**Strengths:**
*   Correctly loads the CSV data.
*   Successfully generates the box plot using `seaborn`.
*   Performs logistic regression and extracts the `Oldpeak` coefficient.
*   Correctly uses `pd.qcut` for discretization.
*   Correctly calculates the proportion of `HeartDisease=1` within each bin using `groupby().mean()`.

**Areas for Improvement (Why it's not a higher score):**

1.  **Completeness - Interpretation of Coefficient:** The prompt explicitly asked to ""interpret its meaning in the context of odds ratios."" The generated code only prints the coefficient, but doesn't calculate or interpret the odds ratio (e.g., `np.exp(coefficient)` and explaining what it means for a 1-unit increase). This is a significant missing piece as it directly relates to the ""report and interpret"" part of the logistic regression section.
2.  **Code Quality - Plot Customization:** While the box plot is created, it lacks titles, axis labels, and `plt.show()` is placed immediately after the boxplot definition, preventing further plot customization or context. The ground truth example shows good practices for plot labeling.
3.  **Robustness/Error Handling:** There's no error handling for `FileNotFoundError` if `heart.csv` is missing, which could cause the script to crash ungracefully.
4.  **Clarity/Output Information:** The output for the logistic regression simply prints ""Oldpeak coefficient: [value]"". It would be more informative to explicitly state what is being printed and add context (e.g., ""Logistic Regression Coefficient for Oldpeak:"").
5.  **Small Detail - Bin Labels:** The prompt example uses 'Low', 'Medium', 'High' (capitalized) for `qcut` labels, whereas the generated code uses 'low', 'medium', 'high'. While functionally identical, adhering to the example's specifics is better.

Overall, the code provides a functional backbone for the task but lacks the polish, detailed interpretation, and robustness expected from a complete solution.",gemini-flash-1.5,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load data
df = pd.read_csv('spotify_2023.csv')

# 2. Convert streams to numeric and handle NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df.dropna(s...",7,"The generated code correctly addresses most of the requirements. It loads the data, correctly handles the `streams` column conversion and NaN values, creates the `is_popular` target, selects the specified features, performs one-hot encoding, splits the data, trains the Logistic Regression model, and evaluates it using F1-score and ROC AUC. It also displays the confusion matrix.

However, there are a few areas for improvement:
1.  **One-Hot Encoding Implementation:** The way one-hot encoding is handled is slightly less robust than the ground truth. It drops the original columns *before* `pd.concat` and also assumes that the index of `X` and `encoded_df` will align perfectly, which might not be guaranteed after dropping rows, especially if `X` wasn't reset_index() or if `df.dropna` happens before `X` is created. The ground truth uses `pd.get_dummies` directly on `X`, which is a more streamlined and idiomatic way for this task.
2.  **Dataset Index Discrepancy:** When `df.dropna()` is called initially, the DataFrame's index is not reset. When `X` is created as `df[features]`, it retains this potentially non-contiguous index. The subsequent `pd.concat([X, encoded_df], axis=1)` could lead to `NaN` values if the indices don't align perfectly. Although `OneHotEncoder` creates a new `pd.DataFrame` (`encoded_df`) with a default 0-based index, this `encoded_df` may not align with the `X`'s index from the dropped rows. This is a common pitfall. The ground truth's `pd.get_dummies` directly on `df` before splitting handles this more robustly.
3.  **Visualizations:** The request explicitly asked to *display* the confusion matrix, and while the generated code imports `matplotlib.pyplot` and `seaborn` and uses them to plot the confusion matrix, the prompt didn't *require* a visual plot, just the display (which `print(cm)` achieves). The additional code for plotting isn't harmful, but it's extra and not strictly necessary per the prompt, adding unnecessary dependencies. The prompt simply asked to ""Display the confusion matrix"" which usually means printing the numpy array/pandas DataFrame.
4.  **Error Handling for File Loading:** While not explicitly required, robust code would include a `try-except` block for `pd.read_csv`, as demonstrated in the ground truth, to handle `FileNotFoundError` gracefully.
5.  **`stratify` in `train_test_split`:** The ground truth includes `stratify=y` during the train-test split, which is good practice for imbalanced datasets (which is likely the case here as only 25% of songs will be popular). The generated code omits this.",gemini-flash-1.5,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_csv(""spotify_2023.csv"")

# 2. Select features and drop NaN rows
selected_features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_selected = df[selected_features].dropna()

# 3. Standardize features
scaler = ...",8,"The generated code successfully completes almost all aspects of the task.

**Strong Points:**
*   **Correctness and Completeness:**
    *   Loads data correctly.
    *   Selects specified audio features and handles NaNs by dropping rows.
    *   Standardizes features using `StandardScaler`.
    *   Applies K-Means clustering with `K=4`, `random_state=42`, and `n_init='auto'`.
    *   Applies PCA to 2 components with `random_state=42`.
    *   Creates a scatter plot of the two principal components, colored by cluster labels, with a title and axis labels.
    *   Calculates and displays the mean values of the **original (unstandardized)** selected audio features per cluster. This is crucial and correctly implemented by adding the cluster labels back to the `df_selected` (which holds the original, unstandardized features after NaN removal).

*   **Code Quality:**
    *   Proper imports are present.
    *   The code is logically structured, following the steps outlined in the task.
    *   Variable names are clear.

**Areas for Minor Improvement (preventing a perfect 10):**
*   **Error Handling (File Not Found):** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust code. The generated code lacks this.
*   **Clarity in Output:** While calculating means per cluster, the generated code iterates and prints for each cluster, which is functional. However, the ground truth's use of `df_original_for_analysis.groupby('cluster')[audio_features].mean()` produces a cleaner, single table output for the cluster means, which is generally preferred for data presentation. The generated code's output is a bit more verbose due to the loop.
*   **Copying DataFrames:** In the ground truth, `df_features = df[audio_features].copy()` and `df_original_for_analysis = df_features.copy()` are used to explicitly create copies. While `df_selected = df[selected_features].dropna()` implicitly creates a new DataFrame, it's good practice to be explicit when you intend to modify a subset of a DataFrame, especially before you attach new columns like `cluster` to avoid potential `SettingWithCopyWarning` warnings (though in this specific case, `dropna()` would return a new DataFrame, making it less critical here). The ground truth's `df_original_for_analysis` explicitly retains the original, unstandardized data *before* scaling, which is a slightly cleaner separation than re-attaching labels to the `df_selected` after scaling. However, the generated code's approach for mean calculation still works correctly.",gemini-flash-1.5,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_csv(""spotify_2023.csv"")

# 2. Convert 'streams' to numeric
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 3. Filter data
df = df[(df['streams'] > 10000000) & (df['released_year'] >= 2010) & (df['released_year'] <= 2023)]
df.dropna(subset=['streams'], inplace=True)

# 4. Group data by year
grouped = df.groupby('released_year')

# 5. Calculate average audio features
average_features = group...",8,"The generated code correctly addresses all the core requirements of the task. It loads the data, correctly converts `streams` to numeric with error coercion, applies all filtering conditions (streams > 10M, released_year between 2010-2023, and drops NaNs from streams), groups by year, calculates the average audio features, and generates the requested line plots with appropriate titles and labels.

Areas for minor improvement to achieve a higher score:
1.  **Order of Filtering for `streams` NaN**: The generated code filters `df = df[(df['streams'] > 10000000) & ...]` *before* `df.dropna(subset=['streams'], inplace=True)`. While `NaN > 10000000` evaluates to `False` in pandas, it's generally clearer and safer to drop NaNs *first* if they are explicitly part of the filtering criteria (as implied by ""Drop rows with NaN `streams` after conversion""). The ground truth drops NaNs first, which is a slightly more robust approach. In this specific case, it doesn't change the outcome for `streams` because `NaN` will never satisfy `> 10000000`, but for other operations, the order can matter.
2.  **Robustness (File Not Found)**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code assumes the file will always be present. While not strictly required by the prompt, it's good practice.
3.  **Plot Aesthetics**: While the plots are correct and labeled, they lack some of the aesthetic enhancements present in the ground truth (e.g., markers on lines, specific colors, grid, x-axis label rotation). These are minor but contribute to better visualization.
4.  **Clarity of Steps**: The ground truth explicitly separates each step of the analysis with comments and print statements, which significantly improves readability and debugging. The generated code is more concise but less verbose in explaining its steps.",gemini-flash-1.5,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('youtube_trendings.csv')

for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].astype(str).str.replace(',', '').astype(float).fillna(0)

df['published'] = pd.to_datetime(df['published']).dt.year.fillna(0).astype(int)

df['Cat...",9,"The generated code is largely correct and addresses most of the task requirements.

**Strong Points:**
- **Correct Data Loading:** Successfully loads the `youtube_trendings.csv` dataset.
- **Correct Cleaning of Numeric Columns:** Accurately removes commas, converts to float, and fills NaNs with 0 for 'Likes', 'Dislikes', and 'Video views'. The use of `astype(float).fillna(0)` is efficient.
- **Correct Year Extraction:** Extracts the year from 'published' and converts it to integer.
- **Correct Handling of Missing Categories:** Fills missing values in 'Category' with 'Unknown'.
- **Correct One-Hot Encoding:** Applies one-hot encoding to 'Category' using `OneHotEncoder` and correctly appends the new columns to the DataFrame for feature selection.
- **Correct Feature and Target Definition:** `X` and `y` are correctly defined using the specified columns.
- **Correct Data Splitting:** Splits the data with the correct test size and `random_state`.
- **Correct Model Training:** Trains a `LinearRegression` model.
- **Correct Evaluation Metrics:** Calculates and prints both R-squared and MAE.

**Areas for Minor Improvement (Why not a perfect 10):**
- **Robustness in `published` column handling:** The ground truth code correctly extracts `published_year` and explicitly handles potential NaNs introduced by `pd.to_datetime` conversion errors or missing columns. The generated code chains `.fillna(0)` directly after `.dt.year`, which might not cover cases where `pd.to_datetime` itself produces NaNs (though `dt.year` would then also be NaN). A separate step for `fillna` specifically for the year column after full datetime conversion and year extraction might be slightly more robust, or at least ensuring `pd.to_datetime` has `errors='coerce'` as the default. More importantly, the `published` column itself is treated as numeric after `pd.to_datetime(df['published']).dt.year.fillna(0).astype(int)` is applied to it, which is correct for using it as the year.
- **Implicit Column Creation:** The ground truth explicitly creates a new 'published_year' column, while the generated code overwrites the 'published' column. While functionally equivalent for this task, explicitly creating new columns for derived features can sometimes improve readability and prevent accidental overwriting of original data if it were needed for other purposes.
- **Error Handling/Warnings:** The ground truth includes `try-except` for file loading and checks for column existence with warnings. While not explicitly asked for in the prompt, good production code often includes these. For a benchmark evaluation, strictly meeting the prompt is key, but it's a minor point for code quality.

Overall, the generated code successfully completes the task efficiently and accurately.",gemini-flash-1.5,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('youtube_trendings.csv')
df['Video views'] = df['Video views'].astype(str).str.replace(',', '').astype(float).fillna(0)
df['published'] = pd.to_numeric(df['published'].astype(str).str[:4], errors='coerce').fillna(0).astype(int)
grouped = df.groupby('published')
avg_views = grouped['Video views'].mean()
unique_videos = grouped['Video'].nunique()

fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))

axes[0].plo...",8,"The generated code successfully addresses most of the task requirements.

**Strong Points:**
- **Correct Data Loading:** It correctly loads the `youtube_trendings.csv` dataset.
- **`Video views` Cleaning:** It correctly removes commas, converts to numeric (`float`), and fills NaNs with 0 for the `Video views` column. This is very good.
- **`published` Column Handling:** It correctly extracts the year from the `published` column (though it assumes `published` is a string with year at the beginning, which might not always be the case for real-world `publishedAt` columns, but for the given instructions, it's a reasonable interpretation since the prompt used ""published (year)""), converts it to numeric, fills NaNs with 0, and casts to `int`.
- **Correct Grouping and Metric Calculation:** It correctly groups by the `published` year and calculates both the average `Video views` and the count of unique `Video` titles.
- **Two Subplots:** It successfully creates two subplots as requested.
- **Appropriate Titles and Labels:** The plots have correct titles and axis labels.
- **Proper Imports:** Necessary libraries (`pandas`, `matplotlib.pyplot`) are imported.

**Areas for Improvement to achieve a 9/10 or 10/10:**
- **Robustness for `published` column:** The prompt mentions `published` column (year), but CSVs often have `publishedAt` which is a full timestamp. The ground truth correctly assumes `publishedAt` and converts it to datetime objects before extracting the year. The generated code directly parses `df['published'].astype(str).str[:4]`. While this might work for a column named `published` that contains only the year or a string that starts with the year, it's less robust than converting to `datetime` first if the actual column is `publishedAt`. *However, given the phrasing ""published (year)"", the generated code's interpretation is acceptable within the strict reading of the prompt.* If the actual CSV column was `publishedAt` (a common name for date/time values), this would be an issue.
- **Explicit `Video` column for `nunique`:** The prompt stated ""total number of unique videos (count of unique `Video` titles)"". The generated code uses `unique_videos = grouped['Video'].nunique()`. This implies a column named `Video`. If the unique identifier was actually `Video Title` or similar, it might be off. *However, again, based on the prompt implicitly using ""Video"" for counting unique videos, this is acceptable.* The ground truth used `'Video'`, so it's consistent.
- **No `numpy` import:** While not strictly needed for this solution, `numpy` is often imported alongside `pandas` for numerical operations. It's a minor point.
- **No function encapsulation:** The ground truth wraps the logic in a function, which is good practice for reusability and clarity, especially when handling file paths or other parameters. The generated code is a script.
- **Error Handling:** The generated code lacks error handling for `pd.read_csv`, which could fail if the file doesn't exist. This is a common best practice for robust scripts.

Overall, the generated code is highly functional and meets almost all explicit requirements very well. The points for improvement are mostly about robustness and best practices that go slightly beyond the absolute minimum requirements of the prompt.",gemini-flash-1.5,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

df = pd.read_csv('youtube_trendings.csv')

df['title_word_count'] = df['Video'].str.split().str.len()

df['Video views'] = df['Video views'].astype(str).str.replace(',', '', regex=False).astype(float).fillna(0)

correlation = df['title_word_count'].corr(df['Video views'])
print(f""Correlation between title word count and video views: {correlation}"")

df...",7,"The generated code correctly addresses most of the requirements. It loads the data, creates `title_word_count`, cleans `Video views`, calculates the correlation, filters titles based on the year, and finds the most common words.

**Strengths:**
- Correctly uses `str.split().str.len()` for `title_word_count`, which is concise.
- Cleans `Video views` in a single line, which is efficient.
- Correctly handles punctuation removal and lowercasing for word processing.
- Uses `nltk` stopwords and `collections.Counter` as required.

**Areas for Improvement:**
- **Robustness in date filtering:** `df['published'].str.contains('2023') | df['published'].str.contains('2024')` is a brittle way to filter dates. It assumes the year string '2023' or '2024' will always appear in the `published` string and could incorrectly match if ""2023"" or ""2024"" appears elsewhere (e.g., in a month name like ""Feb 2023"" or a day ""2023-01-01""). The ground truth's approach of converting to datetime objects and then extracting the year is much more robust and standard for date-based filtering.
- **Handling of potential `FileNotFoundError`:** The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which is good practice.
- **Output clarity:** The correlation print statement is slightly less descriptive than the ground truth. The print statement for the most common words is also a bit less formatted, printing a list of tuples rather than individual word-count pairs.
- **Word processing refinement:** The ground truth includes `.isalpha()` which helps remove words that might be numbers or empty strings after processing, and also includes a `try-except` block for downloading NLTK stopwords, making it more robust. The current code might include numbers as ""words"" if they are not removed during `re.sub`.
- **Completeness:** The prompt asked for ""Convert words to lowercase, remove punctuation, *and then remove common English stop words.*"" The generated code correctly converts to lowercase and removes punctuation *before* splitting, but the stop word removal is done *after* collecting all words, which is slightly less efficient for very large datasets (though functionally correct). The ground truth processes and filters words within the loop for each title.",gemini-flash-1.5,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder

# 1. Load data
df = pd.read_csv('mobiles_2025.csv')

# 2. Clean Launched Price (USA) and create is_expensive_usa
df['Launched Price (USA)'] = df['Launched Price (USA)'].astype(str).str.replace('USD ', '', regex=False).str.replace(',', '', regex=...",6,"The generated code attempts to address all requirements of the task. However, there are several critical issues, primarily related to data cleaning, NaN handling, and feature preparation, that lead to incorrect behavior and could produce misleading results.

**Critical Issues:**
1.  **Redundant/Incorrect NaN Handling for `is_expensive_usa`:**
    *   The line `df['is_expensive_usa'] = (df['Launched Price (USA)'] > 1000).astype(int)` directly converts the boolean result to an integer (0 or 1). This means any NaNs resulting from `pd.to_numeric` on 'Launched Price (USA)' will NOT propagate to `is_expensive_usa` as NaN but will instead become `0` (False).
    *   The task explicitly states: ""Create a binary target `is_expensive_usa`: 1 if cleaned `Launched Price (USA)` > 1000, else 0. Drop rows where `is_expensive_usa` is NaN."" The ground truth correctly uses `np.nan` for cases where the price is unparseable, then drops these rows. The generated code's approach will incorrectly classify unparseable prices as `0` (not expensive) rather than dropping them.
    *   The line `df.dropna(subset=['is_expensive_usa'], inplace=True)` will drop no rows after the `astype(int)` conversion, effectively ignoring the requirement to drop rows with NaN `is_expensive_usa`.
2.  **Incorrect Feature Cleaning (Back/Front Camera and Dropping NaNs):**
    *   The `extract_first_mp` function uses `str.extract(r'(\d+)')`. This will only extract whole numbers (e.g., '10.5MP' would become '10'). The ground truth uses `r'(\d+(\.\d+)?)\s*MP'` which correctly captures decimal values.
    *   The default for `fillna` in `extract_first_mp` is `0`. The task implies that if a camera value is missing or unparseable, it should be treated as NaN to be dropped later, not filled with 0 at the cleaning stage. This silently imputes missing camera values with 0 instead of propagating NaNs which should be dropped.
3.  **One-Hot Encoding and NaN Handling for `X` and `y`:**
    *   The `OneHotEncoder` is initialized with `handle_unknown='ignore'`. While this is generally fine, `sparse_output=False` is used, which is good.
    *   The `df.dropna(subset=features + ['is_expensive_usa'], inplace=True)` line modifies `df` *after* `X` and `y` have already been assigned (`X = df[features]`, `y = df['is_expensive_usa']`). This means `X` and `y` will still contain NaNs from the original `df`.
    *   The lines `X = X.dropna()` and `y = y.dropna()` are then called *independently*. This can lead to `X` and `y` having different row counts or misaligned rows if NaNs are present in different rows for `X` and `y`. NaNs should be dropped jointly from `X` and `y` based on their combined presence, which the ground truth does correctly by concatenating or using the index.
    *   The `X = X.astype(float)` line is generally unnecessary and could mask issues if there are non-numeric types remaining.
4.  **Implicit Assumptions and Missing Error Handling:**
    *   No error handling for `pd.read_csv` (e.g., `FileNotFoundError`) is present.
    *   No print statements for progress or shapes, which makes debugging and understanding execution flow harder.

**Minor Issues:**
*   `regex=True` is not consistently used where it might be beneficial (e.g., `str.replace(r'[^\d.]', '', regex=True)` in `Launched Price (USA)` cleaning, but `regex=False` elsewhere). While not strictly wrong for what's intended, consistency or specific reasons for `False` should be considered.
*   The `clean_numeric` helper is good, but `extract_first_mp` has the precision issue mentioned above and should ideally produce `NaN` for unparseable data rather than `0`.

Overall, the code attempts to implement the steps but fails to correctly handle the nuances of NaN propagation, data type conversion, and feature extraction as specified, leading to a potentially misprocessed dataset for model training. The independent NaN dropping for X and y is a significant flaw.",gemini-flash-1.5,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price_str):
    try:
        price_str = price_str.replace('PKR ', '').replace('INR ', '').replace('CNY ', '').replace('USD ', '').replace('AED ', '').replace(',', '')
        return float(price_str)
    except:
        return np.nan

df = pd.read_csv('mobiles_2025.csv')

price_cols = ['Launched Price (Pakistan)', 'Launched Price (India)', 'Launched Price (China)', 'Launched Price (USA)', 'Launched Price (Dubai)']
for col in price...",7,"The generated code correctly addresses most of the task's requirements.

**Strengths:**
*   **Correct Task Execution:** It correctly loads the dataset, applies a cleaning function to the specified price columns, calculates the average global price considering available data, and identifies/displays the top 5 models.
*   **Cleaning Function:** The `clean_price` function correctly removes specified currency symbols and commas and converts to float, returning `NaN` on error.
*   **`skipna` equivalent behavior:** While not explicitly using `skipna=True` in `mean()`, the default behavior of `mean()` for pandas DataFrames is to skip NaN values, so `df[price_cols].mean(axis=1)` correctly computes the average only for available prices.
*   **Efficiency:** Using `df.nlargest()` is an efficient way to find the top N rows.

**Areas for Improvement (reflected in the difference from Ground Truth):**
*   **Robustness of `clean_price`:**
    *   The `clean_price` function in the generated code assumes price strings always have a space after the currency (e.g., `'PKR '`). If a price is `'PKR100'` without a space, it won't be cleaned correctly. The ground truth handles this by replacing the symbol without assuming a space.
    *   It doesn't `str()`-wrap the input to `clean_price` before string operations (like `replace`). If the input `price_str` is already a NaN (e.g., `np.nan`), calling `.replace()` on it will raise an `AttributeError`. The ground truth explicitly checks for `pd.isna(price_str)` first or handles conversion to string.
*   **Error Handling (File Loading):** The generated code lacks `try-except` for `pd.read_csv`, which means it would crash if the file is not found. The ground truth includes robust file loading.
*   **Clarity of `mean()` parameter:** Although the default behavior of `df.mean()` handles `skipna=True`, explicitly stating `skipna=True` would make the intention clearer and more directly align with the requirement ""only consider countries for which price data is available (not NaN)"".
*   **Final Sorting (Redundancy):** When using `nlargest`, the result is already sorted by the specified column in descending order. The final `.sort_values(by='Average Global Price', ascending=False)` on `top_5` is redundant, although it doesn't cause harm.",gemini-flash-1.5,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('mobiles_2025.csv')

data['RAM'] = pd.to_numeric(data['RAM'].str.replace('GB', '').str.strip(), errors='coerce')
data['Battery Capacity'] = pd.to_numeric(data['Battery Capacity'].str.replace('mAh', '').str.strip(), errors='coerce')
data['Launched Price (USA)'] = pd.to_numeric(data['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').str.strip(), errors='coerce')

data.dropna(su...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong points:**
- **Correct Cleaning:** Successfully cleans `RAM`, `Battery Capacity`, and `Launched Price (USA)` columns by removing specified strings and converting to numeric with error handling (`errors='coerce'`).
- **Correct NaN handling:** Correctly drops rows where the cleaned columns are NaN.
- **Correct Filtering:** Accurately filters the data for 'Apple' products.
- **Correct Grouping and Plotting:** Iterates through unique launched years and generates separate figures with three subplots for each year, displaying box plots for RAM, Battery Capacity, and Launched Price.
- **Appropriate Titles and Labels:** Each subplot has a descriptive title including the year and metric, and correct y-axis labels.
- **Proper Imports:** Includes all necessary libraries (pandas, matplotlib.pyplot, seaborn).

**Areas for minor improvement:**
- **Robustness in Cleaning (Edge Case):** While `pd.to_numeric` with `errors='coerce'` handles much, the `str.replace` operations assume the columns are initially strings. If `RAM`, `Battery Capacity`, or `Launched Price (USA)` could contain non-string types that might cause an issue with `.str` accessor *before* conversion to string, an `.astype(str)` might be safer, as seen in the Ground Truth. (However, for typical CSV data, this is often not an issue).
- **Explicit `Launched Year` Cleaning/Handling:** The prompt asked to clean `RAM`, `Battery Capacity`, and `Launched Price (USA)` specifically, and then drop rows where *any of these three* were NaN. The generated code did this correctly. However, a `Launched Year` column might also have non-numeric values. The Ground Truth code explicitly cleans `Launched Year` to numeric and drops NaNs from it before proceeding, which adds a layer of robustness. While not strictly required by the prompt's `dropna` criteria, it's good practice for grouping. The generated code implicitly relies on `Launched Year` being clean enough for `unique()`.
- **Plot Style/Aesthetics:** The ground truth includes `plt.style.use('seaborn-v0_8-darkgrid')` for better aesthetics. While not explicitly asked, it enhances presentation. The generated code produces functional plots but lacks this stylistic touch.
- **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for user-friendly scripts. This was not explicitly required by the prompt but adds robustness.

Overall, the generated code successfully completes the core task, demonstrating a strong understanding of data manipulation and visualization in pandas and matplotlib/seaborn. The identified areas for improvement are minor details that enhance robustness or aesthetics rather than core functionality.",gemini-flash-1.5,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import IterativeImputer, KNNImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClas...",5,"The generated code attempts to address many parts of the prompt but falls significantly short on correctness, completeness, and adherence to advanced methodologies specified.

**Primary Reasons for Failure & Critical Missing/Incorrect Aspects:**

*   **Inadequate Data Preprocessing:**
    *   **No Custom Transformer for Feature Engineering:** The prompt explicitly asks for a custom transformer for ""advanced feature engineering (polynomial features, interaction terms, text mining from names)"". The generated code performs these steps in a linear fashion *before* the train-test split, leading to data leakage and making it impossible to encapsulate this advanced preprocessing within a scikit-learn pipeline for consistent application to new data or within cross-validation. This is a fundamental flaw in robust ML workflow design.
    *   **Handling Missing Values:** While it mentions ""multiple imputation strategies,"" it only uses `IterativeImputer` *after* a basic `fillna` on 'Age', 'Embarked', and 'Fare'. The `IterativeImputer` is then applied to `df_poly` which is already partially imputed. Crucially, the preprocessing is done *before* the train-test split, causing data leakage.
    *   **Polynomial Features/Interaction Terms:** These are applied globally *before* the train-test split, and the resulting `df_poly` is used. This means the polynomial features and their fitting are done on the *entire* dataset, leading to data leakage. The ground truth correctly integrates this within a `ColumnTransformer` and `Pipeline` after the split, ensuring proper training and testing environments.
    *   **Text Mining (Names):** While it extracts `Title`, it doesn't align with the advanced text mining specified (e.g., length, Is_Married, handling rare titles comprehensively as done in the ground truth). The implementation of `Title` mapping is fixed values, which is less flexible than one-hot encoding after custom handling.
    *   **Cabin Feature:** Completely ignored despite the ground truth's advanced handling (extracting 'Deck').

*   **Incorrect Pipeline Design for Preprocessing & Models:**
    *   The generated code performs all preprocessing *before* the `train_test_split`. This is a critical error as it causes data leakage and makes it impossible to correctly fit the preprocessing steps (like scaling, imputation, one-hot encoding) only on the training data.
    *   Models are trained directly on `X_train` which is already preprocessed. This means there's no overall `Pipeline` for the entire workflow (preprocessing + model), which is essential for robust deployment and cross-validation without manually repeating preprocessing steps. The `GridSearchCV` and `RandomizedSearchCV` calls are on the base models directly, not on pipelines that include preprocessing, making their utility limited in a truly end-to-end setting.

*   **Hyperparameter Tuning:**
    *   Uses `GridSearchCV` only. The prompt specifically asks for `GridSearchCV *or* RandomizedSearchCV`. `RandomizedSearchCV` is generally preferred for large search spaces. The parameter grids are also less extensive compared to the ground truth.

*   **Ensemble Models:**
    *   The `estimators` for stacking and voting are passed the `GridSearchCV` objects (`rf_best`, `gb_best`, `svm_best`), which themselves contain the best estimator, but the approach in the ground truth where the best *pipeline* is passed ensures consistency in preprocessing for the ensemble.
    *   The SVM in the `estimators` for stacking/voting is `svm_best`, but the `svm` object for `GridSearchCV` ensures `probability=True`. This is good, but the overall lack of a full pipeline makes it less robust.

*   **Feature Selection:**
    *   While `SelectKBest` and `RFECV` are used, they are applied directly to the already preprocessed `X_train` *outside* of a pipeline. This is problematic for proper deployment and comparison, as feature selection should ideally be part of the pipeline.
    *   The prompt asked for ""feature selection using *multiple techniques and compare their impact*"". The comparison of impact is missing; it merely performs the selection but doesn't compare model performance on the feature-selected data vs. full data.

*   **Extensive Model Evaluation:**
    *   **Cross-Validation:** It doesn't perform comprehensive `cross_val_score` on *all* models (including ensembles) using stratified k-fold. The `GridSearchCV` uses CV internally, but an explicit overall CV for all final models to compare means and std dev (as done in the ground truth) is missing.
    *   **Learning/Validation Curves:** While mentioned, the implementation of learning and validation curves is not robust. For example, `validation_curve` for `rf_best` (which is already a `GridSearchCV` object, not a simple `RandomForestClassifier`) would likely fail unless `rf_best.best_estimator_` is used, and even then, it doesn't account for the entire preprocessing pipeline.
    *   **Statistical Significance Tests:** Only a single `ttest_ind` example between RF and GB is provided, which is insufficient. The prompt implies comparing all models comprehensively, likely against the best ensemble, with proper tests (e.g., paired t-test on CV scores, as done in GT). The `ttest_ind` here is misused as it's comparing probability arrays, not overall model performance metrics from CV folds.

*   **Model Interpretability (SHAP/LIME):**
    *   **SHAP:** Only `shap.TreeExplainer` is used with a `RandomForestClassifier`. The prompt asked for ""SHAP *or* LIME"" and the ground truth provides both, with SHAP for XGBoost (which has an optimized TreeExplainer) and LIME for the StackingClassifier (as a model-agnostic example). The generated code only uses SHAP on `X_test` directly, without the proper context of the full pipeline's transformations (e.g., SHAP values for polynomial/one-hot encoded features might be harder to interpret without mapping back).
    *   **LIME:** Completely missing. The prompt specifically asked for ""SHAP *or* LIME"".

*   **Code Quality/Structure:**
    *   Lack of custom transformer class, which was explicitly requested.
    *   Many steps are hardcoded (e.g., direct imputation before split, direct polynomial features before split), leading to a less modular and reusable solution.
    *   Missing `warnings` import and suppression, `re` for regex in name parsing, `lime` import.
    *   `use_label_encoder=False` for XGBoost is best practice but not included in generated code.
    *   Helper function for `get_feature_names_after_preprocessing` was critical in the ground truth for interpretability and feature selection, and entirely missing.

The solution demonstrates a basic understanding of the components but fails to integrate them into a robust, advanced, and correctly structured machine learning pipeline as required by the ""expert code evaluator"" prompt. The fundamental error of applying preprocessing steps *before* the train-test split and not encapsulating the entire workflow within pipelines undermines the scientific validity and practical applicability of the solution.",gemini-flash-1.5,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

df = pd.read_csv('imdb_top_250.csv')
X = df[['Year', 'Runtime']]
y = df['Rating']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_...",6,"The generated code correctly implements the core logic for loading the data, defining features and target, splitting data, training a Linear Regression model, and evaluating it with R-squared and MAE. It includes all necessary imports and follows the specified steps.

However, it lacks robustness and completeness compared to the ground truth. Crucially, it does not handle potential issues with the 'Runtime' column (e.g., '142 min' format) or missing values, which are common in real-world datasets and addressed by the ground truth. It also doesn't include error handling for file not found or missing columns, making it less resilient to real-world data issues. The print statements are minimal, lacking the informative output of the ground truth.

To improve, add data preprocessing steps for 'Runtime' and handle potential NaNs. Implement error handling for file loading and missing columns. Enhance output clarity.",gemini-flash-1.5,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')
X = df[['Rating', 'Runtime']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
kmeans.fit(X_scaled)
labels = kmeans.labels_
plt.scatter(df['Runtime'], df['Rating'], c=labels)
plt.title('K-Means Clustering of IMDB Top 250 Movies')
plt.xlabel('...",6,"The generated code correctly loads the data, selects and standardizes features, and applies K-Means clustering with the specified parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`). It also generates a scatter plot and calculates mean rating and runtime per cluster.

However, there are several areas for improvement:
1.  **Missing Robust Data Cleaning/Preparation**: The ground truth includes robust data cleaning for the 'Runtime' column (handling ' min' suffix, converting to numeric, and coercing errors to NaN) and handles potential missing 'Rating' and 'Runtime' columns, as well as dropping rows with NaN after conversion. The generated code implicitly assumes 'Runtime' is already numeric and doesn't handle potential string formats or `NaN` values, which is a common issue with CSV data and critical for proper execution. This could lead to errors if the 'Runtime' column is not purely numeric.
2.  **Visualization Completeness**: The scatter plot in the generated code lacks a figure size, `cmap` for coloring, a color bar to indicate cluster labels, and grid lines, which are present in the ground truth and improve plot readability and professional appearance.
3.  **Clarity of Cluster Assignment**: The ground truth assigns `cluster_label` directly back to the DataFrame (`df['cluster_label'] = kmeans.fit_predict(scaled_features)`), which makes subsequent steps like `groupby` more straightforward and readable. The generated code uses `labels = kmeans.labels_` and then filters the original DataFrame, which is functional but less integrated than adding the labels directly to the DataFrame.
4.  **Error Handling**: The ground truth includes basic `try-except` for `FileNotFoundError` and checks for the presence of 'Rating' and 'Runtime' columns, making it more robust. The generated code lacks this.

Overall, it's a good attempt that covers the core logic, but it misses important considerations for real-world data handling and producing a more complete/professional output.",gemini-flash-1.5,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter

nltk.download('punkt')
nltk.download('stopwords')

df = pd.read_csv('imdb_top_250.csv')
df['title_length'] = df['Title'].apply(len)
correlation = df['title_length'].corr(df['Rating'])
print(f""Pearson correlation between title length and rating: {correlation}"")

stop_words = set(stopwords.words('english'))
df['processed_title'] = df['Ti...",7,"The generated code correctly addresses most of the requirements but has some areas for improvement.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly calculates `title_length`.
*   Attempts to calculate correlation (though with a potential issue).
*   Correctly handles lowercase conversion.
*   Correctly uses `word_tokenize`.
*   Correctly removes stopwords.
*   Correctly uses `collections.Counter` to find most common words.
*   Includes all necessary imports and directly downloads NLTK resources.

**Areas for Improvement:**
*   **Correlation Calculation**: The prompt specifically asks for Pearson correlation and the ground truth code includes error handling for `Rating` column conversion (`pd.to_numeric(errors='coerce')`) and dropping NaNs, which is crucial for robust correlation calculation. The generated code directly calls `.corr()` without these robustness checks, which might lead to errors or incorrect results if the 'Rating' column isn't perfectly numeric or contains NaNs. The print statement for correlation also implies a generic calculation rather than explicitly mentioning Pearson.
*   **Punctuation Removal**: The prompt specified ""remove punctuation (characters that are not alphanumeric or whitespace)"". The generated code uses `''.join(c for c in x if c.isalnum() or c.isspace())`. While this works, it's less robust than using regex (`re.sub`) for more complex punctuation removal scenarios, as shown in the ground truth. The ground truth also explicitly uses `re` which is a more powerful way to handle regex. The generated code imports `string` but doesn't actually use it.
*   **Output Formatting**: The prompt asks for ""print the 10 most common words along with their frequencies"". The generated code prints `10 most common words: [...]` which is a list of tuples, while the ground truth code provides a more readable line-by-line output (`'word': count`).
*   **Code Structure/Clarity**: The ground truth separates the processing steps more clearly (e.g., creating `processed_title`, then `tokenized_title`, then `filtered_title`). The generated code reassigns to `processed_title` multiple times, which works but can be slightly less readable as each step overwrites the previous transformed version.
*   **Prompt adherence (Punctuation removal detail)**: The ground truth code used `re` module for punctuation removal, which is a more standard and robust way to handle this. The generated code uses a list comprehension approach, which is functional but less common for this specific task. The ground truth also explicitly used a function for readability.",gemini-flash-1.5,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_selector as selector

df = pd.read_csv('penguins.csv')
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g',...",6,"The generated code attempts to use pipelines and ColumnTransformer, which is an advanced and often preferred way to handle preprocessing. This shows a good understanding of `sklearn`'s capabilities.

However, it has several critical issues that prevent it from fully satisfying the requirements:
1.  **Dropping NAs after `fit_transform`:** The line `X_processed = X_processed.dropna()` is incorrect. Missing values should ideally be handled within the pipeline. If `imputer` is used, there should be no NAs left in `X_processed`. If there are NAs, this signifies an issue with the imputation strategy or an edge case not handled (e.g., all values in a column are NA, and `most_frequent` results in NA, or `handle_unknown='ignore'` causes issues). More critically, dropping rows *after* preprocessing breaks the alignment with `y`. The `y = y.iloc[X_processed.index]` line does not correctly preserve the original indices before the `fit_transform` call, which creates a `y` that is unaligned with `X_processed` if rows were dropped. The problem states ""if a row still has NA in `island` after mode imputation...drop those rows"", but this drop should happen *before* the main transformation or be explicitly handled in the pipeline to maintain alignment.
2.  **Target Encoding:** The problem explicitly states ""`species` target (which should be label encoded if not already numeric)"". The generated code does not perform any label encoding for `y`. `RandomForestClassifier` can sometimes handle string targets, but it's not guaranteed behavior across all `sklearn` versions and often leads to warnings or errors, making explicit encoding best practice and a requirement here.
3.  **`handle_unknown='ignore'` for OneHotEncoder:** While `handle_unknown='ignore'` is good practice, it's not strictly necessary for this dataset during training unless unseen categories are expected *during cross-validation*, which `cross_val_score` handles by applying the transformer to each fold. The main issue is that missing values in categorical features need to be properly imputed *before* one-hot encoding or handled by `SimpleImputer` within the pipeline. If `SimpleImputer` returns `NaN` (e.g., if all values are `NaN` for `most_frequent` strategy), `OneHotEncoder` might still fail or produce unexpected results. The ground truth explicitly details the edge case for `island` where *all* values are NA, requiring dropping rows. The pipeline approach would need a custom transformer or more advanced logic to handle such specific dropping rules if `SimpleImputer` cannot resolve it by itself.
4.  **`X_processed.toarray()` and `pd.DataFrame(...)`:** `ColumnTransformer` with `sparse_output=False` by default (or if any transformer is a scaler) will return a dense NumPy array directly. Calling `.toarray()` on an already dense array or on the output of `ColumnTransformer` (which might be a sparse matrix if all transformers return sparse outputs and `sparse_output=True`) is an unnecessary or problematic step. More importantly, converting `X_processed` back to a DataFrame and then immediately dropping rows without managing `y`'s alignment is the source of the alignment issue.
5.  **Lack of `LabelEncoder` import:** This is a minor point, but stems from the absence of target encoding.

To improve, the generated code should:
*   Add `LabelEncoder` for `y`.
*   Address the potentially misaligned `X_processed` and `y` due to `dropna` and `iloc` after `fit_transform`. This `dropna` is problematic for `cross_val_score` workflow, where the imputer should deal with NaNs. The specific requirement for `island` (dropping if mode imputation results in NA) is tricky to handle within a standard `ColumnTransformer`/`Pipeline` setup and might require a custom transformer or initial data cleaning before the main pipeline.
*   More robustly handle the `island` imputation edge case as per the prompt's specific instructions.
*   Ensure the `ColumnTransformer` output is correctly handled for `cross_val_score`.",gemini-flash-1.5,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

df = pd.read_csv(""penguins.csv"")
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
df = df.dropna(subset=features)
X = df[features]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
kmea...",8,"The generated code correctly performs all the core tasks: loading data, selecting features, handling NAs, standardizing, applying KMeans, applying PCA, and generating both required plots.

**Strengths:**
- Loads the dataset correctly.
- Selects the specified features.
- Correctly drops NA values from the selected features.
- Standardizes the features using `StandardScaler`.
- Applies K-Means with the specified parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`).
- Applies PCA to reduce to 2 components.
- Generates two scatter plots with correct data (`X_pca`) and appropriate coloring (K-Means labels and original species).
- Titles are correct.

**Areas for Improvement (Why it's not a 10):**
- **Plotting Aesthetics and Clarity:** While the plots are functionally correct, they lack the visual polish and clarity of the ground truth.
    - The ground truth uses `seaborn` for better aesthetics and easier control over legends, colors, and point attributes (`s`, `alpha`).
    - The generated code uses `plt.scatter` directly. While functional, it doesn't automatically add a legend that maps colors to cluster/species names. For the species plot, `pd.factorize` is used, which maps categories to numbers, but the legend would still just show numbers, not the actual species names. The ground truth explicitly labels the legend.
    - Missing `xlabel`, `ylabel`, and `grid` in the plots, which are present in the ground truth and improve readability.
- **Robustness/Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code assumes the file exists.
- **Intermediate Data Structure for PCA:** The ground truth creates a `pca_df` which makes adding K-Means labels and original species labels more explicit and aligns data for plotting cleanly. While the generated code directly plots using numpy arrays, managing labels can become less intuitive with more complex operations.

Overall, it's a very good solution that fulfills the core requirements, but could benefit from enhanced plotting and minor robustness improvements.",gemini-flash-1.5,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('penguins.csv')

for col in ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']:
    df[col] = df[col].fillna(df[col].mean())

df['sex'] = df['sex'].fillna(df['sex'].mode()[0])
df.drop...",6,"The generated code addresses most parts of the task but has several inaccuracies and omissions that reduce its score.

**Correctness Issues:**
*   **Imputation Order and `dropna`:** The `dropna(subset=['island'])` is performed *after* numerical and sex imputation. If a row has a missing `island`, it's dropped, potentially discarding the imputed values, and more importantly, it might drop rows that *were* used to calculate means/modes, leading to a slight inconsistency. The ground truth correctly drops `island` NAs first.
*   **One-Hot Encoding and Index Alignment:** The most critical issue is how `OneHotEncoder` is used with `pd.concat`.
    *   `island_encoder.fit_transform(df[['island']]).toarray()` returns a NumPy array. When this is converted to a DataFrame (e.g., `pd.DataFrame(...)`), it gets a *new, zero-based index*.
    *   `X` starts with the original `df`'s index.
    *   When `pd.concat` is used with `axis=1` without `ignore_index=True` or resetting indices, it aligns on the index. If `df` had any dropped rows (due to `dropna`), the indices will be misaligned, leading to `NaN`s in the `X` DataFrame for the one-hot encoded columns, or incorrect feature assignment. The ground truth correctly operates on a `df_processed` copy that maintains its index integrity or drops rows consistently before feature extraction. The generated code also does not pass `df` to `OneHotEncoder` *after* the `dropna` operation, which means the `df` passed to the encoder still contains rows with `island` NA.
    *   The `OneHotEncoder` should ideally be applied to the DataFrame *after* all preprocessing and dropping of rows, ensuring feature alignment.
*   **Feature Importance Naming:** While `feature_names = list(X.columns)` correctly gets the names for the `X` DataFrame *before* the train-test split, the `X` used for `model.fit` is `X_train`. While `X_train` *usually* has the same columns, it's a minor point. The main issue is the `OneHotEncoder` index misalignment described above, which could lead to `X` having incorrect values, thus making the feature importances unreliable even if named correctly.
*   **Train-Test Split:** The task description *does not* ask for a train-test split. While it's good practice for model evaluation, it's not requested for *feature importance analysis* or *exploratory data visualization* in this context. Including it introduces unnecessary complexity and potential for error if not handled carefully (as seen with the feature naming). Feature importance is typically calculated on the full training data the model saw.

**Code Quality and Completeness Issues:**
*   **Redundant Import:** `sklearn.model_selection.train_test_split` is imported but not explicitly requested and incorrectly used in the context of the requested feature importance.
*   **Clarity of DataFrame Operations:** Modifying `df` in place repeatedly can sometimes make debugging harder. Using a copy (`df_processed = df.copy()`) is generally safer, as seen in the ground truth.
*   **Plotting:**
    *   The plotting is done in a loop, creating a separate figure for each species. While it works, the request was for ""For each `species`, create box plots...grouped by the imputed `sex`"". The ground truth's `sns.boxplot(data=df_processed, x='species', y='flipper_length_mm', hue='sex', palette='viridis')` creates a single, more concise plot visualizing all species and sexes on one graph, which is generally preferred for comparison. The generated code produces multiple individual plots, making direct comparison across species less straightforward.
    *   `plt.show()` is called inside the loop, leading to multiple pop-up windows if run in a non-interactive environment, which isn't ideal. It's better to show once after all plots are generated or arrange them using `plt.subplots`.

To improve, the code should:
1.  Perform `dropna` for `island` first.
2.  Ensure `OneHotEncoder` operations result in correctly indexed DataFrames that align perfectly with the numerical features. It's often safer to apply `fit_transform` and then create a DataFrame with the same index as the original `df`.
3.  Remove the `train_test_split` as it was not part of the requirements for feature importance calculation or EDA.
4.  Consolidate the box plots into a single figure that groups by `species` and then by `sex` using `hue`.",gemini-flash-1.5,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Separate numerical and categorical features
numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
categorical_cols = ['sex']

# KNN imputation for numerical features
imputer = KNNImputer(n_neighbors=3)
df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

# Mode imputation for ca...",9,"The generated code successfully accomplishes all parts of the task. It loads the dataset, correctly applies KNN imputation to the specified numerical columns using `n_neighbors=3`, and imputes NA values in the 'sex' column with its mode. Finally, it prints the number of remaining NA values to verify the imputation, as required.

**Strong Points:**
- Correctly identifies and separates numerical and categorical columns for distinct imputation strategies.
- Accurately uses `KNNImputer` for numerical columns and `fillna` with `mode()` for the categorical column.
- The verification step iterates through all columns, providing a complete NA summary.
- Concise and readable code.

**Areas for Minor Improvement (for a perfect score):**
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice. The generated code assumes the file exists. While the prompt states to assume the CSV is in the current directory, robust code would include this.
- **Output Clarity:** The ground truth provides more descriptive print statements (e.g., ""Dataset loaded successfully"", ""KNN imputation applied to numerical columns""). While not strictly required by the prompt, it enhances user understanding.
- **Unnecessary Import:** `sklearn.preprocessing.LabelEncoder` is imported but not used. This is a minor point but indicates a slightly unoptimized import list.

Overall, the code is highly functional and correctly implements the core logic required by the task. The minor improvements are mostly about robust error handling and slightly better output for the user, which are not explicitly primary requirements stated in the evaluation criteria.",gemini-flash-1.5,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Filter for Adelie and Gentoo species
penguins_filtered = penguins[(penguins[""species""] == ""Adelie"") | (penguins[""species""] == ""Gentoo"")]

# Handle NA values in culmen_length_mm
penguins_filtered = penguins_filtered.dropna(subset=[""culmen_length_mm""])

# Separate data for Adelie and Gentoo
adelie_culmen = penguins_filtered[penguins_filtered[""species""] == ""Adelie""][""culmen_...",9,"The generated code correctly performs all the required steps: loading the data, filtering, handling NAs, performing the t-test, and interpreting the results. The code is clean and adheres well to the task specifications.

**Strengths:**
*   **Correctness:** All steps are implemented accurately. The filtering, NA handling, and t-test execution are correct. The interpretation of the p-value against the alpha level is also spot on.
*   **Conciseness:** The code is direct and to the point, solving the task efficiently without unnecessary complexity.
*   **Readability:** Variable names are clear, and the flow of logic is easy to follow.

**Areas for Improvement (Minor):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` when loading the CSV. The generated code assumes the file exists, which is acceptable for a direct task solution but less robust for production-level code.
*   **Function Encapsulation:** Encapsulating the logic within a function (as in the ground truth) would make the code more reusable and modular, especially if the task were part of a larger script.
*   **Informative Prints:** While the current print statements are correct, adding a bit more context (e.g., initial and final DataFrame shapes, number of rows dropped due to NA) would enhance the output, aiding in debugging and understanding the data processing steps, similar to the ground truth.
*   **`equal_var` parameter in `ttest_ind`**: The ground truth explicitly sets `equal_var=True`. While `True` is the default for `scipy.stats.ttest_ind`, explicitly stating it makes the assumption clear and the code more robust if the default ever changes or if someone reading the code isn't aware of the default.

Overall, it's a very strong submission that fully meets the core technical requirements. The minor improvements are more about robustness and best practices rather than core functionality.",gemini-flash-1.5,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Define features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'

# Preproces...",6,"The generated code attempts to address all requirements but falls short in several areas, particularly regarding the preprocessing steps and proper use of `ColumnTransformer` and `Pipeline` for robustness.

**Correctness:**
*   **Numerical Imputation:** Correctly uses `median()` for numerical columns.
*   **Sex Imputation:** Correctly uses `mode()` for sex.
*   **Sex Encoding vs. OHE:** This is a major correctness issue. The prompt explicitly asks for ""c) One-hot encode `island` and `sex` (after mode imputation for sex)."" The generated code applies `LabelEncoder` to 'sex', which is incorrect for a nominal categorical feature when building a model like Random Forest that expects numerical input.
*   **Island OHE:** Correctly applies `OneHotEncoder` to 'island'.
*   **Target Encoding:** Correctly uses `LabelEncoder` for 'species'.
*   **Data Splitting:** Correctly splits with `stratify=y` and `random_state`.
*   **Model Training/Evaluation:** Correctly trains `RandomForestClassifier` and evaluates with `accuracy_score` and `classification_report`.

**Code Quality:**
*   **Manual Preprocessing vs. Pipelines:** The biggest code quality issue is the manual, in-place preprocessing of the DataFrame. This means:
    *   The `features` list initially defined is not truly the feature list used for `X` after transformations. The code implicitly modifies `df` and then re-selects `X = df[features]`, but `features` still contains old names and new OHE columns are not explicitly added. This leads to `KeyError` as `island` and `sex` columns are dropped or transformed in place, and `X = df[features]` will fail after `island` and `sex` are processed. The `X = df[features]` line would need to be `X = df.drop(columns=[target])` after all transformations.
    *   The manual preprocessing approach makes it difficult to apply the same transformations consistently to new, unseen data (like `X_test`). While it works for the single train/test split here because `X_train` and `X_test` are sliced *after* the initial `df` transformations, it's not a scalable or reproducible approach. Pipelines and ColumnTransformers are designed to handle this robustly.
    *   The `LabelEncoder` `le` for sex is reused for species without re-instantiating, which is fine, but less clear if they were truly distinct uses.

**Completeness:**
*   **Missing Features in `X`:** The `features` list `['culmen_length_mm', ..., 'island', 'sex']` is defined at the beginning. However, after `island` is dropped and `sex` is label encoded, `X = df[features]` will likely cause a `KeyError` because `island` no longer exists as a column name and `sex` is no longer a distinct column name (it's been overwritten). The prompt implies `island` and `sex` should be part of the final feature set, but their transformed versions are not properly included in the `X` selection.
    *   The `X = df[features]` line will break because `island` has been dropped and its OHE columns are separate, and `sex` has been overwritten with label encoded values. The code *should* have done something like `X = df.drop(columns=[target])` AFTER all transformations if it wanted to stick to manual methods. This is a critical error preventing the code from running as intended without modification.
*   **Sex OHE Requirement:** As mentioned, `sex` was label encoded instead of one-hot encoded as requested.

To improve, the code needs to:
1.  **Crucially fix the `sex` encoding:** Change `LabelEncoder` for `sex` to `OneHotEncoder`.
2.  **Rethink feature selection:** Post-preprocessing, the `X` should directly use the transformed columns. The best way to do this is to use `ColumnTransformer` and `Pipeline` as shown in the ground truth code. This implicitly handles dropping original categorical columns and adding new OHE columns, and ensures proper application of transformations to `X_train` and `X_test` within the model pipeline.
3.  **Address the `X` selection logic:** Explicitly define `X` and `y` *before* detailed column-wise preprocessing, then use `ColumnTransformer` to apply transformations. If sticking to manual, `X` should be `df.drop(columns=[target])` after all transformations and the `features` list (as defined at the start) would become largely redundant for `X` selection.",gemini-flash-1.5,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

correlation_experience = df[""PerformanceScore""].corr(df[""...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly identifies the feature with the stronger absolute correlation.

**Strong points:**
*   **Correctness:** All calculations and logical comparisons are accurate.
*   **Clarity:** The code is straightforward and easy to understand.
*   **Conciseness:** It achieves the task with minimal lines of code.

**Areas for minor improvement:**
*   **Explicit `method='pearson'`:** While `df.corr()` defaults to Pearson, explicitly stating `method='pearson'` (as in the ground truth) is good practice for clarity and robustness, especially in a context where different correlation methods might be applicable for other tasks.
*   **Tie-breaking condition:** The generated code lacks a specific condition for when `abs(correlation_experience) == abs(correlation_training)`. In such a case, it would default to printing ""TrainingHours"" as the stronger feature. While this specific dataset doesn't have such a tie, a more robust solution (like the ground truth) would handle this edge case gracefully by printing a message indicating both are equal.
*   **Formatting of print statements:** The ground truth code formats the correlation coefficients to a specific number of decimal places (e.g., `.4f`), which can improve readability for numerical outputs. This is a stylistic choice but generally considered good practice.

Overall, the generated code is highly functional and completes almost all aspects of the task effectively. The minor points are about robust error handling/edge cases and stylistic consistency.",gemini-flash-1.5,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
from lifelines import KaplanMeierFitter, CoxPHFitter
from scipy import stats
import statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import pymc3 as pm
import matplotlib.pyplot as plt
from lifelines.statistics import proportional_hazard_test

df = pd.read_csv('titanic.csv')
df['Age'].fill...",4,"The generated code attempts to address several requirements but falls short in correctness, completeness, and statistical rigor, particularly concerning the interpretation and application of survival analysis concepts and advanced statistical methods.

Primary Reasons for Failure:

1.  **Fundamental Misinterpretation of Survival Analysis (Kaplan-Meier & CoxPH):**
    *   **Time-to-event (T) and Event Indicator (E):** The most critical flaw is the misunderstanding of `T` (duration/time-to-event) and `E` (event observed) in `lifelines`. The prompt explicitly states to treat ""voyage as time-to-event data (with survival as event indicator)"". The ground truth correctly interprets this, setting `T` as a constant (since all survived or died at the end of the voyage) and `E` as `1 - Survived` (death is the event).
    *   **Generated Code's Interpretation:**
        *   `kmf.fit(df['Survived'], event_observed=df['Survived'])`: This is fundamentally incorrect. `df['Survived']` (a boolean indicating survival) cannot serve as both the duration and the event indicator. This would essentially model the ""survival of survival"" which makes no sense in a time-to-event context.
        *   `cph.fit(df, 'Survived', event_col='Survived')`: Similar issue here. Using 'Survived' as both the duration column and the event column is wrong. `lifelines` expects the duration column to be numeric representing time and the event column to be boolean/binary indicating if the event happened at that time. Given `Survived` marks the outcome, it can only be the event, not the time.
    *   **Impact:** This severe misapplication renders the Kaplan-Meier and Cox Proportional Hazards analyses meaningless and demonstrates a lack of basic understanding of survival analysis principles.

2.  **Incomplete and Incorrect Propensity Score Matching:**
    *   The generated code calculates propensity scores but does not perform any actual matching (e.g., nearest neighbor, caliper matching). It simply adds the score back to the DataFrame.
    *   It also misses the crucial step of imbalance checking (e.g., SMD) which is a core part of PSM validation.
    *   The `LogisticRegression` for PSM uses the *outcome* (`Survived`) as the target variable for predicting propensity scores, which is incorrect. Propensity scores are typically estimated based on observed covariates that predict *treatment assignment* (e.g., in the ground truth, Pclass 1 vs Pclass 3 is the ""treatment"").

3.  **Bootstrap Resampling:**
    *   The `bootstrap_cox` function is conceptually okay, but it returns `cph_boot.summary` which is a string/dataframe and not directly usable for calculating confidence intervals numerically from coefficients. It should return the `params_` attribute (coefficients) from the fitted CoxPHFitter object.
    *   It also doesn't present the results (e.g., CIs for coefficients or C-index) as requested.

4.  **Bayesian Analysis (PyMC3):**
    *   The Bayesian model for ""survival factors"" is effectively a logistic regression, which is okay, but `df['Pclass']` and `df['Sex']` are used directly in the linear model without proper handling of categorical variables (e.g., one-hot encoding or using `pm.Data` for observed variables). The ground truth correctly handles this by pre-encoding, standardizing, and using `pm.Data` if the data is passed separately.
    *   The `mu` line directly multiplies `pclass*df['Pclass']` which assumes `Pclass` is a continuous variable and `pclass` is a single coefficient, which isn't standard for discrete classes without dummy variables.
    *   The trace plot is generated but no summary (e.g., `az.summary`) or interpretation of the Bayesian results is provided.

5.  **Advanced Statistical Tests and Model Diagnostics:**
    *   **Multivariate Analysis:** The `statsmodels.formula.api` logistic regression is a good attempt at multivariate analysis, but it's disconnected from the survival analysis context.
    *   **Interaction Effects:** `Fare:Age` is an interaction, which is good, but broader exploration of interaction effects beyond one example is missing.
    *   **Model Diagnostics:**
        *   The VIF calculation `variance_inflation_factor(model.exog, i)` is appropriate for multicollinearity in OLS/Logistic Regression, but not specifically for *CoxPH* model diagnostics (which would involve things like assumption checks, martingale residuals, etc.).
        *   For CoxPH, the `proportional_hazard_test` is good, but the input `df` in `proportional_hazard_test(cph, df)` is potentially problematic because `df` contains categorical columns that weren't one-hot encoded for `cph.fit`, which can lead to errors or incorrect results. The `lifelines` model expects numeric inputs.

6.  **Completeness:**
    *   **Survival Analysis:** Kaplan-Meier plots are requested but the implementation is flawed.
    *   **Advanced Statistical Tests:** Multivariate and interaction effects are touched upon but not fully integrated or comprehensively explored within the survival analysis framework. Model diagnostics are very minimal and not fully relevant to CoxPH.
    *   **Propensity Score Matching:** Only calculates scores, does no matching or balance checks.
    *   **Bootstrap Resampling:** Fails to present interpretable results (CIs).
    *   **Bayesian Analysis:** Implemented but lacks proper data handling, results summarization, and interpretation.
    *   **Model Validation/Sensitivity Analysis:** Completely missing. This was a key requirement (comparing imputation strategies, prior sensitivity, model specification sensitivity).

7.  **Code Quality/Clarity:**
    *   Imports are somewhat scattered (e.g., `proportional_hazard_test` appears late).
    *   Data preprocessing is minimal (`fillna` is done, but no explicit feature engineering or one-hot encoding for all categorical variables *before* being used in models, which causes issues for `cph.fit` later). `df['Sex']` is used directly in PyMC3 linear model for example, without explicit conversion.
    *   There's no output or print statements for many of the analyses (e.g., bootstrap, PSM effect after matching), making it hard to see results.

Overall, the generated code demonstrates some familiarity with the tools but fundamentally misapplies the core survival analysis concepts and delivers incomplete or incorrect implementations of the advanced statistical methods.",gemini-flash-1.5,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Create the DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77], 
        'Final_Score': [90, 88, 82, 92, 80, 91...",9,"The generated code correctly addresses all requirements of the task. It successfully creates the DataFrame, calculates and prints the Pearson correlation, generates a scatter plot colored by subject with proper labels and title, correctly assigns grades, and prints the grade distribution.

Strengths:
- Correctly imports necessary libraries (pandas, matplotlib.pyplot, scipy.stats).
- Accurately calculates and prints the Pearson correlation coefficient.
- The scatter plot correctly uses `plt.scatter` in a loop to color points by subject and includes a legend, appropriate axis labels, and a title. This is a valid approach, although using Seaborn would be more concise as shown in the ground truth.
- Correctly assigns grades and correctly uses `value_counts()` to display the distribution.
- The code is well-structured and easy to understand.

Areas for minor improvement (not critical enough to significantly reduce the score):
- The `import seaborn as sns` was suggested in the prompt's requirements, but `sns.scatterplot` was not used. While the `matplotlib` loop approach works, Seaborn often produces more aesthetically pleasing and concise plots, especially for categorical coloring.
- The correlation output `:.2f` is a minor formatting difference from the ground truth's `:.4f`, but both are acceptable.
- The `Overall_Grade` assignment could use `np.select` for potentially better performance with larger datasets and is often considered more ""pandastic"" than `apply` with a custom function, but `apply` is perfectly functional and clear for this task.

Overall, it's a very good solution that fulfills all the core requirements.",gemini-flash-1.5,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']), 
        'Ticker': ['TECHCO']*10, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0], 
        'Low': [149.5, 150....",10,"The generated code is excellent. It correctly implements all requirements of the task:

1.  **DataFrame Creation and Indexing**: The DataFrame is created accurately, and 'Date' is set as the index as requested.
2.  **Daily Percentage Return**: The calculation for 'Daily_Return' is correct: `(Close_t / Close_t-1 - 1) * 100`.
3.  **Simple Moving Averages**: Both 3-day and 5-day SMAs are calculated correctly using `rolling().mean()`.
4.  **Plotting**:
    *   A line plot is generated showing 'Close', 'SMA_3Day', and 'SMA_5Day' on the same chart.
    *   The title 'TECHCO Close Price & Moving Averages' is correct.
    *   Axes are labeled 'Date' and 'Price'.
    *   A legend is included.
    *   `plt.grid(True)` is added, which is good practice.
    *   The plot is displayed using `plt.show()`.
5.  **Bullish Crossover Identification**: The logic for identifying bullish crossovers (where 'SMA_3Day' crosses above 'SMA_5Day') is precisely implemented: `(df['SMA_3Day'] > df['SMA_5Day']) & (df['SMA_3Day'].shift(1) <= df['SMA_5Day'].shift(1))`. The identified dates are then successfully printed in a list.

The code is clean, well-structured, and directly addresses all parts of the prompt efficiently. Imports are correct and necessary. No major issues were found, making it a perfect solution.",gemini-flash-1.5,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', '...",9,"The generated code correctly addresses all requirements of the task. It successfully creates the DataFrame, calculates total sales revenue per product, generates and displays the horizontal bar chart as specified, calculates the average order value, and identifies customers with more than one purchase. The imports are correct, and the code structure is logical and easy to follow.

Minor points for improvement (not critical enough to strongly penalize the score):
- The `plt.barh` function is used directly, which is perfectly fine. The ground truth uses `product_revenue.plot(kind='barh')`, which is a pandas-specific plotting method often preferred for its conciseness when plotting Series/DataFrames directly. Both achieve the same visual output.
- The `Category` column is present in the initial data but not used anywhere in the calculations or visualizations. While not explicitly asked to use it, if it were removed from the initial data, the output would be identical. This is a very minor observation that doesn't detract from the solution's correctness.
- The formatting for the ""Customers with more than one purchase"" output is slightly less structured than the ground truth's table-like format, but it still clearly presents the information.",gemini-flash-1.5,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create DeviceType_Numeric
df[""DeviceType_Numeric""] = df[""DeviceType""].map({""Desktop"": 0, ""Mobile"": 1})

# ...",10,"The generated code correctly defines the DataFrame, creates the ""DeviceType_Numeric"" feature by mapping ""Desktop"" to 0 and ""Mobile"" to 1. It also correctly calculates ""PagesPerMinute"" and handles division-by-zero by replacing `np.inf`, `-np.inf`, and `NaN` values with 0, which is crucial for robust data preparation. Finally, it correctly selects the specified features and prints the head of the resulting DataFrame. The code is concise and efficient, demonstrating a good understanding of pandas operations. The approach to handling `inf` and `NaN` is correct and effective (`.replace([np.inf, -np.inf], 0).fillna(0)`). The imports are correct, and the code is well-structured and complete, addressing all requirements of the task.",gemini-flash-1.5,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = pd.DataFrame(data)
...",9,"The generated code correctly performs all the required steps: defines the DataFrame, calculates the mean and standard deviation for 'TemperatureCelsius', identifies outliers based on the 1.5 standard deviation rule, and prints the specified details for each outlier.

**Strengths:**
- Correctly initializes the DataFrame as per the task description.
- Accurately calculates the mean and standard deviation of 'TemperatureCelsius'.
- Correctly applies the 1.5 standard deviation threshold to identify outliers.
- Iterates through the identified outliers and prints the 'SensorID', 'Timestamp', and 'TemperatureCelsius' as requested.
- The code is concise and readable.

**Areas for minor improvement (not critical issues, hence the high score):**
- **Timestamp Conversion (Minor):** The ground truth converts the 'Timestamp' column to datetime objects using `pd.to_datetime`. While not strictly required for this specific outlier detection logic, it's generally good practice when dealing with time-series data in pandas and improves robustness if further time-based operations were needed. The generated code does not do this, but the output format requested `Timestamp: {row['Timestamp']}` works fine with the string format.
- **Output Formatting/Clarity (Minor):** The ground truth includes more descriptive print statements (e.g., ""Identified Temperature Outliers:"", ""Global Mean Temperature:"", ""Outlier Upper Bound:"", etc.) and formats the temperature to one decimal place, making the output more user-friendly and informative. The generated code directly prints the outlier lines without much preceding context.
- **Handling No Outliers (Minor):** The ground truth explicitly handles the case where no outliers are found and prints a message. The generated code would simply print nothing in that scenario, which is not incorrect but less informative.

Overall, the generated code successfully meets the core requirements of the task.",gemini-flash-1.5,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the `EngagementScore`, and efficiently determines the `PostType` with the highest average `EngagementScore` for each `Platform`. The use of `groupby().idxmax()` is a very clean and efficient way to achieve the desired result, arguably more concise than the `sort_values().drop_duplicates()` method used in the ground truth for this specific task. The output format is also clear and follows the requirements. The code quality is high with proper imports and a logical flow. All requirements of the prompt are fully met.",gemini-flash-1.5,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",9,"The generated code correctly performs all the required steps: defines the DataFrame, calculates ""LengthOfStay"", computes average stay and patient count per diagnosis, prints the summary, and identifies the diagnosis with the longest average stay.

**Strong points:**
*   **Correctness:** All calculations and logical steps yield accurate results as per the problem description.
*   **Conciseness:** The code is more concise than the ground truth while still being readable and understandable. For instance, using `.agg([""mean"", ""count""])` directly on the grouped series is a very efficient way to get both aggregates.
*   **Clarity:** Variable names are appropriate and reflect their purpose.

**Areas for minor improvement:**
*   **Formatting of output:** While the `diagnosis_stats` DataFrame is printed correctly, the ground truth code used f-strings and explicit formatting to present the summary in a more human-readable, columnar fashion. The generated code's printout of `diagnosis_stats` is technically correct but less user-friendly for a final report.
*   **No function encapsulation:** The code is written as a script rather than encapsulated in a function, which is often preferred for reusability and organization in larger projects. However, for this specific task, it's not a critical flaw.

Overall, it's a very good solution that successfully addresses the core requirements.",gemini-flash-1.5,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",9,"The generated code correctly performs all requested operations: it defines the DataFrame, calculates the ""WeightedRating"", adds it as a new column, and accurately identifies and prints the title and weighted rating of the movie with the highest value.

The code quality is good, with proper imports and a clear, concise structure. It directly addresses all requirements of the task.

One minor area for improvement, though not critical for correctness in this specific dataset, would be to add a check for `total_votes == 0` before performing the division to prevent a `ZeroDivisionError` in more general cases. This is a common best practice when dealing with potential division by zero. However, given the provided data where `total_votes` is clearly non-zero, its omission doesn't lead to an error for this particular task. The ground truth code includes such a check, which makes it slightly more robust.",gemini-flash-1.5,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}
df = pd...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and prints the resulting grouped DataFrame. It uses the `groupby()` and `agg()` methods effectively, which is a standard and efficient way to perform these operations in pandas.

The code is concise and directly addresses all the requirements of the task.

Minor improvements could be:
1.  Using named aggregations (e.g., `Average_PriceUSD=(""PriceUSD"", ""mean"")`) as in the ground truth for more descriptive column names in the output DataFrame. The current output column names `PriceUSD` and `Mileage` are technically correct for the averages but could be more explicit.
2.  Adding a descriptive print statement before the DataFrame, similar to the ground truth, to make the output more user-friendly.

These are minor stylistic points and do not detract from the correctness or efficiency of the solution.",gemini-flash-1.5,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",10,"The generated code correctly defines the DataFrame, creates the ""TotalStats"" column, accurately identifies the Pokemon with the highest total stats, and correctly calculates the average speed per ""Type1"". All requirements outlined in the task are met precisely. The code is clean, efficient, and well-structured, following common pandas idioms. The use of `idxmax()` is an elegant way to find the index of the maximum value. The output formatting is also clear and concise.",gemini-flash-1.5,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.decomposition import PCA, TruncatedSVD
import umap
from scipy.spatial.distance import cdist
from gap_statistic import OptimalK
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import tra...",5,"The generated code attempts to address some aspects of the prompt but falls significantly short in terms of advanced feature engineering, comprehensive cluster validation, and detailed analysis, as well as several correctness issues.

**Primary Reasons for Failure & Critical Missing/Incorrect Points:**

*   **Feature Engineering (Missing Advanced Features):**
    *   **Social Network Analysis:** The ground truth code engineers `TicketGroupSize` and `FarePerPerson` from `Ticket`, and handles `Fare` imputation before `FarePerPerson` calculation. The generated code calculates `FarePerPerson` using `FamilySize` (which is less accurate for shared tickets) and `TicketGroup` is only a count, not used for `FarePerPerson`. It completely misses the robust `TicketPrefix` and `CabinDeck`/`HasCabin` features.
    *   **Socioeconomic Indicators:** While `Title` extraction is present, the grouping of rare titles is less comprehensive than the ground truth.
    *   **Text-based Features:** `NameLength` is missed. `TicketPrefix` and `CabinDeck` are entirely absent.
    *   **Interaction Features:** Features like `Age_Class`, `Fare_Class`, `FarePerPerson_Class` are completely missed.

*   **Preprocessing and Scaling (Correctness Issues):**
    *   **Missing Value Handling:** Only `Age` and `Embarked` are filled. `Fare` is not explicitly filled within the preprocessing step, though `FarePerPerson` would implicitly handle it if `Fare` was filled, but the ground truth handles it carefully.
    *   **Categorical Encoding:** `LabelEncoder` is used for categorical features. While it serves a basic purpose, `OneHotEncoder` (as in the ground truth) is almost always preferred for nominal categorical features in clustering to avoid introducing spurious ordinal relationships, especially when combined with `StandardScaler`. This is a critical error for clustering. The ground truth uses `ColumnTransformer` and `OneHotEncoder` correctly.
    *   **Pipeline:** No `Pipeline` or `ColumnTransformer` is used, making the preprocessing less robust and harder to manage, especially with different types of features.

*   **Clustering Algorithms (Correctness & Completeness):**
    *   **Gaussian Mixture Model:** The generated code initializes `GaussianMixture` with `n_clusters`, but `GaussianMixture` needs `n_components`. This is a minor syntax error but reflects a lack of careful implementation.
    *   **Optimal Number of Clusters:**
        *   The use of `gap_statistic.OptimalK` is a valid approach for gap statistic, but the prompt explicitly asked for ""multiple validation metrics (silhouette score, elbow method, gap statistic, calinski-harabasz index)"". The generated code only uses gap statistic for `n_clusters` determination and misses explicit calculation and plotting of silhouette, elbow (inertia for K-Means), and Calinski-Harabasz/Davies-Bouldin for *all* algorithms within the specified range.
        *   DBSCAN does not have an `n_clusters` parameter in the same way; it's density-based. The code correctly skips `OptimalK` for DBSCAN but then doesn't provide any method for determining its optimal parameters (`eps`, `min_samples`), which is crucial for DBSCAN.
        *   The clustering results are then stored directly in `results` dictionary without storing the actual fitted models, which makes it impossible to use `model.predict` or `model.labels_` later for *all* models.

*   **Dimensionality Reduction (Incorrect Implementation):**
    *   The `results` dictionary is overwritten with PCA results in the loop, meaning the actual clustering models are lost. `results[name] = pca_result` replaces the fitted model with PCA transformed data. This breaks the subsequent steps that expect the fitted models to retrieve labels.
    *   The visualization tries to plot `X_scaled[:,0]` and `X_scaled[:,1]` for DBSCAN, which is just the original scaled data, not a dimensionally reduced representation.
    *   t-SNE is completely missing.

*   **Analysis and Visualization (Incomplete & Incorrect):**
    *   **Cluster Characteristics:** No thorough analysis of cluster characteristics (mean/median/mode of original features per cluster) is performed.
    *   **Feature Importance:** No explicit method to determine feature importance for each segment is provided.
    *   **Visualizations:** The visualization section is very basic, only showing scatter plots on the first two dimensions of *something* (often incorrectly assigned) for each cluster. It's missing:
        *   Proper cluster distributions on PCA, t-SNE, and UMAP.
        *   Survival rates per cluster (bar plots).
        *   Violin plots/count plots for continuous/categorical feature distributions across clusters.
        *   Heatmaps of scaled feature means.
    *   The plotting logic `plt.scatter(result[:, 0], result[:, 1], c=results[name].labels_)` is flawed because (as mentioned above) `results[name]` no longer holds the model capable of generating labels.

*   **Cluster Stability Analysis (Completely Missing):** This entire section was requested and not implemented.

*   **Consensus Clustering (Completely Missing):** This entire section was requested and not implemented.

*   **External Validation:** While a random forest classifier is used, it's not truly validating the *clustering results*. It's training a classifier on the original features to predict survival, not evaluating how well the *clusters* align with survival outcomes or other external criteria. The ground truth explicitly assesses survival rates *within* each cluster, which is the correct approach for external validation of clustering against a known outcome.

*   **Code Quality:**
    *   Imports are generally okay, but some (like `TruncatedSVD`) are imported but not used.
    *   Lack of clear logical sections with comments makes it harder to follow.
    *   Error handling for file loading is missing.
    *   Overwriting `results` dictionary (a critical error) shows a lack of careful state management.

In summary, the generated code provides a skeletal structure touching on some concepts but fails to implement the advanced techniques, comprehensive validation, and robust analysis required by the prompt. The foundational errors in feature engineering (especially categorical encoding), model handling, and dimensionality reduction prevent it from producing meaningful results.",gemini-flash-1.5,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",8,"The generated code successfully creates the DataFrame, calculates the book count and average price per author, and identifies the author with the most books in stock.

**Strong Points:**
*   **Correctness:** All calculations (book count, average price, total stock per author, and identifying the author with most stock) are performed correctly.
*   **Conciseness:** Uses `groupby().agg()` effectively for multiple aggregations, which is a very pandas-idiomatic approach.
*   **Readability:** Variable names are clear and descriptive.

**Areas for Improvement (to reach a 9 or 10):**
*   **Printing for Task 1 & 2:** The prompt specifically asked to ""Print this information, showing Author, Count of Books, and Average Price."" While `print(author_stats)` does display the DataFrame with these columns, the ground truth code used `iterrows()` and f-strings for a more user-friendly, formatted output. The generated code's output for this part is a DataFrame representation, which is functionally correct but less explicitly formatted as requested.
*   **Completeness in Printing:** The generated code identifies the author with the most books but does not explicitly print the *total stock value* for that author, which the ground truth did and provides useful context. This is a minor omission for completeness.
*   **Clarity of Output:** Adding some descriptive print statements (like ""--- Author Book Statistics ---"" and ""--- Author with Most Books in Stock ---"") as seen in the ground truth would enhance the clarity and user-friendliness of the output, especially for a benchmark task.

Overall, the code is very good and functionally achieves the main goals. The suggestions are primarily about output formatting and minor completeness details in printing, rather than core logical errors.",gemini-flash-1.5,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold, learning_curve, validation_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.impute import KNN...",6,"The generated code attempts to address many requirements but falls short in several advanced aspects and overall implementation quality compared to the ground truth.

**Strong Points:**
- Includes basic data preprocessing steps like imputation, outlier treatment, and polynomial features.
- Defines multiple base models.
- Attempts basic ensemble methods (VotingClassifier, though not Stacking or Blending in a comprehensive way).
- Incorporates some evaluation metrics like accuracy and ROC AUC.
- Shows an attempt at model optimization using GridSearchCV.
- Includes SHAP for interpretability.
- Shows an attempt at model calibration.

**Areas for Improvement / Missing Features:**

1.  **Comprehensive Data Preprocessing:**
    *   **Multiple Imputation Strategies:** The code uses `KNNImputer` and `SimpleImputer` separately, leading to two different preprocessed datasets (`X_poly_knn`, `X_poly_simple`), but then primarily uses `X_train_knn` for model training. The ground truth uses `IterativeImputer` (MICE), which is generally considered a more advanced and integrated approach for multiple imputation. The code does not clearly demonstrate how multiple imputation *strategies* are integrated or compared.
    *   **Outlier Detection and Treatment:** The `treat_outliers` function is a basic IQR-based capping. While it's a start, the ground truth uses a custom `OutlierCapper` class which is more robust and fits into the `Pipeline` structure properly. The current implementation modifies `X_imputed_knn` and `X_imputed_simple` in place, which can be problematic if not managed carefully, and doesn't integrate into a `Pipeline`.
    *   **Advanced Feature Engineering:** Polynomial features are included, but the integration into a proper `Pipeline` with `ColumnTransformer` is missing, making it less robust for real-world scenarios or when dealing with mixed data types. The ground truth properly uses `ColumnTransformer` and `Pipeline` for a streamlined preprocessing flow.

2.  **Model Optimization:**
    *   **GridSearchCV:** While `GridSearchCV` is used for Random Forest, it's explicitly done only for one model and not consistently applied to all base models as required. The ground truth applies it to all models defined.
    *   **Bayesian Optimization:** Not implemented or even attempted in the generated code, whereas the prompt asked for `GridSearchCV` *or* Bayesian optimization.

3.  **Ensemble Models:**
    *   **Stacking and Blending:** Only `VotingClassifier` is directly implemented in a `Stacking`-like declaration, but it's misnamed (i.e., `stacking = VotingClassifier(...)`). True stacking and blending (which requires a held-out set for training meta-learner) are not properly implemented. The ground truth has robust implementations of all three.

4.  **Advanced Evaluation:**
    *   **Stratified K-Fold:** Used in `GridSearchCV` and for `cross_val_score`, but the overall evaluation structure misses the comprehensive, consistent application across all models and metrics found in the ground truth.
    *   **Learning Curves & Validation Curves:** Only placeholders are left, and they are not fully functional or integrated as in the ground truth.
    *   **Bootstrap Sampling:** A placeholder, not implemented. The ground truth provides a full implementation for generating confidence intervals.
    *   **Comprehensive Evaluation Metrics:** While basic metrics are mentioned, the calculation and storage into a single dashboard or dictionary (like `all_model_eval_results` in the ground truth) are missing. Metrics like precision, recall, F1-score, and Brier score are not explicitly calculated for all models.

5.  **Model Interpretability (SHAP/LIME):**
    *   SHAP is used, but the `explainer(X_train_knn)` might be computationally expensive if `X_train_knn` is large. The ground truth carefully selects the dataset for SHAP and provides more illustrative plots (`summary_plot`, `dependence_plot`). LIME is not included.

6.  **Statistical Significance Testing:**
    *   A placeholder is left; no actual paired t-tests or comparison of CV scores. The ground truth explicitly performs this.

7.  **Model Comparison Dashboard:**
    *   Only a placeholder is left. The ground truth provides a structured approach with a DataFrame and plots for performance comparison, including confidence intervals.

8.  **Model Calibration:**
    *   `CalibratedClassifierCV` is used, but the calibration curve plotting for comparison (calibrated vs. uncalibrated) as in the ground truth is missing. Brier score comparison is also missing.

**Code Quality Minor Issues:**
- Lack of a structured pipeline for preprocessing, making the code less modular and prone to errors.
- Inconsistent variable naming (`X_poly_knn`, `X_poly_simple` are created, but then `X_train_knn` and `X_test_knn` are used after scaling without explicitly mentioning which imputed/poly data they correspond to).
- The `cross_val_score` function needs to be imported: `from sklearn.model_selection import cross_val_score`.
- `use_label_encoder=False` is deprecated in newer XGBoost versions, though it works for now.
- `shap.Explainer` might require `shap.TreeExplainer` or `shap.KernelExplainer` depending on the model, `explainer = shap.Explainer(best_rf)` is a newer generic interface but might not be optimal compared to specific explainers for tree models.

Overall, the generated code outlines the steps but a significant portion of the advanced implementation details requested are either missing, incomplete, or placeholders, leading to a score of 6.",gemini-flash-1.5,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer
from miceforest import MiceForest
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from pygam import LogisticGAM
import pymc3 as pm
import statsmodels.formula.api as smf
from scipy import stats
from sklearn.ensemble import Isol...",4,"The generated code attempts to address many parts of the prompt but fails significantly in implementation details, completeness, and correctness for a task requiring ""advanced statistical modeling.""

**Primary Reasons for Failure/Critical Issues:**

1.  **Missing Data Imputation:**
    *   The prompt specifically asks for ""MICE, KNN, iterative imputer"" and ""compare their effectiveness.""
    *   The generated code uses `miceforest.MiceForest()`, `KNNImputer()`, and `IterativeImputer()`. While `miceforest` is a MICE implementation, the code *then* uses `df_mice` for *all subsequent operations* without comparing the ""effectiveness"" of the different imputation methods (e.g., via descriptive statistics, distribution comparisons, or model performance). This misses a key requirement.
    *   `MiceForest()` is instantiated without any parameters and `fit_transform` is called on the *original DataFrame `df`*. This is often not the intended use; `miceforest` usually expects `df.values` or a specific subset of columns. It's likely to default to incorrect behavior or just impute a subset.

2.  **Multivariate Statistical Models:**
    *   **Logistic Regression:** Implemented correctly in principle (`LogisticRegression(penalty='l1', solver='liblinear')`).
    *   **GAM:** Implemented using `pygam.LogisticGAM()`. Good.
    *   **Bayesian Logistic Regression (PyMC3):**
        *   The PyMC3 model setup is **fundamentally incorrect** for logistic regression. `pm.invlogit(pm.math.dot(X_b, beta))` is correct for calculating the probability, but the `pm.Bernoulli('obs', p=p, observed=y_b)` should be based on `p`, not `beta`. *Correction*: `pm.Bernoulli('obs', p=p, observed=y_b)` *is* correct. My apologies for misreading.
        *   However, the `trace` is sampled, but no post-sampling analysis is done (posterior summaries, trace plots, diagnostics, or *predictions on the test set*). This is a critical omission for a Bayesian model.
        *   It uses `pm.Data` for `X_train` and `y_train` but doesn't handle prediction on `X_test` with `pm.set_data` to showcase its predictive capability, which is necessary for model comparison.

3.  **Advanced Hypothesis Testing:**
    *   This section is almost entirely absent. It has comments like `Manova requires appropriate data structuring` and `Permutation test (example) ...` but no actual implementable code. This is a major failure of the prompt. The prompt explicitly asks for ""multivariate ANOVA, permutation tests, and bootstrap confidence intervals"" with *code*.

4.  **Anomaly Detection:**
    *   `IsolationForest().fit(X)`: Correct.
    *   `LocalOutlierFactor().fit_predict(X)`: `fit_predict` returns labels, generally not used for scores directly. For LOF, you'd typically use `negative_outlier_factor_` after `fit`.
    *   `OneClassSVM().fit(X)`: Correct.
    *   **Missing Evaluation:** None of these anomaly detection methods have their *results* displayed or analyzed (e.g., number of anomalies detected, visualization of scores). The prompt asked to ""identify unusual water quality patterns,"" meaning results should be shown.

5.  **Dimensionality Reduction:**
    *   PCA and Factor Analysis are implemented, but `n_components=2` is hardcoded. While this is an example, it doesn't demonstrate how to choose components (e.g., explained variance for PCA).
    *   No analysis or visualization of the reduced dimensions (e.g., scatter plots of PCA components).

6.  **Robust Statistical Methods:**
    *   This section is completely stubbed out with `...`. The ground truth uses Robust Linear Model (RLM).

7.  **Model Diagnostics:**
    *   Only a basic residual plot for Logistic Regression is shown.
    *   The prompt asks for ""residual analysis, influence measures, and goodness-of-fit tests."" This is barely touched upon.
    *   No Q-Q plot of residuals from the *logistic regression* (the example Q-Q plot is for *linear regression residuals*, which is inappropriate for binary classification).
    *   **No leverage plots.**
    *   Only `classification_report` is used, which is a basic metric, not an ""advanced diagnostic.""

8.  **Advanced Statistical Visualizations:**
    *   Only a Q-Q plot is shown, which is incorrect for binary classification residuals.
    *   ""Leverage Plots"" and ""Diagnostic Dashboard"" are commented out `....`.
    *   The prompt asked for ""Q-Q plots, leverage plots, and diagnostic dashboards"" and implicitly for insightful, advanced visualizations.

9.  **Imports:**
    *   Many necessary imports are missing (e.g., `statsmodels.api`, `statsmodels.graphics.gofplots`, `statsmodels.robust.robust_linear_model`, `statsmodels.genmod.generalized_linear_model`, `statsmodels.genmod.families`, `arviz`, `sklearn.preprocessing.StandardScaler`).
    *   The use of `from miceforest import MiceForest` is an external library not directly covered by the prompt's implied `sklearn` or `statsmodels` ecosystem, although it is a MICE implementation. The prompt listed `pandas, statsmodels, pymc3, scipy, sklearn, matplotlib, seaborn, numpy`, implying standard libraries.

10. **Assumptions and Structure:**
    *   No print statements or clear section headers to indicate progress or output a summary of results, making the code hard to follow and verify its execution.
    *   No data scaling (e.g., `StandardScaler`) for models like Logistic Regression or PCA, which is standard practice and often critical for performance and interpretation.

In summary, the code attempts to cover the breadth of the requirements but lacks significant depth, correctness, and completeness in crucial areas like hypothesis testing, comprehensive model diagnostics, and proper implementation/evaluation of PyMC3 and anomaly detection outputs. It feels more like a skeleton with placeholders than a fully functional solution.",gemini-flash-1.5,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture, SpectralClustering
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA, FactorAnalysis
from umap import UMAP
from sklearn.manifold import TSNE
from scipy.spatial.distance import cdist
from gap_statistic import GapStatistic
i...",6,"The generated code attempts to address most aspects of the prompt but lacks completeness and robustness in several key areas.

**Strengths:**
*   **Imports:** Most necessary libraries are imported.
*   **Basic Structure:** Follows a logical flow from data loading to feature engineering, scaling, clustering, and DR.
*   **Multiple Clustering Algorithms:** Implements K-means, Hierarchical, DBSCAN, GMM, and Spectral.
*   **Multiple DR Techniques:** Includes PCA, t-SNE, UMAP, and Factor Analysis.
*   **Cluster Profiling:** Attempts to describe clusters using `describe()`.
*   **Basic Anomaly Detection:** Includes a simple distance-based anomaly detection.

**Weaknesses and Areas for Improvement:**

1.  **Feature Engineering (Significant Deficiency):**
    *   The prompt explicitly asked for ""advanced features including water quality indices, chemical balance ratios, contamination risk scores, and temporal patterns if applicable.""
    *   The generated code only implements `ph_balance`, `nitrate_risk`, `turbidity_risk`, and `chemical_balance`. `ph_balance` is computed incorrectly (dividing by `max()` of `fillna(mean())` rather than scaling). `nitrate_risk` and `turbidity_risk` are simple binary flags, not composite scores. The range of features engineered is very limited compared to the request and the ground truth.
    *   It completely misses the concept of a comprehensive Water Quality Index, general contamination risk scores, and temporal patterns (though the dataset is static, a mention of it for future generalizability would have been good).

2.  **Optimal Number of Clusters (Incomplete/Incorrect Implementation):**
    *   It attempts to use Elbow, Gap Statistic, Silhouette, Davies-Bouldin, and Calinski-Harabasz.
    *   **Gap Statistic:** It attempts to use `gap_statistic.GapStatistic`, but this external library is not standard and not included in the basic `scikit-learn` ecosystem, meaning it's an assumption that this external library would be available. The ground truth used a manual implementation (or plotting for visual inspection).
    *   **Decision of `n_clusters`:** It arbitrarily sets `n_clusters = 3` after calculating the metrics, stating ""Based on above analysis"". The code doesn't show the analysis or a clear decision-making process based on the calculated metrics, nor does it plot them for visual inspection as the ground truth did. This makes the optimal cluster determination section incomplete.

3.  **Cluster Stability (Very Weak):**
    *   The prompt asked for ""bootstrap resampling and consensus clustering.""
    *   The generated `cluster_stability` function for bootstrap resampling is fundamentally flawed for assessing cluster stability in the way typically expected. It just runs the algorithm on resampled data and returns labels. It doesn't compare these labels across runs or to a baseline to quantify stability (e.g., using ARI or co-occurrence matrices for consensus as the ground truth did).
    *   Consensus clustering is entirely missing. This is a critical omitted component.

4.  **Cluster Profiling (Basic, Lacks Depth):**
    *   It uses `cluster_data.describe()`, which provides summary statistics but isn't as easily digestible for ""profiles"" as mean values of specific metrics (as done in the ground truth) and lacks the potability rate comparison.
    *   It doesn't include radar charts for visualization of profiles, which was explicitly requested.

5.  **Visualization Dashboard (Missing Interactive Components/Radar Charts):**
    *   Only shows one static PCA plot.
    *   Missing other 2D/3D plots (t-SNE, UMAP, PCA 3D, UMAP 3D) as requested.
    *   Crucially missing radar charts for cluster profiles and interactive visualizations (which `plotly` excels at).

6.  **Anomaly Detection (Simplistic):**
    *   The provided anomaly detection is a very basic distance-from-centroid approach.
    *   It does not implement ""cluster-based anomaly detection"" in a more sophisticated way (e.g., using Isolation Forest *within* each cluster, or thresholding based on distributions within clusters, which the ground truth included).

7.  **External Validation (Misinterpreted):**
    *   The code includes an `RandomForestClassifier` for `Potability` prediction. While predicting `Potability` is related to water quality, ""external validation of clustering results using external criteria and domain knowledge"" typically refers to evaluating how well the clusters align with known external labels (like the `Potability` column itself), or interpreting clusters against domain standards, not building a predictive model. The ground truth example correctly focused on comparing cluster characteristics to water quality standards.

8.  **Data Handling:**
    *   The ground truth code correctly handles missing values *before* feature engineering and scaling. The generated code drops NaNs *after* some feature engineering, which might lead to inconsistent results or loss of data if NaNs exist in columns used for engineering. It also assumes `nitrates` and `turbidity` are present in `df` before dropping NaNs.
    *   It also adds a `nitrates` column for `nitrate_risk` to the dataframe, but `nitrates` is not part of the original columns in `water_potability.csv`, which has `Chloramines`, `Sulfate`, etc. This indicates a potential misunderstanding of the dataset's columns.

In summary, the code provides a basic framework, but falls short on the ""advanced,"" ""comprehensive,"" and ""domain-specific"" aspects requested in the prompt, particularly in feature engineering, cluster validation, and visualization.",gemini-flash-1.5,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression

df = pd.read_csv('winequality-red.csv')

correlations = df.corr()['quality'].drop('quality')
top_pos = correlations.nlargest(3)
top_neg = correlations.nsmallest(3)

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()

most_pos = correlations.idxmax()
most_neg = correlations.idxmin()

plt.figu...",9,"The generated code correctly addresses all parts of the task. It calculates correlations, identifies top positive and negative features, creates a heatmap, and generates scatter plots with regression lines.

**Strong points:**
- Correctly identifies top 3 positive and negative correlations.
- Uses `sns.heatmap` for the correlation matrix.
- Uses `sns.regplot` which conveniently draws scatter plots and fits linear regression lines, simplifying the code compared to manual `linregress` and plotting.
- Proper use of `subplot` for the scatter plots.
- Includes all necessary imports.

**Minor areas for improvement (to reach a perfect 10):**
- While `sns.regplot` handles the linear regression visually, it doesn't explicitly print the R-squared value or the regression equation, which the ground truth does by using `linregress`. The task asks for linear regression lines, which is fulfilled, but showing the R-squared value makes the plots more informative for correlation analysis.
- The plots don't include gridlines or explicit legends as the ground truth does, which can enhance readability.
- The printed output of the top correlations is slightly less descriptive than the ground truth (e.g., doesn't explicitly state ""Correlation:"" for each feature). This is a minor stylistic point.
- No error handling for file not found, which is good practice for robustness.
- The `pearsonr` import is not used, indicating a potential leftover or alternative approach that was not fully implemented. `df.corr()` handles Pearson correlation implicitly.",gemini-flash-1.5,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
from scipy import stats
from scipy.stats import chi2_contingency

df = pd.read_csv(""video_games_sales.csv"")
df.dropna(inplace=True)
df['Year'] = df['Year'].astype(int)

nintendo_sales = df[df['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = df[df['Publisher'] != 'Nintendo']['Global_Sales']
t_statistic, p_value_nintendo = stats.ttest_ind(nintendo_sales, other_sales)

action_sales = df[df['Genre'] == 'Action']['Global_Sales']
sports_sales = df[df['Genre'] == ...",6,"The generated code addresses most of the statistical tests required but falls short on several critical aspects, leading to a lower score.

**Correctness:**
*   **Data Cleaning:** The `df.dropna(inplace=True)` is too aggressive. It drops *all* rows with *any* missing value, which might remove vast amounts of data unnecessarily, especially if cleaning strategy for different columns (like `Year_of_Release`, `Publisher`, `Genre`, `Platform`, `Global_Sales`) is not considered carefully. The ground truth selectively drops NaNs based on importance for the tests.
*   **`Year` column:** The prompt specifies `Year_of_Release`, but the generated code tries to access `df['Year']`. This will lead to a `KeyError` unless the CSV implicitly renames it, which is not mentioned in the task description. This is a critical bug.
*   **Chi-square Test Filtering:** The chi-square test's contingency table creation is incorrect. It applies `isin` filter separately to 'Genre' and 'Platform' before creating the crosstab, rather than filtering the DataFrame first and *then* creating the crosstab from the filtered DataFrame.
    *   `df[df['Genre'].isin(top_5_genres)]['Genre']` will return the entire 'Genre' series, only filtered for rows where Genre is in top 5, but the corresponding 'Platform' column for *those* exact rows is not filtered for `top_5_platforms`. This means the `contingency_table` is built from a subset of genres but all platforms, not the intersection of top 5 genres *and* top 5 platforms.
*   **Interpretation:** The code only prints p-values, but the task explicitly asks to ""Display p-values and interpret results with significance level α=0.05"". This is a major omission.
*   **Alternative Hypothesis:** For the first t-test (`Nintendo` vs `Others`), the prompt asks to test whether Nintendo games have *significantly higher* global sales, implying a one-tailed test. The generated code uses the default `ttest_ind`, which is two-tailed. The ground truth correctly uses `alternative='greater'`.

**Code Quality:**
*   **Error Handling:** No error handling for `FileNotFoundError` or insufficient data for tests.
*   **Clarity:** Variable naming is generally clear.
*   **Redundancy:** `from scipy.stats import chi2_contingency` and `from scipy import stats` imports are slightly redundant as `chi2_contingency` can be accessed via `stats.chi2_contingency`.

**Completeness:**
*   **Interpretation:** As mentioned, the lack of interpretation of results based on the p-value and alpha is a significant incompleteness.
*   **Cleaning Details:** The cleaning step is very minimal and generalized (`dropna()`) compared to the ground truth's specific handling of `Global_Sales`, `Year_of_Release`, `Publisher`, `Genre`, `Platform`, and `User_Score`. While `User_Score` wasn't used, the other columns are directly relevant to the tests and their specific cleaning needs (e.g., `Year_of_Release` to `int`) are crucial.
*   **Missing `Global_Sales` cleaning:** The prompt explicitly states to ""clean any missing values."" While `dropna()` addresses this globally, it doesn't emphasize the importance of `Global_Sales` for most tests, which the ground truth does by dropping its NaNs first.

To get a higher score, the generated code needs:
*   Correct `Year_of_Release` column name.
*   More robust data cleaning, specifically focusing on columns used in tests.
*   Correct filtering logic for the chi-square test.
*   Proper interpretation of *all* test results based on the p-value and significance level.
*   Using the correct alternative hypothesis for the one-tailed t-test.
*   Adding checks for sufficient data before performing statistical tests.",gemini-flash-1.5,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('video_games_sales.csv')
df.fillna(0, inplace=True)
df['High_Sales'] = (df['Global_Sales'] > 1).astype(int)
le = LabelEncoder()
for col in ['Platform', 'Genre', 'Publisher']:
    df[col] = le.fit_tr...",7,"The generated code largely addresses the requirements but has critical issues in handling missing values and feature selection, which can significantly impact model performance and lead to incorrect results.

**Correctness:**
*   **Missing Value Handling (Critical Issue):** `df.fillna(0, inplace=True)` is generally a poor approach for handling missing values in this dataset, especially for categorical columns or `Year`. For `Year`, 0 would imply a game released in year 0, which is nonsensical. For categorical columns, 0 would be treated as a valid label, potentially creating a new, unrelated category. The ground truth correctly identifies and drops NaNs from crucial columns like `Year`, `Publisher`, and sales figures, which is a more robust strategy here.
*   **Feature Selection:** The generated code drops 'Name', 'Global_Sales', and 'Year'. While 'Global_Sales' is correctly excluded (as it's used to derive the target), 'Year' is an important feature for 'Years_Since_Release' and should not be dropped from `X` directly unless 'Years_Since_Release' is guaranteed to be a robust replacement for all its information. Also, `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` are included in `X` even though `Total_Regional_Sales` is calculated from them. While not strictly wrong, it adds collinearity and potentially redundant information. The ground truth explicitly lists the features to be used, ensuring clarity and control.
*   **Data Types:** The generated code doesn't explicitly convert 'Year' to an integer type. If `fillna(0)` was applied and 'Year' had NaNs, it might result in a float column, which could cause issues for calculations or model interpretation if not handled. The ground truth explicitly casts 'Year' to `int`.

**Code Quality:**
*   **Readability/Structure:** The code is concise but lacks comments and proper sectioning as requested by the prompt (""Include proper imports and assume the CSV file is in the current directory. Provide only the Python code without explanations."") and shown in the ground truth. This makes it harder to follow the different steps of the MLOps process.
*   **Error Handling:** No `try-except` block for file loading, which is good practice.
*   **Random State:** `random_state` is used appropriately in `train_test_split` and `RandomForestClassifier`.
*   **Imports:** Imports are correct.

**Completeness:**
*   **Target Variable Creation:** Correctly creates 'High_Sales'.
*   **Feature Encoding:** Correctly uses `LabelEncoder` for categorical features.
*   **Additional Features:** Correctly creates `Total_Regional_Sales` and `Years_Since_Release`.
*   **Data Splitting:** Correctly splits data (though `stratify=y` is missing, which is important for imbalanced classes).
*   **Model Training:** Correctly trains a Random Forest Classifier.
*   **Evaluation Metrics:** Correctly calculates and prints accuracy, precision, recall, F1-score, and confusion matrix.
*   **Feature Importance:** Correctly calculates and displays feature importance.

**Recommendations for Improvement:**
*   Implement a more thoughtful approach to missing value handling beyond a blanket `fillna(0)`. Consider dropping rows for critical identifiers or using more appropriate imputation strategies (e.g., mode for categorical, mean/median for numerical).
*   Explicitly define the list of features (`X.columns`) to be used for consistency and clarity, similar to the ground truth, rather than relying on dropping columns. This helps avoid accidentally including or excluding important features.
*   Add `stratify=y` to `train_test_split` to ensure proportional representation of classes in training and test sets, especially if the target variable 'High_Sales' is imbalanced.
*   Add comments to explain major steps, improving readability and maintainability.
*   Consider adding a `try-except` block for `pd.read_csv` to handle `FileNotFoundError` gracefully.",gemini-flash-1.5,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('video_games_sales.csv')

df.fillna({'User_Score': df['User_Score'].mean(), 'Critic_Score': df['Critic_Score'].mean(), 'Rating': df['Rating'].mode()[0]}, inplace=True)

df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']
df['Sales_Ra...",6,"The generated code addresses most of the requirements but has several issues in correctness and completeness, particularly in handling missing values, applying PCA, and handling polynomial and interaction features.

**Correctness & Completeness Issues:**

1.  **Missing Value Handling:**
    *   The generated code only handles `User_Score`, `Critic_Score`, and `Rating` for missing values. The task explicitly asks to handle missing values for 'Year', 'Publisher', and 'Genre' which are crucial for subsequent steps. The `User_Score`, `Critic_Score`, and `Rating` columns are not even present in the `video_games_sales.csv` dataset, indicating a potential hallucination or misunderstanding of the dataset.
    *   The `Year` handling is incorrect; it's handled *after* other feature engineering steps when it should be at the beginning. The `Sales_Momentum` calculation relies on `Year` being clean beforehand.
    *   `Sales_Ratio_NA_EU`: While it handles `inf` values, it fills them with `0`, which might not be the most appropriate strategy. `NaN` for division by zero makes more sense, or careful consideration for these cases. The ground truth adds a small epsilon, which is a better approach to prevent `inf`.

2.  **PCA:**
    *   The PCA step attempts to `fit` PCA directly on `df[numerical_cols]` without *scaling* the data first. Scaling is a critical preprocessing step for PCA to ensure features with larger values don't dominate the components.
    *   The PCA plots `explained_variance_ratio` directly without `cumulative_explained_variance`, which is less informative for determining the number of components. The subplot showing the cumulative sum should be used.
    *   It doesn't *transform* the data into principal components or explain the variance in detail as requested.

3.  **Polynomial Features:**
    *   The `PolynomialFeatures` is applied to `sales_cols`, but the output `poly_df` is created without preserving the original DataFrame's index. When `pd.concat` is used, this leads to misalignment if the original `df` had any rows dropped or reordered, which it does implicitly due to the incorrect missing value handling. The ground truth correctly uses `index=df.index`. `include_bias=False` is also generally preferred for polynomial features.

4.  **Interaction Features (Year and Genre):**
    *   The generated code creates a simple string concatenation `df['Year_Genre'] = df['Year'].astype(str) + '_' + df['Genre']`. This is not a numerical interaction feature as typically understood (e.g., `Year` * `OHE_Genre`). The task specifically asks for ""interaction features between Year and Genre"", which implies a numerical product of Year and one-hot encoded genre columns. The ground truth correctly implements this using one-hot encoding and multiplication.

5.  **Outlier Handling:**
    *   The outlier handling drops entire rows if *any* numerical column has an outlier. This is a very aggressive approach and can lead to significant data loss. A more common and robust method, as shown in the ground truth, is *capping* the outliers (e.g., setting them to the `lower_bound` or `upper_bound`).
    *   The numerical columns are selected with `df.select_dtypes(include=np.number).columns`. This includes *all* numerical columns, many of which might not be appropriate for outlier treatment (e.g., `Rank`, or even the newly created interaction and polynomial features which might contain complex patterns). The prompt implies applying outlier handling to specific, relevant numerical features, typically the sales columns and newly engineered ones.

**Code Quality:**

*   Imports are mostly fine, but `SimpleImputer` was needed for robust missing value handling as in the ground truth.
*   No warnings suppression.
*   Lack of comments and print statements to explain steps and show intermediate results makes it harder to follow, compared to the ground truth which serves as a good example of clarity.
*   The `df.fillna` line for `User_Score`, `Critic_Score`, `Rating` is a critical error as these columns are not in the provided dataset, indicating it hasn't actually analyzed the dataset provided. This points to a reliance on general schema knowledge rather than actual data inspection.

Overall, the generated code attempts to address the major steps but falls short in implementation details, particularly in robust data handling, standard PCA practices, and the correct interpretation of ""interaction features"" and ""outlier handling"" strategies.",gemini-flash-1.5,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency
import statsmodels.formula.api as sm

df = pd.read_csv('diabetes.csv')
df = df.replace(0, np.nan)
for col in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:
    df[col] = df[col].fillna(df[col].median())

t_statistic, p_value_glucose = stats.ttest_ind(df[df['Outcome'] == 1]['Glucose'], df[df['Outcome'] == 0]['Glucose'])

statistic, p_value_bmi = stats.mannwhitneyu(df...",6,"The generated code correctly performs most of the statistical tests required by the task. However, it falls short in several areas related to data cleaning robustness, proper interpretation of results, and specific test parameters as requested.

**Strengths:**
*   Correctly identifies and applies the requested statistical tests (t-test, Mann-Whitney U, chi-square, Pearson correlation, ANOVA).
*   Handles the data loading and initial zero-to-NaN replacement.
*   Performs median imputation.
*   Correctly creates age groups and BMI categories.
*   Calculates and prints the p-values for all tests.

**Weaknesses and Areas for Improvement:**
1.  **Data Cleaning - Imputation Logic:** The ground truth meticulously defines `cols_to_impute` and clarifies that `Pregnancies` and `Outcome` can legitimately be zero. The generated code incorrectly replaces ALL zeros with NaN first (`df = df.replace(0, np.nan)`) and *then* selectively imputes. This means `Pregnancies` and `Outcome` zeros would be replaced by `NaN` and then imputed, which is incorrect and can lead to data integrity issues. The instruction was to replace zeros *in relevant columns* with NaN, then impute.
2.  **Missing Interpretation:** The primary deficiency is the lack of interpretation of the results based on the p-values and the significance level (α=0.05). The task explicitly asked to ""Display p-values and interpret results with significance level α=0.05"". The generated code only prints the p-values, without comparing them to alpha or stating conclusions (reject/fail to reject null, and what that means in context). This is a critical part of a statistical analysis task.
3.  **T-test Parameters:** The ground truth used `equal_var=False` (Welch's t-test) for the t-test, which is generally a safer choice as it doesn't assume equal variances, and is often preferred when assumptions aren't explicitly checked. The generated code uses the default, which is `equal_var=True`. While not strictly ""wrong"" if the assumption holds, it's less robust.
4.  **ANOVA Implementation:** While `statsmodels` is a valid way to perform ANOVA, the task implicitly suggested using `scipy.stats.f_oneway` for a more direct comparison to `scipy` for other tests, and because `f_oneway` directly takes group arrays. The `statsmodels` approach adds complexity (creating a model, then anova_lm) that wasn't necessary for this specific request and makes extracting means per group harder. It also required an extra import (`statsmodels.formula.api`).
5.  **Robustness of Grouping for ANOVA:** The ground truth explicitly handles cases where categories might be empty before passing them to `f_oneway`, preventing potential errors. While not an issue with this specific dataset, it demonstrates a higher level of robustness.
6.  **Code Structure and Readability:** The generated code is functional but somewhat terse, lacking comments and clear sectioning that would improve readability, especially for a benchmark solution aiming for clarity and completeness. The ground truth provides excellent code structure with clear headings and comments.
7.  **Error Handling:** The ground truth includes basic error handling for file not found, which is a good practice for robust code. The generated code lacks this.

To achieve a higher score, the generated code needs to implement the full interpretation logic for each test and refine the data cleaning step to precisely match the requirement.",gemini-flash-1.5,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score...",6,"The generated code addresses most of the requirements but has several significant deficiencies that prevent it from being a high-quality solution.

**Strengths:**
- Loads dataset and handles missing values (zeros to median).
- Creates the three specified additional features ('Glucose_BMI_ratio', 'Age_group', 'Insulin_level').
- Performs one-hot encoding for categorical variables.
- Splits data into training and testing sets.
- Trains and compares multiple classification models.
- Uses cross-validation to evaluate models.
- Performs hyperparameter tuning (for Random Forest, as an example).
- Evaluates models using accuracy, precision, recall, F1-score, and ROC-AUC.
- Generates confusion matrices and ROC curves.

**Areas for Improvement / Deficiencies:**

1.  **Preprocessing (Feature Scaling):** The generated code misses a crucial step in preprocessing: scaling numerical features. While it uses `ColumnTransformer` for `OneHotEncoder`, it completely skips `StandardScaler` (or similar) for numerical features. This is critical for models like Logistic Regression and SVM, potentially leading to poor performance and incorrect results. The ground truth code correctly applies `StandardScaler` to numerical features within the `ColumnTransformer`.

2.  **Feature Importance Display:**
    *   **Incorrect `get_feature_names_out()`:** The line `feature_importances = pd.Series(model.feature_importances_, index=ct.get_feature_names_out())` is incorrect. The `ColumnTransformer` (`ct`) here only transforms `Age_group` and `Insulin_level`. The `remainder='passthrough'` is implied as default for columns *not* specified, but these original columns are not part of `ct.get_feature_names_out()`. The correct way to get feature names after preprocessing all columns (numerical + encoded categorical) requires a more sophisticated approach, as shown in the ground truth using `numerical_features + list(ohe_feature_names)`.
    *   **Missing numerical feature names:** Due to the above, the feature importance visualization will only show names for encoded categorical features (likely `x0_Young`, `x0_Middle-aged`, etc., and `x1_Low`, `x1_Normal`, etc.) and not for the original numerical features like 'Glucose', 'BMI', etc., which are highly important. This renders the feature importance section largely inaccurate and uninformative.
    *   **Generalization issue:** The `ct.get_feature_names_out()` call for feature importance only works if `ColumnTransformer` also applied `fit_transform` on `X` before `X_train,X_test` split, which is done here. However, `ct` here should instead be part of a `Pipeline` or applied to `X_train` and `X_test` separately, then feature names gathered. The ground truth manages this elegantly by first preprocessing, then creating a DataFrame with correct column names before training.

3.  **Hyperparameter Tuning & Best Model Integration:**
    *   The `best_rf` model is used for evaluation, but the overall `final_results` dictionary overwrites the original Random Forest entry. This is acceptable, but the ground truth explicitly names the tuned model ""Random Forest (Tuned)"" for clarity in comparisons, which is a better practice.
    *   Only `Random Forest` is tuned. The ground truth explicitly states it picks `Random Forest` as the ""best model"" for tuning, which is fine, but if the task implied tuning for *the* best model (after initial comparison), this part isn't fully robust.

4.  **Redundant Evaluation Loop:** The code first performs cross-validation, then has a separate loop for training and evaluating models on the test set, including confusion matrices and ROC curves *within* this loop. While this works, it leads to multiple plots (one for each model's CM, one for each model's ROC curve), which can be overwhelming. The ground truth consolidates these plots for a better comparison of all models on a single figure, which is a more effective visualization strategy.

5.  **Code Structure and Readability:**
    *   No comments explaining different sections of the code, unlike the ground truth which is well-commented and structured with headers for each task step.
    *   Error handling for `csv` file loading is missing.
    *   The `df = df.replace(0, np.nan)` line without careful column selection would replace 0s in `Pregnancies` and `Outcome` with `NaNs`, which is incorrect. The ground truth correctly identifies specific columns where 0s represent missing values.
    *   The age group bins (`[0, 30, 60, 100]`) and labels (`Young`, `Middle-aged`, `Senior`) are simpler than the ground truth's, but the ground truth's are more detailed and aligned with common age group classifications.
    *   `SVC(probability=True)` is correctly set. `LogisticRegression(max_iter=1000)` setting is good but `solver='liblinear'` chosen by ground truth is more suitable for smaller datasets.

6.  **Minor Issues:**
    *   The confusion matrix and ROC curve plots appear one after the other, which can be disruptive. A single figure with subplots (as in the ground truth) would be better for comparison.
    *   The feature importance plot for tree-based models doesn't specifically call out `Gradient Boosting` as also having feature importance. While it's in the loop, providing two separate and titled plots as in the ground truth is clearer.
    *   The call to `confusion_matrix` requires separate `ConfusionMatrixDisplay` or `sns.heatmap` for visualization; the code uses `sns.heatmap` directly which is acceptable.

In summary, the code provides a functional prototype that covers the core steps. However, it lacks robustness in preprocessing (missing scaling), has critical errors in feature importance identification, and could benefit from better code structure, specific data handling for ""0s as missing"", and improved visualization presentation.",gemini-flash-1.5,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score

#1. Load and preprocess
df = pd.read_csv('...",6,"The generated code attempts to address all parts of the task, but several critical details and best practices are missed, leading to reduced correctness and completeness.

**Strengths:**
*   **Structure:** Follows the requested sequential steps.
*   **Imports:** Includes most necessary libraries.
*   **Basic Implementation:** Performs the required operations like loading, feature engineering, scaling, PCA, selection, SMOTE, and importance.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical Flaw):**
    *   **Incorrect Imputation:** `df.fillna(df.mean(), inplace=True)` is used. This is problematic for `diabetes.csv` because 0s in `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI` are *actual* missing values that should be handled specifically (e.g., replaced with NaN and then imputed). `df.mean()` will not correctly treat these 0s as NaNs, leading to inaccurate imputation for these crucial features. The ground truth correctly identifies these 0s as missing and imputes them with the median.
    *   **No `SimpleImputer`:** The `SimpleImputer` class, which is a standard and robust way to handle missing values, is not used where it would be beneficial for clarity and correctness (as in the ground truth).

2.  **Engineered Features (Minor Issues/Alternative Interpretations):**
    *   **'Metabolic_Score' definition:** The generated code uses `Glucose * BMI * Insulin`, while the ground truth uses `Glucose + BMI + Insulin`. Both are valid interpretations of a ""combination,"" but the ground truth's sum makes more intuitive sense for a direct ""score"" based on these metrics.
    *   **'Risk_Factor_Count' definition:** The generated code uses counts `(df[high_risk_cols] > df[high_risk_cols].mean()).sum(axis=1)`. This is a generic approach. The ground truth provides more medically relevant thresholds (`> 140`, `> 90`, `> 30`, `> 50`), which is superior for this specific task.
    *   **'Pedigree_Age_product':** The ground truth uses `DiabetesPedigreeFunction * Age`, which is more relevant to the task's context (diabetes). The generated code uses `Pregnancies * Age`, which is less directly relevant to ""Pedigree"".

3.  **Polynomial Features:**
    *   The generated code applies polynomial features to `df` *before* separating X and y and before scaling. While functionally it might work, the subsequent scaling then operates on the raw original columns + polynomial features. The ground truth handles this by specifically selecting `poly_cols`, creating the polynomial features, and then dropping the original columns before merging, which is cleaner and avoids redundant features if `include_bias=False`.
    *   The original `num_cols` are NOT dropped from the main dataframe after poly features are created, which means the original columns still exist alongside their polynomial transformations, leading to high multicollinearity.

4.  **Feature Scaling (Logical Flow):**
    *   The generated code scales `df.drop('Outcome', axis=1)` twice (once for StandardScaler, once for MinMaxScaler) but then only uses `df_std` (`X`) for subsequent steps. This is acceptable but the explanation for which scaled data is used downstream is missing. The ground truth explicitly states which scaled data (`X_processed = X_scaled_standard_df`) will be used.

5.  **PCA (Completeness):**
    *   The generated code applies PCA directly to `df_std` without considering the impact of the correlation filtering first. The ground truth applies PCA *after* correlation filtering, leading to a more refined dataset for PCA.
    *   The task asked for ""explain variance ratios,"" and while it prints `pca.explained_variance_ratio_`, it doesn't provide the cumulative variance or a summary as the ground truth does, which is a minor point.

6.  **Feature Selection (Clarity and Best Practices):**
    *   **SelectKBest `k` value:** `k=5` is a very small number for a dataset that could have many relevant features, especially after feature engineering. The ground truth uses `k=15`, which is a more reasonable starting point.
    *   **RFE estimator (RandomForest vs LogisticRegression):** While RandomForest can be used, LogisticRegression (used in the ground truth) is often preferred for RFE due to its interpretability and general stability for coefficient-based elimination.
    *   **RFE `n_features_to_select`:** Similar to KBest, `n_features_to_select=5` is very low. The ground truth uses `10`.
    *   **Output:** The generated code performs the selection but doesn't print the *selected feature names* (`get_support()`), which is crucial for understanding the result of feature selection.

7.  **Correlation-based Filtering (Order and Logic):**
    *   The correlation filtering happens relatively late compared to the ground truth, particularly *after* some feature selection steps have been initiated on the `df_std`. The ground truth performs this step earlier as a pre-processing step to reduce redundancy before PCA and more complex feature selection, which is a better flow.
    *   The ground truth also correctly applies PCA *after* correlation filtering, which was missed in the generated code.

8.  **SMOTE (Order and Data Split):**
    *   The generated code applies SMOTE on the *entire* `df_filtered` (which was `df_std` after correlation filtering) and `df['Outcome']` directly, without a `train_test_split` *before* SMOTE. **This is a critical error.** SMOTE should *only* be applied to the training data to prevent data leakage and an overly optimistic evaluation of model performance. The ground truth correctly performs `train_test_split` first, then applies SMOTE only to the training set.

9.  **Feature Importance Visualization:**
    *   The visualization is basic but functional. It sorts the importances but doesn't explicitly limit to ""top features"" (e.g., top 15) like the ground truth does, potentially showing too many features for readability. The x-axis labels might overlap if there are too many features.

**Overall:**
The critical errors in missing value imputation and data leakage due to incorrect SMOTE application significantly reduce the correctness and reliability of the generated code for a data science benchmark. The less nuanced feature engineering and feature selection choices also contribute to a lower score.",gemini-flash-1.5,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import mannwhitneyu, chi2_contingency

df = pd.read_csv(""world-happiness-report-2021.csv"")
df.dropna(inplace=True)

western_europe = df[df['Regional indicator'] == 'Western Europe']['Ladder score']
other_regions = df[df['Regional indicator'] != 'Western Europe']['Ladder score']
t_statistic, p_value_t1 = stats.ttest_ind(western_europe, other_regions)

top_25 = df.nlargest(int(len(df)*0.25), 'Ladder score')['...",6,"The generated code correctly attempts to perform all the requested statistical tests and calculates the p-values. It also correctly handles the initial data loading and performs basic missing value handling by dropping rows.

However, there are significant shortcomings that prevent a higher score:
1.  **Missing Interpretation of Results:** The prompt explicitly asked to ""Display p-values and interpret results with significance level α=0.05"". The generated code only prints the p-values and does not provide any interpretation (e.g., ""Reject/Fail to reject null hypothesis"" and what that means in the context of the test). This is a major omission.
2.  **Inadequate Missing Value Handling:** The ground truth code identified `critical_columns` and dropped NaNs only from those. The generated code uses `df.dropna(inplace=True)`, which drops rows if *any* column has a NaN, potentially losing more data than necessary for the required analyses. For a robust solution, one should consider the columns relevant for *each* specific test or use a more targeted `dropna`.
3.  **No Error Handling for File Not Found:** The generated code lacks `try-except` for `pd.read_csv`, which is crucial for robust code.
4.  **Lack of Readability and Structure:** The code is a continuous block of operations without comments or clear separation/headings for each test, making it harder to follow compared to the ground truth which clearly delineated each step.
5.  **Small Details/Improvements:**
    *   For the t-tests, `equal_var=False` (Welch's t-test) is generally preferred unless there's strong evidence of equal variances, as shown in the ground truth. The generated code uses the default `equal_var=True`.
    *   The `happiness_categories` using `pd.qcut` is a more robust way to create categories than arbitrary single thresholds, which is good.
    *   The prompt asked for ""proper imports"", and while the necessary functions are imported, `from scipy.stats import mannwhitneyu, chi2_contingency` is slightly redundant when `from scipy import stats` already provides access via `stats.mannwhitneyu` and `stats.chi2_contingency`, but it's not a critical flaw.

To improve, the code needs to implement the interpretive logic for each statistical test based on the p-value and alpha, and refine its data preprocessing steps including `try-except` for file loading. Adding comments and clear sections for each task would also significantly enhance its quality.",gemini-flash-1.5,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import m...",3,"The generated code attempts to address all requirements but falls significantly short in correctness and completeness, particularly in data preprocessing and feature handling, which renders many subsequent steps incorrect or misleading.

Here's a breakdown of the issues:

1.  **Incorrect Feature Selection and Preprocessing (Critical Flaw):**
    *   **Hardcoded/Incorrect Features:** The ground truth meticulously defines the `FEATURE_COLUMNS` based on the prompt's specific wording (e.g., 'Logged GDP per capita', 'Freedom to make life choices'). The generated code uses `X = df[['GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom', 'Generosity', 'Perceptions of corruption']]`. The dataset's actual column names are 'Logged GDP per capita' and 'Freedom to make life choices'. 'GDP per capita' and 'Freedom' as used in the generated code do not exist in the dataset, leading to a `KeyError` or an empty DataFrame if the error were somehow suppressed, making all subsequent model training invalid. This is a fatal error.
    *   **Incorrect OneHotEncoding:** It applies `OneHotEncoder()` to *all* selected features (`[0,1,2,3,4,5]`), which are intended to be numerical. This is fundamentally wrong as it will try to one-hot encode numerical values, creating a sparse matrix of meaningless features. The prompt stated these are numerical features; no encoding is necessary for them. The dataset does have categorical features like 'Regional indicator', but they were not specified to be used as model inputs. If they were, only appropriate categorical columns should be encoded.
    *   **Missing Value Handling:** The generated code uses a simple `df = df.dropna()`. While it handles missing values, the ground truth explicitly checks `df[FEATURE_COLUMNS + [TARGET_COLUMN]].isnull().sum()` and drops only rows where *relevant* columns have missing values. The generated code drops *any* row with *any* missing value in *any* column, which might be overkill and discard useful data not directly related to the model's features.
    *   **Scaling Order:** Scaling happens *after* `X` is already transformed by `ColumnTransformer` (which incorrectly applied one-hot encoding). If the features were correctly numerical, scaling after the split is correct, but the specific application here is flawed due to the OHE issue.

2.  **Lack of Robustness:**
    *   No `try-except` block for file loading, which is good practice.
    *   No `random_state` for `Ridge` or `Lasso` (minor, but good for reproducibility).

3.  **Incomplete/Incorrect Visualizations:**
    *   **Residual Plot:** The residual plot uses `y_pred_best - y_test` on the y-axis, which is `predicted - actual` (y_hat - y). The standard residual plot shows `actual - predicted` (y - y_hat) on the y-axis (or sometimes `predicted` vs `residuals`). More importantly, its x-axis is `y_test` (actual values), whereas common practice is `y_pred` (predicted values) on the x-axis for homoscedasticity checks.
    *   **Feature Importance:** The line `feature_importances = pd.Series(best_model.feature_importances_, index=df.columns[:-1])` is incorrect. `df.columns[:-1]` will incorrectly map the original `df` columns to the *transformed* features, which have changed significantly due to the faulty one-hot encoding. `best_model.feature_importances_` will correspond to the features after the `ColumnTransformer`, not the original named features. This visualization will be meaningless.

4.  **Prediction on New Data (Incorrect):**
    *   `new_countries = [[6,7,6,8,5,4],[3,4,5,6,2,1]]` are numerical lists. These need to be converted to a DataFrame with the *correct column names* that match the training data, so that `scaler.transform` and `best_model.predict` can correctly interpret them. Without column names, the order is implicitly assumed, but the numerical values themselves often need to align with original scales or ranges. More critically, the `ColumnTransformer` was applied to `X` (and assumed to have transformed features). If `new_countries` should represent original features, they should *also* go through the same `ColumnTransformer` and then `StandardScaler` pipeline. The generated code only applies the `StandardScaler`, completely missing the critical (though in this case, error-prone) `ColumnTransformer` step. This makes the predictions for new countries invalid.

5.  **Output and Clarity:** The code lacks print statements to show progress, model performance summaries in an organized way, or analysis. The ground truth code clearly prints information at each step, making it much easier to follow and debug.

In summary, the core issue is a complete misunderstanding of the specified features from the dataset and an incorrect application of preprocessing steps (OneHotEncoder on numerical features), leading to erroneous data fed into the models. This propagates errors to almost every subsequent step (training, evaluation, visualizations, predictions).",gemini-flash-1.5,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, RFE, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('world-happiness-report-2021.csv')

df['Happiness_Ef...",6,"The generated code attempts to address all parts of the prompt but has several issues in correctness, completeness, and robustness, particularly concerning data preprocessing, feature engineering logic, and the application of dimensionality reduction and feature selection techniques.

**Critiques:**

1.  **Missing Preprocessing & Robustness:**
    *   **No FileNotFoundError handling:** The ground truth includes a `try-except` block for file loading, which is good practice. The generated code assumes the file exists.
    *   **No column selection for core DataFrame:** The ground truth selects relevant columns upfront, making the DataFrame cleaner and avoiding issues with non-numeric or irrelevant columns in subsequent steps. The generated code operates directly on the entire DataFrame, which can lead to errors when trying to perform numeric operations on string columns (e.g., 'Country name', 'Regional indicator').
    *   **Missing Value Handling:** The generated code does not explicitly handle missing values *before* feature engineering or scaling. The ground truth fills NaNs in 'Perceptions of corruption' and then `dropna()`, which is crucial. This is a significant omission.
    *   **Infinite Value Handling:** Engineered features can sometimes produce `inf` or `-inf`. The ground truth explicitly handles this before scaling, which the generated code does not. This would cause errors in `fit_transform`.

2.  **Incorrect/Incomplete Feature Engineering:**
    *   **'Social_Wellness_Index' and 'Governance_Score' Calculation:** The generated code divides by 2 for these features. The prompt just asks for a ""combination"" (sum), not an average. The ground truth correctly uses summation.
    *   **'Regional_Happiness_Rank':** The generated code uses `method='dense'` for ranking. While this is a valid method, the prompt implies a standard rank within the region, and `rank()` without `method` or with `method='average'` or `min` is more common. The ground truth uses the default, which is usually `average` or `min` depending on pandas version/context, making `dense` a deviation from the prompt's simplicity.
    *   **Polynomial Features:** The generated code correctly applies polynomial features. However, it blindly concatenates `poly_df` to `df` *without* handling potential index misalignment, which could be an issue if `df` had rows dropped due to NaN prior to this step or if the original df index was not preserved (though in this specific case it might work). The ground truth correctly uses `df_processed.index` when creating `df_poly` to ensure perfect alignment. Also, the ground truth correctly sets `include_bias=False` for `PolynomialFeatures`, which is generally preferred to avoid adding a constant column if one already exists or is implicitly handled by models.

3.  **Flawed Dimensionality Reduction (PCA & t-SNE) Application:**
    *   **Incorrect `scaled_dfs` usage:** The code iterates through `scaled_dfs` (StdScaler, MinMaxScaler, RobustScaler) and applies PCA and t-SNE to *each* of them *sequentially*. This is not what the prompt implies. The typical workflow is to chose *one* scaling method (usually StandardScaler for PCA) and then apply DR. The prompt asks to ""Perform feature scaling using multiple methods"" and then ""Apply PCA and t-SNE for dimensionality reduction and visualization."" The ground truth correctly scales data using *multiple methods* and then explicitly states it uses the `StandardScaler` output for *subsequent steps* (DR, FS), which is the standard practice. The generated code's iteration for DR implies independent processing, which is less useful for a single analytical pipeline.
    *   **No Visualization:** The prompt explicitly asks for visualization of PCA and t-SNE results. The generated code calculates them but doesn't plot them. This is a critical omission.
    *   **t-SNE Perplexity:** The perplexity parameter for t-SNE should ideally be related to the number of samples. A fixed 30 might not be optimal for all dataset sizes. The ground truth dynamically calculates this.

4.  **Flawed Feature Selection:**
    *   **`SelectKBest` and `RFE` Application:** The generated code performs `SelectKBest` and `RFE` on `scaled_dfs[0]` (StandardScaler output), which is good. However, it does not explicitly print or store the *names* of the selected features, making the output less informative. The ground truth explicitly prints the feature names.
    *   **RFE Estimator:** The prompt requests RFE, but the generated code uses `RandomForestRegressor` as the estimator for RFE. While this is valid, it makes RFE computationally more expensive. The ground truth uses `LinearRegression`, which is a common and efficient choice for RFE when the specific model doesn't matter as much as the feature ranking. The prompt doesn't specify the RFE estimator, so `LinearRegression` is a more reasonable default.
    *   **Correlation-based Filtering:** The general approach is there, but the generated code calculates correlations with 'Ladder score' (target), which is more about feature relevance than *redundancy*. The prompt asks for ""correlation-based feature filtering,"" which usually means removing features highly correlated *with each other* (collinearity). The ground truth correctly calculates the correlation matrix of `X` (features) and drops highly correlated *features* to reduce redundancy, not features correlated with the target. This is a significant conceptual error.
    *   **Mutual Information:** The generated code correctly calculates and prints mutual information. However, it sets a fixed threshold of `0.05` for `selected_features_mi`, which might not be robust. The ground truth just displays the ranked scores, letting the user decide on a threshold or use it for `SelectKBest`.

5.  **Interaction Features Placement:**
    *   Interaction features are created *after* the polynomial features are created and concatenated, and *after* the scaling and dimensionality reduction are defined. This means they are not included in the feature scaling/DR process if `X_scaled` was created earlier. The ground truth creates all engineered features *before* any scaling or splitting, ensuring they are part of the main feature set for subsequent operations.

6.  **No `Ladder Score` Dropping in X:**
    *   When creating `X` and `y` for modeling, `X` should not contain the target variable ('Ladder score'). The ground truth explicitly removes it from `numerical_features` before creating `X`. The generated code does `X = scaled_dfs[0].drop('Ladder score', axis=1)` which is correct *if* `scaled_dfs[0]` actually contained 'Ladder score' as a feature. However, `scaled_dfs` is created from `df.select_dtypes(include=np.number)`, which *would* include 'Ladder score'. So functionally, this is correct for `SelectKBest` and `RFE`, but the overall `scaled_dfs` still contains the target, which is not ideal for generic use.

7.  **Code Structure and Comments:**
    *   The generated code lacks the clear section headings and comments that the ground truth provides, making it harder to follow the different steps and their purpose.

**Positive Aspects:**

*   Correctly imports necessary libraries.
*   Attempts to implement all required steps.
*   Basic feature engineering (addition/division) is present.
*   Correctly applies `StandardScaler`, `MinMaxScaler`, and `RobustScaler`.
*   Correctly initializes PCA and t-SNE.
*   Correctly uses `SelectKBest` with `f_regression` and `mutual_info_regression`.
*   Correctly uses `RFE` with `RandomForestRegressor`.
*   Calculates and visualizes Random Forest feature importance.",gemini-flash-1.5,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
usa_data = data[data['location'] == 'USA'].set_index('date')
weekly_data = usa_data['new_cases'].resample('W').sum()
decomposition = sm.tsa.seasonal_decompose(weekly_data, model='additive')
plt.figure(figsi...",7,"The generated code correctly addresses most of the task requirements, performing data loading, filtering, resampling, decomposition, ADF testing, and plotting.

However, there are a few areas for improvement:
1.  **Decomposition Plot**: The task explicitly asks for `decomposition.plot()` which combines all components into a single figure for easier comparison, using `plt.figure()` and `plt.subplot` manually is less ideal for this specific request. While the output is similar, adhering to the common method is better.
2.  **Period for Decomposition**: The `seasonal_decompose` function is called without a `period` argument. For weekly data, a `period` of 52 (weeks in a year) is typically crucial for accurate seasonality identification, especially in time series analysis. Without it, `statsmodels` might default to a smaller period like 1, which wouldn't capture annual seasonality, leading to incorrect decomposition. The ground truth specifically sets `period=52`.
3.  **Error Handling**: The code lacks error handling for file not found, empty data, or insufficient data points for certain operations (e.g., ADF test or decomposition for very short series), which makes it less robust. The ground truth includes comprehensive checks.
4.  **Plot Labels/Titles**: While the current plots show the series, they lack descriptive titles for the overall figure and subplots. The ground truth adds more context to the plots.
5.  **Re-testing ADF**: The code checks the p-value after differencing but doesn't explicitly print the conclusion regarding stationarity for the differenced series, as the ground truth does.
6.  **Function Encapsulation**: The code is not encapsulated within a function, making it less reusable. The ground truth wraps the logic in a `perform_time_series_analysis` function.

For a higher score, the code should address the decomposition period, improve error handling, and enhance plot labeling as seen in the ground truth.",gemini-flash-1.5,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

df = pd.read_csv('winequality-red.csv')

df['pH_group'] = pd.cut(df['pH'], bins=[0, 3.2, 3.4, float('inf')], labels=['Low', 'Medium', 'High'], right=False)

avg_quality = df.groupby('pH_group')['quality'].mean()

fvalue, pvalue = stats.f_oneway(*[group['quality'] for name, group in df.groupby('pH_group')])

sem = df.groupby('pH_group')['quality'].sem()
ci = 1.96 * sem

plt.figure(figsize...",6,"The generated code correctly performs the basic steps of pH categorization, mean quality calculation, ANOVA testing, and visualization. However, there are several areas for improvement that prevent it from achieving a higher score:

1.  **pH Group Labels:** The generated code uses simplified labels ('Low', 'Medium', 'High') instead of the requested detailed labels ('Low pH (< 3.2)', 'Medium pH (3.2 to 3.4)', 'High pH (> 3.4)'). This is a direct miss on a specific task requirement.
2.  **Confidence Interval Calculation:** The calculation `ci = 1.96 * sem` for a 95% confidence interval is an approximation assuming a normal distribution and a large sample size. For smaller sample sizes (which can occur within groups), a t-distribution should be used, requiring `scipy.stats.t.interval` as used in the ground truth. Also, the error bar should represent the *range* from the mean to the CI boundaries, not just `ci`.
3.  **Visualization Detail:**
    *   The `yerr` in `plt.bar` is incorrectly passed as simply `ci`. It should ideally be an array of lower and upper bounds of the confidence interval, or at least the distance from the mean to the confidence interval edge if symmetric error bars are shown. The ground truth correctly calculates `error_bar_length`.
    *   The visualization lacks title clarity (e.g., ""95% Confidence Intervals"" is generic).
    *   It does not set the `ylim` or add a grid, which are good practices for readability.
    *   The ground truth also displayed the average quality value on top of each bar, enhancing readability.
4.  **ANOVA Output:** The ANOVA results are printed after the plot is shown, which is less ideal for a flow where analysis often precedes visualization.
5.  **Robustness:** There's no error handling for `pd.read_csv`, which is included in the ground truth for robust operation.
6.  **Missing Report:** The ground truth code specifically calculated and printed the average quality with the 95% confidence intervals numerically, which the generated code only shows visually. While the `avg_quality` is printed, the confidence interval values themselves are not reported numerically.
7.  **Clarity and Structure:** The ground truth code is well-structured with clear comments, configuration settings, and sections for each task, improving readability and maintainability. The generated code is more monolithic.",gemini-flash-1.5,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

df = pd.read_csv('synthetic_covid19_data.csv', p...",7,"The generated code correctly addresses most of the requirements and demonstrates a good understanding of the data preprocessing and model training pipeline using `ColumnTransformer` and `Pipeline`. However, there are a few significant issues that prevent it from achieving a higher score:

**Strengths:**
*   Correctly loads data and parses 'date'.
*   Identifies features and target.
*   Correctly uses `SimpleImputer` for numerical columns.
*   Uses `OneHotEncoder` for the 'continent' column.
*   Uses `StandardScaler` for numerical features.
*   Correctly splits data into train/test sets.
*   Trains both Linear Regression and Random Forest Regressor models.
*   Evaluates models using MAE, MSE, and R-squared.
*   Uses a helpful `evaluate_model` function for clean output.
*   The use of `ColumnTransformer` and `Pipeline` for preprocessing is a good practice, even though the implementation isn't perfectly aligned with the prompt's steps.

**Areas for Improvement/Issues:**

1.  **Missing Value Handling for Target:** The prompt explicitly states: ""Handle missing values in selected features and target by imputing with the median."" The generated code applies `SimpleImputer` to `y` (target), but the `imputer` object has already been `fit_transform`ed on `X`. A separate `SimpleImputer` instance should be used for the target variable, or the target should be imputed based on its own median. More importantly, imputing the target variable with its median *before* splitting the data violates data leakage principles, as the median would be influenced by the test set. The ground truth correctly handles target imputation before splitting but ensures it's based on `df_selected`'s median. The generated code's approach to `y = imputer.fit_transform(y.values.reshape(-1,1))[:,0]` is problematic.
2.  **Imputation Order and Data Leakage:** The `SimpleImputer` is applied to `X` and `y` *before* the train-test split. While this might be permissible for `X` if the imputer only uses statistics from the training set, here it uses statistics from the *entire* dataset for `X` and `y`. For the target, this is directly problematic as it uses the test set's data to impute missing values in the target. The ground truth first selects the relevant data, then imputes missing values (median of the *entire* relevant column), and *then* splits train/test. This is a common and acceptable pattern for imputation before split, where the imputation logic is applied to the combined dataset. The issue in the generated code is that `sklearn`'s `SimpleImputer`'s `fit_transform` for `X` and `y` before the split uses the *entire* dataset for calculating medians, which is acceptable if done consistently. However, `ColumnTransformer` (which takes care of numerical features and categorical features) should ideally be applied *after* the split for scaling, to prevent data leakage in scaling. The generated code applies it *before* the split.
3.  **Scaling and Encoding Logic:** The `ColumnTransformer` is applied `fit_transform` on the entire `X` *before* the train-test split: `X = preprocessor.fit_transform(pd.DataFrame(X, columns=features))`. This means the `StandardScaler` and `OneHotEncoder` `fit` on the entire dataset, leading to data leakage from the test set into the training process for scaling and encoding. `ColumnTransformer` should be `fit` on the training data and then `transform` both training and test data, just like the `StandardScaler` in the ground truth.
4.  **Feature Indexing for `ColumnTransformer`:** Using hardcoded numerical indices (e.g., `numeric_features = [0,1,2,3,4,5,6]`, `categorical_features = [7]`) assumes the order of features is fixed in `X`. While technically correct given how `X` is created, it's less robust than referring to column names (e.g., `numeric_features = df[features].select_dtypes(include=np.number).columns.tolist()`) or passing column names directly to `ColumnTransformer` as the ground truth implicitly handles through pandas dataframes. `X` is created as a NumPy array after the initial `imputer.fit_transform(X)`, thus losing column names, making indexing necessary but less readable than using named columns with `ColumnTransformer`.
5.  **Handling `continent` imputation:** The prompt asks to handle missing values, then encode `continent`. The generated code assumes `continent` will be handled directly by `OneHotEncoder` via `handle_unknown='ignore'`, but if `continent` itself had missing values (NaNs), `OneHotEncoder` would treat `NaN` as a category unless explicitly imputed. The prompt implied `continent` should be *encoded*, meaning no *imputation* for it, but the general missing value handling step should apply to *selected features and target*. The ground truth explicitly excludes `continent` from median imputation and lets `get_dummies` handle it, which is appropriate. The generated code's initial `SimpleImputer` is implicitly applied to `X` which still contains `continent`, which is incorrect for a categorical column. `SimpleImputer` used without specifying strategy for categorical data can lead to issues.
6.  **Minor Output Formatting:** While the evaluation function is good, the output formatting slightly differs from the ground truth's requested format.

The most critical issues are the data leakage from scaling/encoding *before* the split and the problematic global imputation of the target variable. Addressing these would significantly improve the solution.",gemini-flash-1.5,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
import statsmodels.formula.api as sm

data = pd.read_csv('synthetic_covid19_data.csv')

cols_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for col in cols_to_impute:
    median_val = data[col].median()
    data[col].fillna(median_val, inplace=True)

# ANOVA
asia = data[data['continent'] == 'Asia']['reproduction_rate']
europe = data[data['continent'] == 'Europe']['reproduction_ra...",6,"The generated code correctly implements the core statistical tests (ANOVA, Pearson, t-test) and handles missing values. However, it lacks robustness and completeness in several areas, leading to potential errors or incorrect interpretations in a real-world scenario.

Key issues:
1.  **Missing `date` to datetime conversion**: The ground truth converts the 'date' column to datetime objects using `df['date'] = pd.to_datetime(df['date'])`. The generated code misses this crucial step, which is necessary for accurately identifying the ""latest entry per country"" in the t-test part. Without this, `data.loc[data.groupby('location')['date'].idxmax()]` might pick an arbitrary row if dates are strings, not necessarily the latest in chronological order.
2.  **Handling of empty groups in ANOVA**: The `stats.f_oneway` function can raise an error or produce meaningless results if one of the input arrays is empty or contains only NaNs. The ground truth explicitly filters for `continents_of_interest` and then checks if `rates.empty` before appending to `reproduction_rates_by_continent`, and finally checks if `len(reproduction_rates_by_continent) >= 2`. The generated code directly passes `asia, europe, north_america` which might contain NaNs or be empty if no data exists for a continent, potentially leading to errors or warnings that are not handled.
3.  **Missing value imputation for the derived death rate ratio**: While initial median imputation is done for `total_deaths` and `population`, when `data['death_rate'] = data['total_deaths'] / data['population']` is calculated, it can result in `inf` if `population` is 0 or NaN. The ground truth handles this by replacing `inf` with `nan` and dropping `nan` values from the `death_rate_ratio` (`latest_country_data.dropna(subset=['death_rate_ratio'], inplace=True)`). The generated code performs no such handling which might lead to incorrect 75th percentile calculations or `inf` values polluting the `high_death_rate_country` classification.
4.  **No null hypothesis statements**: The task explicitly asks to ""state the null hypothesis"" for each test. The generated code only prints the p-value and a conclusion, but not the null hypotheses themselves.
5.  **Output clarity**: While it prints the p-values and conclusions, it doesn't clearly label which p-value belongs to which test in the final output block, which can be confusing.
6.  **Unused import**: `statsmodels.formula.api as sm` is imported but not used.
7.  **No error handling for file loading**: The generated code assumes the CSV file exists. The ground truth includes a `try-except FileNotFoundError` block, which is good practice.",gemini-flash-1.5,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('clash_royale_cards.csv')
numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
correlation_matrix = df[numerical_cols].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

```",9,"The generated code correctly performs all the requested tasks: loading the dataset, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it with a heatmap using seaborn. The imports are correct, and the code is concise.

It achieves a high score because it fulfills the core requirements. For minor improvements to achieve a perfect score, it could:
- Add error handling for `FileNotFoundError` if the CSV isn't present.
- Use a slightly more descriptive title for the heatmap, reflecting the specific dataset.
- Possibly include `fmt="".2f""` in `sns.heatmap` for consistent annotation formatting as in the ground truth, though `annot=True` by itself often provides reasonable defaults.",gemini-flash-1.5,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

df = pd.read_csv(""clash_royale_cards.csv"")
result = df.groupby('rarity').agg({'elixirCost': 'mean', 'Win Rate': 'mean', 'Usage': 'mean'})
print(result)

```",9,"The generated code is highly correct and completes the task effectively. It correctly loads the CSV, groups by `rarity`, and calculates the mean for the specified columns. The use of `.agg()` with a dictionary is a perfectly valid and often preferred way to specify aggregate functions for multiple columns, making the code explicit. While it meets all functional requirements, the ground truth explicitly defines `file_path` as a variable before use and includes comments explaining each step, which contributes to slightly better readability and maintainability for someone new to the codebase. However, for a direct functional comparison, the generated code is excellent.",gemini-flash-1.5,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv('clash_royale_cards.csv')
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)
features = df[['elixirCost', 'Usage', 'rarity']]
encoder = OneHotEncoder(handle_unknown='ignore')
encoded_rarity = encoder.fit_transform(features[['rarity']]).toarray()
encoded_df = pd.DataFrame(encoded_rarity, columns=encoder.get_feature_names_out(['rarity']))
feat...",9,"The generated code correctly performs all requested steps: loading data, creating the target column, selecting features, one-hot encoding, and splitting the data.

**Strong Points:**
- **Correctness:** All requirements from the prompt are met accurately.
- **Imports:** Necessary libraries are imported.
- **Target Column Creation:** The `is_high_win_rate` column is created correctly.
- **Data Splitting:** `train_test_split` is used with the correct parameters (`test_size=0.2`, `random_state=42`).

**Minor Area for Improvement:**
- **OneHotEncoder Usage:** While functionally correct for this dataset, the use of `OneHotEncoder` with `handle_unknown='ignore'` and then manually dropping the original column and concatenating the encoded ones is a bit more verbose than `pd.get_dummies()`. `pd.get_dummies()` often integrates more seamlessly into a pandas workflow for simple one-hot encoding directly on a DataFrame and is generally preferred for its simplicity when no complex encoder fitting is needed beyond typical one-hot encoding. However, using `OneHotEncoder` itself is not incorrect, and `handle_unknown='ignore'` is a good practice for productionizing.
- **Feature Selection:** The line `features = df[['elixirCost', 'Usage', 'rarity']]` is correct for selecting the features. However, the one-hot encoding process for `rarity` is applied *after* this selection. This means `features` is a DataFrame in its current state. The subsequent `features = features.drop('rarity', axis=1)` and `features = pd.concat([features, encoded_df], axis=1)` successfully build the final `X` DataFrame. This sequence is functional but could be slightly more streamlined by creating `X` directly with the non-categorical features first, then adding the encoded `rarity`. This is a matter of style preference more than correctness issue.",gemini-flash-1.5,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

iris = pd.read_csv('iris.csv')
X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = iris['Species']
le = LabelEncoder()
y = le.fit_transform(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
knn = KNeighb...",9,"The generated code correctly performs all the specified steps for data preparation and KNN classification, including loading data, defining features and target, encoding the target, splitting data, training the model, and reporting accuracy. All required imports are present.

The main difference from the ground truth is the omission of the `stratify=y_encoded` parameter in `train_test_split` and the `try-except` block for `FileNotFoundError`. While the `try-except` is good practice, it wasn't explicitly requested as a functional requirement for the task. The `stratify` parameter is important for classification tasks, especially with imbalanced datasets, to ensure representative splits. Although the Iris dataset is balanced, including `stratify` is generally a good practice in similar scenarios. Also, the print statement just prints the numerical accuracy value without any descriptive text.",gemini-flash-1.5,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy import stats

data = pd.read_csv('iris.csv')
versicolor = data[data['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica = data[data['Species'] == 'Iris-virginica']['PetalLengthCm']
t_statistic, p_value = stats.ttest_ind(versicolor, virginica)

print(f""T-statistic: {t_statistic}"")
print(f""P-value: {p_value}"")

alpha = 0.05
print(f""Reject null hypothesis: {p_value < alpha}"")
```",6,"The generated code correctly performs the t-test and prints the t-statistic and p-value. However, it falls short on several requirements. It does not explicitly state the null and alternative hypotheses, which was a specific task requirement. The interpretation of the p-value is too concise, only printing a boolean rather than a clear statement on whether to reject the null hypothesis and what that conclusion means in context, as requested. Error handling for `FileNotFoundError` is also missing. To improve, the code needs to print the hypotheses, provide a more complete interpretation of the test result, and add basic file handling.",gemini-flash-1.5,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('iris.csv')

features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']

for feature in features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Species', y=feature, data=df)
    plt.title(f'Box Plot of {feature} by Species')
    plt.show()

```",9,"The generated code correctly addresses the prompt's requirements. It properly loads the CSV, iterates through the features, generates box plots grouped by species, and includes appropriate titles. The use of `matplotlib.pyplot` and `seaborn` is correct, and the plots are displayed individually.

**Strengths:**
*   **Correctness:** The code successfully generates the requested box plots with the correct features and grouping.
*   **Clarity and Readability:** The code is concise and easy to understand.
*   **Proper Imports:** All necessary libraries are imported.
*   **Appropriate Plotting Tool:** Uses Seaborn, which is well-suited for this type of visualization.
*   **Titles:** Each plot has a clear title indicating the feature.

**Minor Area for Improvement (Not critical for this task, but good practice):**
*   **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`, which is good practice for robust applications. The generated code assumes the file will always be present. While not explicitly asked for in the prompt, it's a valuable addition for real-world scenarios.
*   **Aesthetics/Labels:** The ground truth code adds `plt.xlabel`, `plt.ylabel`, `plt.grid`, and `plt.tight_layout()` for enhanced plot aesthetics and readability. While the generated code's plots are perfectly functional and meet the requirements, these additions can improve the presentation. However, for a data science benchmark, meeting the core functional requirements often takes precedence over minor aesthetic tweaks unless explicitly requested.

Overall, it's a very good solution that directly solves the problem.",gemini-flash-1.5,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Select features and target
X = data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']]
y = data['HeartDisease']

# C...",9,"The generated code correctly addresses all requirements of the task. It loads the data, selects the specified features and target, correctly applies one-hot encoding using `ColumnTransformer` within a `Pipeline`, splits the data with the correct proportions and `random_state`, trains a `LogisticRegression` model, and reports the accuracy. The code structure is clean and uses appropriate sklearn tools (`ColumnTransformer`, `Pipeline`), which is excellent practice.

Minor points for improvement (not critical for a lower score, but for perfection):
- The `LogisticRegression` model in `sklearn` often requires `max_iter` to be increased for convergence on certain datasets. While not explicitly asked for, adding `max_iter=1000` (as in the ground truth) can prevent `ConvergenceWarning` in some cases.
- The print statement for accuracy is `print(accuracy)` which is functional, but `print(f""Model Accuracy on Test Set: {accuracy:.4f}"")` as in the ground truth provides a more user-friendly output.
- The `handle_unknown='ignore'` argument in `OneHotEncoder` within the `ColumnTransformer` is a good practice for robustness in case new categories appear in the test set that were not present in the training set.

Overall, a very strong solution.",gemini-flash-1.5,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

df = pd.read_csv('heart.csv')
male_chol = df[df['Sex'] == 'M']['Cholesterol']
female_chol = df[df['Sex'] == 'F']['Cholesterol']

t_statistic, p_value = ttest_ind(male_chol, female_chol)

print(f""T-statistic: {t_statistic}"")
print(f""P-value: {p_value}"")

alpha = 0.05
print(f""Reject null hypothesis: {p_value < alpha}"")
```",6,"The generated code correctly performs the core task of loading data, separating groups, and executing a t-test. It also prints the t-statistic and p-value and correctly implements the boolean logic for rejecting the null hypothesis.

However, it falls short on several requirements:
1.  **Hypotheses Statement:** The prompt explicitly asked to ""Clearly state the null and alternative hypotheses for this test."" The generated code does not include this.
2.  **Interpretation of P-value:** While it prints `p_value < alpha`, it doesn't provide a textual interpretation or conclusion about *why* the null hypothesis is rejected or not, or what that means in the context of cholesterol levels as explicitly requested: ""Based on a significance level of alpha = 0.05, interpret the p-value and state whether to reject the null hypothesis."" The ground truth provides a comprehensive interpretation.
3.  **Robustness and Error Handling:** The prompt implies a robust solution by including ""assume the CSV file is in the current directory"" but doesn't explicitly ask for error handling. However, the ground truth includes `try-except` blocks for `FileNotFoundError` and checks for sufficient data, which are good practices for production-ready code. The generated code lacks any error handling for file loading or insufficient data.
4.  **`equal_var` parameter in `ttest_ind`:** The ground truth correctly uses `equal_var=False` (Welch's t-test), which is generally more robust as it doesn't assume equal variances. The generated code uses the default `equal_var=True` (Student's t-test), which might be less appropriate depending on whether variances are actually equal. While not explicitly stated to use Welch's, it's a detail that impacts the correctness of the statistical test chosen.
5.  **Function encapsulation:** The prompt did not strictly require a function, but wrapping the logic in a function (as in the ground truth) is better practice for reusability and clarity, especially for a benchmark.

For a score of 8 or above, the code would need to address the missing verbal hypotheses and comprehensive interpretation. For a perfect score, it would also incorporate the robust error handling and the more statistically robust `equal_var=False` for the t-test.",gemini-flash-1.5,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv(""winequality-red.csv"")

def treat_outliers_iqr(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    num_outliers = len(outliers)
    df_treated = df.copy()
    df_treated[col] = np.clip(df[col], lower_bound, upper_bound)
    return num_outliers, df_treate...",7,"The generated code correctly identifies outliers using the IQR method and performs winsorization (capping) using `np.clip`. It also compares the mean and standard deviation before and after treatment, fulfilling the core requirements.

However, there are a few areas for improvement:
1.  **Sequential Treatment Issue**: The `treat_outliers_iqr` function is called sequentially, passing `df_treated_va` to the `residual sugar` treatment. This means that if 'volatile acidity' treatment significantly changes the distribution, it could indirectly affect the IQR calculation for 'residual sugar' *if* the `residual sugar` column itself contained outliers whose values were altered by the first treatment. While in this specific case, treating 'volatile acidity' won't change 'residual sugar' values, it's generally better practice to treat each column on a *copy of the original* DataFrame or ensure the sequential treatment doesn't interfere with independent calculations if the columns were somehow related. The ground truth creates a single `df_winsorized` copy and updates columns within it based on original IQR bounds, which is a more robust approach.
2.  **Reporting Number of Outliers**: The generated code reports the number of outliers *before* treatment for each column, which is correct. However, it doesn't state that the outliers have been capped or clearly explain the process to the user beyond just showing the stats.
3.  **Encapsulation/Clarity**: The `treat_outliers_iqr` function returns a treated DataFrame, but the overall approach leads to `df_treated_va` and `df_treated_rs` being separate dataframes that are then implicitly merged by using `df_treated_rs` for comparison (which contains both treatments). A single `df_winsorized` for all treatments, as in the ground truth, would be cleaner and more consistent.
4.  **Error Handling**: There's no error handling for `FileNotFoundError` as present in the ground truth.
5.  **Output Readability**: The output format is concise but less verbose and descriptive than the ground truth, which provides more context (e.g., Q1, Q3, IQR, bounds). While not strictly required by the prompt, clear output is good for benchmarks.

Overall, it's a good attempt that gets the job done, but the sequential treatment of columns needs careful consideration, and the overall structure could be more robust.",gemini-flash-1.5,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('heart.csv')

plt.figure(figsize=(10, 6))
sns.countplot(x='ChestPainType', hue='HeartDisease', data=df)
plt.title('Heart Disease Count by Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.legend(title='Heart Disease')
plt.show()

```",7,"The generated code correctly loads the dataset and generates a grouped bar chart. It includes proper imports, a title, labeled axes, and a legend, satisfying most of the requirements.

However, there are a few areas for improvement to reach a higher score:
1. **Robustness/Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV, which is good practice for real-world applications. The generated code lacks this.
2. **Clarity of `HeartDisease` legend:** The `hue='HeartDisease'` uses 0 and 1 directly in the legend. The ground truth maps these to ""No Heart Disease"" and ""Heart Disease"", which significantly improves readability and interpretability for the user.
3. **Encapsulation:** The ground truth wraps the plotting logic in a function, `create_heart_disease_chest_pain_grouped_bar_chart`, making the code reusable and modular. The generated code is sequential script.
4. **Order of `ChestPainType`:** While not explicitly stated as a hard requirement to enforce a specific order, the ground truth explicitly sets the categorical order for `ChestPainType` (`TA`, `ATA`, `NAP`, `ASY`). This ensures consistent visualization if the dataset's internal order or default sorting changes. The generated code relies on `seaborn`'s default ordering.",gemini-flash-1.5,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')

df['in_shazam_charts'] = pd.to_numeric(df['in_shazam_charts'].str.replace(',', '', regex=False), errors='coerce')
print(f""Number of NaNs in 'in_shazam_charts': {df['in_shazam_charts'].isna().sum()}"")

grouped = df.groupby(['key', 'mode'])[['danceability_%', 'valence_%']].mean()
print(grouped)

```",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, cleans the `in_shazam_charts` column by removing commas and converting to numeric, reports the NaN count, groups by `key` and `mode`, and calculates the average `danceability_%` and `valence_%`.

The use of `regex=False` in `str.replace` is a good practice for simple string replacements. The NaN count reporting is accurate.

The only minor area for improvement (and why it's not a 10) is the lack of a `try-except` block for file loading, which is present in the ground truth and makes the code more robust. Also, the output formatting of the results could be slightly more descriptive, though the current output is perfectly functional.",gemini-flash-1.5,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

df = pd.read_csv('spotify_2023.csv')
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'], inplace=True)
X = df[['in_spotify_playlists', 'energy_%']]
y = df['streams']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size...",9,"The generated code correctly implements all the functional requirements of the task: loading data, coercing 'streams' to numeric and handling NaNs, selecting features and target, splitting data, training a Linear Regression model, and reporting R2 and MSE. The code is concise and uses appropriate libraries and functions.

The only minor improvements that could be made, which are present in the ground truth but not strictly required by the prompt's output format, are:
*   Error handling for `FileNotFoundError` when loading the CSV. While the prompt assumes the file is present, robust code usually includes this.
*   More explicit print statements to show the progress and intermediate results (e.g., shapes of X and y, training/testing data sizes, confirmation of model training). These are good for debugging and user feedback but not strictly part of the core task output requirement.
*   Formatting the output of R2 and MSE to a specific number of decimal places for better readability (e.g., `:.4f`).

Overall, it's a very good solution that directly addresses all specified requirements.",gemini-flash-1.5,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

df = pd.read_csv('spotify_2023.csv')

df['release_date'] = pd.to_datetime(df['released_year'].astype(str) + '-' + df['released_month'].astype(str) + '-' + df['released_day'].astype(str), errors='coerce')

df['days_since_release'] = (pd.to_datetime('2023-12-31') - df['release_date']).dt.days.astype(int)

print(df[['track_name', 'release_date', 'days_since_release']].head())

```",8,"The generated code successfully completes the core tasks. It loads the dataset, correctly combines the date components into a `release_date` column using `errors='coerce'` to handle invalid dates, and calculates `days_since_release` from '2023-12-31'. It also correctly displays the required columns for the first 5 tracks.

Areas for improvement:
1.  **Robustness for `days_since_release` dtype**: While `astype(int)` works, if `release_date` contains `NaT` values, `(pd.to_datetime('2023-12-31') - df['release_date']).dt.days` would produce `NaN`. Casting `NaN` to `int` will raise an error. Using `astype('Int64')` (pandas nullable integer type) as shown in the ground truth is more robust as it allows `NaN` values to propagate naturally where `release_date` was `NaT` and still maintain an integer type for valid entries.
2.  **Zero-padding**: The generated code concatenates `month` and `day` as strings directly. This *could* lead to issues with `pd.to_datetime` if `month` or `day` values are single digits (e.g., '2023-1-1' vs '2023-01-01'). While `pd.to_datetime` is quite robust with different formats, explicitly zero-padding with `.str.zfill(2)` as in the ground truth ensures a consistent 'YYYY-MM-DD' format, which is generally better practice for date string construction.
3.  **Error Handling for File Loading**: The ground truth includes `try-except` blocks for file loading, which is good practice in real-world scenarios to inform the user if the file is missing or corrupted. This was not strictly asked for in the prompt but is a mark of higher code quality.
4.  **No Explanations**: The prompt specifically asked for ""only the Python code without explanations"", which the generated code adheres to perfectly. The ground truth, however, includes extensive comments and configuration variables, which would typically be seen as good practice but go against the ""only the Python code"" constraint of the prompt. However, given the nature of a benchmark and the specific instruction, the generated code followed the ""only code"" part well.",gemini-flash-1.5,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

df = pd.read_csv('youtube_trendings.csv')

for col in ['Video views', 'Likes']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

df['Category'] = df['Category'].fillna('Unknown')

grouped = df.groupby('Category')

result = grouped[['Video views', 'Likes']].sum()

print(result)
```",8,"The generated code correctly performs all the steps required by the task: loading the data, cleaning and converting 'Video views' and 'Likes' columns, filling missing 'Category' values, grouping by 'Category', and calculating the sum of views and likes. The code is concise and efficient.

Areas for improvement (to achieve a higher score):
1.  **Error Handling (File Not Found):** The generated code does not include error handling for the `FileNotFoundError` when `pd.read_csv` is called. The ground truth code includes a `try-except` block for this, which is good practice for robust applications.
2.  **Column Existence Checks:** While not strictly failing the task if the columns are present, robust code often includes checks to ensure columns like 'Video views', 'Likes', and 'Category' actually exist in the DataFrame before attempting operations on them, as seen in the ground truth. This prevents `KeyError` if the CSV structure changes. For a benchmark, however, we can assume the CSV structure is as expected.
3.  **Display Clarity:** The ground truth includes a print statement ""Sum of Video Views and Likes per Category:"" which makes the output clearer to the user. The generated code just prints the DataFrame, which is sufficient but could be enhanced for user experience.

Overall, it's a very good solution that directly addresses the problem's core requirements. The suggested improvements are primarily about robustness and user experience rather than core correctness.",gemini-flash-1.5,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

df = pd.read_csv('youtube_trendings.csv')
df['Likes'] = df['Likes'].astype(str).str.replace(',', '').astype(float, errors='coerce').fillna(0)
df['Category'] = df['Category'].fillna('Unknown')
music_likes = df[df['Category'] == 'Music']['Likes']
sports_likes = df[df['Category'] == 'Sports']['Likes']
t_statistic, p_value = stats.ttest_ind(music_likes, sports_likes)
print(f""T-statistic: {t_statistic}"")
print(f""P-value: {p_value}"")
alpha = 0.05
...",7,"The generated code correctly performs the core task of loading data, cleaning the 'Likes' column, filling missing 'Category' values, and conducting an independent two-sample t-test. It correctly prints the t-statistic and p-value and interprets the result against the given alpha.

However, it falls short on several requirements:
- **Hypotheses:** It does not explicitly state the null and alternative hypotheses as required by the prompt.
- **Robustness/Error Handling:** It lacks error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth and is good practice.
- **Clarity/Readability:** While concise, combining cleaning steps for 'Likes' into a single chain makes it slightly less readable than breaking it down, especially for complex transformations. The ground truth's use of an intermediate 'Likes_cleaned' column and step-by-step processing improves clarity.
- **Completeness of Output:** It only prints the rejection/failure to reject, not the full conclusion specified in the requirement ""state whether to reject the null hypothesis. Include proper imports (pandas, scipy.stats) and assume the CSV file is in the current directory."" The ground truth concludes with more detailed statements about the statistical significance.
- **Welch's t-test:** The generated code uses `ttest_ind` with default parameters, which assumes equal variances (`equal_var=True`). The ground truth specifically uses `equal_var=False` (Welch's t-test), which is generally safer when there's no strong assumption about equal variances. While not explicitly stated in the prompt, using `equal_var=False` often leads to more robust results in real-world scenarios.",gemini-flash-1.5,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

df = pd.read_csv(""youtube_trendings.csv"")

for col in ['Likes', 'Dislikes']:
    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

df['TotalEngagement'] = df['Likes'] + df['Dislikes']
df['LikeDislikeRatio'] = df['Likes'] / (df['Dislikes'] + 1)

print(df[['Video', 'TotalEngagement', 'LikeDislikeRatio']])

```",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, cleans the 'Likes' and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0. It then correctly calculates 'TotalEngagement' and 'LikeDislikeRatio' and finally displays the requested columns.

**Strong Points:**
*   **Concise and idiomatic pandas:** The use of a loop for cleaning 'Likes' and 'Dislikes' is efficient and Pythonic.
*   **Correct handling of errors and NaNs:** `pd.to_numeric(errors='coerce').fillna(0)` is the correct approach.
*   **Correct calculation of new features:** Both `TotalEngagement` and `LikeDislikeRatio` are calculated as specified, including the `+1` for the ratio.
*   **Proper display of results:** The final `print` statement shows the required columns.

**Areas for minor improvement (not critical for score deduction given the prompt, but good practice):**
*   **Error handling for file loading:** The ground truth includes `try-except` blocks for file loading, which is robust for production code. While not explicitly asked for in the prompt, it's a good practice for real-world scripts.
*   **Explicit `astype(float)`:** While `pd.to_numeric` typically results in a numeric type, explicitly converting to `float` after `fillna(0)` can sometimes ensure consistency, though `numpy` often handles this implicitly. In this case, it's not strictly necessary as the operations will promote to float.
*   The `import numpy as np` is not strictly necessary as `numpy` functions are not directly called, but `pandas` uses `numpy` under the hood, so it's a common import.",gemini-flash-1.5,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

df = pd.read_csv('mobiles_2025.csv')

df['Launched Price (USA)'] = df['Launched Price (USA)'].astype(str).str.replace('USD ', '', regex=False).str.replace(',', '', regex=False)
df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'], errors='coerce')

df['Screen Size'] = df['Screen Size'].astype(str).str.replace(' inches', '', regex=False)
df['Screen Size'] = pd.to_numeric(df['Screen Size'], errors='coerce')

result = df.groupby(['Company Name', 'Laun...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, cleans both the 'Launched Price (USA)' and 'Screen Size' columns, handles conversion errors by coercing to NaN, groups the data by 'Company Name' and 'Launched Year', and calculates the mean for the cleaned columns. The code is concise and efficient.

One minor area for improvement, though not strictly required by the prompt, would be to use different column names for the cleaned data (e.g., 'Cleaned Price (USD)') instead of overwriting the original columns. This makes the data transformation clearer and preserves the original raw data if needed for later inspection or debugging. However, for the given task, overwriting is acceptable and still produces the correct final result. The ground truth code does this, which is good practice. Also, it lacks error handling for file loading, which the ground truth includes, but this wasn't explicitly requested as a requirement for the generated code. Overall, a very good solution.",gemini-flash-1.5,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def clean_camera(camera_spec):
    match = re.search(r'\d+', camera_spec)
    if match:
        return int(match.group(0))
    else:
        return 0

df = pd.read_csv('mobiles_2025.csv')

df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera)
df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera)
df['Total Camera MP'] = df['Cleaned Front Camera'] + df['Cleaned Back Camera']

print(df[['Model Name', 'Cleaned Front Camera', 'Cleaned...",7,"The generated code correctly performs the core tasks: loading the dataset, cleaning camera specifications using regex to extract the first digit sequence, creating the `Total Camera MP` feature, and displaying the requested columns for the first 10 rows. The imports are correct.

However, there's a significant robustness issue in the `clean_camera` function. It does not handle NaN values or non-string inputs in the `camera_spec` column, which could lead to `TypeError` if those columns contain `NaN` (which is common in datasets) or other non-string types. The ground truth code explicitly handles this by checking `pd.isna` and converting to string, making it more robust for real-world datasets. This lack of robustness is a major area for improvement. The ground truth also includes error handling for file not found, which is a good practice for robustness. The output formatting of the `head(10)` is also slightly better in the ground truth using `.to_string()`, but this is minor.",gemini-flash-1.5,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

df = pd.read_csv('mobiles_2025.csv')

for col in ['RAM', 'Battery Capacity', 'Screen Size', 'Launched Price (USA)']:
    if col == 'Launched Price (USA)':
        df[col] = df[col].astype(str).str.replace('USD ', '').str.replace(',', '').astype(float)
    else:
        df[col] = df[col].astype(str).str.repl...",8,"The generated code correctly addresses most of the task requirements, including loading the dataset, performing column cleaning, splitting data, training a Linear Regression model, and evaluating it.

**Strong points:**
*   **Correct imports:** All necessary libraries are imported.
*   **Correct data loading:** `pd.read_csv` is used correctly.
*   **Correct feature/target definition:** `X` and `y` are defined as specified.
*   **Correct data splitting:** `train_test_split` is used with the correct `test_size` and `random_state`.
*   **Correct model training:** `LinearRegression` is initialized and trained.
*   **Correct evaluation metrics:** R-squared and MAE are calculated and printed.
*   **NaN handling:** Uses `dropna` on the correct subset of columns after cleaning.

**Areas for improvement (why not a 10):**
*   **Cleaning logic for non-price columns:** For `RAM`, `Battery Capacity`, and `Screen Size`, the generated code uses `str.replace(r'[^\d.]', '', regex=True)`. While this will often work for ""GB"", ""mAh"", "" inches"", it's an overly aggressive regex that might remove other legitimate characters if they were present (e.g., a decimal point `.` if it were a different format). The ground truth's specific `str.replace` for each pattern is more robust and safer against unexpected variations in the data format for these specific examples. For instance, if 'RAM' was '10 GB' or '10GB', the generated code would remove the space too but it's the `GB` that needs to be removed. The Ground Truth is more explicit and precise for the exact cleaning needed.
*   **Error handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV and checks if `df` is empty after cleaning, which is good practice for robust code but not strictly required by the prompt's core logic.
*   **Output formatting:** The output for R-squared and MAE in the generated code is just raw numbers. The ground truth adds f-string formatting (`:.4f`, `:.2f`) for better readability and precision, which is a minor but good detail.

Overall, the generated code is highly functional and fulfills the core requirements, making it a very good solution. The cleaning logic is the only slightly less precise aspect compared to the ground truth.",gemini-flash-1.5,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

df = pd.read_csv(""titanic.csv"")

# Chi-square test
contingency_table = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
cramers_v = np.sqrt(chi2 / (df.shape[0] * (min(contingency_table.shape)-1)))

# T-test
survived_ages = df[df['Survived'] == 1]['Age'].dropna()
not_survived_ages = df[df['Survived'] == 0]['Age'].dropna()
t_statistic, p_t = stats.ttest_ind(survived...",7,"The generated code correctly performs the three requested statistical tests (Chi-square, T-test, ANOVA) and calculates effect sizes. It also interprets the results based on the p-value and a specified alpha level.

However, there are several areas for improvement that prevent a higher score:

**Correctness & Completeness:**
*   **Cramer's V calculation:** The formula used for Cramer's V `np.sqrt(chi2 / (df.shape[0] * (min(contingency_table.shape)-1)))` is incorrect. It should be `np.sqrt(chi2 / (n * min(rows-1, cols-1)))` where `n` is the total number of observations (`contingency_table.sum().sum()`), and `min_dim` is `min(rows-1, cols-1)`. The generated code uses `df.shape[0]` (number of rows in the *original dataframe*) instead of `n` (total count in the contingency table) and `min(contingency_table.shape)-1` is correct for `min_dim` but still loses its meaning if `df.shape[0]` is incorrect.
*   **Cohen's d calculation:** The formula for Cohen's d is correct for the pooled standard deviation, but it lacks checks for empty groups or potential division by zero if `(len(survived_ages)+len(not_survived_ages)-2)` is zero or negative (though less likely in this dataset).
*   **Eta-squared calculation:** The formula `eta_squared = fvalue / (fvalue + df.shape[0] - 3)` for Eta-squared in ANOVA is incorrect. Eta-squared is typically calculated as `SS_between / SS_total`. The provided formula seems to be a rough approximation or incorrect application of the formula for F-statistic in terms of eta-squared, but it's not the direct and standard calculation of eta-squared. This calculation often leads to incorrect effect sizes.
*   **Handling of missing values:** While the code uses `.dropna()` for T-test and ANOVA, it does so inline directly in the function calls, which is less robust. The ground truth separates the cleaning and then performs the tests, ensuring that only the relevant data is used and allows for more explicit checks (e.g., if groups become empty after dropping NaNs).
*   **Interpretation clarity:** While the ""Significant association/difference"" string is produced, a more detailed conclusion (e.g., ""reject the null hypothesis: there is a statistically significant association..."") as in the ground truth is better for explaining the results of a hypothesis test.
*   **Assumptions for T-test/ANOVA:** The code does not check for assumptions like homogeneity of variances (e.g., using Levene's test) for T-test and ANOVA, which can impact the validity of the p-values. While the prompt didn't explicitly ask for assumption checks, they are part of ""proper statistical methodology."" The ground truth includes Levene's test conditionally.
*   **Robustness:** The code assumes `titanic.csv` exists without error handling.
*   **Output formatting:** The output is very minimal compared to the ground truth, which provides context, hypotheses, and more detailed interpretations.

**Code Quality:**
*   Lack of comments and explanations for each section or complex calculation.
*   No clear separation of concerns (e.g., loading data, performing each test, displaying results). The ground truth has clear section headers.
*   The placement of `alpha = 0.05` after all the calculations is unconventional; it should be defined at the beginning.

Overall, the code provides a basic, functional solution, but the inaccuracies in effect size calculations and the lack of robustness and detailed interpretation significantly impact its quality as an ""expert"" solution for a benchmark.",gemini-flash-1.5,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('imdb_top_250.csv')
grouped = df.groupby('Year').agg({'Rating': 'mean', 'Runtime': 'mean'})
plt.figure(figsize=(10, 6))
plt.plot(grouped.index, grouped['Rating'])
plt.title('Trend of Average Rating Over the Years')
plt.xlabel('Year')
plt.ylabel('Average Rating')
plt.grid(True)
plt.show()

```",9,"The generated code correctly performs all the requested tasks: loading the CSV, grouping by year, calculating average rating and runtime, and plotting the average rating trend with a title and labels. It uses proper imports and follows a clear, concise structure.

**Strong points:**
- Correctly loads the data.
- Accurately groups the data and calculates the required averages.
- Generates a correct line plot with appropriate title and axis labels.
- Uses standard libraries (pandas, matplotlib) effectively.

**Areas for improvement (minor):**
- **Robustness:** The ground truth code includes robust data cleaning/type conversion for 'Year', 'Runtime', and 'Rating' columns (e.g., handling non-numeric characters, `min` string in runtime, and potential conversion errors). The generated code assumes these columns are already in a clean numeric format, which might lead to errors if the CSV has inconsistencies. For data science benchmarks, robust data handling is crucial.
- **Error handling:** The ground truth code includes `try-except` blocks for `FileNotFoundError`, `KeyError`, and general exceptions, making it more robust in a real-world scenario. The generated code lacks this.
- **Readability/Best Practices:** While the submitted code is concise, wrapping the logic in a function and potentially defining the dataset path as a constant (as in the ground truth) are good practices for reusability and maintainability.

Overall, it's a very good solution that directly addresses the problem statement effectively, with only minor points for robustness and defensive programming that are often beyond the scope of a direct ""task fulfillment"" but are good practices in general.",gemini-flash-1.5,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

df = pd.read_csv('imdb_top_250.csv')

group1 = df[df['Year'] < 1990]['Rating']
group2 = df[df['Year'] >= 1990]['Rating']

t_statistic, p_value = stats.ttest_ind(group1, group2)

print(f""T-statistic: {t_statistic}"")
print(f""P-value: {p_value}"")

alpha = 0.05
if p_value < alpha:
    print(""Reject the null hypothesis."")
else:
    print(""Fail to reject the null hypothesis."")

```",7,"The generated code correctly performs the core task of loading data, splitting into groups, and running a t-test, which are strong points. It also correctly interprets the p-value against the given alpha. However, it falls short on several explicit requirements from the prompt:

1.  **Hypotheses Statement:** The prompt explicitly asks to ""Clearly state the null and alternative hypotheses for this test."" The generated code does not include any print statements for the hypotheses. This is a significant omission.
2.  **Robustness:** The ground truth includes error handling for `FileNotFoundError` and robust `pd.to_numeric` conversions with `errors='coerce'` and `dropna`, which are crucial for real-world data and prevent potential errors if the CSV structure isn't perfectly clean or the file is missing. The generated code assumes the file exists and the `Year` and `Rating` columns are perfectly numeric, which isn't always a safe assumption.
3.  **Completeness of Output:** While it prints the t-statistic and p-value, it doesn't format them as specifically requested (e.g., `:.4f`).
4.  **Clarity/Verbosity:** The interpretation of the p-value is terse (""Reject the null hypothesis."" vs. ""Since the p-value (X) is less than the significance level (Y), we reject the null hypothesis. Conclusion: ...""). The prompt implied a more detailed interpretation.

To improve, the code should:
*   Add print statements to clearly state the null and alternative hypotheses.
*   Consider adding basic error handling for file loading and data type conversion, especially for 'Year' and 'Rating' columns.
*   Format the t-statistic and p-value output to four decimal places.
*   Elaborate on the conclusion of the hypothesis test, explaining *why* the null is rejected or failed to be rejected, and what that conclusion means in the context of the problem.",gemini-flash-1.5,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

df = pd.read_csv('imdb_top_250.csv')
df['Decade'] = pd.cut(df['Year'], bins=range(1920, 2030, 10), right=False, labels=[f'{x}s' for x in range(1920, 2020, 10)])
print(df[['Title', 'Year', 'Decade']].head(10))

```",9,"The generated code correctly solves the task by loading the CSV, creating the `Decade` column, and displaying the required information. The use of `pd.cut` is a concise and efficient way to create the `Decade` column based on year ranges, which is a strong point.

Areas for minor improvement:
1.  **Robustness**: The ground truth includes error handling (e.g., `FileNotFoundError`, `KeyError`), which makes the code more robust against common issues. The generated code lacks this.
2.  **Explicit Naming**: While `pd.cut` correctly infers the decade ranges, explicitly calculating the decade start year (`((df['Year'] // 10) * 10)`) as seen in the ground truth might be slightly more intuitive for some, but `pd.cut` with calculated labels is also very clear.
3.  **Encapsulation/Modularity**: The ground truth uses functions (`perform_feature_engineering`, `main`) which improves code organization and reusability, although for this small script, it's not strictly necessary.

Overall, the generated code is highly effective and directly addresses the core requirements of the task.",gemini-flash-1.5,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

df = pd.read_csv(""penguins.csv"")

for col in ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']:
    df[col] = df[col].fillna(df[col].mean())

df['sex'] = df['sex'].fillna(df['sex'].mode()[0])

result = df.groupby(['species', 'sex'])['body_mass_g'].mean()
print(result)
```",9,"The generated code correctly addresses all the requirements of the task. It successfully loads the dataset, imputes missing numerical values with the mean, imputes missing categorical values in 'sex' with the mode, and then groups the data to calculate the average 'body_mass_g' as specified.

Strong points:
- **Correctness:** All imputation methods and the final aggregation are performed accurately according to the prompt.
- **Conciseness:** The code is very concise and efficient, particularly the imputation steps.
- **Completeness:** All steps outlined in the task are covered.

Areas for minor improvement (not critical errors, but good practice):
- **Error Handling:** The ground truth includes `try-except` blocks for common issues like `FileNotFoundError` or `KeyError`, which robustifies the code. The generated code lacks this, meaning it would crash if the file isn't found or columns are renamed.
- **Constants:** Defining column names as constants (e.g., `NUMERICAL_COLS`, `CATEGORICAL_COL_MODE`) as in the ground truth makes the code more maintainable, readable, and less prone to typos, especially in larger scripts.
- **Comments:** While functional, adding comments for complex steps (e.g., explaining why `.mode()[0]` is used) would enhance readability for others.

Overall, it's a very good solution that directly answers the prompt's requirements effectively. The minor points are primarily about robustness and maintainability rather than functional correctness for this specific task.",gemini-flash-1.5,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

df = pd.read_csv('penguins.csv')
df = df[(df['species'] == 'Adelie') | (df['species'] == 'Gentoo')]
df = df.dropna(subset=['flipper_length_mm', 'species'])
adelie_flipper = df[df['species'] == 'Adelie']['flipper_length_mm']
gentoo_flipper = df[df['species'] == 'Gentoo']['flipper_length_mm']
t_statistic, p_value = stats.ttest_ind(adelie_flipper, gentoo_flipper)
alpha = 0.05
print(f""T-statistic: {t_statistic}"")
print(f""P-value: {p_value}"")
pri...",6,"The generated code correctly performs data loading, filtering, NA removal, and the t-test, and prints the t-statistic and p-value. However, it misses several explicit requirements from the task description:
1.  **Hypotheses Statement:** The task explicitly asks to ""Clearly state the null and alternative hypotheses for this test."" The generated code does not include this.
2.  **Interpretation of P-value:** While it prints `Reject null hypothesis: {p_value < alpha}`, it does not provide the detailed interpretation requested (""Based on a significance level of alpha = 0.05, interpret the p-value and state whether to reject the null hypothesis.""). The ground truth code provides a more comprehensive conditional statement for this.
3.  **Error Handling:** The ground truth includes `try-except` blocks for file loading and checks for empty or insufficient data groups for the t-test, making it more robust. The generated code lacks these.
4.  **Clarity and Readability:** The ground truth includes comments and print statements that explain each step, which is beneficial for understanding the code's flow and meeting the ""Clearly state..."" requirements for the hypotheses. The generated code is very concise but less explicit.

To improve, the code needs to:
*   Explicitly state the null and alternative hypotheses using `print()` statements.
*   Provide a more comprehensive interpretation of the p-value, similar to the `if/else` block in the ground truth, explaining the conclusion in plain language.
*   Consider adding error handling for file loading and checks for empty dataframes before performing the t-test.",gemini-flash-1.5,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features and target
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = df['species']

# Drop rows with missing values
df_cleaned = df.dropna(subset=['species', 'culmen_length_...",9,"The generated code successfully completes all specified tasks: loading data, selecting features and target, dropping NAs from the specified columns, encoding the target, splitting data, training a Logistic Regression model with `max_iter=1000`, and reporting accuracy. The imports are correct and the code structure is logical.

One minor area for improvement that prevents a perfect score is how the data is cleaned for NAs.
The generated code performs NA dropping on `df` and then re-assigns `X` and `y` from `df_cleaned`. While this achieves the correct outcome, a slightly more robust or direct approach would be to select `X` and `y` first from the original `df`, then concatenate them (or manage columns carefully) for NA dropping, and then re-assign `X` and `y` again from the cleaned DataFrame, ensuring `X` and `y` are perfectly aligned with the cleaned data subsets. The provided code does re-select `X` and `y` after cleaning, so it is technically correct and achieves the desired outcome, but it could be marginally more efficient by selecting the initial `X` and `y` then cleaning *only* those parts, rather than cleaning `df` and then re-selecting from it. However, for a dataframe of this size and complexity, the difference is negligible.

Overall, it's a very solid implementation that addresses all requirements.",gemini-flash-1.5,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
penguins = pd.read_csv(""penguins.csv"")

# Drop rows with NA in body_mass_g or species
penguins = penguins.dropna(subset=['body_mass_g', 'species'])

# Calculate average body mass for each species
average_body_mass = penguins.groupby('species')['body_mass_g'].mean()

# Create the bar plot
plt.figure(figsize=(8, 6))
sns.barplot(x=average_body_mass.index, y=average_body_mass.values)
plt.title('Av...",9,"The generated code correctly performs all requested tasks: it loads the data, correctly handles NAs in the specified columns, calculates the average body mass per species, and generates a bar plot with appropriate labels and title. The use of `average_body_mass.index` and `average_body_mass.values` for the seaborn bar plot is a correct and efficient way to handle the Series returned by groupby().mean().

One minor improvement would be to use explicit column names for `x` and `y` in `sns.barplot` after resetting the index of `average_body_mass`. While the current approach works, resetting the index often makes the code more readable and directly maps to the `data` parameter of seaborn functions. However, this is a style preference and doesn't detract from correctness. The generated code implicitly assumes ""penguins.csv"" is in the current directory and does not include error handling for `FileNotFoundError`, which the ground truth code does, but the prompt only asked to *assume* the CSV is in the current directory, not necessarily to handle the file not found case. Overall, it's a very good solution.",gemini-flash-1.5,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Drop rows with NA values in specified columns
df = df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# Create culmen_area feature
df['culmen_area'] = df['culmen_length_mm'] * df['culmen_depth_mm']

# Select features for correlation analysis
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'b...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values in the specified columns, creates the `culmen_area` feature, calculates the Pearson correlation matrix for the required columns, and visualizes it as an annotated heatmap with the correct title. The code is concise and uses appropriate pandas and seaborn functions.

Improvements for a perfect score (minor):
- The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth. While not explicitly asked for in the prompt, robust code typically includes this.
- The `fmt="".2f""` argument, which formats the annotations to two decimal places, is missing from the `sns.heatmap` call. While `annot=True` works, more specific formatting is often desired for correlation matrices.
- The ground truth includes `plt.xticks(rotation=45, ha='right')` and `plt.tight_layout()`, which are good practices for readability but not strictly necessary for correctness here. The generated code uses `center=0` in `cmap`, which is also a good practice for correlation matrices.",gemini-flash-1.5,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv(""penguins.csv"")

# Select features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']
target = 'species'
df = df[features + [target]]

# Impute numerical features
numerical_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numerical_features:
    df[col] = df[col].fillna(d...",9,"The generated code successfully completes the task as specified in the prompt. It loads the data, selects the correct features, imputes numerical NAs with the mean, handles the 'sex' column according to instructions, and applies one-hot encoding to 'island'. Finally, it prints the shape and head of the processed DataFrame.

Strong points:
- Correctly loads the data.
- Correctly selects the features and maintains the target.
- Correctly imputes numerical NAs using the mean.
- Correctly imputes 'sex' NA values with 'Unknown' and maps the string values to integers.
- Correctly performs one-hot encoding for the 'island' column using `OneHotEncoder` and `pd.concat`.
- Prints the shape and head as required.

Areas for minor improvement (not critical for the task but good practice):
- The `pd.concat` operation after one-hot encoding `island` might cause issues if the `df` index is not contiguous or if there are missing rows (e.g., due to previous `dropna` operations, though not done here). `pd.get_dummies` is generally safer for this task as it handles the index correctly by default. However, `OneHotEncoder` is also a valid approach, and the current implementation is acceptable for this specific dataset and steps.
- The prompt asks for the shape and head of the ""feature matrix"" explicitly, which usually implies `X` (features only), while the generated code prints the shape and head of the entire `df` (features + target). This is a minor deviation but worth noting for precision.
- No error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. While not explicitly required by the prompt, it's good practice.
- The use of `sparse_output=False` in `OneHotEncoder` is good as it produces a dense array convenient for `pd.DataFrame`, but for very large datasets, sparse arrays might be preferred.",gemini-flash-1.5,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

data = {'OrderID': [101, 102, 103, 104, 105, 106, 107], 
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
        'Quantity': [1, 2, 1, 1, 3, 1, 1], 
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150], 
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST002', 'CUST004', '...",8,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates the total spending per customer, identifies and prints the top 3 customers by spending, and calculates the total quantity sold per category.

**Strengths:**
- **Correctness:** All calculations and data manipulations are accurate.
- **Conciseness:** The code is quite concise while remaining readable.
- **Functionality:** It fulfills all specified requirements.
- **Proper Imports:** Includes the necessary `pandas` import.

**Areas for Improvement (to achieve a higher score):**
- **Output Clarity:** While the output is correct, the ground truth code provides more descriptive print statements (e.g., ""--- Creating DataFrame ---"", ""Top 3 Customers by Total Spending (Descending Order):"") which enhance readability and understanding of the output during execution.
- **Output Formatting for Top Customers:** The ground truth code formats the total spending with a currency symbol and two decimal places (e.g., ""$1200.00""), which is a nice touch for financial figures. The generated code simply prints the raw float.
- **Robustness (Minor):** The ground truth includes checks for empty DataFrames/Series before printing results (e.g., `if not top_3_customers.empty:`), which makes the code slightly more robust, although for this specific dataset, it's not strictly necessary.

Overall, it's a very good solution and achieves the core objectives effectively. The improvements are primarily about presentation and user experience rather than fundamental correctness.",gemini-flash-1.5,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
df = pd.read_csv('titanic.csv')

# Data preprocessing
df.drop(['PassengerId', 'Name', ...",6,"The generated code addresses most of the requirements but has significant issues in data preprocessing, which impacts the correctness and interpretability of the results.

**Strong Points:**
- **Basic Structure:** Follows a logical flow of data loading, preprocessing, model training, evaluation, and visualization.
- **Metric Coverage:** Includes all requested evaluation metrics (Accuracy, Precision, Recall, F1-score, ROC-AUC).
- **Visualization:** Generates both Confusion Matrix and ROC Curve plots.
- **Cross-validation:** Performs basic cross-validation.

**Areas for Improvement/Critical Missing Points:**

1.  **Data Preprocessing (Major Issue):**
    *   **Categorical Encoding:** Uses `LabelEncoder` for 'Sex' and 'Embarked'. While it works for 'Sex' (binary), it introduces an ordinal relationship for 'Embarked' (S, C, Q) which is not appropriate for nominal categories and can negatively affect model performance and interpretation, especially with logistic regression. The ground truth uses `OneHotEncoder`, which is the correct approach.
    *   **Feature Scaling/ColumnTransformer:** The `StandardScaler` is applied to `X` *after* label encoding and *before* splitting the data. This is problematic:
        *   It scales *all* columns, including the (now numerical) `Pclass` which should generally be one-hot encoded or treated as a categorical variable.
        *   It does *not* scale the label-encoded categorical features appropriately.
        *   It performs `fit_transform` on the entire dataset `X`, which is a form of data leakage. The scaler should only be fitted on the training data.
        *   The absence of `ColumnTransformer` makes the preprocessing less robust and harder to manage, especially with heterogeneous data types. The ground truth correctly uses `ColumnTransformer` within a `Pipeline`.
    *   **Feature Engineering:** The task explicitly asks for feature engineering (FamilySize in the ground truth), which is missing in the generated code.

2.  **Feature Coefficients Interpretation:**
    *   The generated code simply prints the coefficients with the original column names. After scaling and label encoding, these coefficients are not directly interpretable for categorical features (especially `Embarked`) and are also potentially misleading if `Pclass` is treated numerically by the scaler.
    *   The ground truth code correctly extracts feature names after one-hot encoding and provides a detailed interpretation, which was a specific requirement.

3.  **Visualization:**
    *   The confusion matrix visualization is very basic `imshow`. While it shows the matrix, `ConfusionMatrixDisplay.from_estimator` (as used in ground truth) is more robust and user-friendly, providing labels and better formatting directly.
    *   The ROC curve plot is functional but could benefit from explicitly setting limits and possibly displaying the AUC score on the plot itself, as the ground truth does.

4.  **Reproducibility:** Missing `random_state` for `LogisticRegression` and `numpy.random.seed`.

5.  **Robustness:** The code assumes `titanic.csv` exists; adding a `try-except` block for file loading (like the ground truth) would make it more robust.

The data preprocessing flaws are significant enough to warrant a lower score, as they directly impact the validity and interpretability of the model's output and evaluation.",gemini-flash-1.5,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']), 'Ticker': ['TECHCO']*8, 'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0], 'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5], 'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0], 'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0, 157.0, 154.0], 'Volume': [1000000, 1200000, 110000...",9,"The generated code correctly solves the task. It creates the DataFrame, sets the 'Date' as an index, calculates the 3-day SMA for 'Close' price, and calculates the daily percentage change for 'Volume', handling NaNs appropriately as specified. The `inplace=True` argument for `set_index` was omitted, but assigning the result back to `df` (`df = df.set_index('Date')`) achieves the same outcome. The only minor deviation from the ground truth is the multiplication by 100 for `Volume_Pct_Change`, which presents the percentage change as a whole number rather than a decimal. While not strictly incorrect, the request implied a standard percentage change (decimal). If the output needs to match the ground truth exactly, this would be a slight difference. However, it's a common practice to represent percentages this way, so it's a minor point. The code is clean and follows good practices.",gemini-flash-1.5,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'], 
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'], 
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91], 
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89], 
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}

df = pd.DataFrame(dat...",8,"The generated code correctly creates the DataFrame, calculates the 'Score_Improvement' column, and then calculates and prints the average improvement per subject. It also correctly identifies students meeting the specified score and attendance criteria.

Areas for improvement:
- The output format for printing the `StudentID` of the students meeting criteria is a pandas Series, whereas the ground truth iterated and printed each ID individually. While functionally correct, the ground truth's output is slightly cleaner for this specific requirement.
- The ground truth includes more verbose print statements for clarity which is good practice in a benchmark scenario. These aren't critical for correctness, but they do make the output easier to follow.",gemini-flash-1.5,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

filtered_df = df[(df[""Department""] == ""Tech"") & (df[""Perf...",9,"The generated code correctly performs all requested operations: defining the DataFrame, filtering it based on the specified conditions, and calculating the average number of ""ProjectsCompleted"" for the filtered subset. The code is concise and directly addresses the prompt.

The only minor area for improvement that prevents a perfect score is the lack of handling for the edge case where the `filtered_df` might be empty. While for this specific dataset, `filtered_df` will not be empty, in a more general scenario, if no employees matched the criteria, `filtered_df[""ProjectsCompleted""].mean()` would result in `NaN` and the print statement would output `nan`, which might not be the most user-friendly output. The ground truth code handles this gracefully with an `if not filtered_df.empty:` check.

Overall, it's a very good and correct solution for the given task.",gemini-flash-1.5,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

desktop_users = df[df[""DeviceType""] == ""Desktop""]
mobile_users = df[df[""DeviceType""] == ""Mobile""]

desktop_conversion_rate = (d...",9,"The generated code correctly defines the DataFrame and accurately calculates and prints the conversion rates for both Desktop and Mobile users. The use of `.sum() / len()` to calculate the conversion rate is functionally equivalent to `.mean()` on a boolean series for this specific task and correctly yields the desired percentage.

Strong points:
- Correct DataFrame creation.
- Accurate calculation of conversion rates.
- Clear and properly labeled output printing.
- Concise and readable code.

Area for minor improvement (not critical for correctness in this specific dataset):
- The generated code does not include explicit handling for the edge case where `len(desktop_users)` or `len(mobile_users)` might be zero (i.e., a `DeviceType` has no users in the dataset). In such a scenario, division by zero would occur, raising an error. The ground truth code handles this by checking `.empty` and assigning a rate of 0.0, which is more robust for general cases. While it works perfectly for the given data, robust code would include such checks.",gemini-flash-1.5,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

#Calculate and print average AirQualityIndex for each SensorID
for sensor in df...",9,"The generated code correctly defines the DataFrame and accurately calculates the average AirQualityIndex for each unique SensorID, fulfilling both main requirements of the task. The output format also clearly indicates which average belongs to which sensor.

The code quality is good, with proper imports and a clear, readable structure. It uses a straightforward loop combined with boolean indexing to achieve the desired result, which is functionally correct.

The only minor area for improvement, preventing a perfect 10, is that the `groupby().mean()` method offered by pandas is generally more idiomatic, efficient, and often cleaner for this type of aggregated calculation than iterating through unique values and filtering the DataFrame in a loop. While the generated code's approach works for this small dataset, for larger datasets, `groupby` would perform significantly better due to vectorized operations. However, the current solution is perfectly valid and achieves the task's stated goals.",gemini-flash-1.5,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Calculate EngagementScore
df[""EngagementScore""] = df[""Likes""] + (df[""Shares""] * 2) +...",9,"The generated code successfully defines the DataFrame, calculates the ""EngagementScore"" correctly, filters for Instagram posts, and accurately computes their average ""EngagementScore"". All steps of the task are met. The code is clean and well-structured.

One minor point for improvement, which is common in benchmarks and not a critical flaw here, is that the final print statement `print(average_engagement)` is less descriptive than the ground truth's `print(f""Average EngagementScore for Instagram posts: {average_engagement_score_instagram:.2f}"")`. Adding a descriptive message and formatting the output can improve user readability in real-world scenarios. Also, the ground truth includes a check for an empty DataFrame before calculating the mean, which adds robustness, though for this specific dataset, an empty DataFrame is not a concern. These are minor refinements rather than errors.",gemini-flash-1.5,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is correct, complete, and of high quality. It successfully defines the DataFrame, calculates the ""LengthOfStay"" column, filters for patients with ""Flu"" diagnosis, and accurately calculates and prints their average ""LengthOfStay"". The code uses appropriate pandas functionalities and follows a clear, concise structure. It includes all necessary imports and handles the datetime conversions correctly as specified in the problem description.",gemini-flash-1.5,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

sci_fi_movies = df[df[""Genre""...",9,"The generated code correctly defines the DataFrame, filters for Sci-Fi movies, and accurately calculates their average rating, fulfilling all specified task requirements. The code is concise and directly addresses the problem.

One minor suggestion for improvement, which is not a correctness issue but a best practice, would be to format the output print statement for better readability, similar to the ground truth (e.g., `print(f""The average rating for Sci-Fi movies is: {average_rating:.2f}"")`). Additionally, while not strictly required for this specific dataset given the guarantee of Sci-Fi movies, including an edge case check for an empty `sci_fi_movies` DataFrame (as in the ground truth) would make the code more robust against different input data. However, for the given task and data, the provided solution is fully functional and correct.",gemini-flash-1.5,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

toyota_cars = df[df[""Brand""] == ""Toyota""]
average_mileage = toyota_ca...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately filters for Toyota cars, and calculates their average mileage as requested. The code is concise, clean, and directly addresses all parts of the prompt. No improvements are necessary for this task.",gemini-flash-1.5,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. All requirements of the task are met. The code is clean, concise, and directly achieves the desired outcome.

The only minor area for improvement, which is not critical for correctness but good practice, would be to add a more descriptive print statement (e.g., specifying what the printed number represents) and potentially handle the edge case where no Fire-type Pokemon might be found in a more robust way (though for this specific dataset, it's not an issue). However, given the prompt only asked to ""print their average 'Attack' stat"", the current `print(average_attack)` is technically sufficient. The ground truth example includes a more robust print statement and error handling for an empty filtered DataFrame, which are good practices.",gemini-flash-1.5,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

df = pd.read_csv(""titanic.csv"")
df['Age'] = df['Age'].fillna(df['Age'].median())
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 35, 60, 100], labels=['Child', 'Young Adult', 'Adult', 'Senior'])

fig, axes = plt.subplots(2, 2, figsize=(15, 10))
sns.barplot(x='Pclass', y='Survived', data=df, ax=axes[0, 0])
sns....",7,"The generated code addresses most of the requirements but lacks some polish and detail compared to the ground truth.

**Strong Points:**
- **Correctness:** The code correctly generates a multi-panel figure, a heatmap, violin plots, an interactive scatter plot, and attempts a dashboard-style figure. It uses appropriate libraries for each task.
- **Imports:** Correctly imports `pandas`, `matplotlib.pyplot`, `seaborn`, `plotly.express`, and `plotly.graph_objects`.
- **Data Preprocessing:** Handles missing 'Age' values by filling with the median, which is a good practice.
- **Variety of Plots:** Successfully uses bar plots, heatmaps, violin plots, and scatter plots.

**Areas for Improvement:**
- **Completeness (Details in Visualizations):**
    - **Multi-panel figure (Task 1):** While it creates a multi-panel figure, it only shows three of the four requested demographic groups (class, gender, age groups). The fourth plot is a `countplot` of `Survived` by `Sex` which is not explicitly requested. The ground truth provides three bar plots of *survival rates* by class, gender, and age group, directly addressing the prompt. The generated code also misses proper titles, labels, and consistent aesthetic choices across the subplots.
    - **Heatmap (Task 2):** Correctly implemented, but lacks specific `linewidths` or `linecolor` for better separation as seen in the ground truth.
    - **Interactive Scatter Plot (Task 4):** The interactive plot uses `Pclass` for color, but it doesn't utilize different symbols for survival or `hover_name` for individual passenger names, reducing its storytelling capability compared to the ground truth.
    - **Dashboard (Task 5):** This is the weakest part. The ground truth uses `matplotlib.gridspec` for a flexible and comprehensive layout, including text insights. The generated code uses `plotly.subplots` which is a good attempt, but the chosen subplots are basic and don't form a ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights"" as requested. It essentially just lists four separate plots within a single figure without the advanced layout, descriptive text, or additional insights that the ground truth provides. It uses `mean` for survival rate, which is correct, but doesn't map categories to more descriptive names (e.g., '1st Class' instead of '1').
- **Code Quality & Storytelling:**
    - **Aesthetics and Labels:** Many plots lack clear titles, axis labels, and legends or have less descriptive labels (e.g., `Pclass` instead of 'Passenger Class'). The ground truth goes to great lengths to make plots visually appealing, informative, and consistent.
    - **Data Preprocessing Detail:** The ground truth includes more comprehensive preprocessing steps like converting `Survived` and `Pclass` to descriptive categorical types, which aids in plot readability. The generated code creates `AgeGroup` but doesn't similarly enhance other columns.
    - **Storytelling:** The prompt specifically asks for ""advanced data visualization and storytelling."" While the generated code creates plots, it doesn't explicitly enhance storytelling through annotations, well-chosen labels, or a dedicated insights section in the dashboard, which the ground truth excels at.
    - **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError`.

Overall, the generated code provides functional plots that fulfill the basic requirements, but it falls short on the ""advanced data visualization and storytelling"" aspect, particularly in the comprehensiveness and presentation quality of the dashboard and the detailed annotations in the other plots. The ground truth is significantly more refined in its visual design, clarity, and narrative presentation.",gemini-flash-1.5,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for Dystopian genre, and accurately calculates the average stock for that genre. It addresses all core requirements of the task. The strong points are its conciseness and directness in solving the problem.

The only minor area for improvement that would bring it closer to the ground truth's robustness is the lack of handling for the edge case where no Dystopian books might be found (though not explicitly required by the prompt's output format, it's good practice). In such a scenario, `average_stock` would be `NaN`, and printing it as is might be less informative than a specific message. However, given the provided dataset, this scenario doesn't occur. The ground truth also included optional print statements for verification, which are useful for debugging but not strictly necessary for the final solution. The generated code is efficient and clean for the given problem.",gemini-flash-1.5,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

data = pd.read_csv('water_potability.csv')
data = data.dropna()

alpha = 0.05

cols = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']

for col in cols:
    t_statistic, p_value = stats.ttest_ind(data.loc[data['Potability'] == 1, col], data.loc[data['Potability'] == 0, col])
    print(f'{col}:')
    print(f'  T-test: t = {t_statistic:.3f}, p = {p_value...",6,"The generated code attempts to perform the requested statistical tests but falls short in several areas regarding correctness, completeness, and robustness.

**Correctness:**
*   **Missing Value Handling:** The code uses `data.dropna()` which removes rows with *any* missing values. This is often too aggressive and can lead to significant data loss if many rows have scattered missing values. The prompt specifically asked for ""handling missing values"", and typically imputation (as done in the ground truth) is preferred for numerical data to preserve sample size.
*   **Cohen's d Calculation:** The calculation for Cohen's d is flawed. It uses `np.std(data[col])` for the pooled standard deviation, which is the standard deviation of the *entire* column, not the pooled standard deviation of the two groups being compared. This will lead to incorrect effect size values.
*   **Chi-square Test:** The `stats.chisquare(potability_counts)` performs a goodness-of-fit test against an *expected* distribution of uniformity by default. While this might align with one interpretation of ""check if potability follows expected distribution"" (i.e., 50/50), the ground truth explicitly defines this interpretation and calculates the expected counts. The generated code implicitly assumes this and will work, but lacks clarity regarding the null hypothesis being tested. Also, it only prints the chi2 and p-value, lacking interpretation.
*   **Interpretation:** The code lacks explicit interpretation for the p-values of each test, which was a core requirement. It only calculates and prints the values.

**Completeness:**
*   **Interpretation of Results:** Explicitly stating whether to reject or fail to reject the null hypothesis, and explaining what that means in context, is missing for all tests.
*   **Effect Size Interpretation:** The prompt asked for ""interpretation of results for each test"", which for Cohen's d, includes describing what a small/medium/large effect means. This is missing.
*   **Error Handling:** There's no error handling for file loading (e.g., `FileNotFoundError`).
*   **Clarity and Readability:** Variable names could be more descriptive (e.g., `data` instead of `df`). The output formatting is basic and could be improved with more descriptive headers and separation, as seen in the ground truth.

**Code Quality:**
*   **Imports:** Proper imports are present.
*   **Structure:** The code is structured logically by test type.
*   **Redundancy:** The `cols` list is defined, but `data.loc[data['Potability'] == 1, col]` is repeated multiple times inside the loop, which could be refactored for clarity (e.g., by separating potable and non-potable dataframes once).

Overall, the generated code performs the core statistical calculations but lacks the robustness, detailed interpretation, and proper methodological application (especially for Cohen's d and missing value handling) expected from an expert solution.",gemini-flash-1.5,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt

# Load datase...",6,"The generated code addresses most of the task requirements but has several critical issues that prevent it from being a strong solution.

**Strengths:**
*   **Basic Structure:** Follows a logical flow of data loading, preprocessing, model training, evaluation, and some visualization.
*   **Multiple Models:** Implements Logistic Regression, Random Forest, and SVM.
*   **Comprehensive Metrics:** Calculates accuracy, precision, recall, F1-score, and ROC-AUC.
*   **Visualizations:** Attempts to create confusion matrices and ROC curves.
*   **K-Fold Cross-Validation:** Includes k-fold CV.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical):**
    *   The `df.fillna(df.mean(), inplace=True)` is applied *before* the train-test split. This is a form of data leakage, as the mean used to impute missing values for the training set will be influenced by data from the test set. The Ground Truth correctly handles this by performing imputation *after* the train-test split on the respective subsets (`X_train` and `X_test`).
    *   It uses `df.mean()` which is fine, but the Ground Truth uses `SimpleImputer(strategy='median')` which is often preferred for skewed data or when dealing with outliers (like in potability data).

2.  **Feature Scaling (Critical):**
    *   The `scaler.fit_transform(X)` is applied to the *entire dataset* (X) before the train-test split. This is another form of data leakage. The scaler should be fitted *only* on the training data (`X_train`) and then used to transform both `X_train` and `X_test`.

3.  **Target Encoding:**
    *   `LabelEncoder` is used for the `Potability` column. While `Potability` is 0/1, it's already numerical and doesn't explicitly need encoding. This step is unnecessary and could potentially cause issues if the original column wasn't already numerical (though it is in this specific dataset). The Ground Truth avoids this.

4.  **Cross-Validation Implementation (Major Flaw):**
    *   The Cross-Validation is performed on `X` and `y` (the full dataset) which is *already scaled and imputed* with data leakage.
    *   Crucially, the preprocessing (imputation and scaling) is **not** performed within each fold of the cross-validation. This is a fundamental error in CV methodology. Preprocessing steps (especially `fit` operations) must be part of the cross-validation loop (e.g., using a `Pipeline`) to prevent data leakage and ensure a fair assessment of model generalization. The Ground Truth code demonstrates the correct use of `Pipeline` for this.
    *   Only 'accuracy' is used for `cross_val_score`. The prompt asks for comprehensive metrics (accuracy, precision, recall, F1-score, ROC-AUC) for evaluation, including CV.

5.  **Confusion Matrix Visualization:**
    *   The confusion matrix visualization is basic (`plt.imshow`). While it works, it's not as user-friendly or informative as `seaborn.heatmap` with annotations (`fmt='d'`) and clear labels (e.g., 'Not Potable', 'Potable') as implemented in the Ground Truth.
    *   The `plt.show()` within the loop causes each confusion matrix and ROC curve to appear separately, interrupting the flow. It's better to display them after all models have been processed, perhaps in subplots, or save them to files if many.

6.  **ROC Curve Visualization:**
    *   Uses `roc_curve` and `auc` directly which is fine but `RocCurveDisplay.from_estimator` (as used in Ground Truth) is a more modern and convenient way to plot ROC curves directly from an estimator, often integrating better with subplots.
    *   Similar to confusion matrices, `plt.show()` within the loop is disruptive. It's better to plot all ROC curves on a single graph for easy comparison, as done in the Ground Truth.

7.  **Feature Importance:**
    *   The feature importance extraction is only done for Random Forest if it's the best model.
    *   It plots feature importances using indices (`plt.xticks(range(X.shape[1]), indices)`), but these are just numerical indices, not actual feature names, making the plot uninterpretable. The Ground Truth correctly links feature importances to `X_train.columns`.
    *   It doesn't handle feature importance for Logistic Regression (coefficients) or SVM, whereas the ground truth explicitly covers LR and notes SVM limitations.

8.  **Model Selection Criteria:**
    *   The `best_model_name = max(results, key=lambda k: results[k]['accuracy'])` selects the best model based *only* on accuracy from the single test set split. The task explicitly asks for comparison based on k-fold cross-validation results which are more robust. The Ground Truth correctly selects based on CV F1-score.

9.  **Completeness & Detail:**
    *   Doesn't explicitly compare model performances in a clear, tabular format after evaluation and CV, unlike the Ground Truth which prints `pd.DataFrame`s.
    *   Lacks `random_state` for models (e.g., `LogisticRegression()`, `RandomForestClassifier()`) which can lead to non-reproducible results. The Ground Truth uses `random_state=42`.
    *   Lacks the `try-except` block for file loading.
    *   No warnings suppression.

Overall, the generated code provides a functional starting point but demonstrates significant gaps in robust data preprocessing, cross-validation methodology, and detailed result presentation as per the task requirements and best practices in machine learning.",gemini-flash-1.5,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('water_potability.csv')
df = df.fillna(df.mean())

df['pH_Chloramines'] = df['pH'] / df['Chloramines']
df['Hardness_Solids'] = df['Hardness'] / df['Solids']
df['Sulfate_Trihalomethanes'] = df['Sulfate'] * df['Trihalomethanes']
df['pH_squared'] = df['pH']**2
df[...",6,"The generated code addresses most of the requirements but lacks depth in several areas, leading to a score of 6.

**Positives:**
*   Correctly loads data and handles missing values (though it uses `fillna(df.mean())` which is a good simple approach).
*   Attempts feature engineering (ratios, interaction, polynomial terms).
*   Calculates correlation matrix and identifies top 5 correlated features.
*   Applies SelectKBest and RFE for feature selection.
*   Generates a correlation heatmap, feature importance plot, and pairwise scatter plots.
*   Compares the selected feature sets.

**Areas for Improvement / Missing Aspects:**

1.  **Feature Engineering Quality & Variety:**
    *   The `pH_Chloramines` and `Hardness_Solids` ratios are created, but with a potential for division by zero if `Chloramines` or `Solids` are 0. The ground truth adds a small epsilon to mitigate this.
    *   `Sulfate_Trihalomethanes` is an interaction, not a ratio.
    *   Only two polynomial features (`pH_squared`, `Hardness_squared`) are created. The prompt asked for ""polynomial features"" (plural) and the ground truth uses `PolynomialFeatures` to generate multiple polynomial terms, which is a more comprehensive approach.
    *   The prompt asked for ""meaningful features such as ratios... interaction terms, and polynomial features"". The generated code only has 2 ratios, 1 specific interaction, and 2 specific polynomial terms. This is less comprehensive than expected.

2.  **Missing Initialization/Standardization for Feature Selection:**
    *   `RFE` and `SelectKBest` often benefit from feature scaling (e.g., `StandardScaler`) before application, especially if using a model sensitive to scale, which is not included. The ground truth correctly applies `StandardScaler` first.

3.  **Visualization Details:**
    *   The correlation heatmap `annot=True` makes the plot very cluttered with many features, making it hard to read. `annot=False` (or a small number like 0.1) is generally better for a large number of features.
    *   The x-axis labels for the feature importance plot are overlapping.
    *   The `pairplot` includes a hue for `Potability` but the quality of plots could be improved (e.g., `diag_kind='kde'`).

4.  **RFE `n_features_to_select`:**
    *   The ground truth explicitly defines `n_features_to_select` for RFE (and k for SelectKBest). The generated code uses the default for RFE which might not be 5.
    *   `RFE(rf, n_features_to_select=5)` is correct in the generated code. I missed that on first pass.

5.  **Output Clarity:**
    *   The ground truth provides more verbose and formatted print statements for the comparisons, making the analysis clearer.

Overall, the code provides a functional pipeline but falls short in the depth and robustness of its feature engineering, scaling considerations for feature selection, and the aesthetic quality of some visualizations.",gemini-flash-1.5,LLM_judge_config
