task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        wine_data = pd.read_csv('winequality-red.csv', sep=';')
    except FileNotFoundError:
        print(""The file 'winequality-red.csv' was not found in the current directory."")
        return

    # List all column names and identify the data type of each column
    print(""Column Names and Data Types:"")
    for col, dtype in zip(wine_data.columns, wine_data.dtypes):
        if dtype.kind in 'bifc':
            col_ty...",8,"The generated code correctly loads the dataset, identifies data types, calculates descriptive statistics for numerical columns, and reports dataset dimensions. It also makes a reasonable attempt to identify categorical/ordinal columns, including recognizing that numerical columns with few unique values might be ordinal, which is a good insight for the 'quality' column in this dataset.

Strong points:
- Proper use of pandas for data loading and manipulation.
- Clear and concise output for different sections.
- Intelligent handling of numerical vs. categorical data types using `select_dtypes`.
- The added logic to identify potential ordinal columns (numerical but few unique values) is a good heuristic, especially for the 'quality' column in this dataset.

Areas for improvement:
- **CSV Separator:** The ground truth code assumes the default comma separator, while the generated code correctly identifies that `winequality-red.csv` uses a semicolon (`;`). This is a crucial detail for this specific dataset and makes the generated code more robust for `winequality-red.csv`. For benchmark purposes, if the dataset is not explicitly defined as comma-separated, inferring the correct separator (or stating a common one) is good. However, the prompt did not specify the separator, so assuming the default or providing the correct one are both valid, with the latter being more practical.
- **Categorical Column Identification:** The generated code's check `dtype.kind == 'O'` is less robust than `pd.api.types.is_object_dtype` or `pd.api.types.is_categorical_dtype` from the ground truth. While 'O' generally works for object, it doesn't explicitly catch `CategoricalDtype` if the column was already converted.
- **Strictness for Categorical/Ordinal:** The ground truth code explicitly uses `pd.api.types.is_object_dtype` or `pd.api.types.is_categorical_dtype`, which is more precise. The generated code labels `O` as 'Categorical/Object' effectively. For ""ordinal"", it suggests checking `nunique()` which is a good heuristic but not a definitive type. The prompt asks to identify ""any columns that *appear* to be categorical or ordinal,"" and the generated code's approach for ordinal columns is a good interpretation of ""appear.""
- **Error Handling:** The ground truth provides more verbose error feedback for `FileNotFoundError`. The generated code simply prints a short message. While functional, more user-friendly guidance is always better.

Overall, the generated code is very good and largely fulfills the requirements with practical and efficient pandas operations. The `sep=';'` is a key correction for this specific dataset which improves its correctness beyond merely following the prompt's general assumptions.",llama-4-maverick,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        data = pd.read_csv('video_games_sales.csv')
    except FileNotFoundError:
        print(""The file 'video_games_sales.csv' was not found."")
        return

    # Display the first 10 rows
    print(""First 10 rows of the dataset:"")
    print(data.head(10))

    # Show the shape of the dataset
    print(""\nShape of the dataset (rows, columns):"")
    print(data.shape)

    # List all column names and their data types
...",9,"The generated code correctly performs all the required tasks. It loads the dataset, displays the first 10 rows, shows the shape, lists column names and data types, displays top games by global sales, shows unique platform and genre values, calculates summary statistics for sales columns, and identifies missing values.

Strengths:
- Correctly uses pandas functions for all required EDA steps.
- Handles `FileNotFoundError` robustly.
- Presents output clearly with descriptive print statements.
- Uses `data.nlargest` for top games which is efficient.

Areas for minor improvement (compared to Ground Truth):
- The `data.dtypes` provides a slightly different output format than `df.info()`. While `data.dtypes` fulfills the request for 'data types', `df.info()` provides more comprehensive information including non-null counts and memory usage, which is often preferred in EDA.
- The display of top 10 games only includes 'Name' and 'Global_Sales', whereas the ground truth also included 'Platform', 'Genre', 'Publisher' for a more complete view.
- The missing values section could be enhanced to only show columns with actual missing values, as done in the ground truth, for clearer interpretation in larger datasets.
- The use of `print` statements to delineate different sections is good, but the ground truth's use of repeated characters (`=""*80`) makes the sections even more visually distinct.",llama-4-maverick,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd
import numpy as np

def clean_video_games_sales_data(csv_file):
    # Load the dataset
    df = pd.read_csv(csv_file)

    # Handle missing values in the Year column by filling them with the median year
    df['Year'] = df['Year'].fillna(df['Year'].median())

    # Remove games with missing Publisher information
    df = df.dropna(subset=['Publisher'])

    # Create a new column 'Years_Since_Release'
    df['Years_Since_Release'] = 2023 - df['Year']

    # Remove du...",7,"The generated code correctly addresses most of the requirements but misses some crucial robustness and completeness aspects compared to the ground truth.

**Strengths:**
*   Correctly loads the dataset.
*   Handles missing 'Year' values using the median.
*   Removes games with missing 'Publisher'.
*   Correctly calculates 'Years_Since_Release'.
*   Removes duplicates based on 'Name' and 'Platform'.
*   Filters out low-selling games based on 'Global_Sales'.
*   Displays the final shape and head.
*   Encapsulates logic in a function, which is good practice.

**Areas for Improvement (Why it's not higher):**
*   **Error Handling for 'Year' and 'Global_Sales' Data Types:** The ground truth includes `pd.to_numeric(errors='coerce')` for both 'Year' and 'Global_Sales' columns, followed by `dropna` for `Global_Sales` and `astype(int)` for `Year`. This is a critical step for real-world datasets where these columns might contain non-numeric strings, which would cause the generated code to fail or produce incorrect results (e.g., `median()` might fail, or `Global_Sales >= 0.1` might raise an error). The generated code assumes these columns are already numeric, which is a fragile assumption.
*   **Negative `Years_Since_Release`:** The ground truth specifically handles cases where `Years_Since_Release` might be negative (e.g., if a future year is present in the dataset) by setting it to `max(0, x)`. The generated code does not include this, which could lead to nonsensical negative values.
*   **Clarity on `inplace=True` vs. reassignment:** While the generated code reassigns `df` (e.g., `df = df.dropna(...)`), the ground truth often uses `inplace=True`. Both are valid, but reassigning is generally preferred in most cases if not chaining operations heavily, as it leads to more readable code and avoids potential side effects. However, the use of `inplace=True` or reassignments consistently isn't a major issue here.
*   **Explicit `DATASET_PATH` and `CURRENT_YEAR`:** The ground truth defines these as constants, improving readability and maintainability. The generated code hardcodes `2023`.
*   **No `try-except` block:** The ground truth includes robust error handling for `FileNotFoundError` and general exceptions, which is crucial for production-ready code.",llama-4-maverick,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def load_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load data: {e}"")

def plot_top_platforms(data):
    top_platforms = data.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
    plt.figure(figsize=(10, 6))
    top_platforms.plot(kind='bar')
    plt.title('Top 10 Gaming Platforms by Global Sales')
    plt.xlabel...",6,"The generated code attempts to address all visualization requirements but falls short in several areas.

**Correctness:**
- **Plotting functionality:** The code correctly identifies the types of plots required for each visualization and generally uses the right `matplotlib` functions.
- **Data Loading:** The `load_data` function is a good practice for error handling.
- **Specific Issues:**
    - **Year column:** The ground truth code correctly identifies `Year_of_Release` as the year column. The generated code uses `'Year'` in `plot_games_per_year`, which is incorrect based on the dataset schema (`video_games_sales.csv` typically has `Year_of_Release`). This will likely cause a KeyError or produce an empty plot.
    - **Data cleaning/preprocessing:** The ground truth code includes crucial data cleaning steps (e.g., converting `Year_of_Release` to numeric, handling NaNs in sales columns). The generated code completely omits this, which can lead to errors if the dataset has non-numeric sales entries or NaN year values, making the plots unreliable or causing crashes. This is a significant omission for a data science task.
    - **Plot formatting:** While titles and labels are present, they lack the detail and clarity of the ground truth. For instance, the ground truth explicitly mentions ""Millions"" for sales units, enhancing readability. The scatter plot and histogram from the generated code also lack grid lines, which improve readability. The line plot for games per year does not filter or clean the years, which can lead to spurious data points (e.g., year 0 or very early years if coerced from non-numeric).

**Code Quality:**
- **Modularity:** The use of functions for each plot and a `main` function is excellent for code organization and reusability. This is a strong point.
- **Imports:** Correct imports are present.
- **Error Handling:** Basic error handling for file loading is good.

**Completeness:**
- **Visualization Types:** All required visualization types (bar, pie, histogram, scatter, line) are attempted.
- **Titles and Labels:** Generally included, but the quality/details are less refined than the ground truth.
- **Missing Crucial Steps:** The lack of data cleaning and preprocessing for numeric columns and year values is a major incompleteness, as real-world datasets often require this for accurate plotting. The ground truth explicitly addresses this.

**Areas for Improvement:**
1.  **Crucial Data Preprocessing:** Add steps to handle `Year_of_Release` conversion to numeric and potential NaNs, and ensure all sales columns are numeric before plotting. This is vital for robust code.
2.  **Correct Column Names:** Ensure the `Year` column name is correctly `Year_of_Release` as per the dataset.
3.  **Enhanced Plot Formatting:** Add `plt.grid()` where appropriate (e.g., histogram, scatter plot, line plot) and consider adding units like ""Millions"" to sales labels for clarity, similar to the ground truth.
4.  **Year Filtering:** For the line plot, consider filtering out extreme or illogical `Year_of_Release` values to make the trend more meaningful.",llama-4-maverick,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")

# Perform exploratory data analysis
def exploratory_data_analysis(data):
    # Display the first 10 rows
    print(""First 10 rows of the dataset:"")
    print(data.head(10))

    # Show the shape of the dataset
    print(f""\nShape of the dataset: {data.shape}"")

    # List al...",7,"The generated code provides a good attempt at the requested EDA task. It correctly loads the dataset, displays the head, shape, dtypes, missing values, and summary statistics. However, it misses a few key requirements and has areas for improvement in specificity and robustness compared to the ground truth.

**Strengths:**
*   Correctly loads the CSV and handles potential `FileNotFoundError` (though the error message could be more user-friendly).
*   Correctly implements `head()`, `shape`, `isnull().sum()`, and `describe()`.
*   Separates logic into functions (`load_dataset`, `exploratory_data_analysis`, `main`).

**Areas for Improvement / Missing Requirements:**

1.  **Column Names and Data Types (df.info() vs df.dtypes):** The prompt specifically asked for `df.info()`, which provides more comprehensive output (non-null counts, memory usage) than `df.dtypes`. The generated code uses `data.dtypes`.
2.  **Percentage of Diabetic vs Non-Diabetic Patients:** While it calculates percentages correctly, it doesn't explicitly label which percentage corresponds to ""diabetic"" (1) and ""non-diabetic"" (0) in its output, making it less clear than the ground truth. It also doesn't check if the 'Outcome' column exists before trying to access it, which could lead to an error.
3.  **Identification of Zero Values as Missing:**
    *   The prompt asked to ""Identify any columns with zero values that might represent missing data."" and clarified which columns (`Glucose`, `BloodPressure`, etc.) are typically suspect for this in the diabetes dataset. The generated code simply identifies *all* columns with zero values, which includes 'Pregnancies' and 'Outcome' that can legitimately be zero, diluting the intent of finding *potential missing data*.
    *   The ground truth specifically lists the columns of interest and calculates the percentage of zeros in those columns, which is a stronger indicator of potential missing data than just a raw count.
4.  **Robustness and Error Handling:** The ground truth includes more explicit error handling for file loading and checks for column existence (`TARGET_COLUMN in df.columns`), making it more robust.
5.  **Output Readability:** The ground truth includes print statements with clearer headings and separators (`=""*50""`) which improve the readability of the output for each section of the analysis. The generated code's print statements are less distinct.
6.  **`numpy` import:** While not strictly *needed* for the generated code's implementation, the task specified imports that might be commonly used (like `numpy`), and the ground truth uses it for `np.round`, which is a minor detail.

Overall, it's a functional script, but it misses some nuances and specific elements requested in the prompt, particularly around the zero-value identification and the specific output format for column information.",llama-4-maverick,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

def load_and_preprocess_data(csv_file):
    # Load dataset
    df = pd.read_csv(csv_file)

    # Identify columns with zero values representing missing data
    columns_with_zero_as_missing = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

    # Replace zero values with NaN
    df[columns_with_zero_as_missing] = df[columns_with_zero_as_missing].replace(0, np.nan)

    # Handle miss...",7,"The generated code correctly implements most of the requirements.

**Strong Points:**
- Correctly loads the dataset.
- Correctly identifies and replaces zero values with NaN.
- Correctly fills missing values with the median.
- Correctly removes duplicate rows.
- Correctly creates the 'BMI_Category' column.
- Correctly scales numerical features, excluding 'Outcome'.
- Displays the shape and head of the cleaned dataset.
- Encapsulates the logic in a function, improving reusability.

**Areas for Improvement:**
- **Robustness in File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError`. The generated code assumes the file exists, which could lead to an unhandled error if the file is missing.
- **Improved Missing Value Handling (Median Calculation):** While `df[columns_with_zero_as_missing].fillna(df[columns_with_zero_as_missing].median())` works, it calculates the median for each column across *all* columns in `columns_with_zero_as_missing` and then fills. For a more explicit and potentially robust (though often not practically different for this case) approach, filling each column individually ensures that the median of `Glucose` is used only for `Glucose`, etc., especially if the `median()` call itself is subtly different. The ground truth explicitly iterates and calculates the median per column.
- **BMI Category Bins:** The `bins` for `BMI_Category` in the generated code are `[0, 18.5, 25, 30, np.inf]`. The problem statement specified 'Normal' (18.5-24.9) and 'Overweight' (25-29.9). The generated code's bins would make 'Normal' up to 25 (exclusive) and 'Overweight' up to 30 (exclusive), which is a slight deviation from the specified ranges (24.9 and 29.9). The ground truth used `[0, 18.5, 24.9, 29.9, np.inf]`.
- **Informative Prints:** The ground truth includes more informative `print` statements during execution (e.g., ""Dataset loaded successfully."", ""Removed X duplicate rows."", ""Created 'BMI_Category' column.""), which is helpful for debugging and understanding the flow in a benchmark context. The generated code only prints the final shape and head, which is acceptable but less informative.
- **Handling Non-existent Columns:** The ground truth includes checks like `if col in df.columns:` when iterating through columns to handle missing values or replacing zeros. While the problem statement implies these columns exist, robust code would account for their potential absence. The generated code directly applies operations to `df[columns_with_zero_as_missing]` which would raise a KeyError if any of these columns are missing.

Overall, it's a good attempt that covers most functional requirements but could be more robust and precise in certain details.",llama-4-maverick,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def load_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load data: {e}"")
        return None

def create_correlation_heatmap(data):
    plt.figure(figsize=(10,8))
    sns.heatmap(data.corr(numeric_only=True), annot=True, cmap='coolwarm', square=True)
    plt.title('Correlation Heatmap')
    plt.show()

def plot_histograms(d...",6,"The generated code addresses most of the task requirements but falls short in several areas regarding visualization quality, compliance with specific instructions, and adherence to best practices.

**Strengths:**
*   **Modularity:** The code is well-organized into functions for each visualization type, which improves readability and maintainability.
*   **Correctness (Basic):** It successfully loads data and generates all requested plot types.
*   **Imports:** Includes necessary libraries (pandas, matplotlib, seaborn).

**Areas for Improvement:**
*   **Completeness/Precision:**
    *   **Correlation Heatmap:** The `numeric_only=True` is good, but it should specifically exclude `Outcome` from the correlation on numerical features, as `Outcome` is a categorical target. While `corr()` will treat it as a number, it's generally better to analyze numerical features explicitly. The ground truth explicitly uses `df[numerical_cols]` which correctly isolates only the features to be correlated.
    *   **Histograms:** The `df[numerical_features].hist()` method from pandas is used. While functional, it's less flexible than `sns.histplot` (used in ground truth) for custom styling, KDE, etc., and individual subplot titles are missing. The prompt asked for ""proper titles and labels for all charts.""
    *   **Box Plots:** Each box plot is shown individually (`plt.show()` in a loop). The request implies generating *multiple* box plots, often best presented in a grid of subplots for comparison, similar to histograms in the ground truth. This leads to many separate plot windows popping up which isn't ideal.
    *   **Pie Chart Labels:** The labels for the pie chart are hardcoded as 'Non-Diabetic' and 'Diabetic' without explicitly mapping to the 0 and 1 values. While it might work for this dataset, it's less robust. The ground truth uses `outcome_counts.index` or labels based on `value_counts()` results.
    *   **Scatter Plots:** The legend for `hue='Outcome'` in `create_scatter_plots()` will show '0' and '1' instead of 'Non-Diabetic' and 'Diabetic'. The ground truth explicitly sets the legend labels for better clarity.
    *   **Pair Plot:** The pair plot is generated, but `diag_kind='kde'` and a specific `palette` (as used in the ground truth for better aesthetics) were missed. The `suptitle` also needs to be adjusted in pair plots to avoid overlap with subplots.
*   **Code Quality/Clarity:**
    *   **Titles and Labels:** While titles are present, they are often generic (e.g., ""Correlation Heatmap"") rather than descriptive (""Correlation Heatmap of Numerical Features"") and labels are sometimes missing or generic where the ground truth provides specific, clear labels (e.g., ages, glucose levels).
    *   **Subplot Organization:** As mentioned, histograms and box plots could benefit from being organized into a single figure with multiple subplots for better visual comparison and efficiency. The generated code opens a new figure for every single histogram and box plot.
    *   **`File Not Found` Handling:** The `load_data` function catches a generic `Exception`. While it handles `FileNotFoundError`, it also catches other potential issues. A specific `try-except` for `FileNotFoundError` is often preferred if that's the primary expected error, with a more generic catch afterwards for other issues. The ground truth provides specific feedback for `FileNotFoundError`.
*   **Aesthetics:** The generated plots lack the aesthetic enhancements present in the ground truth, such as `sns.set_style`, specific color palettes, precise figure sizing for readability, and `tight_layout` (which is used in one scatter plot but not consistently). The `figsize` for individual box plots is also very generic.",llama-4-maverick,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        happiness_report = pd.read_csv('world-happiness-report-2021.csv')
    except FileNotFoundError:
        print(""The file world-happiness-report-2021.csv was not found in the current directory."")
        return

    # Display the first 10 rows
    print(""First 10 rows of the dataset:"")
    print(happiness_report.head(10))

    # Show the shape of the dataset
    print(""\nShape of the dataset (rows, columns):"")
    p...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It loads the data, displays initial rows, shows shape, lists column info, handles regional indicators, identifies top countries, calculates summary statistics, and finds missing values, all with proper imports and conditional checks for column existence.

Strong points:
- Uses a `main()` function which is good practice for organizing code.
- Includes `try-except` block for `FileNotFoundError` when loading the CSV, making the script robust.
- Correctly uses `df.head(10)`, `df.shape`, `df.dtypes` (though `df.info()` from ground truth is slightly better for showing non-null counts), `value_counts()`, `nlargest()`, `describe()`, and `isnull().sum()`.
- Implements checks for the existence of columns (`'Regional indicator'`, `'Ladder score'`, `key_happiness_factors`), which prevents errors if the dataset structure changes slightly.
- The use of `nlargest` for top 10 is an efficient and clean way to achieve the goal, similar to `sort_values().head()`.

Areas for slight improvement (minor):
- For ""Column names and their data types,"" `df.info()` as used in the ground truth is often preferred in EDA as it also provides non-null counts and memory usage, which are very useful. `df.dtypes` only shows data types.
- The output formatting is simple but effective. The ground truth added more explicit separators which can make the output clearer in a console context, but this is a stylistic choice.
Overall, a very strong submission that successfully meets the prompt's requirements.",llama-4-maverick,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")

# Handle missing values
def handle_missing_values(data):
    for col in data.columns:
        if data[col].dtype in ['int64', 'float64']:
            data[col] = data[col].fillna(data[col].mean...",8,"The generated code correctly addresses most of the requirements and demonstrates good code organization through the use of functions.

**Strong points:**
*   **Modularity:** Breaking down the tasks into functions (load, handle missing, create features, remove outliers, standardize, encode) improves readability and maintainability.
*   **Missing Value Handling:** Correctly distinguishes between numerical and categorical columns for filling missing values, using `mean` and `mode` respectively.
*   **Derived Features:**
    *   `Happiness_Category`: Uses `pd.cut` which is a clean way to bin numerical data.
    *   `GDP_Category`: Correctly uses `pd.qcut` for quartile-based categorization.
    *   `Well_being_Score`: Calculates the average as requested.
*   **Outlier Removal:** Correctly implements the IQR method for `Ladder score`.
*   **Scaling and Encoding:** Applies `StandardScaler` and `LabelEncoder` as specified.
*   **Output:** Displays the shape and head of the cleaned dataset.

**Areas for Improvement (to reach a higher score):**
*   **File Not Found Handling (Minor):** The `load_dataset` function catches exceptions but only prints an error message. The task explicitly requests exiting the script if the file is not found (as the ground truth does), which is a more robust way to handle a critical dependency before proceeding.
*   **Numerical Features for Scaling (Minor Logic):** The code selects *all* `int64` and `float64` columns for standardization (`numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()`). While this works for the current dataset, it's generally safer to explicitly define which numerical columns should be scaled, especially after creating new categorical numerical features like `Regional indicator` (which becomes numerical after encoding but shouldn't be scaled, although in this specific case, it gets encoded *after* scaling, so it's not scaled). The ground truth implicitly handles this by performing encoding *before* the final print and thus not attempting to scale a label-encoded 'Regional indicator'. The current code scales *all* numerical features, which technically includes the newly created 'Happiness_Category' and 'Well_being_Score', which is correct, but also the 'Regional indicator' if it were numerical *before* encoding. Since `Regional indicator` is encoded *after* scaling, it doesn't get scaled twice. This is a subtle point, but being more explicit about which numerical columns to scale can prevent issues in more complex scenarios.
*   **Docstrings/Comments (General):** While not explicitly asked for, adding docstrings to the functions would further improve code quality and maintainability, especially for a library of utility functions.",llama-4-maverick,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np

# Load the dataset
df = pd.read_csv('world-happiness-report-2021.csv')

# Create a world map visualization showing happiness scores by country
fig = px.choropleth(df, locations=""Country name"", locationmode='country names', 
                    color=""Ladder score"", hover_name=""Country name"", 
                    color_continuous_scale=px.colors.sequential.Plasma)
fi...",6,"The generated code addresses most of the visualization requirements, but several visualizations lack proper formatting, readability, and adherence to best practices.

**Strengths:**
*   **Correctness:** All requested visualizations are attempted and generally produce the correct type of plot.
*   **Completeness:** All 7 types of visualizations are present.
*   **Imports:** Proper libraries are imported.
*   **Renaming:** While not explicit, the code uses the original column names which implicitly handles the data.

**Areas for Improvement:**

1.  **Readability/Consistency:** The ground truth code renames columns for better readability and consistency across plots (e.g., 'Ladder score' to 'Happiness Score', 'Logged GDP per capita' to 'GDP per Capita'). The generated code uses the original column names which can be less intuitive in plot titles and labels.
2.  **World Map:**
    *   The `update_layout` for the map is too minimal. The ground truth adds `geo=dict(showframe=False, showcoastlines=False, projection_type='equirectangular')` for a cleaner map appearance.
    *   The title is very basic (`Happiness Score by Country`) compared to the ground truth's more descriptive one (`World Happiness Score Map (2021)`).
3.  **Horizontal Bar Chart:**
    *   The sorting for the horizontal bar chart in the generated code is `ascending=True`, which is less intuitive for average scores. A common practice is to sort `descending` for better visual comparison of highest to lowest.
    *   The labels for the x-axis and y-axis could be more descriptive (e.g., ""Average Ladder Score"" vs. ""Average Happiness Score"").
4.  **Correlation Heatmap:** The ground truth sets `linewidths` and `linecolor` for better separation of cells in the heatmap, which improves readability.
5.  **Scatter Plots:**
    *   The generated scatter plots use simple scatter plots without distinguishing categories (like 'Regional indicator' as done in the ground truth). Adding a `hue` for 'Regional indicator' would make these plots much more insightful.
    *   The ground truth also sets a `grid` and `alpha` for better aesthetics.
6.  **Box Plots:**
    *   The `xticks` rotation is 90 degrees, which can make labels harder to read than a 45-degree rotation, especially if labels are long. The ground truth uses 45 degrees with `ha='right'`.
    *   The ground truth provides a `palette` for better visual distinction.
7.  **Radar Chart:** This is the most complex plot and has significant issues in the generated code:
    *   **Scaling:** The most critical issue is the lack of feature scaling (normalization) for the radar chart. Without scaling, features with larger inherent values (like 'Logged GDP per capita') will dominate the chart's shape, making comparison unfair. The ground truth uses `MinMaxScaler` which is crucial here.
    *   **Data Structure:** The loop iterates over `top_5` country names and then filters the DataFrame inside the loop. It would be more efficient to select the top 5 rows and then iterate over those rows.
    *   **Labels/Grid:** The `set_thetagrids` is correct for labels but without normalization, the radial grid lines (`yticks` in the ground truth) don't represent a meaningful scale. The ground truth sets `ax.set_ylim(0, 1)` and specific `yticks` because the data is normalized to 0-1.
    *   **Completeness:** The ground truth adds a fill to the radar chart lines (`ax.fill`), which visually distinguishes each country's profile better.
    *   **Missing `math` import:** The `math` module is not imported in the generated code, which would be needed if more advanced calculations (like `math.pi`) were used directly as in the ground truth for angles. However, `np.pi` from numpy is also available.

The code is functional and produces plots, but it misses several opportunities for better data representation, interpretability, and adhering to common data visualization best practices that the ground truth demonstrates. The lack of scaling in the radar chart is a major omission for that specific visualization's correctness.",llama-4-maverick,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

def main():
    # Load the dataset and parse the 'date' column as datetime objects
    data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

    # Display the first 10 rows
    print(""First 10 rows:"")
    print(data.head(10))

    # Show the shape of the dataset (rows and columns)
    print(""\nDataset shape:"")
    print(data.shape)

    # List all column names and their data types
    print(""\nColumn names and data types:"")
    print(data.dtypes)
...",7,"The generated code addresses most of the requirements and demonstrates good understanding of pandas operations.

**Strong Points:**
*   Correctly loads data and parses dates.
*   Successfully displays head, shape, dtypes.
*   Successfully calculates continental summaries.
*   Correctly calculates summary statistics.
*   Correctly counts missing values.
*   Uses a `main` function and `if __name__ == ""__main__"":` which is good practice.

**Areas for Improvement (Why not a 9 or 10):**
1.  **Date parsing for `idxmax()` in Top 5 Locations:** The logic `latest_data = data.loc[data.groupby('location')['date'].idxmax()]` to find the latest data per location is concise and often works, but it implicitly relies on `date` being in chronological order within each location group or `idxmax()` correctly identifying the true maximum index even if dates are not sorted. The ground truth's approach of `sort_values(by=['location', 'date'])` followed by `drop_duplicates(subset=['location'], keep='last')` is more robust and explicitly guarantees the latest date for each location is selected, regardless of the initial row order for the same location. While `idxmax()` often behaves as `max()` for datetime objects, having explicit sorting for `drop_duplicates` makes it clearer and more reliable.

2.  **Handling of `df.info()` vs `df.dtypes`:** The task specifically asked for ""List all column names and their data types."" While `data.dtypes` provides the data types, `df.info()` (as used in the ground truth) is generally preferred in exploratory data analysis as it additionally provides non-null counts and memory usage, which are very useful for understanding data quality and resource consumption. Using `df.info()` would have been a more comprehensive and better demonstration of EDA best practices for this point.

3.  **Robustness (Error Handling/Column existence checks):** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, and it doesn't include checks to ensure columns like 'continent', 'total_cases', 'new_cases', etc., actually exist before attempting operations on them. The ground truth includes these checks, which makes it more robust and production-ready for slightly varying datasets.

4.  **Implicit type conversion for `total_cases` in Top 5 Locations:** The generated code directly uses `nlargest(5, 'total_cases')`. If `total_cases` were not numeric (e.g., if loaded as a string or had non-numeric characters), this line would fail or produce incorrect results. The ground truth explicitly uses `pd.to_numeric` with `errors='coerce'` to handle potential non-numeric values, making it more robust.

5.  **Output formatting for Missing Values:** While `missing_values = data.isnull().sum().reset_index()` works, simply `data.isnull().sum()` (as in the ground truth) is more idiomatic and clear for displaying missing value counts per column. `reset_index()` reshapes it into a DataFrame, which is okay, but less direct for just listing column-wise sums.",llama-4-maverick,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_wine_quality():
    # Load the dataset
    df = pd.read_csv('winequality-red.csv')

    # Calculate and report the average 'quality' of the red wines
    avg_quality = df['quality'].mean()
    print(f""Average quality of red wines: {avg_quality:.2f}"")

    # Group the wines by their 'quality' score and calculate the average 'alcohol' content for each quality group
    quality_alcohol_avg = df.groupby(...",8,"The generated code correctly addresses all three main requirements of the task: calculating the mean quality, grouping by quality to find average alcohol content, and creating a bar chart visualization. It uses appropriate libraries (pandas, matplotlib, seaborn).

**Strengths:**
*   Correctly calculates the average quality.
*   Correctly groups by 'quality' and finds the mean 'alcohol' content.
*   Successfully generates a bar chart with appropriate labels and title.
*   Encapsulates the logic within a function and uses `if __name__ == ""__main__"":`, which is good practice.

**Areas for Improvement:**
*   **Robustness:** The generated code lacks error handling for `FileNotFoundError` when reading the CSV, which is present in the ground truth and is good practice for real-world scenarios.
*   **Output Clarity:** The ground truth provides more detailed print statements for each step's output (e.g., ""1. Average 'quality'..."", ""2. Average 'alcohol'...""). The generated code only prints the average quality.
*   **Plot Aesthetics/Information:** While the plot is correct, the ground truth's plot is more polished with `sns.set_style`, data labels on bars, and `plt.tight_layout`, which enhances readability and informativeness. These are minor but contribute to a more complete and professional visualization. The `palette='viridis'` in `sns.barplot` also adds a nice touch.",llama-4-maverick,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

def clean_covid_data(file_name):
    # Load the dataset and parse the 'date' column as datetime objects
    df = pd.read_csv(file_name, parse_dates=['date'])

    # Handle missing values in 'hosp_patients' and 'icu_patients' by filling them with 0
    df[['hosp_patients', 'icu_patients']] = df[['hosp_patients', 'icu_patients']].fillna(0)

    # Fill missing values in other numerical columns with their respective mean
    numerical_cols = df.selec...",7,"The generated code correctly addresses most of the requirements but has a few issues that prevent it from being perfect. The use of a function to encapsulate the cleaning logic is good for reusability.

**Strong Points:**
*   Correctly loads data and parses dates using `parse_dates`.
*   Handles missing values for `hosp_patients` and `icu_patients` by filling with 0.
*   Correctly calculates `mortality_rate` and `vaccination_rate_per_capita`.
*   Handles `NaN` and `inf` values for calculated columns by replacing them with 0.
*   Filters the dataset for the year 2021.
*   Displays the shape and head of the cleaned data.

**Areas for Improvement/Corrections:**
1.  **Handling Missing Values for Other Numerical Columns:** The prompt specifically asked to fill 'positive\_rate' and 'reproduction\_rate' with their respective means. The generated code attempts to fill *all* other numerical columns with their mean (`numerical_cols = df.select_dtypes(include=[np.number]).columns`). While this might work for these specific columns if they are numerical, it's a generalization that wasn't explicitly requested and could lead to unintended behavior if other numerical columns are not meant to be imputed this way (e.g., ID columns, or columns that should have 0s). The ground truth specifically targets the columns requested.
2.  **Displaying Output:** The task explicitly asked to ""Display the shape of the cleaned dataset and the first 5 rows with the *new columns*"". The generated code `print(cleaned_df[['date', 'mortality_rate', 'vaccination_rate_per_capita']].head())` only shows a subset of columns, not the first 5 rows of the entire DataFrame with the new columns.
3.  **Robustness for `replace` method:** While `df['mortality_rate'].replace([np.inf, np.nan], 0)` works for `nan`, the `replace` method is generally used for direct value replacement. For `NaN` values, `fillna(0)` is typically more explicit and idiomatic. The ground truth uses both `replace([np.inf, -np.inf], 0)` and `fillna(0)`. The generated code only uses `np.inf` and misses `-np.inf`, which could occur in some division scenarios.
4.  **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code doesn't include this.",llama-4-maverick,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def load_and_parse_data(file_name):
    data = pd.read_csv(file_name)
    data['date'] = pd.to_datetime(data['date'])
    return data

def plot_new_cases_over_time(data, location):
    location_data = data[data['location'] == location]
    plt.figure(figsize=(10,6))
    plt.plot(location_data['date'], location_data['new_cases'])
    plt.title(f'New Cases Over Time in {location}')
    plt.xlabel('Date')
    plt.y...",6,"The generated code correctly addresses most of the plotting requirements. However, there are several areas where it deviates from the instructions or has subtle correctness issues compared to the ground truth.

**Positives:**
*   **Modularity:** The use of functions for each plot makes the code well-organized and readable.
*   **Basic Plot Types:** It creates all the requested plot types (line, bar, scatter, histogram, box plot).
*   **Imports:** Proper imports are included.
*   **Titles and Labels:** Basic titles and labels are present.

**Areas for Improvement/Corrections:**
1.  **File Not Found Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which the ground truth gracefully handles.
2.  **Time Series Plot (India New Cases):**
    *   It uses `plt.plot` instead of `sns.lineplot`. While `plt.plot` works, `sns.lineplot` generally offers more aesthetic default styling and better integration with Pandas DataFrames.
    *   Missing `plt.grid(True)` and `plt.tight_layout()`.
3.  **Bar Chart (Total Deaths by Continent):**
    *   **Crucial Logic Error:** The *ground truth* correctly identifies that for `total_deaths` per continent, one should sum the *latest* `total_deaths` value *for each country* (location) within that continent. The generated code simply groups by `continent` and sums `total_deaths` across all entries for that continent. This will *greatly overestimate* deaths, as it sums up cumulative deaths over time for each country. This is a significant logical flaw for a ""total deaths"" summary.
    *   The x-axis labels are rotated to 90 degrees, which can be excessive; 45 degrees (as in ground truth) or careful layout is often better.
    *   Missing `plt.grid(axis='y')` and `plt.tight_layout()`.
4.  **Scatter Plot (GDP vs. Vaccinations):**
    *   **Logic Error:** While it correctly attempts to get the `latest_data` by using `groupby('location').tail(1)`, it does *not* handle `NaN` values for `gdp_per_capita` or `total_vaccinations`. If these columns contain NaNs, `scatterplot` might drop them implicitly or raise errors, but explicit `dropna` as in the ground truth is safer and clearer.
    *   The ground truth also uses `hue='continent'` and `size='population'` to add more information, which is a good practice for such plots and was not included in the generated code.
    *   Missing `plt.xscale('log')` and `plt.yscale('log')`, which are very important for these types of skewed economic/cumulative data to show relationships more clearly.
    *   Missing `plt.grid(True)` and proper legend handling.
5.  **Histogram (Reproduction Rate):**
    *   Doesn't explicitly handle `NaN` values in `reproduction_rate` before plotting. `sns.histplot` often handles NaNs by default by dropping them, but explicit `dropna()` as in the ground truth is robust.
    *   Missing `plt.grid(axis='y')` and `plt.tight_layout()`.
6.  **Box Plot (Stringency Index by Continent):**
    *   Doesn't explicitly handle `NaN` values in `stringency_index` before plotting.
    *   The x-axis labels are rotated to 90 degrees, which could be less readable than 45 degrees.
    *   Missing `plt.grid(axis='y')` and `plt.tight_layout()`.
7.  **Plot Aesthetics:** The generated code generally lacks the professional styling and specific color palettes (`palette='viridis'`, `palette='deep'`, `palette='pastel'`) that `seaborn` offers and that the ground truth utilizes to make the plots more visually appealing and informative. The prompt asked for ""proper titles and labels"" and the ground truth used `seaborn`'s capabilities to enhance these.

The most critical issue is the `total_deaths` calculation oversight, which fundamentally misrepresents the data in that specific plot. The lack of robust `NaN` handling and missing stylistic details also slightly detracts from a perfect score.",llama-4-maverick,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

def main():
    try:
        # Load the dataset
        cards_df = pd.read_csv('clash_royale_cards.csv')
        
        # List all column names and their data types
        print(""Column Names and Data Types:"")
        print(cards_df.dtypes)
        
        # Identify numerical columns of interest
        numerical_columns = ['elixirCost', 'Win Rate', 'Usage']
        
        # Check if the specified numerical columns exist in the DataFrame
        numerical_co...",8,"The generated code successfully addresses most of the task requirements.

**Strong Points:**
*   **Correct Loading and Error Handling:** Properly loads the CSV and includes robust `try-except` blocks for `FileNotFoundError`, `EmptyDataError`, and `ParserError`, which is good practice.
*   **Column Data Types:** Correctly identifies and prints column data types using `df.dtypes`.
*   **Descriptive Statistics:** Calculates descriptive statistics for the specified numerical columns. The use of `.describe()` is concise and provides the requested statistics (mean, std, min, max, and also includes quartiles, which is more than requested but not detrimental).
*   **Total Rows:** Correctly reports the total number of rows using `len(cards_df)`.
*   **Modularity:** Encapsulates the logic within a `main()` function, which is good for larger scripts.

**Areas for Improvement (Why not a 9 or 10):**
*   **Specific Statistics and Format:** The prompt specifically asked for ""mean, median, standard deviation, min, max"". While `.describe()` provides these (except median directly in the output name, though it's easily derivable from 50%), it also includes quartiles (25%, 50%, 75%) which weren't explicitly requested. The ground truth's use of `.agg(['mean', 'median', 'std', 'min', 'max'])` was more precise in matching *only* the requested statistics and would explicitly label the median.
*   **Column Name Listing Clarity:** The ground truth explicitly iterated and printed each column name for better readability, and then printed the data types separately. The generated code directly prints `cards_df.dtypes`, which shows both, but doesn't *list* the column names as a distinct step as clearly as the ground truth. The prompt asked to ""List all column names *and* identify the data type of each column."" The generated code combines them, which is functional but not as distinct as specified.
*   **Output Formatting:** The ground truth code used clear headings (`--- Column Names and Data Types ---`, `--- Descriptive Statistics for Numerical Columns ---`, etc.) for better readability. The generated code has headings, but they are less prominent.",llama-4-maverick,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

def main():
    try:
        # Load the dataset
        cards_df = pd.read_csv('clash_royale_cards.csv')

        # Filter the dataset
        filtered_df = cards_df[cards_df['elixirCost'] <= 3]

        # Select and display the required columns, sorted by 'Win Rate'
        result_df = filtered_df[['Card', 'Win Rate']].sort_values(by='Win Rate', ascending=False)

        print(result_df)
    except FileNotFoundError:
        print(""The 'clash_royale_cards.csv' fil...",9,"The generated code correctly performs all the required tasks: loading the CSV, filtering by `elixirCost` <= 3, selecting the `Card` and `Win Rate` columns, and sorting by `Win Rate` in descending order. It also includes proper imports and basic error handling for `FileNotFoundError`. The code is concise and efficient.

However, it could be slightly improved by:
1.  Using named constants for column names (`elixirCost`, `Card`, `Win Rate`) and the filename, similar to the ground truth. This improves readability and maintainability.
2.  Although wrapping the logic in `main()` and guarding with `if __name__ == ""__main__"":` is good practice for scripts, it's not strictly necessary for this specific benchmark where only the standalone code execution is evaluated. It doesn't detract from correctness but adds a tiny bit of boilerplate.

Despite these minor points, the code is very good and directly addresses the prompt.",llama-4-maverick,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    # Load the dataset
    try:
        cards_df = pd.read_csv('clash_royale_cards.csv')
    except FileNotFoundError:
        print(""The 'clash_royale_cards.csv' file was not found in the current directory."")
        return

    # Check if 'elixirCost' column exists
    if 'elixirCost' not in cards_df.columns:
        print(""'elixirCost' column not found in the dataset."")
        return

    # Gener...",8,"The generated code correctly loads the CSV, generates a histogram for `elixirCost`, and includes a title and labeled axes, fulfilling the primary requirements. The use of `try-except` for file loading is good practice.

Areas for improvement to reach a higher score:
1.  **Histogram bins:** The ground truth uses `bins=range(1, 12)` which creates distinct bars for each integer elixir cost. The generated code defaults to an automatic binning, which might group discrete elixir costs, making the visualization less precise for integer-based data like elixir cost.
2.  **KDE:** The generated code includes `kde=True` by default (or explicitly passed) which plots a Kernel Density Estimate. While not strictly incorrect, for discrete integer values like elixir cost, `kde=False` (as in the ground truth) often provides a clearer representation of the actual counts per discrete value.
3.  **Error messages and exit strategy:** The ground truth provides more specific error messages (e.g., listing available columns if `elixirCost` is missing) and uses `exit()` to clearly terminate the script after a critical error, which is often preferable for standalone scripts. The generated code returns from `main()` which stops further execution within the function but doesn't explicitly terminate the script.
4.  **Figsize and general plot aesthetics:** While the `figsize` is set, the ground truth's use of `color`, `edgecolor`, `xticks`, `yticks`, `fontsize`, and `grid` makes the plot more visually appealing and readable, which could be considered part of ""proper imports"" and ""completeness"" in a broader sense for data visualization tasks.",llama-4-maverick,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

def main():
    # Load the iris dataset
    try:
        iris_df = pd.read_csv('iris.csv')
    except FileNotFoundError:
        print(""The 'iris.csv' file was not found in the current directory."")
        return

    # List all column names and identify the data type of each column
    print(""Column Names and Data Types:"")
    print(iris_df.dtypes)

    # Calculate and report basic descriptive statistics for numerical columns
    numerical_columns = ['SepalLengthC...",8,"The generated code correctly addresses all parts of the task: loading the CSV, identifying data types, calculating descriptive statistics for the specified numerical columns, and reporting unique species counts. It uses `pandas.read_csv`, `df.dtypes`, `df.describe()`, and `df.value_counts()` effectively. The inclusion of a `main` function and a `if __name__ == ""__main__"":` block indicates good Python practice.

However, there are a couple of minor points for improvement compared to the ground truth:

1.  **Column Names Listing:** While `print(iris_df.dtypes)` shows the data types and implicitly lists the column names, it doesn't explicitly print just the column names as requested in ""List all column names"". The ground truth uses `df.columns.tolist()` for this, which is a more direct fulfillment of that specific part of the requirement.
2.  **Robustness for Missing Columns:** The generated code directly indexes `iris_df[numerical_columns]` and `iris_df['Species']`. If any of these columns were missing or misspelled in the actual `iris.csv` file, this would raise a `KeyError`. The ground truth includes checks (`if col in df.columns` and `if species_column_name in df.columns`) to handle such edge cases gracefully, printing warnings or errors rather than throwing an exception. While the prompt assumes a well-formed `iris.csv`, robust code prepares for real-world data issues.
3.  **Specific Descriptive Statistics:** The prompt asked for ""mean, median, standard deviation, min, max"". `describe()` provides these along with count, 25%, 50% (median), and 75% quartiles. While it covers the requested statistics, `describe()` also includes extra information not explicitly asked for. The ground truth's use of `.agg(['mean', 'median', 'std', 'min', 'max'])` is slightly more precise to the exact requirements, although `describe()` is a common and acceptable shortcut.

Overall, the code is highly functional and provides the correct output for the given task and data, making it a strong solution. The areas for improvement are mainly related to defensive programming and strict adherence to minor phrasing in the request.",llama-4-maverick,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

def main():
    try:
        # Load the iris dataset
        iris_df = pd.read_csv('iris.csv')

        # Filter the dataset to include only 'Iris-setosa'
        setosa_df = iris_df[iris_df['Species'] == 'Iris-setosa']

        # Select and display 'SepalLengthCm' and 'SepalWidthCm' columns
        result_df = setosa_df[['SepalLengthCm', 'SepalWidthCm']]
        print(result_df)

    except FileNotFoundError:
        print(""The 'iris.csv' file was not found in the...",9,"The generated code correctly solves all aspects of the task: it loads the CSV, filters for 'Iris-setosa' species, and selects/displays the specified sepal columns. It includes proper imports and handles `FileNotFoundError`. The code is well-structured and easy to read.

The strong points are:
- Correct implementation of all task requirements.
- Uses standard pandas operations effectively.
- Includes basic error handling for `FileNotFoundError`.
- Encapsulates logic within a `main` function and uses `if __name__ == ""__main__"":` which is good practice.

An area for minor improvement, which prevents a perfect score, is the lack of more robust error handling (e.g., for `KeyError` if columns are missing, or general exceptions as seen in the ground truth). While not strictly required by the prompt's explicit criteria, it's a good practice for production-ready code. However, given the prompt's explicit requirements, the code fully satisfies them.",llama-4-maverick,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def main():
    # Load the iris dataset
    iris_df = pd.read_csv('iris.csv')

    # Create a scatter plot
    plt.figure(figsize=(8, 6))
    species_list = iris_df['Species'].unique()
    for species in species_list:
        species_df = iris_df[iris_df['Species'] == species]
        plt.scatter(species_df['SepalLengthCm'], species_df['SepalWidthCm'], label=species)

    # Customize the plot
    plt.title('Sepal Length vs Sepal Widt...",8,"The generated code correctly addresses the core requirements of loading the dataset, creating a scatter plot of `SepalLengthCm` vs `SepalWidthCm`, coloring points by `Species`, and including a title and axis labels. All necessary imports are present.

Areas for improvement to achieve a higher score:
1.  **X-axis and Y-axis label text**: The ground truth uses `SepalLengthCm` and `SepalWidthCm` directly for the labels, which matches the column names and is generally preferred for clarity in data science contexts unless specified otherwise. The generated code uses ""Sepal Length (cm)"" and ""Sepal Width (cm)"", which is also acceptable but slightly deviates from the exact column name labels in the ground truth. This is a minor point.
2.  **Robustness**: The ground truth includes `try-except` blocks for `FileNotFoundError` and `KeyError`, making it more robust. While not explicitly requested in the prompt, good practice in data science code often involves handling potential issues like missing files or columns.
3.  **Aesthetics (minor)**: The ground truth includes `alpha`, `edgecolors`, `linewidth` for scatter points, and a `grid` for better aesthetics and readability, which were not strictly required but enhance the plot. It also defines `DATASET_FILE` as a constant.",llama-4-maverick,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        heart_data = pd.read_csv('heart.csv')
    except FileNotFoundError:
        print(""The file 'heart.csv' was not found."")
        return

    # List column names and data types
    print(""Column Names and Data Types:"")
    print(heart_data.dtypes)

    # Numerical columns to analyze
    numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']

    # Calculate descriptive statistics for numerical columns
   ...",8,"The generated code successfully loads the dataset, lists column data types, calculates descriptive statistics for the specified numerical columns, and reports value counts for 'HeartDisease'.

Strong points:
- Proper use of pandas for data loading and manipulation.
- Correctly identifies data types using `.dtypes`.
- Adequately calculates and presents descriptive statistics for the specified columns.
- Correctly uses `.value_counts()` for the 'HeartDisease' column.
- Includes basic error handling for `FileNotFoundError`.
- Encapsulates the logic within a `main` function, which is good practice.

Areas for improvement:
- **Column names listing:** The prompt asked to ""List all column names and identify the data type of each column."" The generated code only prints `heart_data.dtypes`, which implicitly lists column names with their types. While `dtypes` is sufficient for identifying data types, explicitly listing just the names as requested by ""List all column names"" (e.g., `print(heart_data.columns.tolist())`) would make it fully compliant with this specific part of the requirement. The ground truth explicitly iterates and prints each column name, which is a more direct interpretation of ""List all column names"".
- **Descriptive Statistics Output Format:** The prompt asks to ""calculate and report basic descriptive statistics (mean, median, standard deviation, min, max)."" The generated code iterates through each numerical column and prints stats individually. While correct, using `df[numerical_columns].describe().loc[['mean', '50%', 'std', 'min', 'max']]` as in the ground truth often provides a more concise and readable table for multiple columns, especially if more statistics were requested. The explicit formatting with `:.2f` is a nice touch for readability, but the overall structure could be improved.
- **Error Handling:** Only `FileNotFoundError` is explicitly handled. While sufficient for the given constraints, the ground truth includes a more general `Exception as e` catch, which is generally good practice for robust applications.
- **Clarity of `HeartDisease` output:** The ground truth adds ""Interpretation: 0 = No Heart Disease, 1 = Heart Disease"", which is helpful context for the stakeholder reading the output. This was not explicitly requested but adds value.",llama-4-maverick,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

def main():
    try:
        # Load the dataset
        heart_data = pd.read_csv('heart.csv')

        # Filter the dataset
        filtered_data = heart_data[(heart_data['Age'] > 50) & (heart_data['HeartDisease'] == 1)]

        # Select relevant columns
        relevant_columns = filtered_data[['Age', 'Sex', 'ChestPainType', 'Cholesterol']]

        # Display the result
        print(relevant_columns)

    except FileNotFoundError:
        print(""The 'heart.csv' ...",9,"The generated code correctly loads the dataset, filters it based on the specified age and HeartDisease criteria, and then selects and displays the required columns. It also includes proper imports and basic error handling for `FileNotFoundError`. The code is clean, concise, and directly addresses all parts of the task.

One minor area for improvement, which is present in the ground truth but not strictly required by the prompt, would be to encapsulate the logic within a function that takes the file path as an argument. This makes the code more reusable and testable. Also, the `print(relevant_columns)` line implicitly handles the ""display"" requirement, but explicitly adding a descriptive print statement like ""Filtered data:"" before printing, similar to the ground truth, would make the output clearer to the user. These are minor points that don't detract significantly from its correctness or quality for the given task.",llama-4-maverick,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

def main():
    try:
        # Load the dataset
        data = pd.read_csv('winequality-red.csv')

        # Filter wines based on 'alcohol' and 'pH'
        filtered_data = data[(data['alcohol'] > 12) & (data['pH'] < 3.3)]

        # Check if filtered data is not empty
        if not filtered_data.empty:
            # Find the wine with the highest 'quality' score
            best_wine = filtered_data.loc[filtered_data['quality'].idxmax()]

            # Display a...",9,"The generated code correctly implements all the functional requirements of the task:
1. It correctly filters wines based on a compound condition (`alcohol > 12` AND `pH < 3.3`).
2. It correctly identifies the wine with the highest quality within the filtered subset using `idxmax()`.
3. It displays all features of the identified wine.

The code also includes proper imports and basic error handling for `FileNotFoundError`, `EmptyDataError`, and `ParserError`.

Areas for minor improvement (not affecting core correctness, hence a high score):
- The `main` function is a good structure, but could benefit from an explicit `dataset_path` parameter to make it more reusable, similar to the ground truth's `analyze_wine_quality` function.
- The output format for displaying the wine (`print(best_wine)`) is functional, but `print(best_wine.to_string())` (as used in the ground truth) can be slightly better for ensuring all features are displayed without truncation, especially in some environments.
- The ground truth includes more specific error handling for `KeyError` which could occur if a required column is missing, making it slightly more robust.
- The ground truth provides more verbose print statements for progress tracking and filtering results, which can be helpful for debugging or user understanding.",llama-4-maverick,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    # Load the dataset
    heart_data = pd.read_csv('heart.csv')

    # Generate histogram for Age
    plt.figure(figsize=(8, 6))
    sns.histplot(heart_data['Age'], kde=True)
    plt.title('Distribution of Age')
    plt.xlabel('Age')
    plt.ylabel('Frequency')
    plt.show()

    # Generate bar chart for Sex
    sex_counts = heart_data['Sex'].value_counts()
    plt.figure(figsize=(8, 6))
    sns.ba...",8,"The generated code correctly loads the dataset, generates a histogram for 'Age', and a bar chart for 'Sex'. It properly imports necessary libraries and sets titles and labels for the plots, addressing most requirements.

**Strong Points:**
*   Correctly loads the CSV using pandas.
*   Successfully generates a histogram for `Age` with a title and labeled axes.
*   Successfully generates a bar chart for `Sex` with a title and labeled axes.
*   Uses `matplotlib.pyplot` and `seaborn` effectively.
*   Encapsulates the logic within a `main` function and uses `if __name__ == ""__main__"":`.

**Areas for Improvement:**
*   **Sex Labeling:** The `Sex` column in `heart.csv` typically uses numerical encoding (0 for Female, 1 for Male). The generated code plots 0 and 1 directly on the x-axis for the bar chart. The ground truth code maps these numerical values to 'Female' and 'Male' strings, which is more descriptive and user-friendly for visualization. This is a common practice for interpretability.
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError`, `KeyError`, and other potential exceptions, which is crucial for real-world applications. The generated code lacks this.
*   **Aesthetics/Detailing:** The ground truth code includes more detailed plotting enhancements like `figsize`, `kde=True` (which the generated code also does for age, but could be consistent), `color`, `grid`, `tight_layout`, and `rotation` for x-axis labels (though not strictly necessary for 0/1, it's good practice for general bar charts). While not strictly required by the prompt, these improve readability and robustness. The labels for the generated code (`Age`, `Frequency`, `Sex`, `Count`) are functional but less descriptive than those in the ground truth (`Age (Years)`, `Number of Individuals`).",llama-4-maverick,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        spotify_df = pd.read_csv('spotify_2023.csv')
    except FileNotFoundError:
        print(""The file 'spotify_2023.csv' was not found."")
        return

    # List column names and data types
    print(""Column Names and Data Types:"")
    print(spotify_df.dtypes)

    # Convert 'streams' column to numeric and handle errors
    spotify_df['streams'] = pd.to_numeric(spotify_df['streams'], errors='coerce')
    print(""\n...",9,"The generated code is largely correct and addresses most of the requirements. It successfully loads the data, converts the 'streams' column, reports NaNs, calculates descriptive statistics, and counts unique artists.

**Strong Points:**
- Correctly loads the CSV and handles `FileNotFoundError`.
- Correctly converts the `streams` column to numeric using `errors='coerce'` and reports `NaN` count.
- Calculates descriptive statistics for the specified numerical columns.
- Correctly counts unique artists.
- Uses `if __name__ == ""__main__"":` block for good practice.
- Checks for column existence before operating on them, which is robust.

**Areas for Improvement (to reach a perfect 10):**
- **Descriptive Statistics Detail:** The prompt specifically asked for ""mean, median, std, min, max"". The generated code uses `.describe()` which outputs these, but it also includes 'count', '25%', and '75%'. While not incorrect, the ground truth explicitly filters for only the requested statistics, which is a more precise match to the prompt's wording.
- **Reporting NaNs in `streams`:** The ground truth provides more context around the `streams` conversion, showing the original and new dtypes, and then a small sample of rows with NaNs if they exist, which can be very helpful for debugging or understanding data quality issues. The generated code only prints the count.
- **Column Dtypes Output:** The ground truth used `df.info()` which gives a more comprehensive summary of the DataFrame, including non-null counts per column which is useful, whereas `df.dtypes` just gives the data type. While `df.dtypes` fulfills ""identify the data type of each"", `df.info()` is generally more informative.

Overall, a very good solution that successfully implements the core requirements. The suggested improvements are minor refinements for completeness and clarity.",llama-4-maverick,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

def load_and_process_data(file_name):
    try:
        # Load the dataset
        data = pd.read_csv(file_name)
        
        # Filter the dataset to include only songs released in 2023
        data_2023 = data[data['released_year'] == 2023]
        
        # Select the required columns
        selected_data = data_2023[['track_name', 'artist(s)_name', 'streams']]
        
        # Ensure 'streams' column is numeric
        selected_data['streams'] = pd.to_num...",9,"The generated code correctly implements all requirements: loading the CSV, filtering by `released_year` (2023), selecting the specified columns, coercing `streams` to numeric, and sorting in descending order. The code is well-structured within functions and handles `FileNotFoundError`.

One minor area for improvement is the potential `SettingWithCopyWarning` in `selected_data['streams'] = pd.to_numeric(selected_data['streams'], errors='coerce')`. While not always an error, explicitly using `.loc` for assignment after selection can prevent this warning and ensure modifications are made on a copy rather than a view of the original DataFrame. For instance, `selected_data = selected_data.copy()` before the conversion, or performing the `to_numeric` conversion earlier on the `df` before slicing. However, for a simple script, this is a minor point. The ground truth explicitly handles `SettingWithCopyWarning` by assigning the result of selection to a new variable. It also performs the `dropna` operation which the generated code implicitly skips, but the prompt did not strictly require dropping NaNs, so this is not a correctness issue. The ground truth's early conversion of 'streams' to numeric is a good practice for robustness.",llama-4-maverick,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    data = pd.read_csv('spotify_2023.csv')

    # Generate a histogram for the 'bpm' column
    plt.figure(figsize=(8,6))
    plt.hist(data['bpm'], bins=20, edgecolor='black')
    plt.title('Distribution of BPM')
    plt.xlabel('Beats Per Minute (BPM)')
    plt.ylabel('Frequency')
    plt.show()

    # Create a scatter plot for 'danceability_%' vs 'energy_%'
    plt.figure(figsize=(8,6))
    plt.sca...",9,"The generated code correctly addresses all the requirements of the task. It loads the CSV, generates a histogram for `bpm`, and a scatter plot for `danceability_%` vs `energy_%`. Both plots have appropriate titles and axis labels, and proper imports are included. The use of `plt.show()` after each plot is a minor stylistic choice; usually, one `plt.show()` at the end is sufficient to display all created figures. Also, adding `try-except` blocks for file handling or column existence would improve robustness, similar to the ground truth. However, for the given basic task description, the code is highly effective and correct.",llama-4-maverick,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        df = pd.read_csv('youtube_trendings.csv')
    except FileNotFoundError:
        print(""The file 'youtube_trendings.csv' was not found in the current directory."")
        return

    # List column names and their inferred data types
    print(""Column Names and Data Types:"")
    print(df.dtypes)

    # Clean 'Video views', 'Likes', 'Dislikes' columns
    columns_to_clean = ['Video views', 'Likes', 'Dislikes']
    fo...",8,"The generated code successfully addresses most of the task requirements with good correctness and reasonable code quality.

**Strong points:**
*   **Correctness:** It correctly loads the dataset, cleans the specified numeric columns by removing commas, converting to numeric, and filling NaNs with 0. It also correctly fills missing 'Category' values and reports unique category counts.
*   **Conciseness:** The cleaning logic for numeric columns is more concise than the ground truth while achieving the same result.
*   **Structure:** Encapsulating the logic within a `main` function is good practice.

**Areas for improvement:**
*   **Data type reporting (Task 1):** While `print(df.dtypes)` shows the types, the ground truth's use of `df.info()` provides a more comprehensive summary, including non-null counts, which is often more useful for understanding data quality at a glance. The prompt asked to ""List all column names and their inferred data types"", and `df.info()` is generally a more complete way to satisfy this than just `df.dtypes`.
*   **Descriptive Statistics detail (Task 3):** The prompt specifically asked for ""mean, median, std, min, max"". While `df[columns_to_clean].describe()` outputs these, it also outputs other statistics (count, 25%, 50%, 75% quartiles) which may be considered beyond the exact requirements. The ground truth explicitly extracts and prints only the requested statistics, which is a more direct fulfillment of the task.
*   **Error Handling/Messaging:** The ground truth includes more verbose and specific print statements for each step, including confirming successful cleaning operations and warnings if columns are not found. While not strictly required for functional correctness, such output enhances user feedback and debuggability. For instance, if a column to be cleaned doesn't exist, the generated code would silently fail to process it without informing the user, whereas the ground truth explicitly warns about it.

Overall, it's a very solid attempt that fulfills the core requirements effectively. The minor deductions are for nuance in how certain outputs or task specifications were interpreted/implemented compared to an absolute ""perfect"" match to the prompt's implied expectations.",llama-4-maverick,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

def main():
    try:
        # Load the dataset
        df = pd.read_csv('youtube_trendings.csv')

        # Clean the 'Video views' column
        df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

        # Filter the dataset
        filtered_df = df[df['Video views'] > 50000000]

        # Select and display the required columns
        result_df = filtered_df[['Video', 'Video views']].sort_values(by='Video view...",10,"The generated code successfully implements all requirements of the task. It correctly loads the dataset, cleans the 'Video views' column by removing commas, converting to numeric, and filling NaNs with 0 in a concise manner. It then filters the data based on the specified view count, selects the required columns, and sorts the result in descending order. The code is well-structured, uses proper imports, and includes error handling for `FileNotFoundError`. The use of `main()` and `if __name__ == ""__main__"":` is good practice.",llama-4-maverick,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def clean_and_plot_data(csv_file):
    # Load the dataset
    df = pd.read_csv(csv_file)

    # Clean the 'Likes' and 'Video views' columns
    for col in ['Likes', 'Video views']:
        df[col] = df[col].str.replace(',', '').apply(lambda x: pd.to_numeric(x, errors='coerce')).fillna(0)

    # Create a scatter plot
    plt.figure(figsize=(10,6))
    plt.scatter(df['Video views'], df['Likes'])
    plt.title('Video Views vs. Likes')
 ...",9,"The generated code is very good, correctly implementing all the requirements of the task. It properly loads the CSV, cleans the specified columns by removing commas, converting to numeric, and filling NaNs with 0. It then correctly generates a scatter plot with the specified title and axis labels.

Strong points:
- Concise and correct data cleaning for `Likes` and `Video views` columns. The chaining of `str.replace`, `apply(pd.to_numeric, errors='coerce')`, and `fillna(0)` is efficient and effective.
- Correct use of `matplotlib.pyplot` for plotting, including title and axis labels.
- Encapsulation within a function `clean_and_plot_data` is good practice.

Minor improvements for a perfect score:
- Error handling for `FileNotFoundError` when loading the CSV would make it more robust. The ground truth includes this, which is a good practice for real-world scripts.
- While `clean_and_plot_data('youtube_trendings.csv')` is fine for a quick script, passing the filename as an argument to the function is good for reusability. The current implementation does that, which is good.
- Adding `plt.grid(True)` or `plt.tight_layout()` could slightly enhance the plot's readability, similar to the ground truth, but these are minor aesthetic points.",llama-4-maverick,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_and_analyze_data(csv_file):
    # Load the dataset
    df = pd.read_csv(csv_file)

    # Clean the specified columns
    df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', ''), errors='coerce')
    df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
    df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
    df['Launched Price (USA)'] = pd.t...",8,"The generated code correctly addresses most of the task requirements.

**Strengths:**
*   **Correct Cleaning Logic:** It correctly applies `str.replace` and `pd.to_numeric` with `errors='coerce'` for all specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`).
*   **Correct Handling of `Launched Price (USA)`:** It correctly removes both 'USD ' and commas.
*   **Data Type Listing:** It properly prints `df.dtypes`.
*   **Descriptive Statistics:** It calculates and prints `describe()` for the cleaned columns.
*   **Unique Company Name Count:** It correctly uses `nunique()` for the 'Company Name' column.
*   **Proper Imports:** `pandas` and `numpy` are correctly imported (though `numpy` isn't strictly used directly in this solution, it's a standard import for data analysis with pandas).

**Areas for Improvement:**
*   **Minor Cleaning Detail - `str.strip()`:** While `str.replace` often handles leading/trailing spaces for numeric conversion, adding `.str.strip()` after `replace` is good practice, especially before `pd.to_numeric`, to ensure robustness against various whitespace issues. The ground truth includes this, which makes it slightly more robust.
*   **Redundant Descriptive Statistics Output:** The ground truth extracts specific statistics (mean, median, std, min, max) and presents them in a more compact `describe().loc[['mean', '50%', 'std', 'min', 'max']]` format. The generated code prints the full `describe()` output for each column, which includes counts and quartile information not explicitly requested. While not incorrect, it's less precise to the prompt's request for ""mean, median, std, min, max"".
*   **Lack of Error Handling for File Loading:** The generated code assumes the CSV file exists and can be loaded. The ground truth includes `try-except` blocks for `FileNotFoundError` and other general exceptions during data loading, which is a good practice for robust code.
*   **No ""Median"" Output in `describe()`:** The prompt specifically asked for ""median"". While `describe()` outputs ""50%"", which is the median, explicitly mentioning `df[column].median()` or showcasing the 50% row from `describe()` would be more aligned with the prompt. The ground truth explicitly uses `'50%'` from `describe()`.
*   **Output Formatting/Readability:** The ground truth code has more structured print statements and headers which enhances readability and user experience. For example, it confirms successful data loading, indicates when cleaning starts/finishes, and clearly labels each section of the output. This is a polish point rather than a correctness issue.

Overall, the generated code is very close to perfect in terms of functionality for the core requirements, with minor areas for refinement in robustness, precision of output, and user experience.",llama-4-maverick,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def main():
    # Load the dataset
    df = pd.read_csv('mobiles_2025.csv')

    # Clean the 'Launched Price (India)' column
    df['Launched Price (India)'] = df['Launched Price (India)'].str.replace('INR ', '').str.replace(',', '')
    df['Launched Price (India)'] = pd.to_numeric(df['Launched Price (India)'], errors='coerce')

    # Clean the 'RAM' column
    df['RAM'] = df['RAM'].str.replace('GB', '')
    df['RAM'] = pd.to_numeric(df['RAM'], e...",9,"The generated code correctly performs all the required steps: loading the dataset, cleaning both specified columns (handling errors via `coerce`), filtering the data, selecting the correct columns, and sorting the final result. The `main` function encapsulation is good practice.

Areas for minor improvement:
1.  **Robust cleaning**: For the ""Launched Price (India)"" and ""RAM"" columns, `astype(str)` before `.str.replace()` makes the cleaning more robust if the column initially contains non-string types (e.g., numbers, `NaN`, etc.), preventing `AttributeError`. While the current CSV likely has them as strings, it's a good defensive practice.
2.  **Explicit `ascending=True`**: Although `sort_values` defaults to `ascending=True`, explicitly stating it (as in the ground truth) enhances readability and self-documentation.
3.  **Error handling for file loading**: The generated code doesn't include error handling for `pd.read_csv`, which could crash the script if `mobiles_2025.csv` is not found. The ground truth demonstrates robust `try-except` blocks for this. While not strictly asked for in the prompt's requirements for *code logic*, it's a standard practice for ""proper imports and structure"" in real-world scenarios.
4.  **No `numpy` used**: `import numpy as np` is present but `np` is not used in the code. It can be removed.

Overall, it's a very good solution that directly addresses the prompt's requirements effectively.",llama-4-maverick,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def clean_numeric_column(df, column_name, unit):
    df[column_name] = df[column_name].str.replace(unit, '', regex=False).apply(lambda x: pd.to_numeric(x, errors='coerce'))

def main():
    # Load the dataset
    df = pd.read_csv('mobiles_2025.csv')

    # Clean the RAM and Battery Capacity columns
    clean_numeric_column(df, 'RAM', 'GB')
    clean_numeric_column(df, 'Battery Capacity', 'mAh')

    # Create a s...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- **Correct CSV loading:** `pd.read_csv('mobiles_2025.csv')` is correct.
- **Correct cleaning logic:** The `clean_numeric_column` function correctly removes the specified unit and then converts the column to numeric, handling errors by coercing to `NaN` using `pd.to_numeric(x, errors='coerce')`. This is applied correctly to both `RAM` and `Battery Capacity`.
- **Correct scatter plot generation:** `sns.scatterplot` is used, with `RAM` on the x-axis, `Battery Capacity` on the y-axis, and `Company Name` for `hue` (coloring).
- **Proper title and axis labels:** The plot has a title, x-axis label, and y-axis label as required.
- **Proper imports:** `pandas`, `matplotlib.pyplot`, and `seaborn` are all imported.
- **Good code structure:** Using a `main` function and
`if __name__ == ""__main__"":` block is good practice.

**Areas for minor improvement (why it's not a 10):**
- **In-place modification:** The `clean_numeric_column` function modifies the DataFrame in-place. While it works, creating new columns for the cleaned data (e.g., `RAM_cleaned`) as done in the ground truth is often safer and allows for comparison with the original data if needed. However, the problem statement doesn't explicitly forbid in-place modification.
- **Error handling for file not found:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is robust. The generated code does not. While not explicitly asked for, it's good practice.
- **Plot aesthetics/readability:** The `bbox_to_anchor` for the legend is good, but the scatter plot could benefit from slightly larger figures (`figsize=(12, 8)`), `s` (marker size), and `alpha` (transparency) as shown in the ground truth for better visualization, especially if there's overlapping data. This is more of an aesthetic improvement than a functional requirement.

Overall, the generated code fulfills the core requirements very well and provides a clean, functional solution.",llama-4-maverick,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        titanic_df = pd.read_csv('titanic.csv')
    except FileNotFoundError:
        print(""The 'titanic.csv' file was not found in the current directory."")
        return

    # Display the first 5 rows of the dataset
    print(""First 5 rows of the dataset:"")
    print(titanic_df.head())

    # Show the shape of the dataset
    print(""\nShape of the dataset (rows, columns):"")
    print(titanic_df.shape)

    # List all ...",9,"The generated code is very good and largely fulfills all requirements.

**Strong points:**
*   **Correctness:** All tasks are performed correctly: loading data, displaying head, shape, dtypes, calculating survival rate, and showing missing values.
*   **Robustness:** Includes `try-except` for `FileNotFoundError`, which is good practice.
*   **Clarity:** Variable names are descriptive (`titanic_df`).
*   **Encapsulation:** Using a `main` function and `if __name__ == ""__main__"":` is good for script organization.
*   **Survival Rate Calculation:** The survival rate calculation is correct and includes a check for the 'Survived' column, adding robustness.

**Minor areas for improvement (to reach a perfect 10):**
*   **Data Types Display (`.dtypes` vs `.info()`):** While `titanic_df.dtypes` correctly lists column names and their data types, the ground truth's use of `df.info()` provides a more comprehensive summary, including non-null counts and memory usage, which is often preferred in EDA for a quicker overview of data completeness. For the specific requirement ""List all column names and their data types,"" both are technically correct, but `info()` is more complete.
*   **Output Formatting/Labels:** The ground truth includes more explicit print statements (e.g., ""1. First 5 rows..."", ""2. Shape..."", etc.) which helps in clearly demarcating each section of the output for the user. The generated code has labels, but they are slightly less numbered/explicit.

Overall, this is a very strong submission that passes almost all criteria with flying colors.",llama-4-maverick,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        imdb_df = pd.read_csv('imdb_top_250.csv')
    except FileNotFoundError:
        print(""The file 'imdb_top_250.csv' was not found in the current directory."")
        return

    # List all column names and their data types
    print(""Column Names and Data Types:"")
    print(imdb_df.dtypes)

    # Calculate and report basic descriptive statistics for numerical columns
    numerical_columns = ['Year', 'Rating', 'Runt...",6,"The generated code correctly loads the CSV, lists column data types (though the output format isn't as clean as `df.info()`), and reports the total number of movies. For descriptive statistics, it calculates them for 'Year' and 'Rating' correctly as long as they are numeric.

However, there are significant issues that prevent a higher score:
1.  **Handling of 'Runtime' column:** The ground truth demonstrates robust preprocessing for the 'Runtime' column, which is often a string like ""142 min"" in real datasets. The generated code *assumes* 'Runtime' will be numeric and does not include any preprocessing logic. This is a critical omission for a real-world dataset and would likely cause an error or incorrect output if the 'Runtime' column is not already numerical.
2.  **Specific statistics requested vs. `describe()` output:** The task explicitly asks for ""mean, median, standard deviation, min, max"". While `describe()` includes these, it also includes other statistics like count, 25th percentile, and 75th percentile. The ground truth specifically selects and renames the statistics for better adherence to the prompt.
3.  **Output format for descriptive statistics:** The generated code prints `describe()` output for each column individually. The ground truth provides a consolidated table, which is generally more readable and useful for comparing statistics across columns.
4.  **Column names and data types output:** While `imdb_df.dtypes` is correct, `df.info()` as used in the ground truth provides a more comprehensive summary, including non-null counts and memory usage, which is often preferred for initial data overview.

To improve, the code should:
*   Implement robust handling for potential non-numeric data types in columns like 'Runtime', including conversion steps.
*   Filter the `describe()` output to explicitly show only the requested statistics (mean, median, std, min, max).
*   Consider using a single `describe()` call on relevant numerical columns and then selecting/formatting the output as a table.
*   Use `df.info()` for column information if the goal is a more detailed summary.",llama-4-maverick,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

def main():
    try:
        # Load the dataset
        imdb_data = pd.read_csv('imdb_top_250.csv')

        # Filter the dataset to include movies with Rating >= 9.0
        filtered_data = imdb_data[imdb_data['Rating'] >= 9.0]

        # Select Title and Rating columns
        selected_data = filtered_data[['Title', 'Rating']]

        # Sort by Rating in descending order
        sorted_data = selected_data.sort_values(by='Rating', ascending=False)

        # Dis...",9,"The generated code correctly performs all the requested tasks: loading the CSV, filtering by rating, selecting specific columns, and sorting. It also includes proper imports and basic error handling for `FileNotFoundError`. The use of a `main` function and `if __name__ == ""__main__""` block is good practice for script organization.

Areas for minor improvement (preventing a perfect 10):
1.  **Scope of Error Handling**: While `FileNotFoundError` is handled, the generated code misses handling `KeyError` if 'Rating' or 'Title' columns are absent, which the ground truth gracefully addresses. This makes the error handling slightly less robust.
2.  **Clarity**: The ground truth adds more comments explaining each step which, while not strictly required by the prompt, improves code readability and maintainability for future developers.
3.  **`main()` function call**: The `main()` function is called, but it's not strictly necessary for this simple script given the prompt's scope. However, it's a good practice.
4. **`copy()` not used**: The ground truth uses `.copy()` to avoid `SettingWithCopyWarning`, which, although often not critical for the final display step, is a good habit. The generated code does not include this.

Overall, it's a very good solution that directly addresses the problem requirements.",llama-4-maverick,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    # Load the dataset
    df = pd.read_csv('imdb_top_250.csv')

    # Create a histogram for the Rating column
    plt.figure(figsize=(8, 6))
    sns.histplot(df['Rating'], kde=True)
    plt.title('Distribution of IMDB Ratings')
    plt.xlabel('Rating')
    plt.ylabel('Frequency')
    plt.show()

    # Create a scatter plot for Rating vs Runtime
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x='...",6,"The generated code correctly loads the CSV, creates a histogram for 'Rating', and a scatter plot for 'Rating' vs 'Runtime'. It also includes appropriate titles and axis labels. However, it critically misses the data cleaning/preprocessing step for the 'Runtime' column. The 'Runtime' column in the `imdb_top_250.csv` dataset is typically in a string format (e.g., ""120 min""), not a direct numeric format ready for plotting. The generated code will likely fail or produce errors when trying to plot 'Runtime' directly as a numeric value for the scatter plot, as `sns.scatterplot` expects numeric data for `x`. This significantly impacts its functional correctness for the given dataset. The ground truth includes robust error handling for file loading and a crucial preprocessing step for 'Runtime'. The generated code could also benefit from basic error handling (e.g., FileNotFoundError) for loading the CSV.

To improve, the code should:
1. Implement robust preprocessing for the 'Runtime' column to extract numeric values (e.g., using regex to extract minutes and converting to integer).
2. Add error handling for `pd.read_csv` in case the file is not found.
3. Consider using `plt.tight_layout()` for better plot presentation.",llama-4-maverick,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

def main():
    # Load the penguins dataset
    penguins_df = pd.read_csv('penguins.csv')

    # Report the number of missing values for each column
    print(""Missing Values Count:"")
    print(penguins_df.isna().sum())

    # Select numerical columns of interest
    numerical_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

    # Drop rows with missing values in the selected numerical columns
    cleaned_df = penguins_df.dropn...",8,"The generated code correctly performs most of the required tasks.

**Strengths:**
*   Properly loads the CSV using pandas.
*   Correctly reports the number of missing values for each column using `isna().sum()`.
*   Correctly identifies and uses the specified numerical columns for analysis.
*   Correctly drops rows where *any* of the four specified numerical columns have a missing value using `dropna(subset=...)`.
*   Calculates and reports descriptive statistics (mean, min, max, std, etc.) for the filtered data using `describe()`, which covers most of the requested statistics.
*   Correctly counts and reports unique values for `species` and `island` columns using `nunique()`.
*   Includes proper imports (`pandas`).
*   Encapsulates the logic within a `main()` function and uses the `if __name__ == ""__main__"":` block, which is good practice.

**Areas for Improvement:**
*   **Descriptive Statistics**: The `describe()` method provides mean, std, min, max, and quantiles (25%, 50%, 75%). While it covers mean, std, min, and max, it reports the median as the 50th percentile (50%) instead of explicitly labeling it as 'median'. The ground truth explicitly requested 'median', which `describe()` provides as `50%`. The ground truth used `.agg(['mean', 'median', 'std', 'min', 'max'])` to get the exact requested statistics labelled as such. This is a minor point but could be slightly clearer in output for the user.
*   **Error Handling for File Loading**: The generated code does not include any error handling for `FileNotFoundError` or other potential issues when loading the CSV, unlike the ground truth. This means if `penguins.csv` isn't present, the script will crash.
*   **Reporting Missing Values**: The task asked to ""Report the number of missing values (NA) for each column."" The generated code outputs `penguins_df.isna().sum()`, which shows all columns, not just those with missing values, and doesn't filter out columns with 0 missing values for clarity, as the ground truth did. However, it still reports the count for *each* column, so it fulfills the literal interpretation of the request.
*   **Clarity of Output**: The ground truth provides more descriptive print statements (e.g., ""Successfully loaded..."", ""Original number of rows:"", ""Number of rows after dropping NAs..."", ""--- Analysis Complete ---""), making the output easier to understand for a user.

Overall, the generated code is highly functional and correctly addresses the core requirements of the task. The minor points are mainly about robustness (error handling) and output formatting/completeness with respect to the specific statistics requested vs. what `describe()` provides by default and how `describe()` labels the median.",llama-4-maverick,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

def main():
    try:
        penguins_df = pd.read_csv('penguins.csv')
        
        filtered_df = penguins_df[
            (penguins_df['species'] == 'Adelie') & 
            (penguins_df['island'] == 'Torgersen')
        ][['culmen_length_mm', 'culmen_depth_mm']].dropna()

        print(filtered_df.head(5))
    except FileNotFoundError:
        print(""The 'penguins.csv' file was not found in the current directory."")
    except Exception as e:
        print(f""A...",9,"The generated code correctly solves all aspects of the task, including loading the data, filtering by species and island, selecting specific columns, dropping NaNs, and displaying the first 5 rows. The code is concise and efficient by chaining operations. It also includes basic error handling for `FileNotFoundError`.

The only minor area for improvement is that the `dropna()` method in the generated code is called without specifying the `subset` argument. While in this specific case, because the dataframe `filtered_df` only contains 'culmen_length_mm' and 'culmen_depth_mm' at that point, `dropna()` will by default check all columns (which are only the two specified), and correctly drop rows with NaNs in either of them. However, it's generally good practice to explicitly use `subset` for clarity and robustness, as done in the ground truth, to ensure `dropna` always targets the intended columns, especially if the chain of operations were ever changed. Despite this, the current implementation still yields the correct result for this specific task.",llama-4-maverick,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    # Load the penguins dataset
    penguins = pd.read_csv('penguins.csv')

    # Drop rows with missing values in the required columns
    penguins = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

    # Create the scatter plot
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=penguins, x='culmen_length_mm', y='culmen_depth_mm', hue='species')

    # Set title and l...",9,"The generated code correctly addresses all requirements of the task. It properly loads the CSV, filters out rows with NAs in the specified columns, generates a scatter plot of `culmen_length_mm` vs `culmen_depth_mm` colored by `species`, and includes a title and axis labels. The imports are correct and the structure is clean and functional.

**Strengths:**
- **Correct Data Loading and Filtering:** The code correctly uses `pd.read_csv` and `dropna(subset=[...])` to achieve the required filtering.
- **Accurate Plotting:** It uses `seaborn.scatterplot` effectively to plot `culmen_length_mm` on the x-axis, `culmen_depth_mm` on the y-axis, and color by `species`.
- **Proper Labels and Title:** The plot includes a clear title and correctly labeled x and y axes.
- **Concise and Readable:** The code is well-structured and easy to understand.

**Minor Area for Improvement (Not critical for this task, but good practice):**
- **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError`. While not explicitly requested in the prompt's requirements beyond ""assume the CSV file is in the current directory,"" robust code would include this. For a benchmark where robustness matters, this would be a slight reduction. However, for a data science task focused on core functionality, it's acceptable to omit.
- **Lack of Docstrings/Comments:** The ground truth includes a detailed docstring and comments. While not strictly required by the prompt, adding them improves maintainability and understanding in real-world scenarios.

Overall, the generated code is an excellent solution to the given problem.",llama-4-maverick,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

def main():
    # Read the dataset
    try:
        penguins_df = pd.read_csv('penguins.csv')
    except FileNotFoundError:
        print(""The 'penguins.csv' file was not found in the current directory."")
        return

    # Check if required columns exist
    required_columns = ['culmen_length_mm', 'culmen_depth_mm']
    if not all(col in penguins_df.columns for col in required_columns):
        print(""The dataset is missing one or more required columns."")
     ...",10,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, fills NA values in the specified columns with their means, calculates and prints the mean, median, and standard deviation for these columns, and finally prints the total number of rows and columns. The code is well-structured within a `main` function and includes error handling for `FileNotFoundError`. It also checks for the existence of required columns before proceeding, which is a good robustness measure. While the ground truth code provides more verbose print statements during NA imputation (which can be helpful for debugging/understanding), the generated code computes and applies the imputation correctly and provides concise output for the statistics, which is perfectly acceptable for the task. The use of an f-string to combine statistics output for each column is a clean way to present the data. Overall, it's a very good solution.",llama-4-maverick,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    penguins_df = pd.read_csv('penguins.csv')

    # Drop rows where flipper_length_mm is NA
    penguins_df = penguins_df.dropna(subset=['flipper_length_mm'])

    # Create a histogram of the flipper_length_mm column
    plt.figure(figsize=(8, 6))
    sns.histplot(data=penguins_df, x='flipper_length_mm', kde=False)
    plt.title('Distribution of Flipper Length (mm)')
    plt.x...",9,"The generated code successfully completes all tasks outlined in the prompt: loading data, handling NAs in the specified column, creating a correctly labeled histogram for `flipper_length_mm`, and generating a correctly labeled bar chart for `island` counts. The use of `seaborn` and `matplotlib.pyplot` is appropriate.

**Strong points:**
*   **Correctness:** All 4 steps of the task are correctly implemented.
*   **Clarity:** The code is well-structured and easy to understand.
*   **Appropriate libraries:** Uses pandas for data manipulation and seaborn/matplotlib for plotting, as requested.
*   **NA Handling:** Correctly drops rows where `flipper_length_mm` is NA.
*   **Plotting:** Both plots are generated correctly with appropriate titles and axis labels.

**Areas for minor improvement (prevented a perfect 10):**
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While the prompt states to ""Assume the CSV is in the current directory,"" robust code typically includes such error handling.
*   **KDE in Histogram:** The ground truth explicitly uses `kde=True` in `histplot`, which is often a nice addition for visualizing distribution shape. The generated code defaults to `kde=False` (or explicitly sets it). While `kde=False` is not strictly incorrect, `kde=True` provides more insight into the underlying distribution.
*   **`main` function:** Wrapping the code in a `main` function and using `if __name__ == ""__main__"":` is good practice, but not strictly necessary for this specific script's functionality as given in the prompt, and it was not a requirement stated in the task.
*   **Plot Aesthetics/Details:** Minor details like `figsize` in plots are consistent, but the ground truth added `plt.xticks`, `plt.yticks` font sizes and `plt.grid` lines for slightly better presentation, though this wasn't explicitly requested.",llama-4-maverick,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        penguins_df = pd.read_csv('penguins.csv')
    except FileNotFoundError:
        print(""The 'penguins.csv' file was not found in the current directory."")
        return

    # Filter the data to include only penguins from the 'Torgersen' island
    torgersen_penguins = penguins_df[penguins_df['island'] == 'Torgersen']

    # Count the number of penguins belonging to each species
    species_counts = torgersen_pengu...",9,"The generated code correctly solves the task. It loads the data, filters by 'Torgersen' island, and counts species. The code is well-structured using a `main` function and includes basic error handling for `FileNotFoundError`.

Strengths:
- Correctly implements all three steps of the task.
- Uses standard pandas operations (`read_csv`, boolean indexing for filtering, `value_counts`).
- Encapsulates logic in a `main` function, which is good practice.
- Includes basic `try-except` for `FileNotFoundError`.

Areas for Improvement (minor):
- The `print` statements could be more descriptive, similar to the ground truth, to provide more context to the user (e.g., ""Species count for penguins on 'Torgersen' island:"").
- It doesn't handle the edge case where no penguins are found on 'Torgersen' island after filtering, or if the 'island' or 'species' columns are missing. The ground truth explicitly checks for and handles these scenarios, making it more robust. However, for the given task, these checks might be considered over-engineering if the dataset is guaranteed to have these columns and data.
- The `print` statement at the end just prints the `species_counts` Series, which is correct and readable, but some additional descriptive text could enhance user experience further.",llama-4-maverick,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

print(""Shape of the DataFrame:"", df.shape)
print(""Data T...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified in the task, prints its shape, and lists the data types of all columns using `df.shape` and `df.dtypes` respectively. The code also follows good practices by importing pandas and directly using the specified data dictionary. It fully addresses all requirements of the task. The output formatting for `print` statements is concise and clear.",llama-4-maverick,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def main():
    # Load the Titanic dataset
    titanic_df = pd.read_csv('titanic.csv')

    # Bar chart for survivors vs non-survivors
    plt.figure(figsize=(8, 6))
    titanic_df['Survived'].value_counts().plot(kind='bar')
    plt.title('Count of Survivors vs Non-Survivors')
    plt.xlabel('Survived (0 = No, 1 = Yes)')
    plt.ylabel('Count')
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.savefig('survivors_bar_chart.png...",9,"The generated code correctly addresses all requirements of the task. It creates the four specified plots with proper titles and labels. It correctly handles missing 'Age' values by filling them with the median. The imports are correct, and the use of `main()` function is good practice.

**Strong points:**
- All four required plots are generated.
- Plots have appropriate titles and axis labels.
- Missing 'Age' values are handled correctly using the median.
- Uses `value_counts()` and `plot(kind='bar')` for bar charts effectively.
- Uses `plot.hist()` for the histogram correctly.
- Includes `plt.tight_layout()` for better plot presentation.
- Includes `plt.show()` for immediate display and `plt.savefig()` as a bonus, although saving was not explicitly asked for.
- Encapsulation within a `main()` function is good.

**Areas for minor improvement (to reach a 10):**
- **Readability of 'Survived' plot:** The x-axis labels for the 'Survived' plot are `0` and `1`. While the `xlabel` explains this, directly mapping `0` to 'Non-Survivor' and `1` to 'Survivor' (as done in the ground truth) would make the plot immediately more readable.
- **Labels on Pclass plot:** The Pclass plot could benefit from mapping numerical classes (e.g., 1, 2, 3) to more descriptive labels if available (though the numbers are generally understood for Titanic data).
- **Coloring:** The plots use default matplotlib colors. While functional, more specific or distinct colors for different categories (e.g., male/female, survived/non-survived) would enhance visual appeal and clarity. The ground truth example demonstrates this with specific colors.
- **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` which is a good robustness feature for a data science script. The generated code assumes the file will always be present.",llama-4-maverick,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create a pandas DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}

df = pd.DataFrame(data)

# 1. Calculate and print the average 'Midterm...",9,"The generated code correctly performs all the required tasks: creating the DataFrame, calculating the average midterm and final scores, and identifying the subject with the highest average final score. The code quality is good, with proper imports and a clear structure.

The only minor area for improvement is in the printing of the average scores. The ground truth explicitly states ""Average Midterm Score for all students"" and ""Average Final Score for all students"", while the generated code uses ""Average Midterm Score"" and ""Average Final Score"". While functionally correct, the ground truth's output is slightly more descriptive in adhering to the prompt's implied detail. Additionally, the ground truth's final printout for the subject with the highest score includes the actual average score value for that subject, which adds more informative detail that the prompt didn't strictly require but is good practice. The generated code also uses `reset_index()` and then `loc` with `idxmax()` to get the subject, which is a valid approach, but `idxmax()` directly on the grouped Series (as in the ground truth) is slightly more concise for just getting the index (subject name) of the maximum value. Overall, these are minor differences and the generated code fully solves the problem.",llama-4-maverick,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Create a DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']), 
        'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'], 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5], 
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0], 
        'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0], 
    ...",10,"The generated code is correct, complete, and well-structured. It successfully creates the DataFrame, sets 'Date' as the index, calculates the 'Daily_Range', finds the date with the highest 'Close' price, and prints the updated DataFrame as required by the task. The use of method chaining (`pd.DataFrame(data).set_index('Date')`) is a concise and excellent way to create and index the DataFrame. All necessary imports are included. The output format for the highest close price is slightly different (includes time) from the ground truth, but this is a minor formatting choice and does not impact correctness.",llama-4-maverick,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

def main():
    # Create a pandas DataFrame
    data = {
        'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUS...",9,"The generated code correctly addresses all parts of the task. It successfully creates the DataFrame, calculates the 'Total_Revenue', identifies the product sold the most by quantity, and prints the first 5 rows of the updated DataFrame. The code is well-structured and uses appropriate pandas functions.

The only minor point of difference, not a correctness issue, is the inclusion of `main()` function and `if __name__ == ""__main__"":` block, which is good practice for larger scripts but not strictly necessary for this small, self-contained task as per the prompt's implied direct execution. Also, the prompt requested to print ""The product sold the most in terms of Quantity is: {best_selling_product}"" and the generated code prints ""The product sold the most is: {most_sold_product}"". This is a very minor textual difference and doesn't affect the core functionality. Everything else is perfectly aligned with the ground truth and the task requirements.",llama-4-maverick,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDuratio...",10,"The generated code is excellent.
1.  **Correctness**: It correctly defines the DataFrame, calculates the average session duration using the `.mean()` method, and accurately counts the number of users per `DeviceType` using `value_counts()`. All calculations are precise.
2.  **Code quality**: The code is clean, well-structured, and uses appropriate pandas methods. Imports are handled correctly, and variable names are descriptive.
3.  **Completeness**: It fully addresses all requirements of the task: defining the DataFrame, calculating and printing the average session duration, and counting and printing the number of users for each device type. The output format is clear and readable.

The solution is concise, efficient, and demonstrates a strong understanding of pandas functionalities for the given task.",llama-4-maverick,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

# Create the DataFrame
df = pd.DataFrame(data)

# Find and print the maximum ""Tempera...",10,"The generated code is excellent. It correctly defines the DataFrame exactly as specified in the task, and then accurately finds and prints both the maximum ""TemperatureCelsius"" and the minimum ""HumidityPercent"" using the appropriate pandas methods (`.max()` and `.min()`). The code is clean, well-structured, and directly addresses all requirements of the prompt. The output format for printing is also clear and includes the requested units.",llama-4-maverick,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# Print the total number of ""Likes"" across all posts
total_likes = df['Likes'].sum()
print(f'Total number ...",9,"The generated code correctly defines the DataFrame, calculates the total number of likes, and accurately identifies the PostType with the highest average number of shares. The code is well-structured and uses appropriate pandas functions.

The only minor point for improvement, which keeps it from a perfect 10, is in the string formatting for the total likes output. The ground truth output is `Total number of Likes across all posts: {total_likes}`, while the generated code produces `Total number of likes: {total_likes}`. This is a very minor discrepancy in the output string but doesn't affect the correctness of the calculation. Similarly, for the average shares, the ground truth output is `PostType with the highest average number of Shares: {post_type_highest_avg_shares}`, while the generated code outputs `Post type with the highest average shares: {max_avg_shares_post_type}`. While functional, adhering strictly to the exact wording implied by the prompt is often beneficial in benchmarks.

Regarding the calculation of the `PostType` with the highest average shares, both `df.groupby('PostType')['Shares'].mean().idxmax()` (Ground Truth) and `avg_shares.loc[avg_shares['Shares'].idxmax()]['PostType']` (Generated Code) are valid and efficient ways to achieve the result. The generated code's approach of resetting the index and then using `.loc` is slightly more verbose but equally correct.",llama-4-maverick,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",10,"The generated code is excellent. It correctly defines the DataFrame with the specified data types, accurately calculates the average age, and correctly counts the patients by gender. The code is well-structured, uses proper pandas functions, and addresses all requirements of the task. The output format for the average age is also good, matching the ground truth in precision.",llama-4-maverick,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

def main():
    # Define the data for the DataFrame
    data = {
        ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
        ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
        ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
        ""Year"": [2010, 2008, 1994, 1994, 1999],
        ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
        ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
    }

    # Create the Data...",9,"The generated code correctly defines the DataFrame, identifies the highest-rated movie by title, and calculates the average release year. The use of `idxmax()` is an efficient way to find the index of the maximum rating, which is good. The code is well-structured and uses standard pandas operations. The `main` function and `if __name__ == ""__main__"":` block demonstrate good Python practice for organizing code, although it was not strictly required by the prompt.

One minor point for improvement, not impacting correctness but slightly impacting robustness: if there were multiple movies with the exact same highest rating, `idxmax()` would only return the index of the first occurrence. The ground truth code explicitly handles this by filtering the DataFrame which would allow for retrieval of all if needed, though for ""the title of the movie"" singular, either approach is acceptable. The prompt implied finding *a* movie title, not *all* of them if duplicates exist. So, for this specific prompt, the generated code is perfectly fine. The printing statements are also clear.",llama-4-maverick,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create a DataFrame representing car sales
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}
df = pd.DataFrame(data)

# Print the total number ...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates and prints the total number of cars, and correctly calculates and prints the average ""PriceUSD"". The use of f-strings for clear output and `len(df)` for total cars is perfectly valid and idiomatic. The formatting of the average price to two decimal places is also very good. All requirements of the task are fully met with high-quality, concise code.",llama-4-maverick,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}

# Create the DataFrame
df = pd.DataFrame(dat...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, and accurately identifies and prints the names of the Pokemon with the highest HP and lowest Defense using efficient pandas methods (`idxmax()` and `idxmin()`). The code is clean, well-structured, and directly addresses all requirements of the task. The use of f-strings for printing is also good practice.",llama-4-maverick,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

def clean_titanic_data(csv_file):
    # Load the dataset
    df = pd.read_csv(csv_file)

    # Identify columns with missing values
    print(""Columns with missing values:"")
    print(df.columns[df.isnull().any()].tolist())

    # Fill missing Age values with the median age
    df['Age'] = df['Age'].fillna(df['Age'].median())

    # Fill missing Embarked values with the most frequent port
    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

    # D...",8,"The generated code correctly performs most of the required data cleaning steps. It loads the data, identifies missing values, fills missing 'Age' and 'Embarked' values appropriately, and drops the 'Cabin' column. It also successfully extracts the 'Title' from the 'Name' column using a vectorized string operation which is efficient. The code structure is clean with a function encapsulating the cleaning process and an `if __name__ == ""__main__"":` block, demonstrating good practice.

Areas for improvement:
1.  **Strictly adheres to display requirements:** The prompt explicitly asked to *display* the identified columns with missing values (`print(df.isnull().sum()[df.isnull().sum() > 0])` in ground truth). The generated code identifies and prints `df.columns[df.isnull().any()].tolist()` which is fine, but not exactly what was requested for the first step.
2.  **No error handling for file not found:** The ground truth includes a `try-except FileNotFoundError` block, which is a robust practice for external file dependencies. The generated code lacks this.
3.  **Title standardization (minor missing feature):** The ground truth includes a step to standardize less common titles (e.g., 'Mlle' to 'Miss', 'Dr' to 'Rare'). While the prompt only asked to *create* the 'Title' column, standardizing it is a common and often necessary subsequent step in data cleaning for titles, and the `get_title` function in the ground truth is designed to facilitate this. The generated code's `str.extract` gets the title correctly but doesn't handle this standardization. This is a minor omission given the prompt's wording (""Create a new column 'Title' extracted from the Name column""), but worth noting as a good practice that the ground truth exemplifies.",llama-4-maverick,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame representing a book inventory
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75...",9,"The generated code correctly defines the DataFrame, accurately identifies the title of the book with the highest price, and calculates the total stock. All requirements of the task are met. The code is concise and leverages pandas functionalities effectively (e.g., `idxmax()` and `sum()`).

The only minor area for improvement, preventing a perfect 10, is the output format. While `print(df.loc[df['Price'].idxmax()]['Title'])` and `print(df['Stock'].sum())` are functionally correct, the ground truth uses f-strings to provide more descriptive output, which is generally better for readability and user understanding in real-world applications (e.g., ""The title of the book with the highest price is: '...'""). However, for a code-only task focused on correctness, this is a minor stylistic point and does not detract from the core functionality.",llama-4-maverick,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        water_potability_df = pd.read_csv('water_potability.csv')
    except FileNotFoundError:
        print(""The file 'water_potability.csv' was not found."")
        return

    # Display the first 10 rows
    print(""First 10 Rows:"")
    print(water_potability_df.head(10))

    # Show the shape of the dataset
    print(""\nDataset Shape:"")
    print(water_potability_df.shape)

    # List all column names and their data t...",8,"The generated code correctly addresses all aspects of the task, including loading the dataset, displaying its shape and head, showing column types, calculating potability percentages, displaying missing values, and summarizing numerical columns. The code structure is good with a `main` function and proper import.

**Strong points:**
*   Correctly loads the CSV and handles `FileNotFoundError`.
*   Effectively uses `head()`, `shape`, `dtypes`, `isnull().sum()`, and `describe()` for the respective tasks.
*   The `value_counts(normalize=True) * 100` for potability is a concise and correct way to get percentages.
*   It explicitly checks for the `Potability` column before attempting calculations, which is good practice.
*   It explicitly selects numerical columns using `select_dtypes` for `describe()` although `describe()` on its own typically handles this.

**Areas for improvement:**
*   **Column Names and Data Types:** While `df.dtypes` is correct, `df.info()` as used in the ground truth provides a more comprehensive view, including non-null counts, which is often more useful in EDA.
*   **Missing Values Display:** The generated code prints `water_potability_df.isnull().sum()`, which shows all columns, even those with no missing values. The ground truth `missing_values[missing_values > 0]` is slightly better as it only shows columns with actual missing data, making the output cleaner when most columns are complete.
*   **Output Clarity:** The ground truth code includes more descriptive print statements and separators (`=""*50""`) which enhance readability and help delineate each section of the analysis. The generated code's print statements are a bit more minimal.
*   **Redundant `select_dtypes` for `describe()`:** `describe()` by default operates only on numerical columns. Explicitly selecting them using `water_potability_df.select_dtypes(include=['int64', 'float64'])` is not strictly necessary unless you want to ensure specific numerical types are included/excluded (e.g., if you had boolean columns represented as `int64` and didn't want them in `describe`). In this specific context, `water_potability_df.describe()` would have been sufficient and more concise.",llama-4-maverick,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

def clean_water_potability_data(csv_file):
    # Load the dataset
    df = pd.read_csv(csv_file)

    # Identify columns with missing values
    missing_value_cols = df.columns[df.isnull().any()].tolist()
    print(f""Columns with missing values: {missing_value_cols}"")

    # Store initial statistics
    initial_stats = {
        ""missing_values_count"": df.isnull().sum().sum(),
        ""dataset_shape"": df.shape
    }

    # Fill missing values usi...",7,"The generated code addresses most of the requirements but has a few shortcomings compared to the ground truth.

**Strong points:**
*   Correctly loads the dataset.
*   Identifies and fills missing numerical values with the mean.
*   Correctly removes duplicate rows.
*   Attempts to filter data based on reasonable ranges.
*   Provides a summary of before and after statistics for missing values and shape.
*   Displays the head of the cleaned dataset.
*   Uses a function which can be good practice for reusability.

**Areas for improvement:**
*   **Missing Value Identification and Reporting:** The generated code prints `Columns with missing values: [...]` at the beginning, but the ground truth more explicitly prints *which* columns had missing values and their counts, making it clearer what was imputed. The final check for missing values is also less explicit; it only prints `Final Missing Values Count`, where the ground truth explicitly shows `Missing Values After Cleaning:` and if any remain, lists them.
*   **Range Validation Logic:** The ground truth code validates `pH` specifically (0-14) and then checks *all other numerical columns for non-negativity*. The generated code uses hardcoded `reasonable_ranges` for several columns. While admirable for establishing upper bounds, some of these upper bounds might be too restrictive or arbitrary without domain knowledge (e.g., Sulfate up to 1000, Solids up to 100000). More importantly, the ground truth's approach of enforcing non-negativity for *all* relevant parameters (like Hardness, Conductivity, Organic_carbon, etc.) is a simpler, more robust, and less opinionated data validation step given the general prompt. The generated code also does not include `Conductivity` in its `reasonable_ranges` dictionary, which can take on a range of positive values.
*   **Error Handling:** The ground truth includes basic `try-except` for `FileNotFoundError` when loading the CSV, which is a good practice for robust code. The generated code lacks this.
*   **Summary Detail:** While a summary is provided, the ground truth's summary is more explicit by showing exactly *which* columns had missing values and their counts before cleaning, and explicitly stating ""No missing values remaining"" if successful. The generated code only provides a total count, which is less informative.
*   **Clarity on Rows Removed by Range Check:** The ground truth explicitly states how many rows were removed due to out-of-range values, whereas the generated code only implicitly shows this through the `final_stats['dataset_shape']`.",llama-4-maverick,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    # Load the dataset
    df = pd.read_csv('water_potability.csv')

    # Bar chart for potable vs non-potable water samples
    plt.figure(figsize=(8, 6))
    df['Potability'].value_counts().plot(kind='bar')
    plt.title('Distribution of Potable vs Non-Potable Water Samples')
    plt.xlabel('Potability')
    plt.ylabel('Count')
    plt.xticks([0, 1], ['Non-Potable', 'Potable'])
    plt.tight_layou...",8,"The generated code successfully fulfills the task requirements, creating four distinct visualizations with proper titles and labels.

**Strong Points:**
*   **Correct Visualizations:** All four requested visualizations (bar chart, histograms, boxplot, correlation heatmap) are correctly generated.
*   **Proper Imports:** Necessary libraries (pandas, matplotlib, seaborn) are imported.
*   **Clear Titles and Labels:** Each plot has a title and appropriate axis labels.
*   **Handling Missing Values (Implicitly):** `histplot` and `boxplot` in seaborn automatically handle NaNs by ignoring them, and `df.corr()` also handles them pairwise.
*   **Good Structure:** The code is organized within a `main` function, which is good practice.
*   **Output Files:** The inclusion of `plt.savefig()` is a thoughtful addition, although not explicitly requested, it allows for easy saving of the plots.

**Areas for Improvement:**
*   **Bar Chart Type:** While `df['Potability'].value_counts().plot(kind='bar')` works, using `sns.countplot` (as in the ground truth) is generally preferred for categorical distribution plots in seaborn as it's more expressive and allows for easier customization of aesthetics.
*   **Numerical Feature Selection for Heatmap:** `df.select_dtypes(include=['int64', 'float64']).columns` is a robust way to select numerical features. However, for this specific dataset, simply `df.corr()` would also yield the same result as all features are already numerical.
*   **Consistency in `plt.show()`:** The code uses `plt.show()` after each plot block, which is correct for displaying plots one by one. The ground truth also follows this, ensuring each plot is displayed before the next is generated.
*   **Minor Customization:** The ground truth adds more visual polish like `sns.set_style(""whitegrid"")`, `palette` arguments, `fontsize`, `grid` customization, `xticks` rotation for the heatmap, and specific colors for histograms, which make the plots more visually appealing and informative. These are not critical for functionality but enhance presentation. For example, explicitly setting `xticks` labels for the bar chart as ""Not Potable"" and ""Potable"" without requiring `plt.xticks([0, 1])` then manually specifying them as `labels=['Non-Potable', 'Potable']` could be slightly clearer if the `Potability` column's values were directly mapped. However, the current approach is perfectly functional.",llama-4-maverick,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

# Load dataset
df = pd.read_csv('winequality-red.csv')

# Create new features
df['sulphates_to_chlorides'] = df['sulphates'] / df['chlorides']
df['alcohol_times_density'] = df['alcohol'] * df['density']
df['fixed_acidity_to_...",6,"The generated code correctly attempts to fulfill most of the requirements but has several shortcomings in terms of robustness, best practices, and completeness.

**Correctness:**
- **Feature Engineering:** Creates 4 new features, which is good. However, it implicitly assumes `chlorides` and `volatile acidity` will never be zero, leading to potential `ZeroDivisionError` or `inf` values. This is a common oversight that robust code should handle (e.g., adding a small epsilon).
- **Feature Selection:** Correctly applies RFE with RandomForest and SelectKBest with f_regression.
- **Data Scaling:** Does not scale the data (`X_train`, `X_test`). While `RandomForestRegressor` and `f_regression` are somewhat robust to feature scaling, it's generally good practice, especially when comparing importance scores or when using other models later.
- **Handling NaNs/Infs:** Does not explicitly check for or handle `NaN` or `inf` values that might arise from feature engineering (e.g., divisions by zero or very small numbers). This can lead to errors or incorrect results in subsequent steps.
- **RFE Importance Output:** The `rfe_importances` array returned by `rfe_selector.estimator_.feature_importances_` corresponds to the importances of the features selected *by the estimator during its final fit on the selected features*, not the features in the order of `X_train.columns`. If `n_features_to_select` was smaller, the `rfe_importances` only refer to those features. The code correctly lists the selected features and their importances, but it doesn't explicitly link them back to a full ranking if that's what was intended. The ground truth re-trains the model on the *selected* features to get their importances, which is a more appropriate way to attribute importance specifically to the chosen subset.

**Code Quality:**
- **Imports:** Imports are generally correct. `PolynomialFeatures` is imported but not used.
- **Structure:** The code is logically structured, moving from data loading to feature engineering and then selection.
- **Readability:** Variable names are descriptive.
- **Robustness:** Lacks error handling for file loading or potential division by zero.

**Completeness:**
- **""At least three new meaningful features"":** Achieved, creating four.
- **""Apply at least two different feature selection techniques"":** Achieved (RFE and SelectKBest).
- **""Identify the top 5-7 most important features"":** Achieved (selects top 6).
- **""Compare and contrast the feature sets selected by each method"":** It prints the common features, which is a basic comparison. It does not provide any textual analysis or deeper insight into *why* the sets might differ, as the ground truth does. This was explicitly requested in point 3 of the task (""Compare and contrast..."").
- **""Display the final selected features with their importance scores or rankings"":** Achieved.
- **""Proper imports... assume the CSV file is in the current directory"":** Achieved.
- **""Provide only the Python code without explanations"":** The generated code does start with a small amount of explanation via comments.

**Areas for Improvement:**
1.  **Robust Feature Engineering:** Add a small epsilon to denominators to prevent division by zero, or handle potential `nan`/`inf` values introduced.
2.  **Data Preprocessing:** Incorporate `StandardScaler` for the features before applying feature selection methods. This is a best practice, especially for models sensitive to scale; while less critical for RandomForest and f_regression, it's still good form.
3.  **Comprehensive Comparison:** Add a section that provides a textual comparison and contrast of *why* the selected feature sets are similar or differ. This was a direct prompt in the task description.
4.  **Clarity for RFE Importances:** While the current method of getting RFE importances from `rfe_selector.estimator_.feature_importances_` is not wrong, ensuring clarity that these scores are for the *final model trained on the selected features* is important. The ground truth's approach of re-training on the selected features explicitly clarifies this.
5.  **Remove Unused Imports:** Remove `PolynomialFeatures` as it's not used.
6.  **Avoid printing comments as part of actual output:** The ground truth shows how to include comments as part of the desired output. The current code prints descriptive text like ""RFE Features:"", ""KBest Selected Features with Scores:"", etc., which is acceptable given the overall goal.",llama-4-maverick,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot a...",6,"The generated code attempts to address all requirements but falls short in several critical aspects, primarily in robust data preprocessing and evaluation methodology.

**Strengths:**
*   **Implementation of Ensemble Methods:** Random Forest, Gradient Boosting, XGBoost, AdaBoost, Voting, and Stacking regressors are all implemented.
*   **Hyperparameter Tuning:** GridSearchCV is used for one model.
*   **Evaluation Metrics:** MAE, MSE, RMSE, and R² are calculated.
*   **Visualization:** Feature importance plots and residual plots are generated.

**Weaknesses and Areas for Improvement:**

1.  **Data Preprocessing (Critical Flaw):**
    *   **Missing Values:** The code only drops rows with missing `Global_Sales` and then proceeds with `LabelEncoder`. This is insufficient for numerical columns (like `User_Score` or `Critic_Score` which often have NaNs in this dataset) or general categorical columns that might have NaNs (though `LabelEncoder` can handle NaNs by encoding them as -1, it's not a proper imputation strategy). The ground truth correctly uses `SimpleImputer` and `ColumnTransformer` for robust handling.
    *   **Categorical Encoding:** `LabelEncoder` is used for all object columns. This is inappropriate for nominal categorical variables (like `Genre`, `Publisher`, `Platform`) as it introduces an arbitrary ordinal relationship, which can severely mislead tree-based models. One-Hot Encoding (as in the ground truth) is standard practice for such cases.
    *   **Pipeline:** The generated code does not use `Pipeline` for preprocessing combined with models, meaning preprocessing steps (like imputation and encoding) are fitted and transformed *once* on the entire dataset (or `X_train`). This leads to data leakage if applied before `train_test_split` and improper cross-validation if not part of the `Pipeline` per fold. The ground truth correctly integrates preprocessing into pipelines for each model, ensuring proper cross-validation.

2.  **Cross-Validation Evaluation:**
    *   The `cross_val_score` is used, but it's only on `X_train`. The final evaluation metrics are computed on `X_test` after training on `X_train` *once*. This mixes cross-validation results with a single train-test split evaluation, making comparisons difficult and not fully leveraging the benefits of cross-validation for evaluation of the ensemble models and the tuned model. The ground truth performs the 5-fold cross-validation explicitly for each model, calculating mean metrics across folds.
    *   The results of model evaluations are only printed to console, not stored systematically for easy comparison (e.g., in a dictionary or DataFrame as in the ground truth).

3.  **Hyperparameter Tuning Scope:**
    *   Only `RandomForestRegressor` is chosen for tuning, and the selection of this model seems arbitrary rather than based on performance analysis. The ground truth dynamically selects the best performing individual model for tuning and evaluates the tuned model with cross-validation.

4.  **Feature Importance:**
    *   The feature importance plot for `XGBRegressor` might fail or be incorrect if the `LabelEncoder` has created a -1 value for NaNs, as `XGBoost` expects numerical input.
    *   The feature names for importances are taken directly from `X.columns`, which are the original columns. After `LabelEncoder`, these columns are numerical, but their relative importance might be skewed if `LabelEncoder` was problematic. If OneHotEncoding were properly applied, the feature names would need to reflect the encoded categories.

5.  **Residual Plots:**
    *   Residual plots are generated for *all* models individually, which is good. However, they are based on `X_test` which is a single split, not a comprehensive view from cross-validation.

6.  **Completeness/Precision:**
    *   The task explicitly asked for ""cross-validation with 5 folds to evaluate *each* model."" The generated code uses `cross_val_score` for individual models but then re-fits them on `X_train` and evaluates on `X_test` for the final metric prints, which can be confusing and less robust than the full CV approach.
    *   The problem statement mentioned ""handling missing values and encoding categorical variables"" as specific steps. The current implementation of these steps is inadequate.

Overall, the generated code provides a basic structure but lacks the sophistication and robustness required for expert-level machine learning tasks, particularly in data preprocessing and systematic cross-validation.",llama-4-maverick,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error
import pymc3 as pm
from scipy import st...",4,"The generated code attempts to address several requirements but falls short in correctness, completeness, and adherence to best practices for a ""data science expert"" level.

**Correctness & Completeness Issues:**

1.  **Data Loading and Aggregation:**
    *   The `Year` column is used as the index in `yearly_sales_ts`, but the original `data` dataframe uses `Year` as a general column, leading to potential inconsistencies.
    *   The `Year` column in the CSV is specified as `Year_of_Release` in the ground truth, but the generated code assumes `Year`. This is a critical `FileNotFoundError` or `KeyError` if the CSV header isn't exactly 'Year'.

2.  **ARIMA Modeling:**
    *   The generated code uses `statsmodels.tsa.arima.model.ARIMA`, which requires manual order selection (`(1,1,1)`). The task implies a more robust approach (like `auto_arima` from `pmdarima` in the ground truth) for ""expert"" level, or at least a documented process for selecting the order.
    *   Forecasting `steps=5` is done, but without plotting or clear interpretation.
    *   Confidence intervals are requested (`y_pred_int = y_pred_interval.summary_frame(alpha=0.05)`), but their display and proper context are missing.

3.  **Seasonal Decomposition:**
    *   Seasonal decomposition is performed, but for yearly data, identifying a meaningful ""season"" for `seasonal_decompose` without a `period` argument is problematic. The ground truth correctly identifies this limitation and provides a workaround. The generated code defaults to an implicit period, which is likely 1 for yearly data, rendering the 'seasonal' component empty.

4.  **Multivariate Regression with Interaction Terms:**
    *   Uses `LabelEncoder` for categorical features, which introduces ordinality that is inappropriate for nominal variables (Genre, Platform, Publisher) in a linear model. One-hot encoding (`pd.get_dummies`) is the correct approach.
    *   Interaction terms are poorly implemented. `PolynomialFeatures(interaction_only=True)` on already label-encoded features will create interactions of numerical codes, which is not what's typically meant by interactions with categorical variables. The task also specifically asked for `Year` interactions with `Genre`, `Platform`, and `Publisher`, which is not correctly implemented.
    *   `train_test_split` is used, which is not suitable for time series data. Time-series cross-validation is explicitly requested.

5.  **Polynomial Regression with Regularization:**
    *   Polynomial features are applied to `X_train` which includes all original features (Year, Genre (encoded), Platform (encoded), Publisher (encoded)). This would create high-order polynomial interactions of encoded IDs, which is not generally useful. The ground truth correctly identifies `Year_of_Release` as the feature for polynomial regression.
    *   Uses `train_test_split` again.

6.  **Bayesian Regression (PyMC3):**
    *   The model definition is too simplistic. It implies `beta1` is a single coefficient for all features, or it's a vector that somehow maps to `X_train`. Given `X_train` has multiple columns, `pm.math.dot(X_train, beta1)` assumes `beta1` is a vector of coefficients matching the number of features.
    *   It samples but doesn't interpret the trace or provide prediction/confidence intervals from the Bayesian model (which is a primary strength of Bayesian methods).
    *   No handling of categorical variables (they are still label-encoded from previous steps). Bayesian regression generally works better with proper categorical handling (e.g., one-hot encoding or hierarchical models).

7.  **Change Point Detection:**
    *   Pelt model `l2` is used, which is acceptable. `pen=10` is an arbitrary penalty value; the ground truth uses a more data-driven approach (`2 * np.log(n_samples_signal)`).

8.  **Survival Analysis:**
    *   The definition of 'game longevity' is incorrect. It defines `Profitable` based on `Global_Sales > 0` but then uses `Year` in `kmf.fit(data['Year'].dt.year, event_observed=data['Profitable'])`. This is incorrectly applying Kaplan-Meier. Kaplan-Meier requires a `duration` (how long something 'survived') and an `event` (whether the event of interest happened). Here, `data['Year'].dt.year` is a point in time, not a duration. The task asked for ""life-time"" and ""years a game remains profitable,"" implying duration.
    *   No CoxPH model is implemented despite being requested.

9.  **Confidence/Prediction Intervals:**
    *   Only for ARIMA, and the plot is missing. The other models (multi-reg, poly-reg, Bayesian) are missing proper interval generation and interpretation.

10. **Time Series Cross-Validation:**
    *   The `TimeSeriesSplit` is correctly used structurally, but the `ARIMA` model inside the loop keeps `order=(1,1,1)` fixed, which lessens the validation effectiveness. The ground truth manually implements rolling ARIMA, which is a good approach for TS-CV.
    *   The cross-validation for regression models is missing (only done for ARIMA).

**Code Quality Issues:**

*   **Imports:** Some unnecessary imports (e.g., `adfuller`, `concordance_index` which isn't used).
*   **Variable Names:** Inconsistent use of `data` vs `df`.
*   **Reproducibility:** No random seed set for PyMC3 sampling or `train_test_split`.
*   **Error Handling:** Lacks robust error handling (e.g., for file not found, model convergence issues).
*   **Clarity:** Code lacks comments and proper separation between sections, making it hard to follow.
*   **Visualization:** No plots are generated where they are crucial for interpretation (e.g., ARIMA forecast, survival function).

Overall, the generated code contains several fundamental conceptual errors related to statistical modeling principles (e.g., categorical encoding, time series data splitting, survival analysis duration) and doesn't fully implement all requested models or their validation.",llama-4-maverick,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.metrics.cluster import adjusted_rand_score, normalized_mutual_info_score
from scipy.cluster.hierarchy import dendrogram, link...",6,"The generated code attempts to address all requirements of the task, which is commendable. However, it falls short in several areas regarding correctness, completeness, and robust implementation.

**Strengths:**
*   **Structure:** The code is well-structured using functions for different parts of the analysis, which improves readability and modularity.
*   **Comprehensive Task Coverage:** It attempts to cover almost all aspects of the prompt, including multiple clustering algorithms, dimensionality reduction, optimal cluster determination, consensus clustering, anomaly detection, association rule mining, and recommendation systems, as well as visualization and validation.
*   **Use of Libraries:** Correctly imports and uses relevant libraries like `sklearn`, `umap`, `mlxtend`, `matplotlib`, `seaborn`.

**Areas for Improvement / Weaknesses:**

1.  **Data Preprocessing and Feature Engineering:**
    *   **Dropping NaNs:** `data = data.dropna()` is too aggressive. Missing values in sales columns should ideally be filled with 0, not dropped, as they signify no sales in that region. The `Year_of_Release` is also dropped, which is a major feature.
    *   **Regional Sales Distribution:** The calculation `data['Regional_Sales_Distribution'] = data[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].apply(lambda x: x / x.sum(), axis=1).tolist()` is problematic. First, `x.sum()` can be zero, causing NaNs. Second, `tolist()` creates a list of lists, which is not suitable for direct use in `sklearn` models or as a standalone feature column in `pd.DataFrame`. The ground truth creates separate proportion columns, which is the correct approach.
    *   **Publisher_Portfolio_Size:** `data['Publisher'].map(data['Publisher'].value_counts())` calculates the *number of occurrences* of each publisher, not the *number of unique games* published by them (which is what ""portfolio size"" usually implies and what the ground truth does).
    *   **Feature Selection for Scaling/Encoding:** The code selects arbitrary `numerical_cols` for scaling (`Critic_Score`, `Global_Sales`, etc.) and `categorical_cols` for `get_dummies` (e.g., 'Rating' which is not in the `video_games_sales.csv` dataset, and omits 'Name', 'Year_of_Release' from the features for clustering, which are important). The ground truth correctly uses `ColumnTransformer` and `Pipeline` for a more robust and clear preprocessing step on selected features.

2.  **Optimal Cluster Determination:**
    *   The `determine_optimal_clusters` function only uses K-Means to calculate silhouette/Calinski-Harabasz/Davies-Bouldin scores. This is insufficient for determining optimal clusters for GMM or Hierarchical Clustering, or for doing proper comparative analysis across algorithms. The ground truth calculates these metrics for all algorithms, which is more appropriate.
    *   It lacks the Elbow Method and Gap Statistic (prompt asked for all three).

3.  **Clustering Algorithms - DBSCAN:**
    *   `DBSCAN(eps=0.5, min_samples=10)` uses hardcoded parameters, which are highly data-dependent and almost certainly suboptimal without proper tuning (like k-distance graph). Many implementations will fail or return single large clusters/all noise. The ground truth provides a more robust (though still heuristic) approach to estimate `eps`.

4.  **Consensus Clustering:**
    *   The implementation `consensus_labels = np.apply_along_axis(lambda x: np.bincount(x).argmax(), 1, consensus_labels)` effectively performs a majority vote for each data point's cluster label. While a form of consensus, it's a very simplistic approach that might not achieve truly stable clusters, especially if algorithms disagree significantly. True consensus clustering often involves building a co-occurrence matrix and then clustering that matrix, as done in the ground truth.

5.  **Cluster Profiling and Statistical Significance:**
    *   **Profiling:** `cluster_data.describe()` provides means and standard deviations, but doesn't explicitly highlight the *top modes* for categorical features as requested or done in the ground truth, which is crucial for qualitative profiling.
    *   **Statistical Testing:** The prompt asked for significance testing with *cluster profiles*. The code applies `f_oneway` to categorical columns and `kruskal` to numerical ones.
        *   `f_oneway` (ANOVA) is for comparing means of *numerical* data across groups, not for categorical data. For categorical data, Chi-squared test on contingency tables is appropriate, as done in the ground truth.
        *   The use of `kruskal` for numerical data is fine, but `f_oneway` (ANOVA) is also a strong choice for normally distributed data.

6.  **Anomaly Detection:**
    *   Uses `LocalOutlierFactor`, which is a valid algorithm. However, `contamination` parameter isn't set, and there's no clear output or analysis of the results (e.g., printing anomaly scores, displaying anomalous games). The ground truth used `IsolationForest` which is also valid and provides a clear output of detected anomalies.

7.  **Association Rule Mining:**
    *   `data[categorical_cols].apply(lambda x: x.astype(str).tolist(), axis=1).tolist()` for `transactional_data` will convert each row's selected categories into a list of single values. This is not how `TransactionEncoder` typically works for association rules where each *row* should represent a transaction of *multiple distinct items*. For example, a game isn't a transaction; a game's attributes can be items in a transaction. The ground truth one-hot encodes directly and then applies Apriori, which is more standard.
    *   The features used for ARM are limited to categorical variables. The prompt also implies finding patterns between *game attributes*, which could include derived features or cluster labels. The ground truth includes `Consensus_Cluster`.

8.  **Recommendation System:**
    *   The recommendation system merely collects game names into cluster lists. It lacks the core logic of taking an input game and recommending *other* games from its cluster based on some criteria (e.g., popularity). The ground truth provides a functional example.

9.  **Completeness & Detail:**
    *   Many visualizations are called but `plt.show()` is missing, causing them to not display in some environments.
    *   The `dendrogram` generation for Hierarchical Clustering is present, but it's not connected to the actual clustering process or used to determine `n_clusters`.
    *   The external validation function `external_validate_clustering` is defined but not called, implying a missing ""true label"" dataset.

In summary, the generated code provides a skeleton for the requested analysis, but many implementations are either incorrect, overly simplistic, or lack the depth and robustness expected from an ""expert"" output. The ground truth provides much stronger and more correct implementations for critical steps like data preprocessing, optimal cluster determination, consensus clustering, and statistical significance testing.",llama-4-maverick,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classifica...",7,"The generated code attempts to address all requirements of the task and demonstrates a good understanding of various ensemble methods, optimization, and interpretation techniques. However, there are several key areas where it falls short in terms of correctness, completeness, and best practices compared to the ground truth.

**Strong Points:**
- Implements a variety of ensemble methods (Voting, Bagging, Boosting, Stacking).
- Attempts Bayesian optimization with Optuna.
- Includes SHAP for model interpretation.
- Implements model calibration and Brier score.
- Attempts ensemble diversity analysis.
- Basic data loading and preprocessing are present.

**Areas for Improvement / Missing Aspects:**
1.  **Data Preprocessing and Feature Engineering (Correctness/Completeness):**
    *   The prompt asked for ""comprehensive feature engineering"" and specifically handling ""0s"" in relevant columns as missing values (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`). The generated code performs `StandardScaler` and `PolynomialFeatures` but completely misses the critical step of handling these zero-values as NaNs and then imputing them. This is a significant data quality issue that affects model performance.
    *   The `PolynomialFeatures` are applied after `StandardScaler` on the *entire* dataset before `train_test_split`. This is a data leakage issue; `StandardScaler` and `PolynomialFeatures` should ideally be fitted only on training data and then transformed on both train and test sets, or at least performed with pipelines within cross-validation. The ground truth handles this via a `load_and_preprocess_data` function that applies these in the correct sequence.

2.  **Bayesian Optimization with Optuna (Correctness):**
    *   The `optimize_voting` function in the generated code attempts to tune `VotingClassifier` parameters. However, `GridSearchCV` is used *inside* a `nested_cv` function to optimize `lr__C` only, which doesn't reflect the Optuna study's suggested parameters for `lr__C`, `dt__max_depth`, and `svm__C`. The Optuna optimization is effectively incomplete and misapplied. The `study.optimize` call runs `optimize_voting`, but the `nested_cv` function has its own `GridSearchCV` that overrides or ignores the Optuna suggestion in that context.
    *   The ground truth correctly tunes an XGBoost model, demonstrating a more complex and relevant use case for Bayesian optimization.

3.  **Nested Cross-Validation (Correctness/Completeness):**
    *   The `nested_cv` function itself has issues:
        *   It defines a `GridSearchCV` with a fixed `param_grid` for 'lr__C' only, regardless of the model passed to `nested_cv`. This means it would only tune Logistic Regression, and the other models would not be properly tuned within the nested CV.
        *   It only applies to `VotingClassifier` and specifically tunes one parameter of one base estimator in `VotingClassifier`. This design is not generalized to evaluate ""all models"" as requested by the task.
    *   The generated code then completely *fails to execute* the `nested_cv` function for all the listed models. It merely defines it but doesn't integrate the training and evaluation of all models within a proper nested cross-validation loop. This is a major omission from the core requirement. The ground truth correctly implements a robust nested CV loop that evaluates multiple models, including ensembles.

4.  **Custom Ensemble Methods (Correctness):**
    *   The `CustomEnsemble` class has a very simplistic dynamic weighting strategy: `1 / (1 + np.std(pred))`. While it's a ""custom"" method, it's not truly data-driven in the sense of finding optimal weights. The ground truth uses validation AUCs to derive weights, which is a more sensible approach.
    *   The implementation structure of the `CustomEnsemble` does not seamlessly integrate its `fit` and `predict_proba` methods into the general `models` dictionary due to its custom weights calculation in `predict_proba` which might lead to issues.

5.  **CatBoost (Completeness):**
    *   The prompt specifically requested ""Apply advanced techniques like CatBoost for handling categorical features"". The generated code correctly imports `CatBoostClassifier` but then comments it out with ""not applicable here as there are no categorical features"". While true for the *processed* dataset, the request was about CatBoost's *capability* and implicitly, how one would apply it if there were categorical features. The ground truth includes `cat_features=None` in the CatBoost initialization, acknowledging the lack of explicit categorical features in the *current* dataset but still including the model, which is a better fulfillment of the prompt.

6.  **SHAP Values (Correctness):**
    *   `shap.Explainer(voting_soft)` will work, but `shap.Explainer(model.predict_proba, X_train)` would be more typical, or `shap.TreeExplainer` for tree-based models if applicable. The ground truth uses `TreeExplainer` for XGBoost, which is often more efficient and accurate if the model is a tree ensemble.
    *   `shap.initjs()` should be called once, typically at the beginning.
    *   The SHAP plots are shown at the very end, outside of any specific model evaluation, which makes them less useful for tying back to the performance of a particular model. The ground truth integrates SHAP explanation with the chosen best model.

7.  **Model Calibration (Correctness):**
    *   `CalibratedClassifierCV` is applied to `voting_soft` trained on `X_train, y_train`. This is okay, but for a more robust calibration, especially within a nested CV process as envisioned by the task, it would ideally be done on OOF predictions or a held-out set to prevent data leakage in calibration. The ground truth calibrates the *best* model found through nested CV, which is more aligned with the spirit of unbiased evaluation.

8.  **Ensemble Diversity (Correctness/Completeness):**
    *   The `ensemble_diversity` function calculates ""diversity"" as the mean difference in predictions, which is a valid but very basic metric. The ground truth uses a correlation matrix of Out-Of-Fold (OOF) predictions, which is a more common and informative way to analyze diversity and potential redundancy in ensembles. Crucially, the generated code doesn't collect OOF predictions from nested CV, which is essential for this kind of analysis and meta-modeling.

9.  **Overall Structure and Best Practices:**
    *   No function definitions for data loading, preprocessing, or main model evaluation. The ground truth uses functions (`load_and_preprocess_data`, a main loop for nested CV) which improves modularity and readability.
    *   Hardcoding `X_train`, `X_test` splits for Optuna and calibration, rather than integrating them consistently with the nested CV process.
    *   The final evaluation loop trains models *again* on the split data, but without using the nested CV or Optuna's best parameters. The ground truth stores results consistently from the nested CV iterations.

In summary, the generated code demonstrates awareness of the required components but often implements them partially, incorrectly, or without proper integration, leading to a less robust and less scientifically sound solution than the ground truth. The lack of proper nested cross-validation across all models and the misapplication of Optuna are significant drawbacks.",llama-4-maverick,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Load and preprocess data
df = pd.read_csv('diabetes.csv')
df = df.dropna()  # Simple handling for demonstration; consider more sophisticated methods for real ap...",3,"The generated code attempts to address several requirements but fails significantly in correctness, completeness, and overall quality for an ""expert"" level task.

**Correctness & Completeness Issues:**

1.  **PyMC Version:** The code uses `pymc` (which implies PyMC v5+ due to `pm.Model` and `pm.sample` syntax with return types) but imports `pymc` as `pm` and several functions (`pm.sample_posterior_predictive`, `az.from_pymc`) that are inconsistent with PyMC v5 or ArviZ v0.10. Specifically, `az.from_pymc` is deprecated and should be `az.from_pymc3` or `az.from_inference_data` depending on the input. This suggests a mix-up or an outdated understanding of the libraries. The ground truth explicitly uses `pymc3`.
2.  **Missing Value Handling:** The generated code uses `df.dropna()`, which is a very simplistic and often detrimental approach for a dataset like `diabetes.csv` where 0s represent missing values and dropping rows would lead to significant data loss. The ground truth correctly identifies 0s as NaNs and uses median imputation.
3.  **Hierarchical Model:** The ""Hierarchical Bayesian model"" section is *not* a hierarchical model. It's a standard logistic regression. A hierarchical model in PyMC3/PyMC5 requires defining varying intercepts or slopes based on a grouping variable. The ground truth correctly creates an `Age_Group` for this purpose.
4.  **Mixed-Effects Model:** Similar to the hierarchical model, the ""Mixed-effects model"" is fundamentally incorrect. It attempts to use `u[y_train.astype(int)]` for random effects, which is syntactically flawed and conceptually misapplies random effects to `y_train` indices rather than to actual grouping variables or individual observations. Mixed-effects models in PyMC typically involve group-level varying effects.
5.  **Bayesian Logistic Regression:** This part is a correct implementation of standard Bayesian logistic regression, but it's redundant given the previous (incorrectly named) hierarchical/mixed-effects models.
6.  **Survival Analysis:**
    *   The simulation of `time` using `np.random.exponential` is arbitrary and doesn't connect `time` to the actual diabetes `Outcome` or other features in a meaningful way. The ground truth provides a more reasoned (though still simulated) approach where `time` is inversely related to a derived risk score.
    *   The use of `pymc_experimental.distributions.Weibull` is non-standard and might not be stable. More importantly, the `lambda_` parameter is directly linked to `X_train` without considering the event status on the `df` that contains `time` and `event`. The implementation of the survival model is not a proper application of Bayesian survival analysis (e.g., Cox proportional hazards or AFT models).
7.  **Gaussian Process (GP):**
    *   The GP model setup is problematic: `observed_X=X_train, observed_y=y_train` directly models the output (`y_train`) as a GP *based on all features* (`X_train`), which is computationally very expensive and often not the typical use case for GP regression (usually used for non-parametric relationships between one or two features and an outcome). The task requested non-parametric modeling of relationships between *variables*, not directly predicting a binary outcome using a general GP model. The ground truth correctly demonstrates this by modeling `Glucose` as a function of `Age`.
    *   Using `pm.Bernoulli` for the outcome implies binary, while GP is usually for continuous data unless a link function is applied. The setup implicitly assumes `y_train` is continuous, which it isn't.
8.  **Causal Inference:**
    *   It imports `causalinference` but then only calls `est_via_matching()`. This library is a frequentist approach and does not provide *Bayesian* causal inference as the task implies (or at least could infer from the overall Bayesian context).
    *   Crucially, it *only* implements propensity score matching. The task explicitly asks for ""propensity score matching, instrumental variables"". Instrumental variables are completely missing.
    *   The application `X_train[:, 0]` as the treatment is arbitrary and likely incorrect without defining a specific treatment variable.
9.  **Bayesian Model Comparison:** Only WAIC is calculated; LOO cross-validation is missing despite being explicitly requested.
10. **Credible Intervals and PPCs:** The `az.from_pymc` function is deprecated. `az.plot_ppc` needs `data_pairs` or similar mapping if the observed data isn't named 'observed_data'. Without this information or correct setup, the PPC plots would likely fail or be misleading. The `posterior_predictive='outcome'` and `observed_data='outcome'` parameters are not standard for `az.from_pymc` / `az.from_pymc3` directly.

**Code Quality & Structure:**

1.  **Imports:** Imports `pymc` instead of `pymc3` as specified in the ground truth and implied by task context (the ground truth explicitly states PyMC3/PyMC4). The `pymc_experimental` import is unusual.
2.  **Data Preprocessing:** Hardcodes `df.dropna()` which makes the solution fragile and incomplete. Scaling happens but is not integrated into all models (e.g., survival analysis uses `df` directly).
3.  **Redundancy:** The `beta0` and `beta` parameters are redefined and sampled many times for each model, which is fine, but the overall structure indicates a lack of cohesive design for a comprehensive script.
4.  **Readability:** Variable names are okay, but the inline comments are sparse and do not explain the advanced concepts.

In summary, while the code attempts to touch upon all requirements, most of the advanced statistical modeling techniques (hierarchical, mixed-effects, survival, GP, causal inference) are either incorrectly implemented, conceptually flawed, or incomplete. The PyMC version mismatch further complicates execution and verification.",llama-4-maverick,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.cluster import SpectralClustering, OPTICS
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.metrics.cluster import adjusted_rand_score
fro...",5,"The generated code attempts to address many aspects of the prompt but falls short in implementation details, correctness, and completeness for several key requirements.

Here's a detailed breakdown:

**Correctness & Completeness Issues:**

1.  **Preprocessing & Feature Engineering:**
    *   **Metabolic Syndrome:** The `preprocess_data` function attempts to create `MetabolicSyndrome` but introduces features `Triglycerides`, `HDL`, and `WaistCircumference` which are explicitly *not* present in the Pima Indian Diabetes dataset (`diabetes.csv`). This is a critical error as `diabetes.csv` typically only contains 'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', and 'Outcome'. This makes the `MetabolicSyndrome` feature calculation incorrect and will lead to `KeyError`.
    *   **Missing Value Imputation:** While it replaces 0s with mean/median, the prompt specifically asked for *advanced* feature engineering, including metabolic syndrome indicators *based on available features*. The ground truth correctly uses the available features.
    *   **Scaling:** Correctly applies `StandardScaler`.

2.  **Clustering Algorithms:**
    *   The `cluster_data` function hardcodes `n_clusters=3` for K-Means, Hierarchical, GMM, and Spectral Clustering. This contradicts the requirement to *determine optimal clustering parameters*.
    *   DBSCAN and OPTICS parameters (`eps`, `min_samples`) are hardcoded. The ground truth demonstrates how to *estimate* `eps` for DBSCAN and provides more detailed parameter choices for OPTICS.
    *   The code correctly includes K-Means, DBSCAN, Hierarchical Clustering (Agglomerative), Gaussian Mixture Models, Spectral Clustering, and OPTICS.

3.  **Dimensionality Reduction:**
    *   **Autoencoder:** The autoencoder implementation is flawed. It uses `Dense(2, activation='relu')` for the encoded layer, and then `PCA(n_components=2)` to extract embeddings. This is incorrect. The `PCA` on `encoded` layer is redundant/wrong, and PCA is a linear technique, not an autoencoder. The prompt asks for ""Autoencoder-based embeddings"" which implies using the latent space of the autoencoder directly. The `encoder.predict(scaled_features)` is the correct approach to get autoencoder embeddings, but the network structure and its relation to PCA are confusing. It also uses `tf.keras` not `keras`. The prompt specifies `tensorflow` is needed, implying the fuller framework.
    *   **t-SNE and UMAP:** Correctly calls `TSNE` and `UMAP`.

4.  **Optimal Clustering Parameters:**
    *   The `determine_optimal_clusters` function only considers `KMeans` for silhouette, Calinski-Harabasz, and Davies-Bouldin scores. The prompt required this for *multiple clustering algorithms*, and the ground truth explicitly shows this for K-Means, GMM (BIC/AIC), and Hierarchical/Spectral (silhouette).
    *   It plots these scores but doesn't actually *select* or *use* an optimal number of clusters for the subsequent clustering steps. This is a critical omission.

5.  **Consensus Clustering:**
    *   The `consensus_clustering` function correctly attempts to build a co-occurrence matrix. However, it only calculates the raw sum of co-occurrences and normalizes it. It *does not* then apply a final clustering algorithm (like hierarchical clustering) on this consensus matrix to derive the consensus clusters, which is the whole point of consensus clustering. The output is a matrix, not new cluster labels. The ground truth correctly performs hierarchical clustering on the derived similarity/distance matrix.

6.  **Patient Phenotype Profiles:**
    *   The `patient_phenotype_profiles` function iterates through *all* cluster labels (KMeans, DBSCAN, etc.) without focusing on a single, strong result (like consensus or a best-performing single algorithm). This is less useful for clinical interpretation.
    *   It uses `f_oneway` for continuous variables, which assumes normality and equal variances. The ground truth correctly implements `kruskal` for a more robust non-parametric test.
    *   It lacks statistical significance testing for *categorical* features as per the prompt's request (e.g., Chi-squared for 'Outcome').
    *   The prompt asked for *detailed* profiles, which implies more than just `describe()` and a simple F-test for each feature; it requires comparing features *between* clusters. The ground truth does this by showing mean/std and then statistical comparisons.

7.  **Anomaly Detection:**
    *   Only implements `IsolationForest`. The prompt asked for ""anomaly detection to identify unusual patient profiles or outliers,"" and the ground truth provides `IsolationForest`, `OneClassSVM`, and `LocalOutlierFactor`. This is incomplete.
    *   It returns `anomalies` but doesn't integrate them meaningfully into the DataFrame or visualize them on reduced dimensions as requested.

8.  **Time-Series Clustering:**
    *   The code includes a placeholder `time_series_clustering` function. However, it explicitly assumes a 'time' column which is *not* in the `diabetes.csv` dataset. The ground truth correctly identifies this limitation for the given dataset and provides a conceptual explanation, which is the appropriate response given the data. The generated code attempts to implement it directly which is a mismatch with the dataset.

9.  **Association Rule Mining:**
    *   The `association_rule_mining` function tries to convert the entire DataFrame to `str` and uses `applymap`. This is problematic for numerical data as it bins or discretizes features which is crucial for ARM. The ground truth explicitly discretizes numeric features into bins (e.g., quantiles) which is the correct approach. Simply converting raw numeric values to strings will lead to very sparse and uninterpretable transactions for ARM.
    *   It uses `mlxtend` correctly otherwise.

10. **Patient Risk Stratification:**
    *   The `patient_risk_stratification` function correctly groups by cluster and calculates `Outcome` mean. However, it simply returns a dictionary of mean outcomes, instead of building a *system* that assigns qualitative risk levels (Low/Medium/High) and provides *clinical interpretation*, as explicitly asked by the prompt. The ground truth demonstrates how to map these means to risk levels and provides a detailed interpretation.

11. **Visualization:**
    *   The final visualization section is incomplete and also flawed. It tries to plot `reduced_feature` against `cluster_labels` for *every* cluster algorithm on a single plot. This will result in an unreadable plot with overlapping colors and legends. The intent was to show *each* clustering result on the dimensionally reduced space, or perhaps a selected best one. The ground truth correctly shows individual plots for each dimensionality reduction technique for a refined set of anomalies.

**Code Quality:**

*   **Functions:** Breaking down the code into functions is good, but they are not well integrated (e.g., `determine_optimal_clusters` doesn't pass back the optimal value).
*   **Imports:** Mostly correct, but `keras` should be `tensorflow.keras` for modern TensorFlow.
*   **Clarity:** Lack of comments makes it hard to follow the logic in some sections, especially for the autoencoder and consensus clustering.
*   **Error Handling:** Lacks robust error handling, especially for parameter estimation and in sections like consensus clustering where data issues could lead to crashes (e.g., all 0s in distance matrix).

Overall, while the code covers the breadth of tasks, the depth and correctness of implementation for each task are severely lacking, leading to a low score.",llama-4-maverick,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_er...",6,"The generated code attempts to address all requirements but falls short in several key areas.

**Correctness:**
*   **Data Preprocessing:** The `ColumnTransformer` is missing, which is crucial for handling mixed data types (numerical and categorical) correctly with different preprocessing steps. Simple scaling after dropping the target is insufficient for a real-world dataset like this, which contains categorical features (e.g., 'Regional indicator').
*   **Feature Engineering:** Polynomial features are applied before scaling, which is generally incorrect. Scaling should happen first in the pipeline for most models, especially those sensitive to feature scales. Also, it applies `PolynomialFeatures` without considering numerical vs categorical features properly.
*   **Voting Regressor ""Hard/Soft"":** The `VotingRegressor` in scikit-learn for regression combines predictions through averaging, not ""hard"" or ""soft"" voting in the classification sense. The distinction is typically between simple averaging (`weights=None`) and weighted averaging. The code provides both but the distinction is misleading for regression.
*   **Ensemble Implementations:** The base estimators are assigned to raw models (`model1`, `model2`, etc.) but these models are then trained on `X_train_poly` directly. The ensemble models (`voting_hard`, `bagging_dt`, `adaboost`, `stacking`) are defined but not *fitted* on data in the main script flow, meaning they won't actually be trained. Only the `xgb_model`, `lgb_model`, `catboost` are individually fitted later for interpretation, but not within the context of the ensemble definitions.
*   **Custom Ensemble Weighting:** The attempt at dynamic weight assignment is flawed. Weights are calculated based on `mean_squared_error(y_train, model.predict(X))`, which is in-sample error and leads to overfitting. Proper dynamic weighting for an ensemble requires out-of-fold predictions to avoid data leakage.
*   **Blending/Multi-level Stacking:** These sections are conceptually okay but implemented very superficially. The `blending_meta_learner` is trained only on `X_val_blend` predictions but the base models were trained on `X_train_blend`, this is the correct idea. However, the final `meta_model_level2` is trained on predictions from `meta_model_level1` (which was trained on `val_blend` predictions), but `y_val_blend` is used as target for `meta_model_level2`. This creates a very fragile and likely overfit setup. A true multi-level stacking typically uses OOF predictions from all level-1 models as input for level-2.
*   **Nested Cross-Validation:** The nested CV implementation is incomplete. It sets up an `KFold` for the outer loop but then only fits a single `xgb.XGBRegressor` using the `study.best_params` found *globally* before the nested CV loop. A true nested CV would re-run the Optuna optimization *within each outer fold* on the respective training data for that fold.
*   **Model Calibration:** `CalibratedRegressorCV` in scikit-learn is primarily for classification (predicting probabilities). While it might run for regression, its primary purpose is probability calibration, not direct regression calibration or uncertainty quantification as typically done by, e.g., quantile regression or conformal prediction. The `y_pred_std` calculation is also incorrect based on how `CalibratedRegressorCV` works; it doesn't give standard deviations of predictions directly.
*   **SHAP/LIME:** The SHAP explainer for XGBoost expects `X_test_poly` (which was preprocessed), but `X.columns` is passed to the LIME explainer. `X.columns` represents original feature names, not the polynomial features, leading to a mismatch.
*   **Ensemble Diversity Analysis:** The diversity is calculated as `np.mean((pred_i - pred_j) ** 2)`, which is a simple mean squared difference of predictions. While a metric, it's not a common or robust way to assess ensemble diversity compared to correlation of predictions or more complex ensemble diversity measures. Furthermore, it directly uses `model.predict(X_test_poly)` rather than OOF predictions, which is less ideal.
*   **Model Combination Optimization:** Uses `X_test_poly` and `y_test` for optimization, which is leakage. This should be done using OOF predictions on the training data, not the test set.

**Code Quality:**
*   **Imports:** Mostly good, but `cbt` for CatBoost is less common than `CatBoostRegressor`.
*   **Structure:** Lacks clear functions or classes, leading to a sprawling script. Each major task (preprocessing, ensemble training, hyperparameter tuning, etc.) is a block of code, which is acceptable but less maintainable.
*   **Readability:** Variable names are okay, but the lack of separation (e.g., all ensemble definitions at the top, then their actual training/fitting much later or not at all) makes the flow hard to follow.
*   **Missing Imports:** `matplotlib.pyplot` and `seaborn` are used in the ground truth for plotting interpretation, but not imported in the generated code, which would cause errors if executed fully.

**Completeness:**
*   **Advanced Feature Engineering:** The ""advanced"" aspect is just `PolynomialFeatures`, which is basic. No handling of 'Country name' or 'Regional indicator' for robust encoding, which is essential for this dataset.
*   **Ensemble Methods:** Defines many ensembles but doesn't *fit* them all.
*   **Custom Ensemble:** The dynamic weighting is flawed as described above.
*   **Blending/Multi-level Stacking:** Implemented, but superficially and incorrectly.
*   **Model Interpretation:** Attempts SHAP and LIME but issues with feature names and usage with `CalibratedRegressorCV` limit their utility.
*   **Model Calibration/Uncertainty:** `CalibratedRegressorCV` is misapplied for regression uncertainty, and no other robust UQ methods are used (e.g., quantile regression).
*   **Ensemble Diversity/Optimization:** Attempts are made, but both are fundamentally flawed by using in-sample/test-set data for calculations that require OOF predictions.

Overall, the code demonstrates an understanding of the concepts but struggles with correct and robust implementation details, especially concerning data leakage, proper pipeline usage, and advanced ensemble techniques.",llama-4-maverick,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import pytensor.tensor as pt
from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel
from causalgraphicalmodels import CausalGraphicalModel
import statsmodels.api as sm
from statsmodels.sandbox.regression.gmm import IV2SLS
from scipy.stats import norm

# Load and preprocess data
data = pd.read...",5,"The generated code attempts to address many aspects of the prompt but fails significantly in execution and correctness for several key components.

**Correctness & Completeness Issues:**

1.  **PyMC Version Mismatch:** The generated code uses `pymc` (PyMC v4+ syntax like `pm.Normal(sigma=...)`), but then attempts to convert traces using `az.from_pymc3`, which is for PyMC3. This indicates a fundamental misunderstanding or incorrect porting between `PyMC3` and `PyMC`. The ground truth explicitly asked for `PyMC3 or PyMC4`. The current code mixes syntaxes and `az.from_pymc3` will fail or produce errors on `PyMC` (v4+) traces. `target_accept` is a PyMC4 argument, `sigma` for SD is PyMC4, not `sd`. This inconsistency makes the PyMC models likely non-functional as intended.
2.  **Missing Value Handling:** The code uses `dropna` on a subset of columns, which is very simplistic and can lead to significant data loss, especially with many columns that might have sparse missing values. The ground truth demonstrated comprehensive handling using `SimpleImputer` and `ColumnTransformer` for different data types. The generated code also renames columns for processing but then uses original column names in PyMC models (`data['Ladder score']`) which might cause issues if not carefully managed.
3.  **Hierarchical Model:** The hierarchical model is overly simplified. It only includes `gdp` as a fixed effect and `region` as a random intercept. It doesn't incorporate other significant predictors like `Social support`, `Healthy life expectancy`, etc., which were implicitly expected to be part of the ""advanced statistical modeling"" and the ""informative priors based on happiness research literature"" section (where those variables are typically included). The prompt specifically asked to model ""happiness with regional random effects"".
4.  **Structural Equation Models (SEM):** The SEM implementation is severely limited and not representative of true SEM. It models two separate regressions (GDP -> Social, and (GDP + Social) -> Happiness) but the `social_support_obs` is modeled with a fixed `sigma=1`, which is usually learned. More importantly, it doesn't form a coherent ""structural equation model"" in the typical sense (e.g., using `pymc`'s `Deterministic` variables to link latent constructs or more formally defining path models). The ground truth approximated SEM by linking Bayesian regressions, which is a more reasonable approach within `PyMC`. The generated SEM is just two independent regressions with some paths.
5.  **Causal Inference (IV and PSM):**
    *   **IV:** The `IV2SLS` from `statsmodels.sandbox.regression.gmm` is an outdated or non-standard module and less commonly used for robust IV. The use of `instrument = data['Healthy life expectancy']` as an IV for GDP is highly questionable. Healthy life expectancy is very likely directly correlated with happiness, violating the exclusion restriction. The ground truth code correctly identified this problem and simulated a plausible instrument for demonstration, acknowledging the data's limitations.
    *   **PSM:** The PSM implementation is incomplete. It calculates propensity scores but then directly calculates ATE by taking `treated` and `control` means without explicitly performing actual matching (e.g., nearest neighbors, caliper matching) or weighting. The `propensity_score` is just created, but not used for matching or inverse probability weighting. The ground truth performed NearestNeighbors matching explicitly.
6.  **Bayesian Regression with Informative Priors:** This section is relatively okay, but again, the use of a fixed `sigma=0.1` for errors in the likelihood model is generally not good practice for a full Bayesian treatment unless there's strong prior knowledge of the error variance. It also uses PyMC4 syntax with `az.from_pymc3`.
7.  **Gaussian Processes:** The `sklearn.gaussian_process.GaussianProcessRegressor` is not a *Bayesian* Gaussian Process. The prompt explicitly asked for *PyMC3 or PyMC4* for Bayesian models, and the GP should ideally be implemented within a Bayesian framework (like `pm.gp` in PyMC3/4) to get posterior distributions and seamlessly integrate with other Bayesian tools. This is a critical departure from the ""Bayesian modeling"" requirement.
8.  **Mediation Analysis:** The mediation analysis provided (simple linear regression coefficient calculations `np.cov / np.var`) is a classical approach, not a Bayesian approach. The prompt asked for Bayesian modeling throughout the solution. The ground truth demonstrated a calculation of indirect and total effects by sampling from the posterior distributions of the relevant parameters, which is the correct Bayesian way.
9.  **Difference-in-Differences (DiD):** The simulation of `time` and `treated` variables is random (`np.random.randint`) and not based on any logical or demographic split. This makes the simulation arbitrary and less realistic for demonstrating a policy impact compared to the ground truth which used a GDP threshold for treatment assignment.
10. **Bayesian Model Comparison:** This section attempts WAIC and LOO-CV, but given the earlier issues with PyMC versions and incorrect model specifications, the comparisons may not be reliable.
11. **Credible Intervals, PPC, Sensitivity Analysis:** Calls to `az.plot_posterior` and `az.plot_ppc` are present, but the models themselves are flawed earlier. The sensitivity analysis is only for a simple model and changes priors from informative to non-informative rather than systematically testing different sets of priors.

**Code Quality Issues:**

*   **Imports:** The import `pymc as pm` is correct for PyMC v4+, but the call `az.from_pymc3` is incorrect for PyMC v4+ traces.
*   **Structure:** The code is somewhat monolithic, without clear functions or classes, but for a script of this nature, it's acceptable.
*   **Readability:** Variable renaming is inconsistent (e.g., `data['Ladder score']` vs. `happiness`).
*   **Documentation:** Lack of comments for complex sections, especially for `IV` setup and `SEM` approximation.

In summary, while the generated code touches upon requested components, its significant flaws in implementing key Bayesian concepts, causal inference methods, and PyMC version compatibility, along with simplistic data handling, make it substantially less effective than the ground truth.",llama-4-maverick,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.cluster import SpectralClustering
import hdbscan
from sklearn.manifold import TSNE
import umap
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.metrics.pairwise import pairw...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in correctness, completeness, and structure, leading to a non-functional and highly inaccurate solution.

**Correctness & Completeness Issues:**

1.  **Feature Engineering:**
    *   The `create_happiness_profiles` function creates a boolean `happiness_profile` column ('High'/'Low') based on mean 'Ladder score', which is a very rudimentary profile and doesn't align with ""advanced feature engineering including happiness profiles and regional characteristics"" the ground truth implies (creating interaction terms, ratios, etc.).
    *   `create_regional_characteristics` manually hardcodes a few countries for ""Western Europe"" and ""North America"" and lumps everything else into ""Other"". This is fundamentally incorrect and doesn't handle the actual `Regional indicator` column present in the dataset, nor does it create useful regional characteristics for analysis. The ground truth correctly uses one-hot encoding on the existing 'Regional indicator' column.

2.  **Clustering Algorithms:**
    *   It applies the clustering algorithms but doesn't implement optimal parameter determination *before* applying them. It uses hardcoded `n_clusters=5` for KMeans, Agglomerative, GMM, and Spectral, and `eps=0.5, min_samples=10` (`min_cluster_size=10`) for DBSCAN/HDBSCAN. The ground truth explicitly has a loop to find optimal 'k' using silhouette score for K-Means, Agglomerative, GMM, and Spectral, and tunes DBSCAN/HDBSCAN parameters to find better configurations.
    *   The `fit_predict` method is called directly, but for `GaussianMixture` and `SpectralClustering`, `fit_predict` is not suitable sometimes (e.g., SpectralClustering often uses `assign_labels='kmeans'` after `fit`).

3.  **Dimensionality Reduction:**
    *   It correctly includes t-SNE, UMAP, and PCA. However, it completely misses the Autoencoder requirement, which is a significant omission given its complexity and advanced nature. The ground truth successfully implements a Keras-based autoencoder.

4.  **Optimal Parameter Determination:**
    *   The `determine_optimal_parameters` function for optimal clustering parameters is rudimentary. It only iterates through `n_clusters` for K-Means, Agglomerative, and GMM, and uses Silhouette and Calinski-Harabasz scores. It lacks the ""Elbow method"" plot and ""stability analysis"" mentioned in the prompt (even if stability analysis is conceptually outlined in the ground truth due to complexity, it's addressed). More importantly, it doesn't *use* the optimal parameters found to re-run the clustering algorithms; it just determines them.

5.  **Consensus Clustering:**
    *   The `consensus_clustering` function correctly computes the co-association matrix. However, it *stops there*. It doesn't perform hierarchical clustering on this matrix to derive final consensus clusters, which is the core purpose of consensus clustering. The ground truth leverages this matrix by converting it to a distance matrix and then applying hierarchical clustering (`fcluster`) to derive the actual consensus labels, and even determines the optimal `k` for these consensus clusters.

6.  **Detailed Country Profiles:**
    *   It calculates `groupby().mean()` for cluster profiles, which is a start. However, it attempts statistical significance testing using `f_oneway` for *all* numerical features, including the `cluster` column derived from `labels`, which is nonsensical. It also calculates `p_val` but doesn't display or interpret it clearly, nor does it calculate ""effect size analysis"" (e.g., Eta-squared) as required by the prompt and implemented in the ground truth. The approach to getting original features for the final profiling is also less robust than the ground truth's merge strategy.

7.  **Anomaly Detection:**
    *   It only uses Isolation Forest, while the prompt explicitly asks for ""anomaly detection"". The ground truth implements Isolation Forest, Local Outlier Factor (LOF), and One-Class SVM.

8.  **Network Analysis:**
    *   The `build_happiness_network` function builds a network based on *feature correlations* (`correlation_matrix = original_df.select_dtypes(include=['int64', 'float64']).corr()`). The prompt asks for relationships between *countries* based on happiness factors. While country similarity *is* based on happiness factors, the code attempts correlation between *features*, not between countries, which is a fundamental misunderstanding. The ground truth correctly computes cosine similarity *between countries* (rows of the scaled feature matrix) and builds a graph based on that. It also misses extracting insights like centrality measures.

9.  **Association Rule Mining:**
    *   The `mine_association_rules` function attempts to apply ARM on `df_binary = (original_df.select_dtypes(include=['int64', 'float64']) > original_df.select_dtypes(include=['int64', 'float64']).mean()).astype(int)`. This binarization method (above/below mean) is highly simplistic and might not yield meaningful ""patterns in happiness characteristics"". The ground truth uses `pd.qcut` for more intelligent binning and pre-processes labels for clarity (e.g., 'low\_GDP', 'high\_SocialSupport').

10. **Country Recommendation System & Policy Insights:**
    *   The recommendation system is extremely basic: it just returns `num_recommendations` countries from the *same cluster* as the target country. It doesn't find ""most similar countries within the same cluster"" or any nuanced approach.
    *   The policy insights are generated by calling `.describe()` on cluster characteristics, which provides summary statistics but doesn't produce ""policy insights based on clustering results with geopolitical validation"" as required. The ground truth correctly identifies high/low happiness clusters and outlines insightful comparisons and policy directions based on their profiles, and explicitly mentions geopolitical validation through regional distribution.

**Code Quality & Structure:**

*   **Global Variables vs. Class:** The ground truth uses a well-structured class (`AdvancedHappinessAnalysis`) to encapsulate all methods and manage shared state (raw data, processed data, clustering results, DR embeddings, etc.). The generated code uses scattered functions that rely on global variables (`df`, `df_scaled`, `original_df`, `cluster_results`, etc.) which makes the code harder to manage, understand, and debug, especially for a complex multi-step analysis.
*   **Imports:** Some imports are redundant or missing (e.g., `IsolationForest` is used but not imported in the main block of initial imports, `matplotlib.pyplot as plt` is missing but used, `seaborn` is not imported but would be useful for plotting).
*   **Magic Numbers:** Excessive use of magic numbers (e.g., `n_clusters=5`, `eps=0.5`, `min_samples=10`) without any justification or prior optimization.
*   **Error Handling/Robustness:** Lacks the robustness and error handling (e.g., what if `n_samples` is too small for t-SNE perplexity, or if a clustering algorithm only finds one cluster for silhouette score calculation).
*   **Visualization:** The visualization part is basic (scatter plots of DR results) and doesn't integrate cluster labels for all visualizations as effectively as the ground truth. It also uses PCA where the prompt (and ground truth) emphasize advanced techniques like t-SNE, UMAP, and Autoencoders primarily for visualization.

In summary, the generated code demonstrates a superficial understanding of the requirements. While it contains some correct function calls, the overall implementation of advanced concepts like feature engineering, optimal parameter determination, consensus clustering, anomaly detection, network analysis, and policy insights is either highly inaccurate, incomplete, or missing.",llama-4-maverick,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import numpy as np

# Load data and preprocess
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])
germany_data = data[data['location'] == 'Germany'].copy()
germany_data.set_index('date', inplace=True)

# Handle missing values
germany_data['new_cases'] = germany_...",7,"The generated code correctly implements the core SARIMA forecasting task, including data loading, filtering, auto_arima for order selection, model training, forecasting, and RMSE evaluation with plotting.

However, there are several areas for improvement as highlighted in the ground truth:

1.  **Missing Value Handling:** The generated code uses `interpolate(method='time')` which is a good choice, but it doesn't account for potential NaNs at the beginning or end of the series (which `ffill`/`bfill` or explicit `dropna` would handle). The ground truth provides a more robust multi-step approach (`ffill`, `bfill`, then `interpolate` as a fallback, and a final `dropna` with checks) to ensure no NaNs remain.
2.  **Exogenous Variable Handling for `auto_arima` and `SARIMAX`:** The generated code passes `exogenous_vars` to `auto_arima` which is `train_data[['stringency_index', 'people_fully_vaccinated']]`. This is correct. However, for the `SARIMAX` model training it directly uses `train_data[['stringency_index', 'people_fully_vaccinated']]`. This is also fine, but isolating `y_train`, `X_train` etc., as done in the ground truth, makes the code cleaner and less prone to errors, especially when `X` needs to be consistently handled.
3.  **Forecast Start/End Parameters:** The `predict` method is used with `start=len(train_data), end=len(germany_data) - 1`. While this often works, using the `get_forecast` method with `steps` and
    `exog` (as `exog_test` is provided) is generally more robust and recommended for forecasting future periods with exogenous variables in `statsmodels`. The ground truth does this by first using `get_forecast` to get a forecast object and then extracting `predicted_mean`.
4.  **Plotting:** The plotting implementation is basic. It plots `Actual` and `Forecast` for *only the test period*. The ground truth includes the training data in the plot, which provides better context for the forecast. It also includes more detailed plot labels, titles, and formatting (e.g., RMSE in title, `tight_layout`, `grid`).
5.  **Robustness and Error Handling:** The ground truth includes checks for file existence, sufficient data length for splitting, and detailed print statements for user feedback. The generated code lacks these vital robustness features, which are important for production-level code.
6.  **Negative Forecasts:** In time series, particularly with SARIMA, negative forecasts can occur for non-negative quantities like 'new_cases'. The ground truth explicitly handles this by setting negative forecasts to zero, which is a good practice. The generated code does not.",llama-4-maverick,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from scipy.stats import kruskal
from statsmodels.stats.multicomp import MultiComparison

# Load data
df = pd.read_csv('winequality-red.csv')

# Define quality groups
df['quality_group'] = pd.cut(df['quality'], bins=[0, 4, 6, 10], labels=['low', 'medium', 'high'], right=False, include_lowest=True)

# Separate features
features = df.c...",6,"The generated code addresses most parts of the task but has several significant deficiencies, especially regarding the statistical rigor and error handling.

**Correctness & Statistical Rigor:**
*   **Normality Check:** The normality check `stats.shapiro(group.sample(min(len(group), 5000)).values)` is flawed. `5000` is an arbitrary large number, and `sample()` could lead to different results each run or miss non-normality in the full dataset. More importantly, it should check normality for *all* groups, and the decision to use ANOVA should *also* consider homogeneity of variances (Levene's test), which is missing.
*   **Post-hoc for Kruskal-Wallis:** The code uses `MultiComparison` with `allpairtest(stats.mannwhitneyu, method='bonferroni')` which is a correct approach for post-hoc after Kruskal-Wallis. However, `MultiComparison` is not directly built for Kruskal-Wallis but is generally used for pairwise comparisons in a more flexible/general way. The Ground Truth uses `multipletests` for Bonferroni correction on raw Mann-Whitney U test p-values, which is a common and correct implementation of Dunn's test proxy.
*   **Feature Selection:** `features = df.columns[:-2]` assumes 'quality' and 'quality_group' are always the last two columns. This is fragile. A better approach (like in the Ground Truth) is to explicitly exclude them.
*   **Incomplete Statistical Outputs:** The `results` dictionary stores basic stats but doesn't retain the full post-hoc results in an easily readable format for later printing in the summary. For `pairwise_tukeyhsd`, it stores the object, which is good, but for `MultiComparison`, `allpairtest` returns a structured array/dataframe, and storing `results[feature]['post_hoc'] = mc.allpairtest(stats.mannwhitneyu, method='bonferroni')[0]` means only the first part of the result is stored, not the comprehensive pairwise comparison table (`.summary()`).

**Code Quality & Completeness:**
*   **Error Handling:** There is no error handling for the file not found, which is crucial for real-world scripts.
*   **Clarity in Definitions:** The `bins=[0, 4, 6, 10]` definition for quality groups is slightly different from the exact `score <= 4`, `5 <= score <= 6`, `score >= 7` specified in the task, especially for the high end (`10` vs no upper bound). The Ground Truth's explicit `assign_quality_tier` function is more robust and readable.
*   **Visualization:** The box plots are generated correctly, but the plotting style is basic. The Ground Truth includes `plt.style.use('seaborn-v0_8-darkgrid')` and customizes labels/titles for better presentation.
*   **Summary:** The summary section prints the raw `post_hoc` objects/arrays, which can be less readable than a formatted summary like the Ground Truth's. For `MultiComparison`, printing `results[feature]['post_hoc']` might not print the nicely formatted table.

**Areas for Improvement:**
*   Implement explicit Levene's test for homogeneity of variance before deciding on ANOVA.
*   Improve the normality check to consider all data or a statistically sound sampling strategy if the dataset is extremely large (though not strictly necessary here).
*   Add robust error handling for file operations.
*   Make the feature selection more explicit and less reliant on column order.
*   Ensure that the full, readable post-hoc results are stored and displayed in the summary.
*   Enhance plotting aesthetics.
*   Consider making the `ALPHA` value a configurable constant.",llama-4-maverick,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, f1_score
import shap

# Load data and parse date
data = pd.read_csv('synthetic_covid19...",6,"The generated code attempts to address all aspects of the task, showing a good understanding of the requirements. However, there are several critical issues, primarily in the target variable definition, feature engineering, and the dataset splitting strategy, which significantly impact its correctness and robustness.

**Strengths:**
- **Pipeline and Preprocessing:** Correctly uses `ColumnTransformer` and `Pipeline` for preprocessing numerical and categorical features.
- **XGBoost and Hyperparameter Tuning:** Implements `GridSearchCV` for XGBoost hyperparameter tuning.
- **Evaluation Metrics:** Calculates both ROC-AUC and F1-score as requested.
- **SHAP:** Attempts to use SHAP for feature importance.

**Weaknesses and Areas for Improvement:**

1.  **Target Variable Definition (Major Issue):**
    *   **Incorrect Logic:** The target variable definition `data.groupby('location')['new_deaths'].transform(lambda x: (x > 0) & (x >= x[x > 0].quantile(0.9))).astype(int)` is **fundamentally flawed**.
        *   `x[x > 0].quantile(0.9)` correctly gets the 90th percentile of non-zero deaths.
        *   However, `(x > 0) & (x >= ...)` means that *any* `new_deaths` value on a given day that is greater than *or equal to* the 90th percentile (and also greater than 0) will be marked as 1. The definition was ""top 10%"", which implies *strictly greater than* the 90th percentile, not greater than or equal to.
        *   More critically, `x` in the lambda function refers to the *entire series for that group*. When `x > 0` is applied, it creates a boolean series. When `x[x > 0]` is used, it filters the `new_deaths` series *within that group* to only non-zero values. This part is correct.
        *   The main issue is the `transform` itself: The output of `transform` expects a series of the same length as the input group. The current lambda outputs a boolean series (which is then converted to int). While technically it runs, the condition `x >= x[x > 0].quantile(0.9)` is effectively comparing each `new_deaths` value `x` against *a single scalar value* (the 90th percentile calculated from the group). This seems okay on the surface for a given group, but the `transform` *applies the function row-wise based on the group*. The specific logic needs to ensure it's doing the comparison for *each row* in relation to the *group's 90th percentile*. The ground truth's two-step approach (calculating percentiles per location first, then joining back) is more robust and clearer. The current lambda function's application is slightly ambiguous and could lead to misinterpretations or edge cases where non-zero `new_deaths` that are *not* high are incorrectly classified.

2.  **Feature Engineering (`days_since_first_case`):**
    *   `data.groupby('location').cumcount()` simply counts rows within each group. This **does not correctly calculate 'days since first case'** for a location if there are gaps in dates or if the `date` column isn't continuous. It should be `(current_date - first_date_of_location).dt.days`.

3.  **Missing Value Handling (Critical Flaw):**
    *   `data = data.fillna(0)`: This is applied **too broadly and too early**.
        *   It fills *all* NaNs with 0, including static features like `median_age`, `gdp_per_capita`, etc., which might be better imputed with median/mean or left for the `SimpleImputer` in the `Pipeline`. More importantly, it fills NaNs *before* the `people_fully_vaccinated_per_capita` calculation, which could lead to `0/0` or `X/0` scenarios that aren't handled gracefully (`NaN` or `inf` might remain).
        *   The prompt asked to handle NaNs *from feature engineering* specifically, with suggestions like ""fill with 0 or backfill"". Ground truth used `ffill` then `fillna(0)` for engineered features per group, which is a much more robust and time-series-aware approach than a global `fillna(0)`.
        *   The `SimpleImputer` in the pipeline is then redundant or ineffective for numerical features because `fillna(0)` already took care of all NaNs globally.

4.  **Feature selection `people_fully_vaccinated_per_capita`:**
    *   This feature is created *after* `numerical_features` list is defined. This means `people_fully_vaccinated_per_capita` would not be included in the `numerical_features` list passed to the `ColumnTransformer`, unless the list is re-defined or dynamically built later. It should be created *before* the list definition.

5.  **Data Splitting (Sub-optimal, but not wrong):**
    *   `TimeSeriesSplit(n_splits=5)` is used, but then only the *last fold* (`break`) is used for the train-test split. This means the model is only evaluated on the last fold of `TimeSeriesSplit`. While this correctly creates a time-based final test set, it's not leveraging the full
        `TimeSeriesSplit` for cross-validation within `GridSearchCV`. The prompt asked for ""a time-based split (e.g., last 20% of data for testing)"", which the ground truth did simply by sorting and taking the last 20%. The generated code, by doing `TimeSeriesSplit` and then `break`, essentially creates *a* time-based split, but not necessarily the last 20% and doesn't explicitly use the `TimeSeriesSplit` object for `GridSearchCV`'s `cv` argument (it uses `cv=3`). This is a conceptual mismatch to the prompt's specific request for a static last 20% split for final evaluation, and then using `cv` for cross-validation on the *training set* during tuning.

6.  **SHAP Implementation:**
    *   The SHAP part is okay, but `explainer(X_test_transformed)` implies using the test set for SHAP values. While possible, it's more common and often recommended to compute SHAP values for the training data or a representative sample for global explanations, as the ground truth does.
    *   The `shap.summary_plot` doesn't explicitly pass `feature_names`, which could lead to generic names like ""f0, f1, ..."" if the transformed DataFrame `X_test_transformed` does not retain original column names or if the `ColumnTransformer` defaults to numpy arrays without named columns.

7.  **Punctuation/Format:** The generated code doesn't include imports like `matplotlib.pyplot` or `warnings` which were in the ground truth and generally advisable for the visualization task.

Overall, the generated code has strong structural elements (pipelines, cross-validation setup) but critical errors in the feature engineering and target definition. The `fillna(0)` is a sweeping generalization that undermines the `SimpleImputer` and could mask true data patterns.",llama-4-maverick,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import mixedlm

# Load data and parse 'date'
data = pd.read_csv('synthetic_covid19_data.csv')
data['date'] = pd.to_datetime(data['date'])

# Calculate 'vaccination_percentage'
data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population'] * 100

# Handle missing values
for col in ['reproduction_rate', 'stringency_index', 'vaccination_percentage']:
    data[col] = data...",6,"The generated code attempts to address all aspects of the task, and correctly identifies the need for nested random effects. However, there are several key issues and areas for improvement that prevent it from being a strong solution.

**Correctness Issues:**
*   **Missing Value Imputation:** The median imputation grouped by location is applied *after* linear interpolation only if there are still NaNs. This might not be robust enough. More importantly, it doesn't handle the `population` and `people_fully_vaccinated` missing values before calculating `vaccination_percentage`, which could lead to NaNs in the percentage itself. The prompt also suggests ""interpolation or median imputation grouped by location,"" but the current approach for `reproduction_rate`, `stringency_index`, and `vaccination_percentage` only uses linear interpolation *within* each location and then group-wise median imputation only for remaining NaNs. A global median or similar fallback is usually needed for locations entirely missing data.
*   **Interaction Term in Formula:** The prompt specifies `stringency_index`, `vaccination_percentage`, and *their interaction*. `stringency_index * vaccination_percentage` in `statsmodels.formula.api` correctly generates main effects and their interaction. This part is correct.
*   **Random Effects Specification:** The initial `md` model `vc_formula={'continent': 'C(continent)'}` is an attempt at variance components for continent, but `mixedlm`'s `vc_formula` is for more complex random effects structures (e.g., slopes varying by group) rather than just random intercepts. The `groups` argument defines the grouping variable for the random effects. For ""random intercepts for 'location' nested within 'continent'"", the correct approach is to create a single group ID that combines 'continent' and 'location', as done in `data['continent_location'] = data['continent'] + '_' + data['location']`. This is correctly used in `md_nested`.
*   **Model Fitting Method:** `method=['lbfgs']` is used, which is fine, but the default might be robust enough, and `maxiter` was not specified, which can lead to convergence issues (as alluded to in the warnings section of the ground truth).
*   **Reporting:** The code prints the full `summary()` result, which is good, but doesn't explicitly extract and report the fixed effects coefficients, standard errors, and p-values in a clear, interpretable way as requested by ""Interpret the significance and direction of these effects."" It only lists whether they are significant. Similarly, for variance components, it prints `mdf_nested.cov_re` which is the covariance matrix of random effects; for a random intercept model, this is a 1x1 matrix whose single value is the variance. Printing `mdf_nested.summary()` table of random effects would be clearer.

**Code Quality and Completeness:**
*   **Imports:** Proper imports are made.
*   **Structure:** The code is logically structured, following the steps outlined in the prompt.
*   **Error Handling:** There's no handling for `FileNotFoundError` for the CSV, which could make the script fail silently if the file is missing or misnamed.
*   **Verbosity/Guidance:** The output is a bit terse. For instance, the interpretation section just says ""significant"" or ""not significant"" rather than discussing the direction of the effect or its practical meaning, which was explicitly requested (""Interpret the significance and direction of these effects"").
*   **Assumptions Discussion:** The discussion of assumptions is very brief and doesn't clearly explain *how* they could be checked beyond just naming the plots (e.g., Q-Q plots for normality, residual plots for homoscedasticity). The ground truth provides more detail.
*   **Specific Warnings:** The ground truth includes specific warning suppressions that are common in `statsmodels` mixed models due to convergence or boundary issues. The generated code lacks this, which might lead to noisy output.

**Overall:**
The generated code correctly identifies the core modeling task (mixed-effects with nested random effects) and sets up the formula correctly. However, the data preprocessing (especially missing value handling for `population` and `people_fully_vaccinated` before calculating percentage) is incomplete, and the result reporting and interpretation lack the depth requested in the prompt. The initial attempt at `vc_formula` for random effects also shows a slight misunderstanding of how `groups` and `re_formula` primarily define random effects in `mixedlm` for simpler random intercept/slope models.",llama-4-maverick,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Select features for clustering
features = df[['elixirCost', 'Win Rate', 'Usage']]

# Standardize the features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Determine the number of clusters (K) using the elbow met...",6,"The generated code correctly addresses most of the task requirements but has several areas for improvement in terms of robustness, best practices, and completeness.

**Correctness:**
*   Loads data: Yes.
*   Selects features `elixirCost`, `Win Rate`, and `Usage`: Yes, but the selection `df[['elixirCost', 'Win Rate', 'Usage']]` directly creates a DataFrame `features`, which is slightly less robust than explicitly checking for feature existence like the Ground Truth.
*   Standardizes features: Yes.
*   Determines K (2-7, elbow/silhouette): Yes, it attempts both methods and plots them.
*   Applies K-Means with chosen K: Yes, but the K selection is manual via `input()`, which is not typically how an ""appropriate"" number is determined programmatically based on the plots themselves.
*   Reduces dimensionality with PCA: Yes.
*   Creates scatter plot with colored clusters: Yes.

**Code Quality:**
*   **Imports:** Proper imports are present.
*   **Structure:** The code flows logically according to the task steps.
*   **`n_init` in KMeans:** The generated code omits `n_init=10` or a similar value for KMeans (as recommended in scikit-learn 1.4+), leading to a warning or default behavior that might not be reproducible. The Ground Truth correctly handles this.
*   **Error Handling:** Lacks robust error handling for `FileNotFoundError` or missing columns, which the Ground Truth includes. This makes the code less resilient to common issues.
*   **Clarity of Feature Selection:** `features = df[['elixirCost', 'Win Rate', 'Usage']]` directly overwrites `features` as a DataFrame. While functional, explicitly defining `features = ['elixirCost', 'Win Rate', 'Usage']` and then creating the subset `X = df[features].copy()` as in the Ground Truth is often clearer and allows for better checks.
*   **User Input for K:** Relying on `input()` for K bypasses the idea of *justifying* the choice programmatically or visually without manual intervention. The Ground Truth visually suggests K and picks one for demonstration, which is more aligned with standard practice for code generation.

**Completeness:**
*   **Justification of K:** While it plots WCSS and Silhouette scores, it doesn't *programmatically* make a choice or clearly justify it from the code's perspective beyond plotting. The Ground Truth prompts the user to observe the plot and then explicitly states a chosen K.
*   **Plot Labels/Titles:** The `plt.scatter` for the clusters only uses 'Principal Component 1' and 'Principal Component 2' for axis labels, without specifying the explained variance ratio of each, unlike the Ground Truth, which enriches the plot information. It also lacks a clear title for the final scatter plot, as the `plt.title('K-Means Clustering')` is generic.
*   **Reproducibility:** Missing `random_state` in KMeans, which makes the clustering results non-reproducible. The Ground Truth correctly includes `random_state=42`.
*   **No output/prints:** The generated code doesn't print intermediate information (e.g., dataset loaded, shape, first rows, explained variance) that helps in understanding the execution flow and verifying data steps, unlike the Ground Truth.

To improve, the generated code should:
1.  Add `random_state` and `n_init` to `KMeans` for reproducibility and to avoid warnings.
2.  Implement basic error handling for file not found and missing columns.
3.  Add more informative print statements for crucial steps.
4.  Refine the final plot's labels and title to be more descriptive (e.g., including explained variance).
5.  Consider a programmatic approach or clearer visual indication for choosing K, rather than a direct `input()` prompt, since the task asks to ""determine an appropriate number"" rather than ""ask the user to input.""",llama-4-maverick,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Load dataset
data = pd.read_csv('clash_royale_cards.csv')

# Select features and target
X = data[['elixirCost', 'Usage', 'rarity']]
y...",9,"The generated code is excellent and meets almost all requirements.

**Strong Points:**
*   **Correctness and Functionality:** The code correctly loads the data, selects features and target, preprocesses `rarity` using one-hot encoding, splits the data, trains a `RandomForestRegressor`, and evaluates it using R-squared and MSE.
*   **Robust Preprocessing (Pipelines and ColumnTransformer):** The use of `Pipeline` and `ColumnTransformer` for preprocessing is a very robust and modern approach. It handles numeric and categorical features separately and correctly, including imputation. This is arguably more robust than `pd.get_dummies` alone, especially for handling unseen categories or missing values systematically.
*   **Completeness:** All specified steps (load, select features, preprocess, split, train, evaluate) are covered.
*   **Imports:** Proper imports are included.
*   **Readability:** The code is well-structured and easy to understand.

**Areas for Minor Improvement (Why not a 10):**
*   **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is good practice for robust scripts. The generated code assumes the file exists. While the prompt didn't strictly require error handling, it's a valuable addition for a production-ready script.
*   **Imputation Strategy for Categorical Data:** The `SimpleImputer(strategy='constant', fill_value='missing')` for categorical data is generally good, but for `rarity` (which is likely a fixed set of categories), if there were missing values, it would create a new 'missing' category, which might not be desirable depending on the context. `pd.get_dummies` handles this implicitly if missing values are not present, or if present, they would just become `NaN` in the one-hot encoded columns. For this specific challenge, `rarity` likely doesn't have missing values.
*   **No explicit check for missing columns:** The ground truth included a sanity check to ensure all required features and the target column exist in the DataFrame, which is a good robustness check.

Overall, the generated code demonstrates a high level of understanding of scikit-learn's preprocessing and modeling pipelines, delivering a highly effective and maintainable solution.",llama-4-maverick,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
from scipy.stats import f_oneway

def main():
    # Load the dataset
    data = pd.read_csv('clash_royale_cards.csv')

    # Identify unique categories in the 'rarity' column
    rarity_categories = data['rarity'].unique()

    # Group 'Win Rate' by 'rarity'
    win_rate_groups = [data.loc[data['rarity'] == category, 'Win Rate'] for category in rarity_categories]

    # State the null and alternative hypotheses
    null_hypothesis = ""The mean 'Win Rate' is the same ...",6,"The generated code correctly loads the data, identifies unique categories, states hypotheses, performs the ANOVA test, prints the F-statistic and p-value, and interprets the result. This covers the core requirements.

However, there are several areas for improvement:

1.  **Robustness and Error Handling (Critical omission):** The generated code completely lacks error handling for file not found or data processing issues (e.g., non-numeric 'Win Rate', NaN values). The ground truth code includes `try-except` blocks for file loading and checks for `NaN` values and empty dataframes, which are crucial for real-world data science tasks. The generated code implicitly assumes perfect data, which is rarely the case.
2.  **Data Cleaning:** The ground truth code includes explicit steps to convert 'Win Rate' to numeric and drop NaN values. The generated code does not perform this, which could lead to errors if the 'Win Rate' column contains non-numeric data or NaNs, or incorrect results if those values are implicitly dropped or treated differently by `f_oneway`.
3.  **Handling Insufficient Groups/Data:** The ground truth code checks if there are enough unique rarity categories (at least 2) and if each group has enough observations (at least 2) for a valid ANOVA. The generated code does not perform these checks, which could lead to errors or misleading results if the data is sparse.
4.  **Hypotheses Formatting:** While the hypotheses are stated, the ground truth's formatting is slightly clearer with headings and separators.

The code is functional for ideal data but lacks the robustness expected from expert-level code.",llama-4-maverick,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
df = pd.read_csv('iris.csv')

# Define features and target
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly defines `X` and `y` features and target.
*   Successfully encodes the `Species` target variable.
*   Splits the data into training and testing sets with `random_state=42` and the correct `test_size`.
*   Correctly initializes and trains all three specified models (Logistic Regression, SVC with linear kernel and `random_state=42`, Random Forest Classifier with `n_estimators=100` and `random_state=42`).
*   Calculates and prints accuracy, macro-averaged precision, recall, and F1-score for each model.
*   Identifies and prints the best model based on the highest F1-score.
*   Includes all necessary imports.

**Minor Improvements/Considerations:**
*   **Logistic Regression `max_iter`:** The ground truth includes `max_iter=200` for `LogisticRegression`. While not strictly stated in the prompt, Logistic Regression can sometimes fail to converge with default `max_iter` (100) on some datasets, leading to a `ConvergenceWarning`. Explicitly setting it or handling the warning is good practice. The generated code uses default parameters, which is within the prompt's scope, but `max_iter` is a common issue with `LogisticRegression`.
*   **`stratify` in `train_test_split`:** The ground truth uses `stratify=y_encoded` during the train-test split. This is important for classification tasks with imbalanced classes to ensure that the proportions of target classes are preserved in both training and testing sets. While the Iris dataset is relatively balanced, it's a good practice to include, as it improves reproducibility and robustness, especially when `random_state` is set. The prompt did not explicitly ask for `stratify`, but it's a best practice for `train_test_split` in classification.
*   **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV. While the prompt implied the file would be present, robust code often includes such error handling. This is a minor point for a benchmark task but valuable in real-world scenarios.",llama-4-maverick,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('iris.csv')

# Select features and target
features = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
species = df['Species']

# Standardize features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Apply K-Means...",7,"The generated code correctly loads the data, selects and standardizes features, applies K-Means with the specified parameters, and performs PCA to 2 components. It also generates two scatter plots of the PCA components, one colored by K-Means labels and one by true species labels.

However, there are a few areas for improvement:
1.  **Plot Titles:** The specified task explicitly asks for ""Add a title to this plot"" for *each* plot. While the generated code has titles, they are ""K-Means Clustering on Iris Dataset"" and ""True Species Labels on Iris Dataset."" The ground truth uses more descriptive titles like ""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"" and ""PCA of Iris Dataset: Points Colored by True Species Labels,"" which align better with plots showing PCA components.
2.  **Color bar:** The task did not explicitly ask for color bars, but the ground truth adds them, which significantly improves the interpretability of the plots. Without a color bar, it's hard to tell what the colors represent (especially for K-Means labels). The ground truth also correctly handles the `Species` label encoding to show the actual species names on the color bar.
3.  **Plot Layout/Separation:** The generated code uses `plt.subplot(1, 2, 1)` and `plt.subplot(1, 2, 2)` to place both plots side-by-side in one figure. While functional, the task asked for ""Create a scatter plot..."" and ""Create a second scatter plot..."", implying two separate figures which is how the ground truth implemented it (`plt.figure()` for each). This is a minor point, but maintaining separation generally makes plots cleaner, especially if they are for comparison.
4.  **`n_init` for KMeans:** The generated code uses `n_init='auto'`, which is correct based on the prompt.

Overall, the code is highly functional and fulfills most requirements. The main area for improvement is in the presentation of the plots to make them more informative and adhere more closely to the stylistic implications of the prompt (separate figures, more descriptive titles, and the helpful addition of colorbars).",llama-4-maverick,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# Load the dataset
df = pd.read_csv('iris.csv')

# Create new features
df['PetalArea'] = df['PetalLengthCm'] * df['PetalWidthCm']
df['SepalArea'] = df['SepalLengthCm'] * df['SepalWidthCm']

# Define features X and target y
X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'PetalArea', 'Sep...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, creates the specified new features, correctly defines `X` and `y` (including the new features and encoded target), initializes and trains the Gradient Boosting Classifier with the correct parameters, performs 5-fold cross-validation, and reports the mean accuracy score.

Minor improvements could include:
1. Adding a `try-except` block for file loading, similar to the ground truth, to make the code more robust against `FileNotFoundError`.
2. Including the `scoring='accuracy'` parameter in `cross_val_score` for explicit clarity, although accuracy is the default for classification.
3. Formatting the printed output to be more descriptive, e.g., `print(f""Mean cross-validated accuracy: {scores.mean():.4f}"")` to match the ground truth's output style and readability.

These are minor points and do not detract from the core correctness and functionality of the code. The code is clean, concise, and implements the required logic effectively.",llama-4-maverick,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Define features and target
X = df.drop('HeartDisease', axis=1)
y ...",9,"The generated code is highly correct and complete, successfully addressing almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading and Feature/Target Definition:** The code correctly loads the `heart.csv` dataset and defines `X` by dropping the `HeartDisease` column and `y` as the `HeartDisease` column.
- **Correct Preprocessing:** It correctly identifies and lists the categorical and numerical features. The `ColumnTransformer` is used appropriately for one-hot encoding categorical features (`handle_unknown='ignore'`) and scaling numerical features with `StandardScaler`. This is a robust approach avoiding data leakage.
- **Correct Data Splitting:** `train_test_split` is used correctly with `test_size=0.2` and `random_state=42`.
- **Correct Model Training:** A `Pipeline` is effectively used to combine preprocessing and the `RandomForestClassifier` with the specified `n_estimators=100` and `random_state=42`. The model is trained on the training data.
- **Correct Evaluation Metrics:** Accuracy, macro-averaged precision, recall, and F1-score are calculated and printed correctly. `roc_auc_score` is also calculated, demonstrating a good understanding of model evaluation.

**Minor Issue/Area for Improvement:**
- **Feature Selection for X:** While `X = df.drop('HeartDisease', axis=1)` is a common and often acceptable way to define features, the prompt specifically listed the exact columns to be used for `X` (`Age`, `Sex`, `ChestPainType`, `RestingBP`, `Cholesterol`, `FastingBS`, `RestingECG`, `MaxHR`, `ExerciseAngina`, `Oldpeak`, `ST_Slope`). The `drop` method implies that *all other* columns in the original `df` are implicitly included, which might differ if the dataset had additional columns not specified in the prompt. The ground truth explicitly selected only the specified features, which is slightly more robust to unexpected columns in the `heart.csv` file. This is a very minor point, as for the given `heart.csv` dataset, it likely yields the same `X`.
- **ROC-AUC Calculation for Binary Classification:** For binary classification, `roc_auc_score` typically expects `y_pred_proba` to be the probability of the positive class (i.e., `y_pred_proba[:, 1]`). The generated code passes the full `y_pred_proba` and specifies `multi_class='ovr'`, which technically works, but `'ovr'` is more commonly used when `y_true` and `y_score` are for multi-class problems. For a binary classification problem, passing `y_pred_proba[:, 1]` is more direct and standard. Since `HeartDisease` is a binary target, passing `y_pred_proba[:, 1]` would be the direct approach. However, `multi_class='ovr'` will still work correctly by internally considering each class as a positive class against all others (though for binary, it simplifies to one vs one implicitly).
This is a very minor detail and doesn't affect the correctness of the ROC-AUC calculation in this specific binary case.

Overall, the code is very well-written, clear, and follows best practices for machine learning workflows (e.g., using pipelines, `ColumnTransformer`, handling unknown categories).",llama-4-maverick,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('heart.csv')

# Select numerical features for clustering
features = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']]

# Standardize the selected features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Determine an appropriate numbe...",7,"The generated code correctly performs most aspects of the task, including loading data, selecting features, standardizing, performing K-Means, and applying PCA for visualization.

However, there are a few areas for improvement:
1.  **Hardcoded `k` choice**: Instead of asking for user input (`k = int(input(""Enter the chosen number of clusters: ""))`), the task explicitly states: ""Determine an appropriate number of clusters (K) for K-Means... Use the elbow method (plotting Within-Cluster Sum of Squares - WCSS) and silhouette scores to justify your choice of K."" While the plots are generated, the process of *choosing* K based on them should ideally be handled within the code, perhaps by picking the K that maximizes the silhouette score or identifying an elbow point programmatically (though visual inspection is often best for this case). The ground truth code correctly demonstrates making an *example* choice (`chosen_k = 3`) and stating it's based on visual inspection, which is a better approach than user input for a benchmark.
2.  **Redundant `silhouette_score` import**: `from sklearn.metrics import silhouette_score` is imported inside the loop, which is inefficient and unnecessary as it's already in the initial imports.
3.  **Visualization Detail**: The PCA plot lacks a legend for the clusters, making it difficult to differentiate which cluster corresponds to which color. The ground truth code provides a more robust plotting solution by iterating through clusters and adding a legend. The title of the plot could also be more descriptive, similar to the ground truth.

Strong points:
*   Correctly loads data and selects specified features.
*   Correctly standardizes features.
*   Correctly calculates WCSS and silhouette scores for the specified range of K values.
*   Generates both elbow and silhouette plots.
*   Correctly applies K-Means with `n_init='auto'` and `random_state=42`.
*   Correctly performs PCA to 2 components.
*   Successfully plots the PCA components colored by cluster labels.",llama-4-maverick,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np

# Load data
df = pd.read_csv('winequality-red.csv')

# Create binary target variable
df['quality_binary'] = np.where(df['quality'] >= 7, 1, 0)

# Define features and target
X = df.drop(['quality', 'quality_binary'], axis...",9,"The generated code correctly addresses all the requirements of the task. It successfully implements the binary target variable creation, handles data preprocessing (scaling), splits the data, trains a logistic regression model, evaluates performance using the specified metrics, and identifies important features based on coefficients.

**Strong points:**
- All required imports are present.
- Correct creation of the binary target variable based on the `quality` column.
- Proper separation of features (X) and target (y).
- Correct usage of `StandardScaler` for feature scaling.
- Correct data splitting into train and test sets (80/20).
- Appropriate training and prediction steps for `LogisticRegression`.
- Correct calculation and printing of accuracy, F1-score, and classification report.
- Correct extraction and reporting of feature importances using `model.coef_` and `np.abs()`.

**Areas for minor improvement:**
- The ground truth code includes handling of missing values (even if the dataset doesn't have them), which is a good practice for robustness. The generated code only prints a check but doesn't implement a handling strategy if missing values were present. While the prompt stated ""handle missing values if any"" and this dataset doesn't have them, explicitly showing a handling mechanism (e.g., `fillna`) would make it more robust.
- The `random_state` for `train_test_split` is good, but adding `stratify=y` would ensure the proportion of good/bad wines is maintained in both train and test sets, which is important for imbalanced datasets.
- The scaling is applied to `X` (all data) before splitting, which leads to data leakage. The `scaler.fit_transform` should be applied to `X_train` and then `transform` used on `X_test` to prevent information from the test set influencing the scaling of the training set. The ground truth correctly applies scaling after splitting.
- While the output is clear, the ground truth includes more descriptive `print` statements which can be helpful for debugging or understanding the flow. Adding similar prints for confirmation of steps (e.g., ""Data split successfully"", ""Model trained"") could enhance readability.

The data leakage from scaling `X` before splitting is the most significant issue, but it's easily fixable. Otherwise, it's a very good solution.",llama-4-maverick,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# Load the dataset
df = pd.read_csv('heart.csv')

# Create a box plot showing the distribution of Oldpeak for patients with HeartDisease=0 and HeartDisease=1
plt.figure(figsize=(8,6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=df)
plt.title('Distribution of Oldpeak by HeartDisease')
plt.show()

# Perform a logistic regression with only O...",7,"The generated code correctly addresses most of the task requirements but has some areas for improvement in terms of robustness, completeness, and clarity.

**Strong Points:**
*   Correctly loads the dataset.
*   Successfully generates the specified box plot.
*   Correctly performs logistic regression and extracts the coefficient and odds ratio.
*   Correctly discretizes `Oldpeak` using `pd.qcut` into three bins.
*   Correctly calculates the proportion of `HeartDisease=1` within each bin.

**Areas for Improvement (Why not higher):**

1.  **Interpretation of Logistic Regression Coefficient/Odds Ratio (Missing):** The request specifically asked to ""interpret its meaning in the context of odds ratios."" The generated code prints the coefficient and odds ratio but provides no textual interpretation of what these values signify, which was a core part of point 3 in the prompt.
2.  **Dataset Loading Robustness:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robust data loading. The generated code assumes the file will always be present, which is less robust.
3.  **Missing Value Handling:** The ground truth includes a check and handling for missing values, which is important for real-world data science tasks and was implicitly part of ensuring correct analysis. The generated code does not address this.
4.  **Box Plot Customization:** While the plot is correct, the ground truth added more descriptive labels and a title for better readability, which is good practice.
5.  **Output Clarity:** The ground truth includes more `print` statements to clearly indicate which step of the analysis is being reported, making the output easier to follow for someone reviewing the results. For example, indicating ""Logistic Regression analysis complete"" or ""Box plot displayed"".
6.  **Binning Robustness:** The prompt mentions handling `duplicates='drop'` for `pd.qcut` and the ground truth includes a `try-except` block for `qcut` in case of issues with duplicate values, which can sometimes occur with `qcut`. The generated code does not account for this.
7.  **`reset_index()` for proportions:** While `reset_index()` works for printing the `proportion` DataFrame, for just displaying the proportions, printing the `groupby().mean()` Series directly is often cleaner and sufficient, as the ground truth does. The `reset_index()` adds an unnecessary step unless further DataFrame operations are intended.",llama-4-maverick,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# Load data
data = pd.read_csv('spotify_2023.csv')

# Convert streams to numeric and drop NaN rows
data['streams'] = pd.to_nume...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading and Preprocessing:** Handles `streams` conversion to numeric with error coercing and subsequent NaN dropping. Correctly identifies the 75th percentile for `is_popular` creation.
- **Feature Selection and Encoding:** Selects the specified numerical features and correctly identifies `key` and `mode` for one-hot encoding.
- **Pipelining:** Uses `Pipeline` and `ColumnTransformer` effectively for preprocessing and model training. This is a robust and recommended approach for machine learning workflows, although not explicitly required by the prompt, it demonstrates good practice.
- **Model Training:** Trains a `LogisticRegression` model with `max_iter=1000`.
- **Evaluation:** Calculates and prints F1-score (macro), ROC AUC score, and the confusion matrix as requested. Imports are proper and complete for the chosen method.

**Areas for Minor Improvement (Preventing a perfect 10):**
- **Implicit Feature Handling:** While `ColumnTransformer` is excellent, the prompt specifically asked to ""Combine these with other selected features"" after one-hot encoding, implying a single `X` dataframe before `train_test_split`. The generated code keeps `key` and `mode` in `features` *before* `train_test_split`, and then the `ColumnTransformer` handles their one-hot encoding *within* the pipeline. This is functionally equivalent and often preferred, but deviates slightly from the literal interpretation of the prompt's sequence, which suggested creating the full `X` upfront. The ground truth explicitly creates the full one-hot encoded `X` before splitting.
- **Missing `random_state` for `LogisticRegression`:** While `max_iter=1000` is set, `random_state` is not specified for the `LogisticRegression` model itself. While Logistic Regression is generally deterministic, some solvers might have minor stochastic aspects that could benefit from `random_state` for perfect reproducibility, especially if regularization is involved. The ground truth includes `random_state` for the model.
- **Missing `stratify` in `train_test_split`:** The ground truth used `stratify=y` which is good practice for imbalanced datasets (which `is_popular` likely is, given it's based on the top 25% of streams). The prompt didn't explicitly ask for `stratify`, but it's a detail that would make the split more robust.

Overall, the generated code produces a correct and effective solution using modern scikit-learn best practices. The minor deviations from the literal sequencing in the prompt are more about style/order of operations within the pipeline vs. manual steps, rather than functional correctness or missing requirements.",llama-4-maverick,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('spotify_2023.csv')

# Select audio features and drop rows with NaNs
audio_features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 
                  'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
df_selected = df[audio_features].dropna()

# Standardize fe...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, selects the specified audio features, handles NaNs, standardizes features, applies K-Means clustering, performs PCA, visualizes the clusters, and calculates the mean original audio features per cluster.

Strong points:
- All required libraries are imported.
- The K-Means and PCA parameters (`n_clusters=4`, `random_state=42`, `n_init='auto'`, `n_components=2`, `random_state=42`) are correctly set.
- The visualization correctly uses PCA components for axes and cluster labels for coloring.
- The calculation of mean values for original features per cluster is correct.

Areas for minor improvement/consideration:
- **Plotting Aesthetics**: While functionally correct, the `matplotlib.pyplot` plot could be enhanced with better labels, a color bar, and potentially a more descriptive title, as seen in the ground truth. This improves readability and interpretability for the user.
- **Handling `df_selected` index after `dropna()`**: When `df_selected` is created and `dropna()` is applied, its index might become non-contiguous. When `cluster_labels` (which are derived from `df_scaled`, a NumPy array) are assigned back to `df_selected`, a `SettingWithCopyWarning` could potentially arise in more complex scenarios, or it might lead to misalignment if the original `df_selected` was used for other operations. The ground truth uses `df_original_for_analysis = df_features.copy()` to ensure a clean DataFrame for attaching labels which maintains the alignment and avoids such issues. In this specific case, it works because `df_selected` is only used to create `df_scaled` and then immediately has the `cluster` column added back. However, it's a good practice to be mindful of DataFrame indexing when applying transformations and then attaching labels.
- **File Not Found Error Handling**: The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which makes the code more robust. This is a good practice for real-world applications.

Overall, the generated code is highly competent and meets the core requirements effectively. The points for improvement are mostly related to robustness and presentation, not core functionality errors.",llama-4-maverick,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def load_and_filter_data(file_name):
    # Load data
    data = pd.read_csv(file_name)
    
    # Convert streams to numeric (coerce errors to NaN)
    data['streams'] = pd.to_numeric(data['streams'], errors='coerce')
    
    # Filter for songs with streams greater than 10,000,000 and released_year between 2010 and 2023
    filtered_data = data[(data['streams'] > 10_000_000) & 
                         (data['released_year'] >= 2010...",8,"The generated code addresses all requirements of the task. It correctly loads the data, converts 'streams' to numeric, filters the data based on `streams` and `released_year` criteria, groups by `released_year`, calculates the average audio features, and plots them.

**Strong points:**
*   **Modularity:** The code is well-structured into functions (`load_and_filter_data`, `calculate_average_features`, `plot_average_features`, `main`), which improves readability and maintainability.
*   **Correct filtering:** The filtering logic for `streams` and `released_year` is correct. The `dropna(subset=['streams'])` is correctly applied *after* the `to_numeric` conversion and within the same filtering step, which is efficient.
*   **Correct aggregations:** The grouping and calculation of mean for the specified audio features are accurate.
*   **Proper plotting:** It correctly uses `matplotlib.pyplot` to create line plots with appropriate titles and labels. Using `subplots` is a good way to manage multiple plots.

**Areas for improvement (to reach a 10):**
*   **Error Handling for File Not Found:** The generated code lacks explicit error handling for `FileNotFoundError` when loading the CSV, which the ground truth includes. This makes the code less robust.
*   **`reset_index()` for plotting:** While `reset_index()` works, for `groupby` operations where the group key becomes the index, plotting directly with `df.index` for the x-axis (as in the ground truth) is often slightly more idiomatic and can save a step. However, `reset_index()` is also perfectly valid.
*   **Plotting Subplot Layout:** The generated code uses `plt.subplots(3, figsize=(10, 15))` which creates plots in a single column. The ground truth uses `plt.subplot(1, 3, i)` which creates plots in a single row. While both are valid for displaying the plots, the ground truth's approach might be slightly more compact visually for just three plots, fitting the screen well. This is a minor stylistic preference.
*   **`xticks` rotation/density:** The ground truth includes `plt.xticks(yearly_avg_features.index, rotation=45)` which is excellent for handling potentially dense x-axis labels (years). The generated code does not explicitly set or rotate x-ticks, which could lead to overlapping labels if there are many years. For the given 2010-2023 range, it might not be a major issue, but it's a good practice.",llama-4-maverick,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean and preprocess numeric columns
for col in ['Likes', 'Dislikes', 'Video views']:
    df[col] = df[col].str.replace(',', '').pd.to_numeric(errors=...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being fully correct and robust.

**Issues:**

1.  **Incorrect numeric conversion for `Likes`, `Dislikes`, `Video views`:** The line `df[col] = df[col].str.replace(',', '').pd.to_numeric(errors='coerce').fillna(0)` is syntactically incorrect. `.pd.to_numeric` is not a valid method chain after a string operation. It should be `pd.to_numeric(df[col].str.replace(',', ''), errors='coerce').fillna(0)`. This will cause a runtime error.
2.  **Incorrect `published` column handling:** The task explicitly asks to treat the *year* of the `published` column as a numeric feature. The generated code attempts `df['published'] = pd.to_numeric(df['published'], errors='coerce')`. The `published` column contains full datetime strings (e.g., '2020-08-11T12:00:00Z'), so directly converting it to numeric will result in NaNs for all entries, effectively losing the year information. It should parse it as a datetime and then extract the year, similar to the ground truth.
3.  **OneHotEncoder output handling:** When using `OneHotEncoder`, it returns a sparse matrix. While `pd.DataFrame(category_ohe.toarray(), index=df.index)` is technically correct for converting it to a dense DataFrame, `pd.get_dummies` is generally simpler and more robust for single-column one-hot encoding directly within pandas, as used in the ground truth. The approach here is not wrong but slightly more verbose than necessary for the given requirements.
4.  **Feature selection for `X`:** Due to the incorrect handling of `published` column (treating it as raw string to numeric instead of year), the `published` column in `X` will be all NaNs if the original 'published' column has date strings.
5.  **Missing `dtype=int` for `pd.get_dummies` (if used) or explicit `astype(int)` for numeric conversions**: While not explicitly failed, the lack of `astype(int)` for the cleaned numeric columns (`Likes`, `Dislikes`, `Video views`) might lead to them being floats instead of integers, which is less ideal for count-like features unless floating point precision is specifically needed. The `OneHotEncoder` approach generates float columns by default, which is generally acceptable but specifying `dtype=int` for get_dummies produces cleaner integer representations.

The core logic of defining X, y, splitting, training, and evaluation is correct *assuming* the data preprocessing steps were successfully executed, which they are not. The robust error handling (e.g., `try-except FileNotFoundError`, column existence checks) present in the ground truth is also missing, though not strictly required by the prompt, it makes the ground truth more production-ready.

To improve, the user should fix the `.str.replace` and `pd.to_numeric` chaining, address the `published` column's year extraction, and ideally use `pd.get_dummies` for simplicity for one-hot encoding `Category`.",llama-4-maverick,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    df = pd.read_csv('youtube_trendings.csv')

    # Clean 'Video views' column
    df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)

    # Ensure 'published' column is numeric
    df['published'] = pd.to_numeric(df['published'], errors='coerce').fillna(0).astype(int)

    # Group data by 'published' year
    grouped_df = df.groupby('published')

    # Calculate met...",7,"The generated code largely addresses the requirements of the task, demonstrating good understanding of data manipulation and plotting with pandas and matplotlib.

**Strong Points:**
*   **Correct Data Loading:** Successfully loads the `youtube_trendings.csv` file.
*   **Correct Cleaning of `Video views`:** Accurately removes commas, converts to float, and fills NaNs with 0.
*   **Correct Grouping and Aggregation:** Groups data by the `published` year and correctly calculates both the mean of 'Video views' and the `nunique` of 'Video' titles.
*   **Correct Plotting with Subplots:** Creates two subplots and correctly plots the calculated metrics against the year.
*   **Appropriate Titles and Labels:** Includes titles and axis labels for both plots.

**Areas for Improvement:**
*   **`published` Column Handling (Partial Issue):** The prompt explicitly states, ""Ensure the `published` column (year) is treated as a numeric or integer type."" The generated code tries to convert `df['published']` directly to numeric. However, in the `youtube_trendings.csv` dataset, the year is typically obtained from a column like `publishedAt` which contains full datetime strings (e.g., '2020-08-11T12:00:00Z'). The ground truth code correctly parses `publishedAt` to extract the year. If a 'published' column containing only the year existed, the generated code's approach would be correct, but based on common dataset structures, it might fail or produce incorrect results if `published` itself is a datetime string. This is a crucial detail for data consistency.
*   **Error Handling:** The generated code lacks basic error handling (e.g., `try-except` for `FileNotFoundError` when loading the CSV), which is good practice for robust scripts.
*   **Clarity/Comments:** While the code is relatively straightforward, adding a few comments (e.g., for each step as outlined in the prompt) would improve readability and maintenance for complex tasks.
*   **Function Structure:** The ground truth wraps the logic in a function with a `file_path` argument, making it more reusable. The generated code uses a `main()` function, which is good practice but less flexible for reuse without modification.",llama-4-maverick,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from collections import Counter

# Ensure necessary NLTK data is downloaded
nltk.download('stopwords')

def main():
    # Load dataset
    df = pd.read_csv('youtube_trendings.csv')

    # Create new feature: title word count
    df['title_word_count'] = df['Video'].apply(lambda x: len(str(x).split()))

    # Clean 'Video views' column
    df['Video views'] = pd.to_numeric(df['Video views'].astype(str).str.repla...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading and Initial Processing:** Successfully loads the dataset and correctly computes `title_word_count`, handling potential non-string values.
- **Accurate Cleaning of 'Video views':** The cleaning of 'Video views' (removing commas, converting to numeric, filling NaNs) is implemented correctly and concisely.
- **Correct Correlation Calculation:** The Pearson correlation coefficient is computed and printed as required.
- **Proper Date Filtering:** The filtering of videos published in 2023 or 2024 is done correctly using `pd.to_datetime` and `isin`.
- **Effective Text Processing:** The process for extracting words, lowercasing, removing punctuation, and filtering stop words is well-executed.
- **Correct Common Word Finding:** The `Counter` is used correctly to find and print the 5 most common words.
- **Good Use of Imports:** All necessary libraries are imported.
- **Encapsulation:** The use of `main()` function is good practice.

**Areas for Improvement (Reasons for not being a perfect 10):**
- **NLTK Download Handling:** The `nltk.download('stopwords')` call directly executes, which is fine for direct execution but in an expert evaluation scenario, it's better to wrap it in a `try-except` block with `nltk.data.find` to avoid repeated downloads and inform the user if download is required (as in the ground truth).
- **Error Handling for File Loading:** While not explicitly asked for, adding a `try-except FileNotFoundError` block for `pd.read_csv` would make the code more robust (present in ground truth).
- **Output Formatting for Correlation:** The ground truth specified printing the correlation formatted to 4 decimal places, which the generated code did not explicitly do (`{correlation:.4f}`).
- **Removal of Numeric-Only Words:** While `re.sub(r'[^\w\s]', '', title)` removes punctuation, it doesn't exclude words that might only contain numbers (e.g., ""iphone15"" would become ""iphone15"", but ""2023"" would remain ""2023""). The ground truth adds an `word.isalpha()` check, which is a subtle but important refinement for ""words"".
- **Clarity of Output:** The output for the most common words could be slightly more descriptive (e.g., `print(""5 most common words in titles from 2023/2024 videos:"")` as in the ground truth).

Overall, the generated code is highly functional and correctly implements the core logic for the task. The improvements are minor refinements rather than critical errors.",llama-4-maverick,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Load data
df = pd.read_csv('mobiles_2025.csv')

# Clean Launched Price (USA) and create target
df['Launched Price (USA)'] = df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '')
d...",6,"The generated code addresses most requirements but has a few critical issues and areas for improvement in correctness and robustness.

**Strengths:**
- Correctly loads data.
- Attempts to clean `Launched Price (USA)` and create the `is_expensive_usa` target.
- Attempts to clean all specified numerical features (`Mobile Weight`, `RAM`, `Battery Capacity`, `Screen Size`).
- Attempts to extract MP values for camera features.
- Uses `RandomForestClassifier` with specified parameters.
- Uses `train_test_split` with correct parameters.
- Calculates F1-score (macro) and ROC AUC score.
- Includes necessary imports.

**Areas for Improvement / Issues:**
1.  **`is_expensive_usa` Target Creation (Correctness Issue):** The line `df['is_expensive_usa'] = np.where(df['Launched Price (USA)'] > 1000, 1, 0)` does not correctly handle cases where `Launched Price (USA)` is `NaN` after `pd.to_numeric(errors='coerce')`. It will convert `NaN > 1000` to `False` (which is `0`) instead of keeping them as `NaN` to be dropped later as per the requirement ""Drop rows where `is_expensive_usa` is NaN"". This means rows with unparseable prices are *not* dropped as NaNs but incorrectly labeled as `0` for the target.
2.  **`is_expensive_usa` NaN Handling (Correctness Issue):** `df = df.dropna(subset=['is_expensive_usa'])` is redundant and misleading given the previous point. If `np.where` has already converted all non-numeric prices to `0`, there will be no `NaN`s in `is_expensive_usa` for this line to drop. The ground truth correctly defines `0` for valid prices <= 1000, `1` for valid prices > 1000, and `NaN` for unparseable prices, *then* drops the `NaN` target rows.
3.  **Camera Feature Cleaning (Robustness/Correctness Issue):** The `extract_mp` function is too simple.
    *   `re.search(r'\d+(?:\.\d+)?', camera_str)` only finds *any* number, not specifically those followed by 'MP'.
    *   It does not handle `NaN` input gracefully; `pd.to_numeric(df['Back Camera'].str.replace(' MP', ''), errors='coerce')` could output `NaN`s, and then `apply` to `NaN` will raise an error. The ground truth uses `if pd.isna(camera_str): return 0.0`.
    *   It assumes MP is the *only* value, which is not robust. For example, ""Dual 12MP, 12MP"" or ""64MP wide, 8MP ultrawide"". It should extract the *first numerical MP value* as specified. The ground truth's regex `r'(\d+(\.\d+)?)\s*MP'` is more precise.
4.  **Column Transformer and Feature Combination (Correctness/Complexity):** The way `OneHotEncoder` and `ColumnTransformer` are used, and then `X` is constructed, is overly complex and prone to errors.
    *   `transformer.fit_transform(X).toarray()` will transform *all* columns in `X`, not just the `categorical_features`.
    *   The `remainder='passthrough'` in `ColumnTransformer` is redundant if you only want to transform specific categorical features and then manually combine them with pre-selected numerical features.
    *   The `pd.concat` step is incorrect: `pd.DataFrame(transformer.fit_transform(X).toarray(), index=X.index)` where `X` still contains the original categorical columns. The transformed array will have different column names, and combining them like this will lead to duplicate columns or incorrect feature set. It's much cleaner to use `pd.get_dummies` directly on `df` for the specified categorical columns and then select the features. The ground truth method is much simpler and more robust.
5.  **NaN Handling (Order and Completeness):** The code attempts to drop NaNs in `X` *after* the problematic `ColumnTransformer` step (`X = X.dropna()`), but this might not correctly propagate back to `y`. The requirement was to ""Drop rows with any NaNs in features or target."" The ground truth ensures `X` and `y` are aligned after the final NaN drop. The current sequence `y = y[X.index]` relies on `X.index` being correctly filtered, which is not guaranteed for alignment.
6.  **`Launched Year`:** The ground truth explicitly includes `Launched Year` as a numerical feature. The generated code does not explicitly include it in `numerical_features` list before the `ColumnTransformer`. The `ColumnTransformer` with `remainder='passthrough'` might keep it, but it's not clear or explicit.
7.  **Print Statements:** The ground truth includes helpful print statements for debugging and tracking data transformations and shapes, which is good practice in a benchmark context. The generated code lacks these.",llama-4-maverick,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

def clean_price(price):
    try:
        price = price.replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', '').replace(',', '')
        return pd.to_numeric(price)
    except:
        return np.nan

def main():
    # Load the dataset
    df = pd.read_csv('mobiles_2025.csv')

    # Clean price columns
    price_columns = ['Launched Price (Pakistan)', 'Launched Price (India)', 'Launched Price (China)', 'Launch...",6,"The generated code correctly addresses most of the task requirements but has a critical flaw in the `clean_price` function and a minor omission in handling NaN values for the average calculation.

**Correctness:**
The `clean_price` function fails when the input is `NaN` because `NaN` is a float and does not have a `.replace()` method, leading to an `AttributeError`. This means that if any original price values are missing, the cleaning function will raise an error instead of returning `NaN`, thus failing to correctly clean all columns. The `mean(axis=1)` call implicitly handles `NaN` values by default (`skipna=True`), so it aligns with the requirement, but not explicitly as in the ground truth.

**Code Quality:**
Imports are correct. The code is structured well with a `main` function. However, the error handling in `clean_price` is too broad (`except:` instead of `except AttributeError, TypeError`), and the `main` function is called after its definition which is good practice.

**Completeness:**
*   **Loading dataset:** Satisfied.
*   **Cleaning price columns:** Partially satisfied. The `clean_price` function is applied to all specified columns for symbol and comma removal, but its fundamental flaw with `NaN` input means it might not fully clean the columns as intended if `NaN`s are present in the raw data.
*   **Calculate Average Global Price:** Satisfied. The `mean(axis=1)` handles `NaN` values by default, effectively considering only available data.
*   **Identify top 5 models:** Satisfied using `nlargest`.
*   **Display:** Satisfied.

**Areas for Improvement:**
1.  **Robust `clean_price` function:** The most critical improvement is to ensure `clean_price` can handle `NaN` inputs gracefully. It should check if the input is `pd.isna(price)` and return `np.nan` immediately, or convert `price` to `str` before calling `.replace()`.
2.  **Explicit `skipna=True`:** While `mean()`'s default is `skipna=True`, explicitly stating it (`df[price_columns].mean(axis=1, skipna=True)`) makes the intent clearer and matches the problem description more precisely (""only consider countries for which price data is available (not NaN)"").
3.  **Error Handling for CSV Load:** The ground truth includes a `try-except FileNotFoundError` for loading the CSV, which is good practice for robust code. The generated code lacks this.
4.  **Handling missing price columns:** The ground truth also gracefully handles cases where a `price_column` might not exist in the DataFrame, which is a good robustness check.",llama-4-maverick,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def clean_data(df):
    df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
    df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
    df['Launched Price (USA)'] = pd.to_numeric(df['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', ''), errors='coerce')
    df = df.dropna(subset=['RAM', 'Battery Capacity', 'Launched Price (USA)'])
    return df
...",7,"The generated code correctly addresses most of the task requirements. It loads the dataset, performs the specified cleaning operations, filters for Apple products, groups by launched year, and generates box plots for RAM, Battery Capacity, and Launched Price. The plots have appropriate titles and y-axis labels.

However, there are a few areas for improvement:
1.  **Robustness in Cleaning:** The generated code's cleaning operations are slightly less robust than the ground truth. It assumes `str.replace` will always work directly after `pd.to_numeric` (which is chained correctly due to `errors='coerce'`) without explicitly converting the column to string first (`astype(str)`). While it might work in many cases, making it explicit adds robustness, especially if there are non-string types that `str.replace` cannot operate on directly. Also, it doesn't strip whitespace after removing units (e.g., '10 GB ' might become '10 ' which could cause issues with `pd.to_numeric` if not handled correctly, though `pd.to_numeric` is somewhat forgiving here). The ground truth also explicitly handled `Launched Price (USA)` with `USD ` and `,`, which the generated code did.
2.  **Error Handling for File Loading:** The generated code lacks error handling for `pd.read_csv`, which could crash the program if the file is not found. The ground truth included a `try-except` block for this.
3.  **Visual Style:** The generated code doesn't set a specific plot style like `seaborn-v0_8-darkgrid` used in the ground truth, leading to a less polished visual output by default matplotlib style. While not a functional requirement, it is often a good practice for data science tasks.
4.  **Handling `Launched Year` NaNs:** The ground truth explicitly cleaned `Launched Year` and dropped rows where it might be NaN before grouping, ensuring all `Launched Year` values are valid integers for grouping. The generated code implicitly relies on `Launched Year` being clean enough by default, which might not always be true and could lead to issues if some years are not parseable as numbers.
5.  **Output Clarity:** The ground truth includes print statements that provide feedback on the data loading, cleaning process, and row counts, which is helpful for debugging and understanding the data pipeline. The generated code lacks this.",llama-4-maverick,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute imp...",5,"The generated code attempts to address many requirements but falls short in implementation details, resulting in an incomplete and often incorrect solution for an ""advanced"" ensemble learning model.

**Major Issues:**

1.  **Data Preprocessing and Feature Engineering:**
    *   **Incomplete Feature Engineering:** The prompt asked for ""advanced feature engineering (polynomial features, interaction terms, text mining from names)"". The generated code only extracts 'Title' and basic 'FamilySize'/'IsAlone'. It completely misses deriving 'Deck' from 'Cabin', 'Name_Length', 'Is_Married', and more robust handling of 'Fare_Per_Person'. The `TfidfVectorizer` for 'Name' is wrongly applied. Text mining from names implies more than just extracting a title; it could involve sentiment analysis, specific keyword extraction, etc., which TF-IDF attempts but is then clumsily integrated into a `ColumnTransformer` with `Name` as a single column, which is usually not how TF-IDF is applied within `ColumnTransformer` for tabular data.
    *   **Incorrect `ColumnTransformer` Usage:** The `text_transformer` for 'Name' is placed on `Name` as a single column. `TfidfVectorizer` expects an iterable of strings, and its output (sparse matrix) needs careful handling when combined with other numeric/one-hot features. This setup is highly likely to lead to errors or incorrect behavior later. Also, `pd.get_dummies` is used directly in a `Pipeline` step for `categorical_transformer`, which is problematic within a `Pipeline` as `get_dummies` returns a DataFrame and doesn't conform to scikit-learn's transformer API.
    *   **Missing Multiple Imputation:** The prompt explicitly asked for ""handling missing values with multiple imputation strategies"". The generated code uses `SimpleImputer(strategy='median')` and `SimpleImputer(strategy='constant')`, which is single imputation, not *multiple* imputation. The ground truth correctly uses `IterativeImputer`.
    *   **Polynomial Features Application:** Polynomial features (`poly_features = PolynomialFeatures(degree=2)`) are applied *after* the `ColumnTransformer` and *after* `fit_transform` on `X_train`. This is problematic because the `ColumnTransformer` itself might generate high-dimensional features (especially with TF-IDF and one-hot encoding). Applying polynomial features to this already transformed data can lead to an explosion in dimensionality and potentially introduce features that are hard to interpret or are numerically unstable. A better approach (as in ground truth) is to apply PolynomialFeatures within the numerical pipeline *before* combining with OneHotEncoded categorical features.

2.  **Model Building and Tuning:**
    *   **No XGBoost:** The prompt specifically mentioned including XGBoost as a base model. The generated code includes `import xgboost as xgb` but never instantiates or uses `xgb.XGBClassifier`.
    *   **Hyperparameter Tuning:** It uses `GridSearchCV` (acceptable) but the parameter grids are very small compared to what's expected for ""advanced"" tuning. `RandomizedSearchCV` was requested as an option, which often finds good parameters more efficiently in high-dimensional search spaces.
    *   **Pipeline Definition:** The base models are not built as pipelines that include the preprocessing steps. Instead, preprocessing is done first (`preprocessor.fit_transform(X_train)`) then the models are fit on the transformed data. This makes the models less robust and harder to use (e.g., for `learning_curve` or `cross_val_score` where the original data `X_train` is typically passed). The ground truth correctly integrates preprocessing into each model's pipeline.

3.  **Model Evaluation:**
    *   **Missing Performance Metrics:** Required are ""comprehensive performance metrics"". It only prints `accuracy_score`. ROC AUC is used for `GridSearchCV` scoring but not explicitly reported for final models. Precision, Recall, F1-score, and Confusion Matrix are missing.
    *   **Stratified K-Fold:** While `StratifiedKFold` is defined, its usage in the evaluation loop (`for train_idx, val_idx in skf.split(...)`) is very basic and just retrains and evaluates on selected folds. It doesn't use `cross_val_score` or properly evaluate all models with the defined `skf` object.
    *   **Learning Curves:** The learning curve implementation is manual and incorrect. It subsamples `X_train_poly` and trains the best RF estimator. It then evaluates on `X_test_poly` (fixed test set) instead of performing cross-validation on varying training set sizes as `learning_curve` function (from `sklearn.model_selection`) does. It also uses `accuracy_score` again, not `roc_auc`.
    *   **Validation Curves:** There is no implementation of validation curves.
    *   **Statistical Significance Tests:** Only a `ttest_ind` is performed between two *individual* models (RF and GB) and their scores derived from custom k-fold. The prompt asked for comparison of *all* models and ensembles, implying comparing against a baseline or the best ensemble, and typically uses paired t-tests or McNemar's test for comparing classifiers on the same dataset. The limited statistical test is insufficient.

4.  **Model Interpretability:**
    *   **SHAP:** SHAP is applied, but `TreeExplainer` is used on `rf_grid.best_estimator_`. This is acceptable for tree models. However, the `shap.force_plot` call is missing the `feature_names` argument. More critically, the `X_test_poly` passed to `shap_values` is a NumPy array, which doesn't retain feature names. Getting meaningful SHAP insights for complex feature sets (after polynomial features and one-hot encoding) requires mapping the SHAP values back to interpretable feature names. This mapping is absent.
    *   **LIME:** There is no LIME implementation, which was explicitly requested.

5.  **Code Quality & Structure:**
    *   **Lack of Pipelines for Full Workflow:** The preprocessing is separate from model training, making the code less modular and prone to issues when fitting/predicting new data. The ground truth clearly demonstrates how to build the entire process into a single pipeline for each model.
    *   **Hardcoded Feature Names:** Feature names are hardcoded, which can cause issues if the input data structure changes.
    *   **Missing Imports:** `lime` is missing.
    *   **Clarity and Organization:** The code structure is somewhat jumbled, with feature engineering, splitting, preprocessing, then more feature engineering (polynomials) without clear separation or modularity.

Overall, the generated code has a conceptual understanding of the requested components but fails significantly in advanced implementation details, particularly around robust preprocessing pipelines, comprehensive evaluation methodologies, and accurate interpretability setups.",llama-4-maverick,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Define features and target
X = df[['Year', 'Runtime']]
y = df['Rating']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
mod...",7,"The generated code correctly addresses all the core requirements of the task: loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it using R-squared and MAE. It includes all necessary imports and follows a logical flow.

However, it lacks essential robustness that good production-ready or even robust benchmark code should have. Specifically, it does not handle potential errors like `FileNotFoundError` when loading the CSV, nor does it perform any data cleaning or type conversion for the `Year` and `Runtime` columns. The `Runtime` column, in particular, often contains ' min' strings which would cause an error if not handled, and `Year` and `Rating` should be ensured to be numeric. The ground truth code includes these crucial data preprocessing steps, which makes it more resilient and complete for real-world data. Without these, the generated code might fail or produce incorrect results if the `imdb_top_250.csv` file is not perfectly clean or formatted as expected for direct numeric conversion.",llama-4-maverick,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    data = pd.read_csv('imdb_top_250.csv')

    # Select the features for clustering
    features = data[['Rating', 'Runtime']]

    # Standardize the features
    scaler = StandardScaler()
    standardized_features = scaler.fit_transform(features)

    # Apply K-Means clustering
    kmeans = KMeans(n_clusters=3, n_i...",7,"The generated code correctly implements the core logic for K-Means clustering, including loading data, selecting and standardizing features, applying K-Means with the specified parameters, and visualizing the clusters. It also correctly calculates and prints the mean rating and runtime for each cluster.

However, there are a few important omissions and minor issues that prevent a higher score:
1.  **Data Cleaning for 'Runtime'**: The prompt implies dealing with `imdb_top_250.csv`, which often has 'Runtime' in a string format like '142 min'. The generated code does not include any data cleaning or conversion for the 'Runtime' column, which means it would likely fail or produce incorrect results if the 'Runtime' column is not already numeric (which is a common challenge with this dataset). The ground truth specifically handles this.
2.  **Robustness (Error Handling)**: There's no error handling for `FileNotFoundError` when loading the CSV, nor for missing 'Rating' or 'Runtime' columns.
3.  **Visualization Robustness**: The `plt.scatter` call is missing `cmap` and `s` parameters, which are good practices for clearer visualization. It also lacks a colorbar, which is helpful to interpret cluster labels.
4.  **Assignment of Cluster Labels**: While `cluster_labels` is used for printing and plotting, it's not explicitly added as a new column back to the `data` DataFrame, which sometimes makes inspection or further analysis more convenient. The ground truth adds `cluster_label` to the DataFrame.
5.  **Output Format for Means**: The ground truth uses `groupby().mean()` which is a more compact and idiomatic pandas way to achieve the same result as the loop, and it provides a nicely formatted table.

Overall, it's a good attempt that covers most requirements, but lacks the robustness and attention to common data challenges (like non-numeric 'Runtime') that a production-ready solution would need.",llama-4-maverick,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk

# Ensure NLTK stopwords resource is downloaded
nltk.download('stopwords')
nltk.download('punkt')

def main():
    # Load the dataset
    df = pd.read_csv('imdb_top_250.csv')

    # Create a new feature for title length
    df['title_length'] = df['Title'].apply(len)

    # Calculate and print the Pearson correlation coefficient between...",7,"The generated code correctly addresses most of the requirements. It loads the data, calculates `title_length` and its correlation with `Rating`, preprocesses titles (lowercase, remove punctuation, tokenize), removes stopwords, and finds the 10 most common words.

However, there are a few areas for improvement:
1.  **Punctuation Removal:** The `string.punctuation` method for removing punctuation is less robust than using `re.sub` for the specific requirement of ""characters that are not alphanumeric or whitespace"". `string.punctuation` specificallyTargets a predefined set of ASCII punctuation characters, potentially missing others (e.g., Unicode punctuation) or misclassifying some symbols as non-punctuation when they should be removed per the prompt's `not alphanumeric or whitespace` criteria. Using `re.sub(r'[^a-z0-9\s]', '', text)` is more aligned with the precise instruction.
2.  **Correlation Calculation:** The generated code directly calculates `df[Rating].corr(df[title_length])` without explicitly handling potential non-numeric values or NaNs in the 'Rating' column, which could lead to errors or unexpected results if the column is not perfectly clean. The ground truth explicitly converts 'Rating' to numeric with `errors='coerce'` and then drops NaNs for cleaner correlation.
3.  **Tokenization and Filtering Logic:** The `word.isalpha()` check in `filtered_words = [word for word in words if word not in stop_words and word.isalpha()]` is a good addition for robustness. However, the ground truth's `remove_non_alphanumeric_whitespace` regex approach combined with `word_tokenize` leads to tokens that are already effectively alphanumeric (or empty strings), which then benefit from stopwords removal. The generated code's approach of removing punctuation *before* tokenization using `str.maketrans` on `string.punctuation` might leave some non-alphanumeric characters if they are not in `string.punctuation` (e.g., specific symbols, emojis, etc.), while the `word.isalpha()` partially mitigates this by dropping such tokens, it's not a direct implementation of ""remove punctuation (characters that are not alphanumeric or whitespace)"".
4.  **Overall Structure:** While `main()` and `if __name__ == ""__main__"":` are good practices, the prompt asked for ""Provide only the Python code without explanations"" and didn't specify a function wrapper. The ground truth provides a more linear script as might be expected for data analysis tasks where the steps are executed sequentially. This is a minor point, but worth noting for adherence to the prompt's simplicity.
5.  **Output Formatting:** The output `Pearson correlation coefficient: {correlation}` is generic. The ground truth provides more specific output messages like ""Pearson correlation between title_length and Rating: {correlation:.4f}"" and a clear heading for the most common words, making the output more readable and professional.",llama-4-maverick,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('penguins.csv')

# Define features X and target y
X = df[['culmen_length_mm', 'culmen_depth_m...",7,"The generated code generally addresses most requirements using a robust `Pipeline` and `ColumnTransformer` approach, which is commendable for its structure and maintainability.

However, it has one significant issue related to the N/A handling for the 'island' column, which deviates from the specific requirement. The problem statement explicitly states: ""if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows"".

The `SimpleImputer(strategy='most_frequent')` will correctly calculate the mode. However, if the entire 'island' column is `NaN`, `most_frequent` imputation will also result in `NaN`s, and `OneHotEncoder(handle_unknown='ignore')` may not gracefully handle an entirely `NaN` column, potentially leading to errors or undesirable behavior (like the resulting one-hot encoded columns being all zeros or missing). The ground truth specifically handles the case where *all* 'island' values are NaN, and then drops those rows. The generated code's current setup doesn't explicitly implement this specific dropping logic after imputation, especially for the ""all-NA"" scenario. While `cross_val_score` might drop NAs internally before processing, it's not explicitly following the ""if mode imputation results in NA for island, drop those rows"" instruction.

Additionally, the ground truth explicitly handles missing values in the target `y` *before* any feature processing, which is a good practice to ensure `X` and `y` always have aligned rows. The generated code moves the `y` NA handling
`X = X.loc[~y.isna()]`
`y = y.loc[~y.isna()]`
to just before the `cross_val_score` call. While this works, it's slightly less explicit than handling it right after loading, as feature preprocessing steps might also introduce or change NA patterns in `X` if not handled carefully, though in this case, the `Pipeline` would handle it.

The use of `ColumnTransformer` and `Pipeline` is excellent for production-ready code.

**Areas for improvement:**
*   **Specific `island` NA handling:** Explicitly implement the logic to drop rows if 'island' NAs remain after mode imputation, especially if the mode itself was problematic or if the entire column was N/A. This might require a custom transformer in the pipeline or a pre-pipeline step.
*   **Target NA handling:** Consistently ensure target NAs are removed early in the process.",llama-4-maverick,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    penguins_df = pd.read_csv('penguins.csv')

    # Select features and drop rows with missing values
    features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
    penguins_df = penguins_df.dropna(subset=features)

    # Standardize features
    ...",7,"The generated code correctly implements most of the requirements. It loads the dataset, selects only the specified features, drops NA values (though the order of operations for dropping NA could be slightly improved for clarity as in the ground truth), standardizes the features, applies K-Means, applies PCA, and generates both required plots.

However, there are a few areas for improvement that prevent a higher score:
1.  **Plotting:** The `plt.scatter` function is used directly without leveraging `seaborn`, which is a common and often preferred library for data visualization in data science due to its higher-level interface and aesthetic defaults. While `plt.scatter` works, `seaborn.scatterplot` (used in the ground truth) generates more visually appealing plots, especially for categorical `hue` values, and handles legends automatically.
2.  **Coloring in Second Plot:** For the second plot (by original species labels), the generated code uses `c=pd.Categorical(pc_df['species']).codes`. While this technically works to assign numerical colors, it relies on matplotlib's default colormap, which might not be ideal or clearly distinguishable for categories. A more robust approach is to explicitly map categories to colors using a palette or use `hue` in `seaborn.scatterplot` which handles categorical coloring gracefully.
3.  **Missing `n_init='auto'` in K-Means:** The ground truth correctly specifies `n_init='auto'` for the K-Means model, aligning with modern scikit-learn practices. The generated code omits this parameter, which might trigger a future warning if `n_init` is deprecated or changed in newer versions of scikit-learn default behavior. *Correction: My apologies, the generated code DOES include `n_init='auto'`. This point is incorrect.*
4.  **Aesthetics and Clarity:** The plots lack explicit legends (beyond the default color bar for the first plot, which isn't ideal for discrete clusters) and proper styling. The ground truth uses `sns.set_style(""whitegrid"")` and provides explicit legends with titles, making the plots more informative and presentation-ready.
5.  **Data Cleaning Order:** The ground truth stores `species` alongside the selected features *before* dropping NAs to ensure the `species` column is aligned perfectly with `df_cleaned`. While the generated code's approach of `penguins_df = penguins_df.dropna(subset=features)` also works, ensuring `penguins_df['species']` is aligned with `standardized_features` after dropping NAs is crucial and done more explicitly in the ground truth by passing `species` along for the `dropna` operation. In this specific case, `penguins_df['species'].reset_index(drop=True)` effectively handles the alignment, but it's a detail worth noting for robustness.",llama-4-maverick,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('penguins.csv')

# Impute missing values
df = df.dropna(subset=['island'])
df['culmen_length_mm'] = df['culmen_length_mm'].fillna(df['culmen_length_mm'].mean())
df['culmen_depth_mm'] =...",7,"The generated code correctly loads the data, imputes missing values as specified (numerical with mean, sex with mode, dropping NA island rows). It correctly sets up features and target, and trains a `GradientBoostingClassifier`. It also extracts and prints feature importances.

However, there are a few issues that prevent a higher score:
1.  **Feature Importance Names:** The way `feature_names` are constructed after `ColumnTransformer` is problematic. `preprocessor.named_transformers_['cat'].get_feature_names_out()` only returns the names for the one-hot encoded columns, not aligned with `X` after `fit_transform`. When `X` is transformed by `ColumnTransformer`, `num` features are at the beginning followed by `cat` features. The `feature_names` list concatenation `numerical_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out())` correctly orders the names, but `get_feature_names_out()` by default for `OneHotEncoder` prepends the original column name (e.g., `island_Biscoe`). This is correct. The issue lies in the fact that `X` is *transformed* in place before printing importances, `X = preprocessor.fit_transform(X)`, which makes `X` a NumPy array. The `feature_names` list is constructed *before* this `X` is passed to the model. The feature names should ideally be obtained after `fit_transform` or ensuring `X` remains a DataFrame with correct columns for mapping. The code technically works for printing names, producing `island_Biscoe`, `island_Dream`, `island_Torgersen`, `sex_FEMALE`, `sex_MALE`. The mapping of these names to the *original* X columns (which is now a numpy array) with the importance values generally works because the order is preserved.

2.  **Box Plot Visualization:** The box plot generation is incorrect for the requirement.
    *   The task asks for ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column.""
    *   The generated code creates *separate* box plots for male and female within a species, but not side-by-side grouped by sex on the same `x` axis, which is the standard interpretation of ""grouped by the imputed `sex` column"" for visualization. The ground truth demonstrates this correctly using `hue`. The current implementation creates two separate box plots on a single figure for male and female, but it plots them on `x` axis as `1` and `2`, not labeled categories like `MALE` and `FEMALE` easily identifiable, and it doesn't use the `seaborn` library (`sns.boxplot`) which is typically better for such grouped plots.
    *   The plotting logic of `plt.boxplot([species_df[species_df['sex'] == 'MALE']['flipper_length_mm'], species_df[species_df['sex'] == 'FEMALE']['flipper_length_mm']], labels=['MALE', 'FEMALE'])` correctly labels the boxes but does not produce the desired grouped appearance where sex is a `hue` or a distinct `x` category for each species. It generates separate figures for each species, each with only two boxes (MALE/FEMALE).

3.  **Missing `seaborn` import**: Although `matplotlib.pyplot` is imported, `seaborn` is not, which would be generally preferred for statistical plots in data science and is used in the ground truth for a better aesthetic and easier grouping.

4.  **No `sns.set_theme`**: Minor aesthetic point, but setting a consistent plot theme is good practice.

5.  **`ColumnTransformer` for simple OHE**: While `ColumnTransformer` is powerful, for this specific task where numerical features are passed through and only two categorical features need OHE, a simpler `pd.get_dummies` after imputation would have been more straightforward and less complex for feature importance mapping without `ColumnTransformer`'s column naming conventions. Using `ColumnTransformer` then getting names can sometimes be tricky unless specific transformers like `OneHotEncoder` are used.

Improving the box plot visualization to match the requirement (e.g., using `seaborn` with `hue`) and streamlining the feature importance name retrieval would significantly improve the solution.",llama-4-maverick,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"```python
import pandas as pd
from sklearn.impute import KNNImputer
import numpy as np

# Load the dataset
def load_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load data: {e}"")

# Impute NA values in numerical columns using KNNImputer
def impute_numerical_na(data, numerical_cols, n_neighbors=3):
    imputer = KNNImputer(n_neighbors=n_neighbors)
    imputed_data = imputer.fit_transform(data[numerical_cols...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, applies KNN imputation to the specified numerical columns, imputes NA values in the 'sex' column with its mode, and verifies the remaining NA values. The code is well-structured using functions, which improves readability and modularity.

Strengths:
- **Correctness:** All imputation steps are performed accurately according to the task description.
- **Modularity:** The use of `load_data`, `impute_numerical_na`, and `impute_categorical_na` functions makes the code well-organized and reusable.
- **Completeness:** All aspects of the task, including loading, both types of imputation, and verification, are covered.
- **Robustness (Basic):** The `load_data` function includes a basic `try-except` block for file loading, which is good practice.

Areas for minor improvement (not major flaws):
- **Error Handling for `load_data`:** The `load_data` function prints an error message but returns `None` on failure, which might lead to subsequent errors if `main` doesn't check for this. The ground truth code `exit()`s, which is a more definitive way to handle a critical file loading failure in a script.
- **Direct Access to Main:** The ground truth code performs actions directly, while the generated code uses `main` and conditional execution (`if __name__ == ""__main__"":`). While this is good practice for larger applications, for a simple data cleaning script, the direct approach is also perfectly acceptable and sometimes preferred for brevity in benchmarks.
- **Verification Output Detail:** The ground truth code provides more granular verification by printing NA counts for individual columns after imputation, not just `df.isna().sum()`. While `df.isna().sum()` confirms global NA state, specific column verification aligns better with the problem statement's `print the number of remaining NA values in each column after imputation to verify`.

Overall, this is a very strong submission that fulfills the task requirements effectively.",llama-4-maverick,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
# Import necessary libraries
import pandas as pd
from scipy.stats import ttest_ind

def main():
    # Load the dataset
    try:
        penguins_df = pd.read_csv('penguins.csv')
    except FileNotFoundError:
        print(""The 'penguins.csv' file was not found in the current directory."")
        return

    # Filter the dataset to include only 'Adelie' and 'Gentoo' species and handle NA values
    filtered_df = penguins_df[(penguins_df['species'].isin(['Adelie', 'Gentoo'])) & 
        ...",9,"The generated code correctly addresses all the requirements of the task. It loads the data, filters for the specified species, handles NA values, performs the t-test, and interprets the results based on the p-value. The code is well-structured and uses appropriate libraries.

**Strong points:**
*   **Correct Logic:** All steps of the hypothesis test are implemented correctly.
*   **NA Handling:** The NA handling is concise and effective by combining filtering with NA dropping.
*   **Clear Output:** The print statements clearly present the t-statistic, p-value, and the conclusion regarding the null hypothesis.
*   **Error Handling (Basic):** Includes a basic `FileNotFoundError` check.

**Areas for minor improvement (why it's not a 10):**
*   **Explicit `equal_var` in `ttest_ind`:** While `ttest_ind` defaults to `equal_var=True`, explicitly setting it makes the code's intent clearer, especially for statistical tests where variance assumption is important. The ground truth code does this, which is good practice.
*   **Docstrings/Comments:** The generated code lacks docstrings, which would enhance readability and maintainability, especially for functions. While the ground truth code used a function, the generated code uses a `main` function structure which is also fine, but comments in the `main` function could be slightly more descriptive.
*   **Variable Names:** While `filtered_df` is good, `adelie_culmen_lengths` and `gentoo_culmen_lengths` are a bit long. `adelie_culmen` and `gentoo_culmen` like in the ground truth are concise and clear. This is a very minor stylistic point.

Overall, it's a very good solution that successfully completes the task.",llama-4-maverick,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
df = pd.read_csv('penguins.csv')

# Define features and target
features = ['culmen_length_mm', '...",9,"The generated code successfully addresses almost all requirements of the task.

**Strong Points:**
*   **Correct Logic Flow:** The code follows the specified steps: data loading, feature/target definition, preprocessing, splitting, training, and evaluation.
*   **Correct Preprocessing:**
    *   Numerical median imputation is correctly implemented using `SimpleImputer`.
    *   Categorical mode imputation and one-hot encoding are correctly applied using `SimpleImputer` and `OneHotEncoder` within a `Pipeline` for categorical features.
    *   `ColumnTransformer` is used appropriately to combine numerical and categorical preprocessing.
*   **Correct Target Encoding:** `LabelEncoder` is correctly used for the `species` target.
*   **Correct Data Splitting:** `train_test_split` is used with the correct `test_size`, `random_state`, and `stratify` parameters.
*   **Correct Model Training:** `RandomForestClassifier` is initialized with `n_estimators=100` and `random_state=42` and integrated into a `Pipeline` with the preprocessor.
*   **Correct Evaluation:** `accuracy_score` and `classification_report` are used correctly.

**Areas for Improvement (Why not a 10):**
*   **Dynamic Feature Identification:**
    *   The `numerical_features` and `categorical_features` are identified dynamically using `select_dtypes`. While this is generally good practice for flexibility, the task description explicitly lists the numerical features and states to apply mode imputation and OHE *specifically* to `sex` and `island`. The dynamic assignment *correctly* identifies `island` and `sex` as categorical and the others as numerical, so it works, but it's a slight deviation from the *literal* interpretation of ""One-hot encode `island` and `sex` (after mode imputation for sex)"". However, since the task implicitly defines the types based on the given columns, this is not a critical error.
    *   A minor point if `sex` or `island` were numeric (which they aren't here), the dynamic typing might misclassify them. But for this dataset, it works.
*   **Missing Specific Imputation for `sex` vs. other categoricals:** The prompt specifically said ""For `sex`, impute NAs with the mode"" and ""One-hot encode `island` and `sex`."" The generated code uses `SimpleImputer(strategy='most_frequent')` for *all* `categorical_features`. In this specific case, since `island` is also categorical and would likely also be mode-imputed for this dataset, it produces the correct result. However, if there were other categorical features requiring different imputation strategies, the dynamic approach might need refinement. The ground truth correctly identifies `sex` and `island` for the `categorical_transformer`.
*   **Classification Report Readability:** The `classification_report` in the generated code doesn't use `target_names`. While it still provides the report, passing `label_encoder.classes_` (as done in the ground truth) would make the output more readable by showing the actual species names instead of numerical labels (0, 1, 2). This was missing.
*   **Code Structure/Robustness:** The ground truth includes `try-except` for file loading and print statements for clarity and progress tracking, which are good practices for real-world scripts but not strictly required by the prompt. The generated code is more concise, which can also be a positive attribute. The minor point is just about the `target_names` for the report.",llama-4-maverick,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

# Create DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Calculate Pearson correlation coeffi...",9,"The generated code is highly correct and directly addresses all parts of the task.

**Strong points:**
*   **Correct DataFrame Creation:** The DataFrame is created accurately, matching the specifications.
*   **Correct Correlation Calculation:** It correctly calculates the Pearson correlation coefficients using `df.corr()`. Although `method='pearson'` isn't explicitly stated, `pearson` is the default for `corr()` in pandas, so this is correct.
*   **Correct Printing of Correlations:** Both correlation coefficients are printed as required.
*   **Correct Stronger Correlation Determination:** It correctly uses `abs()` to compare the absolute correlations and identifies which feature has a stronger relationship with ""PerformanceScore"". It also handles the case of equal absolute correlation.

**Areas for minor improvement (to achieve a perfect 10):**
*   **Formatting of Printed Correlations:** The ground truth code formats the correlation coefficients to 4 decimal places using f-strings (e.g., `{:.4f}`). While not explicitly stated as a requirement, this is good practice for numerical output, especially for correlations, and improves readability. The generated code prints the full precision.
*   **Explicitness of Method:** While `pearson` is the default, explicitly adding `method='pearson'` to the `.corr()` calls (as in the ground truth) can make the code slightly more robust and readable, especially if someone is not familiar with pandas defaults. However, this is a very minor point and not a correctness issue.

Overall, the generated code performs very well and meets all essential requirements of the task.",llama-4-maverick,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test
from lifelines.utils import concordance_index
import scipy.stats as stats
import statsmodels.api as sm
import pymc3 as pm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Load the Titanic dataset
df = pd.read_csv('titanic.csv')

# Preprocess the data
df['Survived'...",3,"The generated code fails significantly in meeting the requirements of the prompt, particularly concerning the core survival analysis definitions and advanced statistical methods.

**Critical Issues:**

1.  **Incorrect Kaplan-Meier and Cox Model `duration_col` and `event_col`:** The prompt explicitly states: ""Implement Kaplan-Meier survival analysis treating the voyage as time-to-event data (with survival as event indicator)"". It also states that `T` (time) would be the duration of the voyage, and `E` (event) would be the survival indicator (death/event). The generated code uses `df['Age']` as `duration_col` and `df['Survived']` as `event_col` for both K-M and Cox models. This is fundamentally incorrect for ""time-to-event data"" for a voyage and makes no sense in the context of Titanic survival. Age is a covariate, not the duration of the event (death). `Survived` should be the event indicator, typically 1 for event (death) and 0 for no event (survival), but the prompt implicitly defines it as such by saying ""survival as event indicator"", which often means Survived=0 (death) is the event. The ground truth correctly defines a constant T and E as 1-Survived. This single error invalidates almost all survival analysis components.

2.  **Missing `Pymc3` Installation/Import for Arviz:** The task specifically asked for `pymc3` *or similar* and specified `arviz` as a proper import. The generated code uses `arviz` but doesn't explicitly import it or leverage its full capabilities for plotting or inference data storage. While `arviz` is often installed with `pymc3`, explicitly importing it for plotting is good practice.

3.  **Incomplete Propensity Score Matching:** The prompt requires propensity score matching to `analyze treatment effects (e.g., passenger class impact)`. The generated code uses `causalinference` but then defines `T = df_psm['Pclass_2']` to analyze ""Pclass 2 vs others"". This is not a direct ""passenger class impact"" and lacks the detailed matching, balance checks, and treatment effect analysis that the ground truth provides (e.g., comparing Pclass 1 vs Pclass 3 and comparing survival rates post-matching). The ground truth provides a robust implementation of PSM, including balance checks with SMD and statistical testing of the treatment effect. The generated code also does not handle missing data for PSM covariates explicitly.

4.  **Flawed Bayesian Analysis:** The Bayesian model setup is overly simplistic and likely incorrect for the purpose. It models `lambda_ = pm.math.exp(beta0 + beta1 * df['Sex'])` directly into a Bernoulli likelihood. This implies `lambda_` is a probability (0-1), but an `exp` function can produce values greater than 1, making it unsuitable for a Bernoulli `p`. A `pm.invlogit` (sigmoid) function should be used for logistic regression, and the `df['Sex']` should be `X_bayesian` covering all features, not just `Sex`. This is a fundamental error in statistical modeling. The prompt asked for ""probabilistic modeling of survival factors"", implying a more comprehensive model than just `Sex`.

5.  **Missing Advanced Statistical Tests:**
    *   **Multivariate analysis:** While CoxPH is multivariate, the prompt specifically asked for ""multivariate analysis"". The interaction effects are somewhat covered, but other multivariate analyses are not (e.g., MANOVA if applicable, which wouldn't be for this setup).
    *   **Interaction effects testing:** Only one basic interaction `Age_Sex` is added, which is insufficient for ""testing"" interaction effects comprehensively.
    *   **Model diagnostics:** Beyond C-index (which is also incorrectly calculated using `Age` as duration), there's little evidence of comprehensive diagnostics. The `proportional_hazard_test` is good, but no interpretation is given. The ground truth also explicitly attempts `check_assumptions` with warnings.

6.  **Missing Comprehensive Model Validation and Sensitivity Analysis:** The prompt asked for ""model validation assumptions and sensitivity analysis."" The generated code only superficially touches on this with a single `proportional_hazard_test`. The ground truth implements a sensitivity analysis on imputation strategy and conceptually discusses prior sensitivity for Bayesian models, fulfilling this requirement much better.

7.  **Suboptimal Code Quality and Structure:**
    *   Many imports are unused (`scipy.stats`, `train_test_split`, `StandardScaler`).
    *   Preprocessing steps are basic; missing imputation for `Embarked`, no feature engineering like `FamilySize` or `IsAlone`.
    *   Plots are generated but no titles or labels are added beyond default `lifelines` plots.
    *   Verbose print statements are missing (e.g., ""Starting Kaplan-Meier analysis"").

In summary, the generated code misunderstands the core concept of time-to-event analysis for the Titanic data, leading to fundamental errors in the survival analysis components. Its implementation of advanced features like PSM and Bayesian analysis is either incorrect or too simplistic to meet the detailed requirements of the task.",llama-4-maverick,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Create DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
    'Final_Score': [90, 88, 82, 92...",8,"The generated code correctly performs most of the required tasks.

**Strong Points:**
- **DataFrame Creation:** Correctly creates the pandas DataFrame as specified.
- **Pearson Correlation:** Accurately calculates and prints the Pearson correlation coefficient.
- **Scatter Plot:** Generates a scatter plot with 'Attendance_Rate' on the x-axis, 'Final_Score' on the y-axis, and colors points by 'Subject'. It includes a title, axis labels, and a legend.
- **Grade Assignment:** Successfully adds the 'Overall_Grade' column and prints the count of students in each grade category.

**Areas for Improvement:**
- **Grade Assignment Logic:** While `pd.cut` is a valid approach, the binning logic `bins=[0, 70, 80, 90, float('inf')], labels=['D', 'C', 'B', 'A'], right=False` is slightly off compared to the prompt's definitions.
    - Prompt: 'A' if Final_Score >= 90, 'B' if 80 <= Final_Score < 90, 'C' if 70 <= Final_Score < 80, 'D' if Final_Score < 70.
    - Generated `pd.cut(..., bins=[0, 70, 80, 90, float('inf')], labels=['D', 'C', 'B', 'A'], right=False)`:
        - `[90, inf)` -> 'A' (Correct)
        - `[80, 90)` -> 'B' (Incorrect, it should be `[80, 90)`)
        - `[70, 80)` -> 'C' (Correct)
        - `[0, 70)` -> 'D' (Correct)
    The `right=False` makes the lower bound inclusive and upper bound exclusive.
    For 'B', the prompt states `80 <= Final_Score < 90`. The generated code's bin for 'B' is `[80, 90)`. This seems *correct* for the given ranges. The labels are applied to the bins in order. Let's re-evaluate the bins.
    `bins=[0, 70, 80, 90, float('inf')]`
    `labels=['D', 'C', 'B', 'A']`
    - `[0, 70)` => 'D' (Correct for Final_Score < 70)
    - `[70, 80)` => 'C' (Correct for 70 <= Final_Score < 80)
    - `[80, 90)` => 'B' (Correct for 80 <= Final_Score < 90)
    - `[90, inf)` => 'A' (Correct for Final_Score >= 90)
    My apologies, the `pd.cut` implementation for grades is actually correct and quite elegant, mapping perfectly to the specified ranges.

- **Missing `numpy` Import:** The prompt explicitly stated ""Include necessary imports (pandas, matplotlib.pyplot, seaborn for potentially better aesthetics but not required for coloring, scipy.stats for correlation)"". While the code doesn't directly use `numpy` functions outside of what `pandas` might implicitly handle (e.g., `float('inf')`), it's generally good practice to explicitly import `numpy` if relying on its constants or more advanced functions, especially when the prompt hints at it. However, in this specific case, `float('inf')` works fine without explicit `import numpy as np`.

- **Minor Output Detail (Grade Counts):** The ground truth sorted the grade counts by `sort_index()` which can provide a more logical order (A, B, C, D) if the index is numeric, or alphabetic if string. However, `value_counts()` naturally orders by frequency descending. In this case, `sort_index()` would be 'A', 'B', 'C', 'D' if grades are passed as a CategoricalDtype with a specified order, or 'A', 'B', 'C', 'D' if they are strings and this is the direct result of `sort_index()`. The current output is `D, C, B, A` due to the labels in `pd.cut`. This is a very minor stylistic point. The ground truth example used `np.select` which allows for direct sorting of grades by `sort_index()`. If `pd.cut` is used to get the desired A, B, C, D sort order, one would typically use `pd.CategoricalDtype` to define the order. As it stands, the current `sort_index()` will sort alphabetically (A, B, C, D) if the grades are treated as strings. Since `pd.cut` creates a Categorical series and the labels are defined in order, `value_counts()` will respect that order, which is 'D', 'C', 'B', 'A' as it was passed. If the user then calls `sort_index()` on this, it will sort alphabetically by the string labels, resulting in 'A', 'B', 'C', 'D'. So, the provided code's call to `sort_index()` will produce the desired output order.

The code is very close to perfect. The `pd.cut` solution is concise and correct for grade assignment. The only minor ""missing"" element from the prompt's implied requirements is the `numpy` import, which doesn't cause a functional issue here.",llama-4-maverick,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']), 
        'Ticker': ['TECHCO']*10, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0], 
        'Low': [149.5, 150.5, 1...",9,"The generated code successfully creates the DataFrame, calculates daily returns, and both SMAs. It correctly generates the plot with all required elements (title, labels, legend). It also correctly identifies and prints the bullish crossover dates. The code is well-structured and uses proper imports.

Areas for minor improvement:
1.  **Plotting**: While functional, explicitly passing `df.index` to `plt.plot` (e.g., `plt.plot(df.index, df['Close'])`) is generally clearer and more robust, though in this case, `plt.plot(df['Close'])` defaults to using the index.
2.  **Bullish Crossover Logic Readability**: The ground truth's approach of defining `current_bullish_condition` and `previous_bearish_or_equal_condition` can sometimes be slightly more explicit about the two-part condition for a crossover. However, the generated code's logic `(df['SMA_3Day_Above_SMA_5Day'] == True) & (df['SMA_3Day_Above_SMA_5Day'].shift(1) == False)` is equally correct and concise.
3.  **Output Format**: The `print(bullish_signals.index)` output is functional, but formatting it like the ground truth (`date.strftime('%Y-%m-%d')`) can make the output more user-friendly.

Overall, it's a very strong submission that meets nearly all requirements effectively.",llama-4-maverick,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create DataFrame
data = {
    'OrderID': [101,102,103,104,105,106,107], 
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'], 
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'], 
    'Quantity': [1,2,1,1,3,1,1], 
    'Price_USD': [1200,25,75,300,50,25,1150], 
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST...",9,"The generated code is highly correct and complete, addressing all requirements of the task. It successfully creates the DataFrame, calculates and visualizes sales per product, calculates the average order value, and identifies repeat customers. The imports are correct, and the code structure is logical.

**Strong points:**
*   **Correct DataFrame creation:** The DataFrame is accurately initialized with the provided data.
*   **Correct Revenue Calculation:** The `Revenue` column is correctly calculated.
*   **Correct Product Revenue Grouping:** The revenue is correctly grouped by `Product`.
*   **Correct Chart Generation:** A horizontal bar chart is correctly generated with appropriate labels and title.
*   **Correct Average Order Value Calculation:** The average order value is calculated accurately.
*   **Correct Repeat Customer Identification:** Repeat customers are correctly identified using `value_counts()`.

**Minor areas for improvement:**
1.  **Plot Aesthetics:** While the plot correctly displays the data, the ground truth code includes `plt.grid(axis='x', linestyle='--', alpha=0.7)` and `plt.tight_layout()`, which improve readability and presentation. These could be added to enhance the visual clarity of the plot.
2.  **Output Formatting:** The ground truth provides more verbose and user-friendly print statements, especially for the original DataFrame, the product revenue, and the repeat customers section, which enhances the overall presentation of the results. For `repeat_customers`, the ground truth prints a formatted table, which is slightly more readable than the default pandas Series printout.

Overall, an excellent solution that performs all requested operations correctly. The suggestions are minor polish for better presentation and user experience rather than correctness issues.",llama-4-maverick,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd
import numpy as np

# Create the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Map DeviceType to numerical values
df['DeviceType_Numeric'] = df['DeviceType'].map(...",10,"The generated code is excellent as it correctly addresses all aspects of the task. It properly defines the initial DataFrame, accurately creates the ""DeviceType_Numeric"" feature using the specified mapping, and correctly calculates ""PagesPerMinute"" while robustly handling potential division by zero by replacing NaNs/Infs with 0. Finally, it correctly selects and prints the head of the requested feature DataFrame. The code is clean, concise, and follows good practices (e.g., direct chaining of `replace` and `fillna` for `PagesPerMinute`). All requirements from the prompt are met accurately and efficiently.",llama-4-maverick,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

# Create DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}
df = ...",9,"The generated code correctly defines the DataFrame, calculates the mean and standard deviation for 'TemperatureCelsius', identifies outliers based on the 1.5xSTD threshold, and prints the specified details for each outlier. It also correctly converts the 'Timestamp' column to datetime objects.

Minor areas for improvement (not critical for correctness in this specific task, but good practice):
1.  **Readability of Output:** The ground truth code formats the timestamp and temperature more cleanly in the print statement, and adds informative headers and separators, which enhances readability, especially for a user. The generated code prints the full datetime object which includes seconds, and doesn't format the temperature.
2.  **Handling No Outliers:** The generated code does not explicitly handle the case where no outliers are found. While the `for` loop simply won't execute in that scenario, a message indicating ""No outliers found"" (as in the ground truth) would provide clearer feedback to the user.
3.  **Variable Names:** `mean_temp` and `std_dev_temp` are good, but `outliers` as a DataFrame name could be slightly more descriptive (e.g., `outlier_readings_df` or `temperature_outliers`). This is minor stylistic preference.

Overall, it's a very good solution that addresses all the core requirements.",llama-4-maverick,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150,80,300,200,90,450,120,250,70],
    ""Shares"": [20,15,50,30,10,60,18,40,12],
    ""Comments"": [10,5,25,15,8,30,9...",10,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"" as specified, and accurately identifies the ""PostType"" with the highest average ""EngagementScore"" for each ""Platform"". It then prints the results in a clear and readable format. The logic for finding the max engagement using `groupby('Platform')['EngagementScore'].idxmax()` is an efficient and clean way to solve this part of the problem. All requirements are met, and the code quality is excellent.",llama-4-maverick,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""C...",9,"The generated code correctly performs all the requested tasks: Defines the DataFrame, calculates ""LengthOfStay"", groups by ""Diagnosis"" to calculate average stay and patient count, prints the summary, and identifies the diagnosis with the longest average stay.

**Strong points:**
*   **Correctness:** All calculations (LengthOfStay, average, count) and logic (identifying max) are accurate.
*   **Conciseness:** The code is quite concise, especially the `groupby().agg()` part for calculating both mean and count efficiently.
*   **Readability:** Variable names are clear, and the steps are logically ordered.
*   **Fulfillment of requirements:** All explicit requirements of the prompt are met.

**Areas for minor improvement (why it's not a 10):**
*   **Printing format:** While `print(diagnosis_summary)` does display the required information, the ground truth's formatted print (`f""{'Diagnosis':<15} ..."")` is more user-friendly and explicitly addresses ""print this information (Diagnosis, Average LengthOfStay, Patient Count)"" with specific column names as requested. The generated code's column names after `reset_index()` are slightly different ('Average LengthOfStay' vs 'AverageLengthOfStay').
*   **Refinement of longest stay printout:** The ground truth also prints the *value* of the longest average stay, which is helpful context (e.g., ""The Diagnosis with the longest average Length of Stay is: 'Flu' (7.00 days)""). The generated code only prints the diagnosis name.

These are minor presentational details that don't affect the core functionality or correctness of the computations, which is why it receives a high score.",llama-4-maverick,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes""...",9,"The generated code is highly correct and complete, successfully addressing all aspects of the task. It correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified, adds it as a new column, and accurately identifies and prints the title and weighted rating of the movie with the highest weighted rating. The code quality is good, with clear variable names and logical flow.

**Strong points:**
*   **Correct DataFrame Creation:** The DataFrame is initialized exactly as per the task description.
*   **Accurate WeightedRating Calculation:** The formula `(df[""Rating""] * df[""Votes""]) / total_votes` is correctly applied.
*   **Correct Column Addition:** The `WeightedRating` is added as a new column seamlessly.
*   **Correct Identification of Max WeightedRating Movie:** `idxmax()` is used effectively to find the movie with the highest weighted rating.
*   **Clear Output:** The final print statements clearly present the required information.

**Areas for minor improvement (to reach a perfect 10):**
*   **Error Handling for `total_votes == 0`:** While not strictly necessary for the given dataset (as `total_votes` won't be zero), robust code would ideally include a check for `total_votes == 0` before the division to prevent a `ZeroDivisionError`, similar to how the ground truth thoughtfully handles it.
*   **Output Formatting:** The ground truth used `:.6f` for more decimal places in the output `WeightedRating`, which might be slightly more precise, though `:.4f` is perfectly acceptable for most purposes. This is a very minor stylistic point.

Overall, an excellent solution that performs exactly as required.",llama-4-maverick,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000,...",9,"The generated code correctly performs all the required tasks: it defines the DataFrame as specified, correctly calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and presents this in a new DataFrame. The use of `df.groupby(""Brand"")[[""PriceUSD"", ""Mileage""]].mean().reset_index()` is an efficient and idiomatic pandas way to achieve the desired aggregation and structure for the output.

The primary difference from the ground truth is the naming of the aggregated columns. The generated code's output columns for average price and mileage will retain their original names (""PriceUSD"" and ""Mileage""), whereas the ground truth uses ""Average_PriceUSD"" and ""Average_Mileage"" for clarity. Both are acceptable interpretations, but the ground truth's approach might be slightly more explicit for the output. Also, the print statement in the generated code is simpler, lacking the descriptive header present in the ground truth. These are minor differences that do not detract significantly from correctness or functionality.",llama-4-maverick,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 8...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the ""TotalStats"" column using a concise `sum(axis=1)` approach, correctly identifies the Pokemon with the highest ""TotalStats"" and prints its name, and accurately calculates and prints the average ""Speed"" for each ""Type1"". The code is clean, efficient, and directly addresses all requirements of the prompt. No significant improvements are needed.",llama-4-maverick,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.stats import mode
import matplotlib.pyp...",6,"The generated code attempts to address most of the requirements but falls short in several key areas, leading to a lower score.

**Strengths:**
*   **Attempts advanced feature engineering:** Includes `FamilySize`, `TicketFreq`, `IsAlone`, and `Title` extraction.
*   **Multiple clustering algorithms:** Uses KMeans, AgglomerativeClustering, DBSCAN, and GMM.
*   **Multiple validation metrics:** Implements Silhouette, Calinski-Harabasz, and Davies-Bouldin.
*   **Dimensionality reduction:** Utilizes PCA, t-SNE, and UMAP for visualization.
*   **Visualization of clusters:** Plots clusters in reduced dimensions.
*   **Survival analysis per cluster:** Calculates and visualizes survival rates.
*   **Attempts cluster stability and consensus clustering:** Though implementation is flawed.

**Weaknesses and Areas for Improvement:**

1.  **Correctness & Completeness of Feature Engineering:**
    *   **Social Network Analysis:** The ""social network analysis"" aspects are very minimal (only `TicketFreq` and `FamilySize`). The ground truth explored `TicketGroupSize` and a more robust `FarePerPerson`.
    *   **Socioeconomic Indicators:** It processes `Pclass` and `Fare`, but does not fully capture the complexity of the ground truth (e.g., interaction features like `Age_Class`, `Fare_Class`).
    *   **Text-based Features:** The `Title` mapping is a direct label encoding which assumes an ordinal relationship, which is not ideal for titles. The ground truth uses one-hot encoding after grouping rare titles. `NameLength` and `TicketPrefix` (with proper grouping) are missing.
    *   **Missing Data Handling:** While `Fare` and `Age` are imputed, `Embarked` and `Cabin` are completely ignored in feature engineering. The comprehensive `CabinDeck` and `HasCabin` features from the ground truth are absent.

2.  **Preprocessing and Scaling:**
    *   **Limited features:** Only a small subset of features (`Age`, `Fare`, `FamilySize`, `TicketFreq`, `Pclass`, `Title`, `SibSp`, `Parch`) are selected for clustering, ignoring many potentially valuable engineered features. The ground truth uses `ColumnTransformer` for robust and comprehensive preprocessing of numerical and categorical features.
    *   **Lack of One-Hot Encoding:** Categorical features like `Sex`, `Embarked`, `Pclass` (after converting to category but before codes), and `Title` (if not numerically mapped) are not one-hot encoded, which is crucial for most distance-based clustering algorithms. Converting `Pclass` to category codes and `Title` to numerical mapping without full one-hot encoding can impose arbitrary ordinality.

3.  **Optimal Number of Clusters:**
    *   Only `KMeans` is used to determine optimal clusters. The ground truth evaluates `KMeans`, `GMM`, and `Hierarchical` for the optimal 'k' using all metrics.
    *   The elbow method (inertia) for KMeans is completely missing from the evaluation of `k`.

4.  **Clustering Algorithm Application:**
    *   For `KMeans` and `AgglomerativeClustering`, a fixed `n_clusters=5` is used without determining it from the validation metrics. The task specifically asks to ""Determine optimal number of clusters"" and then ""Apply multiple clustering algorithms with Optimal K"".
    *   `DBSCAN` parameters (`eps`, `min_samples`) are very sensitive and chosen arbitrarily. The task implies proper handling, which means potentially identifying these parameters or at least acknowledging their difficulty.
    *   `OPTICS` was imported but never used.

5.  **Cluster Stability Analysis & Consensus Clustering:**
    *   **Stability Analysis:** The `adjusted_rand_score` is used for stability, but it's measuring the ARI between bootstrapped KMeans runs and *another* KMeans run *on the original scaled data*. A common approach for stability is comparing runs on resampled data or comparing multiple runs to a single ""reference"" run. The ground truth provides a more robust approach of comparing multiple runs against the *first* run to measure internal consistency.
    *   **Consensus Clustering:** The concept is correct (building a co-occurrence matrix), but the implementation of `consensus_labels`, `consensus_matrix`, and the subsequent `KMeans` call within the loop is incorrect. The `consensus_labels` list collects raw label arrays from each run, but then the `consensus_matrix` calculation seems disconnected and incorrectly applies `KMeans` again inside the loop. The ""consensus clustering"" part doesn't then use the consensus matrix to derive a consensus partitioning. The task requires ""consensus clustering,"" which implies creating a partitioning, not just visualizing a matrix.

6.  **Visualization and Analysis:**
    *   **Analysis of Cluster Characteristics:** The `cluster_df.describe()` output is too verbose. The ground truth provides a more concise and interpretable statistical summary (median/mode of key features).
    *   **Feature Importance:** The `feature_importances_` attribute check in the last loop (`if hasattr(algorithm, 'feature_importances_'):`) is not applicable as none of the clustering algorithms (KMeans, GMM, HAC, DBSCAN) directly provide feature importances. This part of the code will never execute a plot. A common way to show feature importance for clustering is to visualize the *average* feature values per cluster, which the ground truth does with a heatmap of scaled feature means.

7.  **General Code Quality:**
    *   Lack of robust error handling (e.g., `FileNotFoundError` for CSV).
    *   Missing `random_state` in many parts, making results non-reproducible (e.g., `KMeans`, `TSNE`, `UMAP`, `GaussianBlur`, `DBSCAN` itself for internal randomness if any, `np.random.choice`).
    *   Undeclared imports (`adjusted_rand_score` is used at the end but imported within the `stability_scores` loop).
    *   Hardcoded `n_clusters=5` in several places after the attempt to find an optimal `k`, which contradicts the goal of finding an optimal `k`.

The generated code provides a basic framework, but critically misses the mark on the ""advanced"" and ""comprehensive"" aspects, especially in feature engineering, robust preprocessing, correct application of cluster stability, and detailed cluster characteristic analysis.",llama-4-maverick,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dyst...",9,"The generated code correctly performs all requested tasks: it defines the DataFrame, calculates book count and average price per author, prints this information, and correctly identifies the author with the highest total stock. The code is well-structured and uses appropriate pandas functions for aggregation.

**Strengths:**
*   **Correctness:** All calculations and identifications are accurate.
*   **Clarity:** The use of `groupby().agg()` with named output columns (`BookCount`, `AveragePrice`) makes the first aggregation result very readable.
*   **Efficiency:** Uses standard pandas operations which are generally efficient.
*   **Completeness:** Addresses all three parts of the task.

**Areas for Minor Improvement:**
*   **Formatting of Author Information:** While the output for ""Author Information"" is correct, it prints the full DataFrame, which is a bit less user-friendly than the ground truth's row-by-row formatted output. The ground truth formats the average price to two decimal places and aligns columns for better readability. This is a minor aesthetic point, not a correctness issue.

Overall, a very strong submission that fully meets the requirements of the task.",llama-4-maverick,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.feature_selection ...",4,"The generated code attempts to address many aspects of the prompt but falls significantly short in implementation details, correctness, and completeness, especially when compared to the advanced requirements.

**Correctness & Completeness Issues:**

1.  **Data Preprocessing:**
    *   **Multiple Imputation:** The prompt asked for ""multiple imputation strategies."" The generated code uses `SimpleImputer(strategy='mean')`, which is a basic single imputation method, not multiple imputation. The `IterativeImputer` (MICE) in the ground truth is a much more appropriate example of advanced multiple imputation.
    *   **Outlier Detection/Treatment:** `IsolationForest` for outlier *detection* is used, but the `mask` directly removes rows, which is not a general ""treatment"" strategy (like capping/Winsorization in the ground truth). This can lead to data loss and is a less robust approach than capping for advanced systems.
    *   **Feature Engineering:** `PolynomialFeatures(degree=2, interaction_only=True)` creates *only* interaction terms, not polynomial features (e.g., x^2, y^2). The prompt asked for ""polynomial and interaction features."" The ground truth correctly includes both by setting `interaction_only=False`.
    *   **Preprocessing Pipeline:** The generated code applies preprocessing steps sequentially without a proper `ColumnTransformer` or `Pipeline` that can be applied to new data (`X_test`) consistently. The data is preprocessed, then split, which is incorrect for a robust pipeline where the splitter should be *within* a pipeline or applied carefully to train/test sets to prevent data leakage (fitting scalers/imputers on test data). The ground truth correctly uses `full_preprocessor` to `fit_transform` on train and `transform` on test.

2.  **Model Optimization:**
    *   The prompt specifically mentioned `GridSearchCV *or* Bayesian optimization`. While `GridSearchCV` is used, the optimization is very basic. The parameter grids are extremely limited (e.g., `max_depth` only up to 15, `alpha` only two values). Advanced optimization typically involves a wider range of hyperparameters and potentially more sophisticated search strategies if Bayesian optimization were to be implemented.
    *   `f1_macro` for `scoring` is okay, but `roc_auc` is generally preferred for imbalanced classification.

3.  **Ensemble Models:**
    *   **Blending:** The generated code implements blending as a `StackingClassifier`, which is fundamentally *not* blending. Blending involves training base models on a subset of the training data and then training a meta-learner on predictions from a *different* subset of the training data (the validation set), then making final predictions on the test set. The generated code simply uses `StackingClassifier` for blending without the proper splitting strategy, making it just another stacking approach. The ground truth has a custom, correct blending implementation.

4.  **Advanced Evaluation:**
    *   **Stratified K-Fold:** Used for `GridSearchCV` and a small loop, but not consistently across all parts for a truly ""advanced"" evaluation.
    *   **Learning Curves/Validation Curves:** Completely missing. These were explicit requirements.
    *   **Bootstrap Sampling:** Missing. This was an explicit requirement for prediction confidence intervals.
    *   **Comprehensive Metrics:** While some metrics are printed, there's no single function or structured way to report all necessary metrics for every model, including precision, recall, F1, ROC AUC, and Brier score.

5.  **Model Interpretability (SHAP):**
    *   The SHAP implementation is partial. It initializes and gets SHAP values, but there's no visualization (e.g., `shap.summary_plot`, `shap.dependence_plot`) which was requested.

6.  **Statistical Significance Testing:**
    *   The `ttest_rel` is applied directly to `y_pred1` and `y_pred2`, which are *binary predictions* derived from a single test set. This is incorrect for comparing models. Paired t-tests for model comparison should be performed on performance scores (e.g., ROC AUC) obtained from *multiple cross-validation folds* (as done in the ground truth), not directly on the predictions from a single test set.

7.  **Model Comparison Dashboard:**
    *   A ROC curve plot is generated, but the broader ""comprehensive dashboard"" (which implies a tabular summary of *all* metrics, confidence intervals, etc., as well as plots for performance metrics) is largely missing.
    *   Prediction confidence intervals were requested but not computed or displayed.

8.  **Model Calibration:**
    *   Basic `CalibratedClassifierCV` is used, but the probability calibration *plots* (`calibration_curve`) are not correctly implemented or displayed to show both uncalibrated and calibrated performance, nor is the Brier score used to quantify pre/post calibration. The ground truth correctly visualizes and quantifies this.

**Code Quality:**

*   **Imports:** Mostly correct, but `scipy.stats` is only partially used and incorrectly.
*   **Structure:** The flow is somewhat logical but lacks the robustness of pipelines and proper function encapsulations for evaluation routines.
*   **Clarity:** Variable naming is reasonable.
*   **Error Handling:** No robust error handling (e.g., if files aren't found, if libraries aren't installed).
*   **Reproducibility:** `random_state` is used in some places but not consistently for all random operations (e.g., `IsolationForest`, `GridSearchCV`'s internal folds, etc.).

In summary, the generated code demonstrates a superficial understanding of many requirements but lacks the depth, correctness, and advanced implementation details expected for an ""advanced ensemble learning and model optimization system."" Many critical components (multiple imputation, robust outlier treatment, comprehensive feature engineering, proper blending, learning/validation curves, correct statistical testing, detailed dashboard, and complete calibration analysis) are either missing or implemented incorrectly.",llama-4-maverick,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
fro...",5,"The generated code attempts to address all requirements but falls short in implementation details, correctness of advanced statistical methods, and overall robustness.

**Major deficiencies:**

1.  **Imputation Comparison:** The ""comparison"" of imputation methods is merely printing `describe()` for each, which doesn't reflect an ""effectiveness comparison."" A true comparison would involve training models on each imputed dataset and comparing their performance, or using statistical metrics like MAE/RMSE on a held-out set of known missing values (if applicable). The `SimpleImputer` is also not mentioned as a requirement for comparison.
2.  **GAM Implementation:** The `statsmodels.gam.api.GLMGam` is used, which is a generalized *linear* model with smoothing terms, but it's not the same as a true Generalized Additive Model (`pygam` was specified). `BSplines` setup is also a bit simplistic and doesn't fully capture the flexibility of GAMs without proper parameter tuning. The task specifically requested `pygam`.
3.  **Bayesian Logistic Regression:**
    *   The `pm.glm.GLM.from_formula` approach is okay but doesn't explicitly show the construction of priors for `betas` and `alpha`, which is a key aspect of Bayesian modeling for demonstrating understanding.
    *   The prediction `y_pred_bayes = pm.trace_to_dataframe(trace)['y_mean'].mean(axis=0)` is incorrect. `y_mean` is not directly part of the `trace` in a standard GLM; you need to use `pm.sample_posterior_predictive` to get predictions on new data, and then process those. The accuracy calculation `accuracy_score(y, y_pred_bayes_class)` uses the full dataset `y` instead of `y_test`.
    *   No trace plots or summary statistics for Bayesian model parameters are provided, which are crucial for diagnostics.
4.  **Hypothesis Testing:**
    *   **Multivariate ANOVA:** `scipy.stats.f_oneway` is a *univariate* ANOVA (for comparing means of several groups on *one* dependent variable), not a *multivariate* ANOVA (MANOVA). The task explicitly asked for MANOVA. The way it's called `f_oneway(X[y==0], X[y==1])` is also problematic as `f_oneway` expects multiple arrays for each group, not a concatenated DataFrame.
    *   **Permutation Test/Bootstrap CI:** While the logic is somewhat there, they are applied to `X.iloc[:, 0]` (only the first feature) instead of being generalized or applied to a relevant statistic (e.g., mean difference between classes for a specific feature, or a model coefficient).
5.  **Robust Statistical Methods:** The code only uses `RobustScaler`, which is a preprocessing step. It does not implement a *robust statistical model* like Robust Linear Models (RLM) from `statsmodels.robust`. The requirement was to apply ""robust statistical methods to handle outliers and non-normal distributions"" which implies robust regression or estimation.
6.  **Model Diagnostics:**
    *   **Residual Analysis for Logistic Regression:** Residuals cannot be simply plotted as a histogram or used directly for `qqplot` with `y_test - y_pred_logreg` in logistic regression, as residuals are not normally distributed. Deviance residuals or Pearson residuals should be used (typically from `statsmodels` GLM results).
    *   **Influence Measures:** `sm.Logit` is good, but the `leverage` plot is just a scatter plot without proper labeling or interpretation, and no mention of Cook's distance was made.
    *   **Goodness-of-Fit Tests:** `het_breuschpagan` is for heteroscedasticity in linear regression, not a general goodness-of-fit test for logistic regression. There's no implementation of a Hosmer-Lemeshow test or similar for classification.
7.  **Factor Analysis:** `FactorAnalysis` was imported, but no output or analysis of its results (e.g., factor loadings, uniqueness) is presented.
8.  **Visualizations:** Q-Q plots and leverage plots are present but not explained for logistic regression properly. The ""diagnostic dashboard"" is a static set of plots, not interactive or particularly ""advanced"" beyond standard EDA visualizations. It does not include specific diagnostic plots like Partial Dependence plots from GAMs or similar.

**Minor issues:**

*   **Imports:** `from sklearn.experimental import enable_iterative_imputer` is missing (though it might work in newer versions without explicit import).
*   **Code Structure:** Missing print statements to indicate progress through each section, making it harder to follow.
*   The selection of features for PyMC3 model was not considered in the generated code.

The code provides a basic scaffold but lacks the depth and correctness required for ""advanced statistical modeling"" in several key areas.",llama-4-maverick,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.stats import ...",6,"The generated code attempts to address most of the requirements but falls short in several areas, particularly in correctness, completeness, and adherence to specific instructions.

**Correctness:**
*   **Feature Engineering:** The engineered features are very simplistic and don't align with the complex indices mentioned in the prompt (e.g., ""TDS"" is just a sum of three often unrelated constituents, ""Chemical_Balance_Ratio"" is problematic from a water chemistry perspective and prone to division by zero). The prompt specifically asked for ""water quality indices, chemical balance ratios, contamination risk scores."" The ground truth uses more sophisticated or at least reasonably plausible derivations.
*   **Missing Value Handling:** No explicit handling of missing values is present. The `df.drop('Potability', axis=1)` might fail if `Potability` is not the only column. The prompt assumes imputation.
*   **Scaling:** The scaling is applied to `df.drop('Potability', axis=1)` before feature engineering is complete on the original `df`. This is incorrect as the engineered features (which are still on `df`) will not be scaled. Scaling should happen after all feature engineering.
*   **Optimal K Determination:** The code simply picks the `n_clusters` that maximizes the silhouette score. While an option, it doesn't present the elbow method, Davies-Bouldin, or Calinski-Harabasz results visually for a human to interpret, which was requested.
*   **DBSCAN Parameters:** Uses fixed `eps=0.5` and `min_samples=10`. The prompt implies proper tuning or at least a demonstration of how it's done (e.g., k-distance graph, which the ground truth provided). DBSCAN is highly sensitive to these, and fixed values are unlikely to be optimal.
*   **Spectral Clustering:** The generated code doesn't specify `affinity` or `n_neighbors` for Spectral Clustering, which can lead to poor performance or errors for certain data distributions.
*   **Cluster Stability Analysis:** The bootstrap resampling for stability analysis stores `cluster_labels_bootstrapped` but does not perform any comparison (e.g., ARI or co-occurrence matrix) to quantify stability. It's an incomplete implementation. The consensus clustering part (co-occurrence matrix, dendrogram) is entirely missing.
*   **Anomaly Detection:** The ""cluster-based anomaly detection"" only identifies small clusters as anomalies, which is a very simplistic and often inadequate definition. It doesn't use distance from centroid or Isolation Forest *within* clusters as the ground truth did for more sophisticated anomaly detection.
*   **Plotly Subplots:** The generated Plotly subplot code for cluster visualization doesn't correctly manage subplots for DR techniques vs. clustering algorithms. It tries to create a grid of all combinations, but the `fig.add_trace` logic for `row=i+1, col=j+1` suggests only one of the results is used per subplot, leading to incorrect or overlapping plots. It also misses 3D plots.
*   **Radar Charts:** The radar chart generation is incorrect: `cluster_profiles.values[0]` will only plot the first cluster's profile. It needs to iterate through all clusters. It also doesn't normalize the features for the radar chart, which is crucial for comparable visualizations.

**Code Quality:**
*   No clear structure (e.g., functions, classes or logical sections with print statements) as in the ground truth, making it harder to follow.
*   Lack of comments for crucial steps.
*   Some `n_init` warnings from KMeans might appear if `n_init` is not explicitly set in newer scikit-learn versions.
*   Missing imports for `SimpleImputer`, `FactorAnalysis`, `NearestNeighbors`, `cdist`, `adjusted_rand_score` (though `adjusted_rand_score` is imported later only for external validation, not for consensus clustering where it could have been used).

**Completeness:**
*   **Feature Engineering:** Missing ""temporal patterns"" (acknowledged they might not apply to static data, but a note like the ground truth would be good). The engineered features are weak.
*   **Clustering Validation:** Missing gap statistic (though not explicitly asked for in the detailed list, it's a common method). The visualizations for optimal K are only for silhouette, DB, CH, not a combined one.
*   **Dimensionality Reduction:** Factor Analysis is requested but missing. 3D visualization (PCA, UMAP 3D) is missing.
*   **Cluster Stability Analysis:** Bootstrap resampling is incomplete (no comparison or metric). Consensus clustering is entirely missing.
*   **Water Quality Profiles:** While it generates mean profiles and attempts radar charts, the radar charts are incorrect. It also omits the ""contamination risk scores"" and ""potability rates"" explicitly in the profile analysis, though ""Potability"" is used in external validation.
*   **Cluster Visualization Dashboard:** The subplot logic is erroneous, 3D plots are missing, and radar charts are incorrectly implemented.
*   **External Validation:** The code evaluates Adjusted Rand Score against 'Potability', but the prompt also asks for ""domain knowledge about water quality standards,"" which the ground truth included.
*   **Cluster-based Anomaly Detection:** Only the small cluster approach, significantly deviates from the requested and more robust methods.

Overall, the code attempts to cover the breadth of the requirements but lacks depth and correctness in several key implementations, especially in feature engineering, optimal K determination visualization, cluster stability analysis, and sophisticated anomaly detection.",llama-4-maverick,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
import numpy as np

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# Determine correlations with 'quality'
correlations = df.corr()['quality'].drop('quality')

# Find strongest positive and negative correlations
pos_corr = correlations.sort_values(ascending=False)
neg_corr = correlations.sort_values(ascending=True)

# List top 3 positive and negative ...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
*   **Correct Correlations:** It correctly calculates and identifies the top 3 positive and negative correlations with 'quality'.
*   **Heatmap Generation:** It successfully creates a correlation matrix heatmap.
*   **Scatter Plots with Regression:** It generates scatter plots for the most positive and negative features with 'quality' and fits linear regression lines.
*   **Proper Imports:** All necessary libraries are imported.
*   **Readability:** The code is reasonably well-structured and easy to understand.

**Areas for Improvement (Why not a 10):**
*   **Robustness:** The ground truth includes error handling for file loading, which is good practice for real-world scenarios. The generated code lacks this.
*   **Regression Line Plotting:** While it plots a regression line, the x-axis for the `plt.plot` function should ideally be based on `x = df[feature]` values directly rather than implicitly relying on `df[feature]` which might cause issues if `df[feature]` is not sorted, potentially creating a ""zigzag"" line rather than a smooth straight line. The ground truth uses `np.array([df[feature].min(), df[feature].max()])` which is a more robust way to define the two points for a straight line.
*   **Information in Scatter Plot Legend:** The ground truth includes R-squared in the legend of the regression line, which is more informative than just the slope.
*   **Minor Plot Customizations:** The ground truth includes `plt.tight_layout()` and font sizes for titles/labels, which makes the plots look slightly cleaner and more professional. Also, `plt.grid` in the scatter plots is a nice touch for readability.
*   **Explicit Printing of Correlation Values:** The ground truth explicitly prints the correlation coefficients for the top 3 positive and negative features, not just iterates through them. While the generated code prints them, the format is slightly less direct with `nlargest/nsmallest`.
*   **DPI and Figure Size Consistency:** The ground truth uses named constants for figure sizes and DPI, making it easier to modify and maintain. The generated code uses hardcoded values, which is less flexible.",llama-4-maverick,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency, mannwhitneyu

def load_and_clean_data(file_name):
    data = pd.read_csv(file_name)
    data.dropna(inplace=True)
    return data

def test_nintendo_vs_others(data):
    nintendo_sales = data[data['Publisher'] == 'Nintendo']['Global_Sales']
    other_sales = data[data['Publisher'] != 'Nintendo']['Global_Sales']
    t_stat, p_value = ttest_ind(nintendo_sales, other_sales, equal_var=False)
    return p_value

def te...",6,"The generated code attempts to address all parts of the task but falls short on several critical aspects, primarily around data cleaning, error handling, and the detail of interpretation.

**Correctness:**
*   **Data Cleaning (Major Issue):** The `load_and_clean_data` function uses `data.dropna(inplace=True)` which is too aggressive. The ground truth code specifically targets relevant columns for NA removal and converts 'Year_of_Release' to `int` and 'User_Score' to numeric, handling `errors='coerce'`. The generated code's blanket `dropna` will likely remove too many rows and might not properly handle type conversions (e.g., 'Year_of_Release' as float or object if not cleaned properly before `dropna`). More importantly, `Year_of_Release` is referred to as 'Year' in the last test, which might be a column name mismatch if it's not renamed or handled.
*   **Missing Data Checks for Statistical Tests (Minor Issue):** The ground truth includes checks like `if len(nintendo_sales) < 2` before performing t-tests to prevent errors or meaningless results from running statistical tests on insufficient data. The generated code lacks these crucial checks, potentially leading to errors if, post-cleaning, a group becomes too small.
*   **Interpretation (Major Issue):** The interpretation of results is very basic. It only states ""Significant difference"" or ""No significant difference."" The task explicitly asks for p-values to be *displayed* and *interpreted*. The ground truth provides detailed interpretations, including the exact null/alternative hypotheses and the conclusion relative to the null hypothesis. The generated code only prints the p-value and a one-liner interpretation.
*   **Alternative Hypothesis for T-test (Minor Issue):** For the ""Nintendo vs. Others"" test, the ground truth uses `alternative='greater'` which aligns with ""significantly higher global sales."" The generated code uses the default `two-sided`, which tests for any *difference*, not specifically 'higher'. This is a conceptual mismatch with the problem statement.
*   **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice. The generated code lacks this.

**Code Quality:**
*   **Structure:** Using functions for each test is good for modularity.
*   **Imports:** Proper imports from `scipy.stats` are used.
*   **Readability:** Generally readable.

**Completeness:**
*   **Data Cleaning:** Incomplete, as detailed above.
*   **Interpretation:** Incomplete, as detailed above.
*   **Display p-values:** It does display the p-values, but the interpretation is very basic.
*   **Assumption of CSV in current directory:** Handled.
*   **No explanation:** Correctly provided only code.

To improve, the generated code needs:
1.  More robust and targeted data cleaning, specifically for 'Year_of_Release' and ensuring correct data types.
2.  Checks for sufficient data points within groups before statistical tests.
3.  More comprehensive printing of results, including the specific hypotheses, p-value, and a more detailed interpretation aligned with the `alpha` value.
4.  Correct specification of `alternative` hypothesis for specific directional tests (like 'greater' for sales comparisons).
5.  Basic file reading error handling.",llama-4-maverick,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def load_data(file_name):
    data = pd.read_csv(file_name)
    return data

def handle_missing_values(data):
    data.fillna(data.mean(numeric_only=True), inplace=True)
    for col in data.select...",6,"The generated code attempts to address all requirements of the task and uses proper imports and a good modular structure with functions for each step. However, there are significant issues in the data handling and feature preparation steps which impact the correctness of the overall solution.

**Specific Issues:**

1.  **Missing Value Handling:**
    *   The `handle_missing_values` function uses `fillna(data.mean(numeric_only=True))` for numerical columns and `fillna(data[col].mode()[0])` for categorical. This is a common imputation strategy, but the prompt specifically implies dropping NaNs for critical columns like `Global_Sales`, `Year`, `Publisher`, and regional sales, as done in the ground truth. The current imputation for `Global_Sales` (which is numerical) would lead to incorrect target variable creation later if it had NaNs. It also does not specifically handle `Year` for converting to int *after* handling NaNs, which can cause issues.
    *   The ground truth explicitly states: ""Drop rows where 'Global_Sales' is NaN as it's crucial for the target variable"" and ""For 'Year', 'Publisher', and regional sales, dropping NaNs is a reasonable approach"". The generated code deviates from this explicit instruction, leading to a different dataset for modeling.

2.  **Target Variable Creation:**
    *   The `create_target_variable` function correctly creates 'High_Sales' based on `Global_Sales > 1.0`. However, if `Global_Sales` had NaNs and was imputed with the mean, this step would be operating on imputed values, which is undesirable for a critical variable like a target.

3.  **Feature Selection in `split_data`:**
    *   The `split_data` function explicitly drops `Global_Sales`, `Name`, and `High_Sales` from `X`. While dropping `Global_Sales` is generally good to prevent data leakage (as it's directly related to the target), the prompt's ground truth code *includes* `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` as features. These *are* related to Global Sales but are often used as features in such models. The generated code implicitly removes these as well because they are part of `Global_Sales` if any of them had NaNs.
    *   More critically, the ground truth code *explicitly lists* the features to be used: `Platform`, `Genre`, `Publisher`, `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales`, `Total_Regional_Sales`, `Years_Since_Release`. The generated code's `X = data.drop(['High_Sales', 'Global_Sales', 'Name'], axis=1)` will include all other columns by default, which might not align with the intended feature set if there are other irrelevant columns. If `NA_Sales`, etc., are present and not dropped, they will be included, which is fine, but the dropping logic is less precise than explicit feature selection.

4.  **`Years_Since_Release` Calculation:**
    *   The prompt specifies creating `Years_Since_Release` as `2023 - Year`. The generated code does this, but it does not handle potential negative or zero values as a safeguard, which the ground truth gracefully adds. While not critical for this specific dataset, it's a minor best practice deviation.

5.  **Stratified Split:**
    *   The prompt asked for an 80-20 split, but the ground truth *also* uses `stratify=y` for the `train_test_split`. This is crucial for imbalanced datasets (which high/low sales can be) to ensure representative splits of the target variable. The generated code omits `stratify=y`.

Overall, while the code structure is good, the implementation details for data preprocessing and feature selection, especially around missing values and the specific features to *keep* or *drop*, deviate from the implied requirements and the ground truth, leading to a potentially different and less robust model.",llama-4-maverick,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures

# Load the dataset and handle missing values
def load_and_clean_data(file_name):
    data = pd.read_csv(file_name)
    data.replace('', np.nan, inplace=True)
    data.dropna(inplace=True)
    return data

data = load_and_clean_data('video_games_sales.csv')

# Create engineered features
data['Sales_Ratio_...",4,"The generated code attempts to address many parts of the task but falls short on correctness, completeness, and robustness, particularly concerning missing values and advanced feature engineering.

**Correctness & Robustness Issues:**
1.  **Missing Value Handling:** The `load_and_clean_data` function uses `data.dropna(inplace=True)`. This is a critical error for a real-world dataset like `video_games_sales.csv`, which has significant missing values in 'Year', 'Publisher', and 'Genre'. Dropping all rows with any NaN will discard a large portion of the dataset and is almost certainly not the intended ""handling missing values"" without specific imputation strategies. The ground truth correctly uses median/mode imputation for `Year`, `Publisher`, and `Genre`.
2.  **`Sales_Ratio_NA_EU`:** The ground truth includes `+ 1e-6` in the denominator to prevent division by zero errors, which is a good practice. The generated code lacks this.
3.  **`Genre_Market_Share` Calculation:** The formula `(data.groupby('Genre')['Global_Sales'].transform('sum') / data['Global_Sales'].sum()) * 100` calculates market share as a percentage of *total global sales*. This is correct.
4.  **PCA Input:** The `numerical_features` variable often includes the 'Rank' column which is an identifier and should ideally be excluded from PCA, or at least discussed. Also, PCA requires scaled data, which the generated code does not perform, unlike the ground truth's use of `StandardScaler`.
5.  **Polynomial Features `pd.concat`:** The `poly_df` created has a default index, while `data` might have a modified index due to `dropna`. This can lead to misaligned concatenation. The ground truth ensures proper index alignment.
6.  **Interaction Features:** The task asked for interaction features *between Year and Genre*, implying numerical interaction (e.g., Year * one-hot encoded Genre). The generated code instead creates a string concatenation (`Year_Genre_Interaction`) and then one-hot encodes *that string*. This is not a numerical interaction feature as typically understood in feature engineering for models. It creates a dummy variable for each unique year-genre combination, which is different from a `Year * Genre_Dummy` interaction.
7.  **Outlier Handling:** The generated code identifies and removes *entire rows* where *any* numerical feature has an outlier using `data = data[~((data[numerical_features] < (Q1 - 1.5 * IQR)) | (data[numerical_features] > (Q3 + 1.5 * IQR))).any(axis=1)]`. This is an aggressive approach that can drastically reduce the dataset size, especially with multiple columns having outliers. The task generally implies capping or transformation, and the ground truth uses capping with `np.where`, which is a much commoner and less destructive method. Furthermore, the `numerical_features` fed into outlier detection will include the newly created polynomial features, which exacerbates the `dropna` issue.

**Completeness & Code Quality:**
1.  **Imports:** All necessary imports are present.
2.  **Structure:** The code is functional but lacks comments and print statements to explain steps and show intermediate results, which were implied by the ""expert code evaluator"" context and are present in the ground truth for clarity.
3.  **Missing Stages:** The generated code does not explicitly show the explained variance plot for PCA, which was requested.
4.  **Error Handling:** The `try-except FileNotFoundError` for loading the CSV is missing.
5.  **Warnings Suppression:** While not strictly mandatory, suppressing warnings (as done in the ground truth) can make the output cleaner.
6.  **Scalability:** Lack of `StandardScaler` before PCA makes the PCA results sub-optimal if feature scales vary widely.

Overall, the generated code demonstrates a basic understanding of the requested operations but lacks the robustness and correctness required for an ""expert"" level, particularly regarding missing values, specific feature engineering interpretations, and outlier handling.",llama-4-maverick,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
def load_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load data: {e}"")

# Clean the dataset by replacing zeros with NaN and then median imputation
def clean_data(data):
    data.replace(0, np.nan, inplace=True)
    for col in data.columns:
        if data[col].dtype in ['int64', 'float64']:
            data[col].f...",6,"The generated code correctly implements most of the requested statistical tests and structures them within functions, which is good for modularity. However, it falls short in several areas as detailed below:

**Areas for Improvement:**

1.  **Data Cleaning - Inaccurate Zero Value Replacement:** The `clean_data` function replaces *all* zeros with `np.nan` in the entire DataFrame, including 'Pregnancies' and 'Outcome'. This is incorrect. The prompt specifically states: ""clean zero values by replacing them with NaN and then median imputation"". For columns like 'Pregnancies', a value of 0 is legitimate. This is a critical error in data preparation that impacts subsequent statistical analyses. The ground truth correctly identifies `cols_to_impute`.

2.  **Imputation Logic:** The `clean_data` function's loop for median imputation `for col in data.columns:` will attempt to impute *all* columns, including 'Outcome' and 'Pregnancies', which might accidentally impute legitimate zeros (if they were kept) or other non-numeric columns. The ground truth correctly applies imputation only to the identified `cols_to_impute`.

3.  **T-test `equal_var` Parameter:** The `t_test_glucose` function uses the default `equal_var=True` for `stats.ttest_ind`. While not explicitly stated in the prompt, in real-world scenarios, it's safer to use `equal_var=False` (Welch's t-test) unless equality of variances is confirmed, as done in the ground truth. This is a less critical but important detail for robust statistical analysis.

4.  **Age Grouping for Chi-square:** For the chi-square test, the generated code uses `bins=[0, 30, 50, np.inf]`. The prompt specified ""young: <30, middle: 30-50, older: >50"". The generated code's upper bound for the first bin is `30`, meaning it includes 30-year-olds in the 'Young' category instead of 'Middle'. The ground truth uses `[0, 29, 50, np.inf]` with `right=True`, which correctly assigns ages like 29 to 'Young' and 30 to 'Middle'. This is a minor but incorrect interpretation of the age groups.

5.  **BMI Category Grouping for ANOVA:** Similar to age grouping, the `BMICategory` bins are `[0, 18.5, 25, 30, np.inf]` with default behavior (exclusive right boundary). The prompt implies inclusive ranges for categories (e.g., ""Normal: 18.5-24.9""). The ground truth uses `right=False` for inclusive lower boundary, which is more aligned with standard BMI categories.

6.  **Interpretation and Output Detail:** While the code provides basic ""Significant"" or ""Not Significant"" output, the prompt asked to ""Display p-values and interpret results with significance level α=0.05"". The ground truth offers much more detailed interpretation, including what the effect means (e.g., ""diabetic patients have significantly higher glucose levels,"" ""older patients tend to have more pregnancies""). This detailed interpretation is crucial for a complete solution.

7.  **Robustness (ANOVA groups):** The ground truth handles the case where ANOVA groups might be empty, making the code more robust. The generated code directly calls `stats.f_oneway(*glucose_by_bmi)` without checking if `glucose_by_bmi` contains empty arrays, which could lead to errors if a category happens to have no data.

8.  **Missing `ALPHA` Constant:** The prompt mentioned ""significance level α=0.05"", and the ground truth explicitly defines `ALPHA = 0.05`. The generated code hardcodes `0.05` multiple times, which is less maintainable.

Overall, the code provides a functional skeleton but misses crucial nuances in data handling, statistical test parameter choices, and detailed output interpretation, leading to a score of 6.",llama-4-maverick,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import Rando...",6,"The generated code addresses most of the requirements but has several issues that affect its correctness and completeness, particularly in the feature engineering and evaluation sections.

**Correctness:**
*   **Missing Value Handling:** The code replaces all zeros with NaN and then fills all NaNs with the median of *all* columns (`df.fillna(df.median(), inplace=True)`). This is incorrect. Only specific columns (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`) should have 0s treated as missing, and their medians should be calculated *before* handling other columns. This global `fillna` might inadvertently alter columns like 'Pregnancies' or 'Age' if they had 0s, where 0 is a valid value. The ground truth correctly identifies and handles `cols_with_zeros_as_missing`.
*   **Feature Engineering - Age_group:** The binning for `Age_group` (`[0, 30, 50, np.inf]`) is much coarser than the ground truth (`[0, 20, 30, 40, 50, 60, np.inf]`). While it creates a categorical feature, it doesn't match the detail implied by the task description which usually suggests more granular grouping.
*   **Hyperparameter Tuning:**
    *   The selection of `best_model_name` for tuning (`max(models, key=...)`) is problematic. It tunes *only one* model based on its cross-validation accuracy, rather than tuning a *specific predetermined model* (like Random Forest for robustness/generalizability as in most benchmarks). The task explicitly asks to ""Perform hyperparameter tuning for the **best model** using GridSearchCV,"" which usually implies the model identified as best *after initial cross-validation comparison* or a robust choice like Random Forest. The current approach will tune the ""best"" out-of-the-box model, which might not be the most complex or the one that benefits most from tuning.
    *   The `param_grid` is conditionally defined, which is okay for demonstration, but it makes the tuning process very rigid and only applies to the *first* ""best"" model identified, not necessarily to a model chosen for comprehensive tuning.
*   **Feature Importance Display:** The `feature_importances_` attribute check and subsequent display are done *inside* the main evaluation loop. `pipeline.named_steps['preprocessor'].get_feature_names_out()` after fitting the pipeline will give the names of the scaled numerical features and the one-hot encoded categorical features, but the ground truth handles this more robustly by getting `all_feature_names` after initial preprocessing. The order of features might be inconsistent if the pipeline is refitted multiple times. SVC does not have feature importances, so the `hasattr` check is needed, but the generalized `param_grid` makes the code less focused on the *specific* random forest feature importance requested.
*   **Visualization:** The code completely omits all visualization (confusion matrices, ROC curves, feature importance plots). The task explicitly requires creating these.

**Code Quality:**
*   **Imports:** Mostly correct.
*   **Structure:** The code is generally sequential, but the model evaluation loop and hyperparameter tuning could be more clearly separated, especially if visualizations were included.
*   **Readability:** Variable names are good.
*   **Lack of Suppressed Warnings/Reproducibility:** No `warnings` module or `np.random.seed` used.
*   **No Pipelines for Cross-Validation:** The ground truth wraps preprocessing and models into a `Pipeline` directly for `cross_val_score`, which is a cleaner and more robust way to perform CV on the entire process. The generated code does this for the final `GridSearchCV` and evaluation, but not consistently during the initial `cross_val_score` calls, which means preprocessing is done outside the pipeline for those initial CVs. *Correction: The generated code *does* use a pipeline for cross_val_score, but the initial missing value handling is done upfront, not within the pipeline, which is fine.*

**Completeness:**
*   **Missing Value Handling (as discussed):** Incorrectly implemented for the specific zero replacement.
*   **Feature Engineering:** The `Age_group` binning is less detailed than expected.
*   **Cross-Validation Metrics:** The generated code only prints accuracy from `cross_val_score`. The task specifies using cross-validation to evaluate using *multiple* metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   **Comprehensive Evaluation:** Although the final evaluation loop prints accuracy, precision, recall, F1, and ROC-AUC, it does so for each *un-tuned* model after re-training them. The task asks to evaluate *all models* using these metrics and to specifically evaluate the *best model* after hyperparameter tuning. The generated code effectively re-evaluates the base models and then separately presents tuning results for a ""best"" model.
*   **Visualization:** This is the most significant omission. The task explicitly asks for confusion matrices and ROC curves for visualization, and feature importance plots. None of these are present.

To improve, the code needs to:
1.  **Correctly handle the zero-replacement for missing values** in specific columns only, not globally.
2.  **Implement visualization components** (confusion matrices, ROC curves, feature importance plots). This is a major gap.
3.  Ensure **cross-validation uses all requested metrics**, not just accuracy.
4.  Clarify the **hyperparameter tuning strategy** to either tune a pre-selected model (like RF) or ensure that the ""best model"" selection and subsequent tuning are clearly articulated as per the task's general intent.
5.  Refine **feature importance display** to make it clearer and potentially plot it as requested implicitly by the task.",llama-4-maverick,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('diabetes.csv')

# Handle missi...",7,"The generated code addresses most of the task requirements but has several areas for improvement in terms of correctness, robustness, and execution flow.

**Strengths:**
*   **Comprehensive Coverage:** Attempts to cover all listed steps: missing value handling, feature engineering, polynomial features, scaling, PCA, SelectKBest, RFE, correlation filtering, SMOTE, and feature importance.
*   **Correct Libraries:** Uses appropriate libraries for each task.
*   **Visualization:** Includes a basic visualization for feature importance.

**Areas for Improvement:**

1.  **Missing Value Handling:**
    *   **Incorrect Assumption:** The original `df.replace(0, np.nan)` replaces 0 in *all* columns, including 'Pregnancies' and 'Outcome', where 0 can be legitimate. The ground truth correctly identifies and handles columns where 0 is truly a missing value.
    *   **Mixed Imputation:** Uses `mean` for 'Glucose', 'BloodPressure', 'BMI' and `median` for 'SkinThickness', 'Insulin'. While not inherently wrong, the task implies a consistent approach, and the ground truth uses median for all, which is generally more robust to outliers.
    *   **Pre-splitting:** Missing value imputation should ideally happen *before* splitting `X` and `y` for consistency, which the generated code does, but the specific columns need to be handled carefully.

2.  **Engineered Features:**
    *   **Metabolic_Score:** The generated code divides by 3, making it an average. The prompt said ""combination,"" and the ground truth uses a sum. While a minor difference, the sum is a more direct ""combination"" for a score.
    *   **Risk_Factor_Count:** The ground truth includes `Age > 50` but not `Insulin > 200`. The generated code includes `Insulin > 200` but not `Age > 50`. The ground truth's choices are more aligned with typical high-risk factors for diabetes.

3.  **Polynomial Features:**
    *   **`include_bias=False`:** The ground truth correctly uses `include_bias=False` to avoid creating a constant term (all ones) that can interfere with some models or feature selections. The generated code does not.
    *   **Column Selection:** The generated code applies polynomial features only to `['Glucose', 'BMI', 'Age']`, while the ground truth applies them to a broader set of original numerical columns. The prompt says ""selected numerical columns,"" so the choice is acceptable, but the ground truth's approach is more comprehensive.
    *   **Overlapping Features:** The generated code `pd.concat([X, X_poly_df], axis=1)` without dropping the original columns used for polynomial features will result in redundant features (e.g., 'Glucose' will exist twice, once as an original and once as a poly feature `x0`). The ground truth correctly handles this by dropping the original columns from `X` before concatenation.

4.  **Feature Scaling:**
    *   **Redundant Scaling:** The code performs both `StandardScaler` and `MinMaxScaler` but then primarily uses `X_scaled_std` for subsequent steps without explicitly stating which one to use throughout the task. The prompt asks to apply both, which it does, but then it's unclear how the two scaled datasets are used or if one is preferred. Ground truth clearly states it will use StandardScaler output.
    *   **Column Names Lost:** When `scaler.fit_transform(X)` is called, the output `X_scaled_std` is a numpy array, but subsequent steps expect a DataFrame with column names (e.g., for `corr()`, `SelectKBest`, `RFE`, `RandomForestClassifier`). While `X_scaled_std_df` is created, it's not consistently used (e.g., PCA is applied to `X_scaled_std` (numpy array), and SelectKBest/RFE on `X_scaled_std` (numpy array)). This loss of column names makes interpretation harder and directly impacts tasks like printing selected feature names. The ground truth consistently works with DataFrames and preserves column names.

5.  **Execution Flow and Data Flow:**
    *   **Correlation Filtering Location:** The correlation-based feature filtering is placed after PCA, SelectKBest, and RFE in the ground truth, but before SMOTE and Feature Importance. In the generated code, it's placed after PCA, SelectKBest, and RFE operations, but the data used for those *earlier* steps is `X_scaled_std` (which is not correlation-filtered). The correct flow would be to apply correlation filtering *before* PCA, SelectKBest, etc., if the intent is to reduce dimensionality before those steps. The ground truth places it earlier and explicitly states it applies PCA/SelectKBest/RFE on the *correlation-filtered data*. This is a significant logical flaw.
    *   **Feature Importance Visualization:** The visualization label `x=top_features` will likely cause issues if `top_features` is not formatted correctly for `sns.barplot` (it expects column names, not the data itself). The ground truth creates a DataFrame of importances for proper plotting.
    *   **Printed Output:** The generated code has very limited print statements, making it hard to follow the progress and intermediate results as requested by a benchmark. The ground truth provides helpful print statements for each step.

6.  **Robustness and Error Handling:**
    *   No `try-except` block for dataset loading, which the ground truth includes.

These issues, particularly the incorrect missing value handling, overlapping features, and flawed execution flow regarding feature filtering and subsequent steps, lead to deductions. The code runs and produces output, but some of the intermediate results and the final selected/transformed features might not be entirely correct or optimally produced based on the task description and best practices.",llama-4-maverick,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, f_oneway, pearsonr, chi2_contingency

# Load the dataset and handle missing values
def load_data(file_name):
    data = pd.read_csv(file_name)
    data.replace([np.inf, -np.inf], np.nan, inplace=True)
    data.fillna(data.mean(numeric_only=True), inplace=True)
    return data

# Main analysis function
def analyze_data(data):
    # Define regions and categories
    western_europe = ['Western Europe']...",5,"The generated code attempts to address all parts of the task but suffers from several critical issues that impact correctness and completeness.

**Critical Issues:**

1.  **Incorrect Missing Value Handling:** The `load_data` function fills missing values with the mean of *all* numeric columns (`data.fillna(data.mean(numeric_only=True), inplace=True)`). The task specifically implies handling missing values relevant to the *analysis columns*. More importantly, the ground truth drops rows with NaNs in critical columns, which is a more appropriate strategy for statistical tests where complete cases are often required. Imputing with the mean can introduce bias and distort statistical relationships, especially if the missingness is not random or if the feature is not suitable for mean imputation. The prompt did not specify the method, but `dropna` for complete cases for statistical tests is generally safer.

2.  **Incorrect Regional Grouping for Test 1 & 5:** The `analyze_data` function modifies the `Regional indicator` column drastically early on: `data['Regional indicator'] = data['Regional indicator'].apply(lambda x: 'Western Europe' if x in western_europe else 'Other')`. This operation reduces all non-Western European regions into a single 'Other' category *permanently* for the DataFrame passed into the function.
    *   **Test 1 (Western Europe vs. Other):** While this transformation works for the *first* t-test, it's problematic for the subsequent Chi-square test (Test 5), where the ground truth expects the *original* detailed regional indicators for association testing.
    *   **Test 3 (ANOVA across all regional indicators):** This is where the error is most pronounced. The generated code reloads the `world-happiness-report-2021.csv` *again* (`original_data = pd.read_csv('world-happiness-report-2021.csv')`) inside the `analyze_data` function specifically for the ANOVA. This is inefficient and shows a lack of proper data management within the single function call. It also re-does the mean imputation, which is generally bad practice.

3.  **Missing Interpretation:** The prompt explicitly asks to ""Display p-values and interpret results with significance level α=0.05."" The generated code only prints ""Significant: True/False"" but completely lacks the required interpretive text. This is a major omission for the ""Completeness"" criterion.

4.  **Inconsistent `alpha` Usage:** The significance level `α=0.05` is hardcoded into each print statement (e.g., `p_val < 0.05`) instead of being defined as a variable at the top, which is a best practice for consistency and easy modification.

5.  **Mann-Whitney U Test Groups:** For the Mann-Whitney U test, the generated code uses `nlargest()` and `nsmallest()` functions to get the top/bottom 25%. This is functionally correct but does not address potential scenarios where the number of countries might not be perfectly divisible, leading to slight inaccuracies if `int(len(data)*0.25)` truncates differently than intended for the specific dataset size. The ground truth's approach using sorting and slicing (`tail`, `head`) is also correct and perhaps slightly more robust if `num_25_percent` is calculated in a way that respects boundary conditions.

6.  **Redundant Data Reloading:** As noted above, the ANOVA step unncessarily reloads and re-processes the entire dataset.

7.  **No Error Handling for File:** The `load_data` function lacks the `try-except FileNotFoundError` block, making it less robust if the file isn't present.

**Areas for Improvement (if the critical issues were fixed):**

*   **Clarity of Output:** While not explicitly requested, the ground truth's structured output with separators (`--- Test X: ... ---`) and clear headers for each test greatly improves readability.
*   **Variable Naming:** Some variable names could be more descriptive (e.g., `data` instead of `df`).
*   **Docstrings/Comments:** More comprehensive comments explaining the rationale behind certain choices (e.g., why a particular test was chosen, what ""significant"" means in context) would enhance code understanding.",llama-4-maverick,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_c...",6,"The generated code attempts to address all requirements of the prompt but has several key issues that reduce its correctness and completeness.

**Correctness Issues:**
1.  **Preprocessing (Categorical Encoding):** The ground truth correctly identifies that the specified features (`Logged GDP per capita`, `Social support`, etc.) are all numerical, and thus no categorical encoding is needed for `X`. The generated code incorrectly applies `LabelEncoder` to *all* columns (`df.columns`) if they are of 'object' type. This is problematic:
    *   It will encode 'Country name' and 'Regional indicator', which are not part of the specified features `X`.
    *   More critically, `LabelEncoder` is usually inappropriate for nominal categorical variables, as it introduces an arbitrary ordinal relationship (e.g., 'Country A' < 'Country B' due to alphabetical order). One-hot encoding is generally preferred.
    *   If `LabelEncoder` is applied to columns *before* feature selection, any subsequent operation like scaling might be applied to these encoded column data, which would be incorrect if they are not features.
    *   The prompt specifically asks for handling categorical variables *if applicable* to the features. In this case, none of the target features are categorical.
2.  **Missing Value Handling:** The generated code uses `df.dropna()` without specifying a `subset`. This will drop rows if *any* column has a missing value, which could be overly aggressive and remove data points that are valid for the selected features. The ground truth correctly uses `dropna(subset=FEATURE_COLUMNS + [TARGET_COLUMN])`.
3.  **Hyperparameter Tuning:**
    *   The `best_model_name` selection for tuning is based on `min(results, key=lambda x: results[x]['RMSE'])`. This is correct for selecting the best model based on an error metric. However, the subsequent `if/elif` structure for `param_grid` only defines grids for `Ridge`, `Lasso`, `Random Forest`, and `Gradient Boosting`. If, for example, `Linear Regression` happened to be the *best* (unlikely for RMSE, but possible), it would fall into the `else` block resulting in an empty `param_grid`, which would likely cause an error or at best do nothing. Even if it didn't error, Linear Regression doesn't typically have hyperparameters to tune this way. The ground truth wisely *chooses* a GBT for tuning, assuming it's usually the best or a strong candidate.
    *   The `param_grid` ranges are very limited for thorough tuning (`[100, 200, 300]` for `n_estimators`, `[None, 5, 10]` for `max_depth`). The ground truth has a more comprehensive grid for GBR.
4.  **Cross-Validation:** The cross-validation for evaluation uses `X_train` and `y_train` (`cross_val_score(model, X_train, y_train, cv=5...)`). While this is a common practice to validate the model's performance on the training data, the prompt implies ""assess model stability"" which is usually done on the *entire dataset* (or at least the full `X`, `y` before the final `test` split) to see if the chosen model generalizes well under different folds. The ground truth uses `X` and `y` (the full cleaned dataset).
5.  **Feature Importance Visualization:** The generated code's feature importance plotting iterates through `models.items()` and plots importance for *all* models, including those that might not have `feature_importances_` (e.g., Linear Regression, Ridge, Lasso), which would raise an `AttributeError`. While it has a `hasattr` check in the residual plotting `if` block, it doesn't have it for the feature importance block, which is problematic. The GT correctly uses `hasattr` and only plots for the best tuned model which is GBR. Additionally, `sns.barplot(x=feature_names, y=feature_importances)` would require `feature_names` to be defined for each `model` in the loop if `X` wasn't scaled, or explicitly pulled from original `X` column names, which the GT handles (`X_train.columns`).

**Completeness Issues:**
1.  **Residual Plotting:** The residual plots lump all models together in a loop. It's usually more informative to plot the residuals for the *best* performing single model (or the tuned model), as done in the ground truth.
2.  **Hyperparameter Tuning Best Model Identification:** The generated code applies tuning, but then just updates `results[best_model_name + ' (Tuned)']`. It doesn't explicitly print the best hyperparameters found or the cross-validation score from `GridSearchCV`'s `best_score_`, which are important outputs of tuning.
3.  **Analysis of Hypothetical Predictions:** The prompt asks for an ""analysis of results"" for the hypothetical countries. The generated code just prints the predictions. The ground truth provides a brief interpretative analysis.
4.  **Imports:** While most imports are there, `KFold` is missing from `sklearn.model_selection`, which is used in the ground truth's CV for more robust splitting, although `cross_val_score` can take an integer `cv` directly.

**Code Quality (Minor):**
1.  Random states are not consistently set for all models (Ridge, Lasso, RandomForestRegressor, GradientBoostingRegressor) which can lead to reproducibility issues across runs. The ground truth sets it for all relevant models.
2.  The output formatting for results is somewhat clunky compared to the GT's use of a DataFrame.

In summary, the generated code has functional components but suffers from incorrect preprocessing choices, limited hyperparameter tuning scope, potential runtime errors in visualization, and less robust cross-validation and output presentation compared to the highly optimized and robust ground truth.",llama-4-maverick,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# Load and preprocess the data...",7,"The generated code largely addresses the requirements of the prompt, demonstrating a good understanding of various feature engineering, dimensionality reduction, and feature selection techniques. However, there are several areas for improvement in terms of correctness, completeness, and code quality.

**Strengths:**
*   Correctly implements the loading and basic preprocessing of the dataset.
*   Successfully engineers the requested new features ('Happiness_Efficiency', 'Social_Wellness_Index', 'Governance_Score', 'Regional_Happiness_Rank', 'GDP_vs_Regional_Average').
*   Applies polynomial features and creates interaction features.
*   Demonstrates the use of multiple scaling methods (StandardScaler, MinMaxScaler, RobustScaler).
*   Applies PCA and t-SNE for dimensionality reduction and includes basic visualization.
*   Utilizes `SelectKBest` (with `f_regression`) and `RFE` for feature selection.
*   Performs mutual information scoring and visualizes Random Forest feature importances.

**Areas for Improvement (leading to a score of 7):**

1.  **Incomplete/Incorrect Feature Engineering and Data Handling:**
    *   `df.dropna()` is too aggressive. The ground truth specifically handles `Perceptions of corruption` NaNs with mean imputation, which is more robust. `dropna()` simply drops rows, which might lose valuable data.
    *   `Social_Wellness_Index` and `Governance_Score` are averaged by dividing by 2 in the generated code. The prompt and ground truth imply a simple summation or combination without normalization unless explicitly stated. This changes the meaning of the combined score.
    *   The `Healthy life expectancy` column name: The generated code uses `'Healthy life expectancy at birth'` for `Social_Wellness_Index` and `key_factors`, while the dataset and ground truth use `'Healthy life expectancy'`. This causes a `KeyError` if the column isn't correctly named.
    *   Polynomial features: After `pd.concat`, the original columns are duplicated if not dropped from `df` before concatenation, leading to a larger number of features for subsequent steps, which isn't ideal. The ground truth handles this by applying polynomial features to a subset and then implicitly concatenating.
    *   The interaction features are added *after* `X_train` and `X_test` are created, meaning `X` (and implicitly `X_train`) does not contain these features when other steps like scaling or dimensionality reduction are performed. This is a significant logical error.

2.  **Repetitive and Inefficient Scaling/Dimensionality Reduction Loop:**
    *   The scaling, PCA, and t-SNE steps are placed inside a loop iterating through `scalers`. This means PCA and t-SNE are performed and visualized *three separate times*, which is redundant and not typical practice for comparison. Usually, you pick *one* scaler (e.g., StandardScaler for PCA) and then proceed. The prompt asked to apply the scaling methods and then apply PCA/t-SNE, implying the use of one of the scaled outputs as input to the next steps. The ground truth correctly uses `StandardScaler` output for subsequent steps.

3.  **Visualization Issues:**
    *   PCA and t-SNE visualizations lack color encoding (`hue=y` as in ground truth), making them less informative for understanding how the target variable relates to the reduced dimensions.
    *   Feature importance visualization `sns.barplot(x=X_train.columns, y=feature_importances)` will result in a `ValueError` because `X_train.columns` is an index, not a list of values matching the `y` (importance scores) array length. This needs to be structured into a DataFrame as done in the ground truth.

4.  **Feature Selection Details:**
    *   Correlation-based feature filtering: The generated code calculates `corr_matrix = X_train.corrwith(y_train)`. This only shows the correlation of each feature *with the target variable*, not the correlation *between features* (which is what's used for multicollinearity reduction, as done in the ground truth). The prompt asked for ""correlation-based feature filtering,"" which usually implies removing highly correlated *features among themselves*.
    *   Mutual information scoring: It's calculated but not explicitly used for filtering or ranking beyond printing. The ground truth provides a sorted DataFrame for better utility and comparison.

5.  **Code Structure and Best Practices:**
    *   The `PolynomialFeatures` creates columns with a `bias` term first (e.g., `1`), which is unnecessary and `include_bias=False` should be used.
    *   `train_test_split` is imported but then largely unused for the bulk of the operations (scaling, dimensionality reduction, most feature selection, and *all* feature importance evaluation are done on `X_train`). This makes it less a ""validation"" and more just a split. The ground truth performs validation on the full data.
    *   Error handling for file loading is missing (e.g., `try-except FileNotFoundError`).
    *   Warnings are not suppressed, which can lead to cluttered output.

To achieve a higher score, the generated code needs to rectify the data handling issues, refine the plotting methods, ensure correct column usage, and implement the feature selection steps more precisely according to their typical use cases (e.g., inter-feature correlation filtering, not just feature-target correlation).",llama-4-maverick,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

def load_and_filter_data(file_path):
    data = pd.read_csv(file_path)
    data['date'] = pd.to_datetime(data['date'])
    usa_data = data[data['location'] == 'USA']
    usa_data.set_index('date', inplace=True)
    return usa_data

def resample_data(data, column):
    wee...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a fully correct and complete solution.

**Issues:**

1.  **Missing `period` in `seasonal_decompose`:** The decomposition function `decompose_time_series` by default uses `period=1`, which is often incorrect for actual time series data (like weekly COVID cases where a yearly seasonal period of 52 weeks would be more appropriate). The ground truth code correctly identifies this and even handles cases where data might be insufficient for a 52-week period. This is a critical omission for proper time series decomposition.
2.  **Incorrect Decomposition Plot:** The `plot_decomposition` function manually creates subplots and plots each component, which is redundant and less robust than using `decomposition.plot()` provided by `statsmodels.tsa.seasonal.seasonal_decompose`. The manual plotting also misses setting proper x-axis labels (dates) for the decomposed components.
3.  **ADF Test Output:** The ADF test output only prints the statistic and p-value, while the ground truth also prints critical values, which are important for a complete interpretation of the test results.
4.  **Error Handling/Robustness:** The generated code lacks any error handling for file not found, empty data after filtering, or insufficient data points for decomposition/ADF tests/ACF/PACF plots. This makes the code less robust to real-world data issues. For instance, if `weekly_new_cases` is empty after resampling, many subsequent operations will fail.
5.  **Implicit `period` in ACF/PACF:** While not explicitly required, specifying `lags` for ACF/PACF plots can be beneficial, especially for seasonal data. The generated code uses default lags, which might not be optimal for weekly data where a 52-week seasonality is expected.

**Strong Points:**

*   **Modular Design:** The code is well-structured into functions for different steps (loading, resampling, decomposition, plotting, stationarity testing, differencing, ACF/PACF). This improves readability and maintainability.
*   **Correct Core Logic:** The fundamental steps for filtration, resampling (`W`), ADF test, and differencing are implemented correctly.
*   **Prompt Differencing:** The code correctly checks the p-value for stationarity and applies differencing if needed, then re-applies the stationarity check and plots ACF/PACF on the potentially differenced series.",llama-4-maverick,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import numpy as np

def main():
    # Load dataset
    data = pd.read_csv('winequality-red.csv')

    # Categorize wines based on 'pH' values
    data['pH_group'] = pd.cut(data['pH'], 
                              bins=[data['pH'].min(), 3.2, 3.4, data['pH'].max()], 
                              labels=['Low pH', 'Medium pH', 'High pH'], 
                              include_lowest=True, right=False)

    # ...",7,"The generated code correctly addresses most of the requirements but has some areas for improvement to achieve a higher score.

**Correctness:**
- **pH Grouping:** The `pd.cut` function uses `data['pH'].min()` and `data['pH'].max()` as bin boundaries. While this might work for the provided dataset, it's not robust for general cases where the min/max pH might fall exactly on 3.2 or 3.4. It's better to use fixed, explicit boundaries like `[0, 3.2, 3.4, np.inf]` as in the ground truth for clarity and robustness.
- **Confidence Interval Calculation:** The calculation of `ci_95` is problematic. `stats.t.ppf(0.975, df)` expects a single `df` (degrees of freedom) value, but it's given a list `[len(data[data['pH_group'] == group]) - 1 for group in avg_quality.index]`. This will cause `ppf` to return an array, but the element-wise multiplication with `sem_qualities` might not be what's intended for independent CIs for each group. The correct approach (as in the ground truth) is to calculate the CI for each group's mean separately using `stats.t.interval` or by multiplying the SEM by the T-score corresponding to that group's degrees of freedom. The current implementation essentially calculates the error margin, but `plt.errorbar` expects the total error amount which is not explicitly passed. `yerr=ci_95` will make the error bars symmetric, which is generally fine for large enough samples, but the calculation method is not ideal.
- **Reporting:** The `avg_quality` prints the mean quality for each group, which is good, but it doesn't print the confidence intervals, which was part of the reporting requirement.
- **Visualization:** The visualization uses `plt.errorbar`, which is a good choice, but it doesn't clearly show the average quality values as bars (as often done for categories), and the error bars should ideally be plotted from the actual average quality. The current plot style (`fmt='o-'`) plots individual points connected by lines, which is less common for categorical group comparisons than bar charts with error bars.

**Code Quality:**
- **Variable Names:** Variable names are generally clear.
- **Function Encapsulation:** Using `def main()` and `if __name__ == ""__main__"":` is good practice.
- **Clarity:** The code is relatively clear, but the CI calculation would benefit from explicit comments or a more standard calculation method.

**Completeness:**
- **Reporting CIs:** The task explicitly asked to ""report the average 'quality' for each pH group"" in a way that implies including CI, and the ground truth explicitly reported them. The generated code only reported the mean.
- **Visualization Type:** While it does create a visualization with error bars, a bar chart is a more common and intuitive choice for comparing means across discrete categories, and adding numerical labels to the bars would enhance readability, as done in the ground truth.
- **Labels in pH groups:** The labels ""Low pH"", ""Medium pH"", ""High pH"" are correct, but they lack the numerical ranges specified in the prompt (e.g., ""Low pH (< 3.2)""). This makes the output less informative.
- **Error Handling:** No explicit error handling for file not found (though `pd.read_csv` would raise an error).

To improve, the generated code should:
1. Refine the `pd.cut` bins to be explicitly defined and use `np.inf`.
2. Correctly calculate and explicitly report the 95% confidence intervals, ideally using `stats.t.interval` or calculating the margin of error for each group individually.
3. Consider using a bar chart for the visualization to better represent the group means and their CIs, and add numerical labels for the means.
4. Include the numerical ranges in the `pH_group` labels.",llama-4-maverick,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load data and parse 'date'
data = pd.read_csv(...",9,"The generated code correctly addresses almost all requirements of the task. It loads the data, selects features, handles missing values, encodes categorical features, scales numerical features, splits data, trains both Linear Regression and Random Forest models, and evaluates them with the specified metrics.

**Strong points:**
*   **Excellent use of `Pipeline` and `ColumnTransformer`:** This is a very robust and standard way to handle preprocessing steps in scikit-learn, preventing data leakage and making the code cleaner and more reproducible. The `Pipeline` correctly integrates imputation, scaling, and encoding within the model training flow.
*   **Correct handling of missing values with `SimpleImputer`:** Using `SimpleImputer(strategy='median')` for numerical features and handling categorical imputation.
*   **Correct feature selection and target definition.**
*   **Correct model instantiation and evaluation metrics.**
*   **Correct use of `random_state` for reproducibility.**

**Areas for improvement/minor issues:**
*   **Handling `y` imputation:** The ground truth code handles `y` (target) imputation as part of the initial data preprocessing on the `df_selected` DataFrame before splitting. The generated code also handles `y` imputation, but it does so separately using `y_imputer` *before* splitting the data into `X` and `y` train/test sets, which is acceptable. However, the task statement mentioned to handle missing values in ""selected features and target by imputing with the median"". The `ColumnTransformer` only handles features, meaning `y` needs separate handling. The current approach is valid.
*   **`drop_first=True` for OneHotEncoder:** The ground truth uses `drop_first=True` in `pd.get_dummies` which is a good practice to avoid multicollinearity in linear models. While `handle_unknown='ignore'` is good, `drop_first=True` (or equivalent in `OneHotEncoder`) would make the preprocessing more robust for `LinearRegression`. For `OneHotEncoder`, this is achieved by setting `drop='first'` not `drop_first=True`. This is a minor point and often not strictly necessary for tree-based models like Random Forest, but best practice for linear models.
*   **`handle_unknown='ignore'` for OneHotEncoder:** While good for deployment (new categories in test data), the problem statement doesn't explicitly mention unknown categories. However, it's a good default.

Overall, the code is very high quality and demonstrates a strong understanding of best practices in machine learning workflows using scikit-learn. The use of `Pipeline` and `ColumnTransformer` makes it more sophisticated and robust than directly manipulating DataFrames and scaling/imputing separately.",llama-4-maverick,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Load the dataset
def load_data(file_name):
    data = pd.read_csv(file_name)
    return data

# Handle missing values by median imputation
def handle_missing_values(data, columns):
    for column in columns:
        data[column] = data[column].fillna(data[column].median())
    return data

# Test if mean 'reproduction_rate' is significantly different across continents
def...",6,"The generated code attempts to address all parts of the task but has several inaccuracies and omissions which lead to incorrect results or less robust code.

**Correctness Issues:**

1.  **ANOVA - Missing Data Handling:** The ANOVA test uses `filtered_data[filtered_data['continent'] == continent]['reproduction_rate']` directly in the `f_oneway` call. If any of these groups are empty or contain NaNs (which can happen even after imputation if a continent doesn't appear in the `reproduction_rate` column or has nulls after filtering), this could cause issues or incorrect results. The ground truth explicitly uses `.dropna()` on the specific series before passing it to `f_oneway` and checks if a group is empty.
2.  **Pearson Correlation - Missing Data:** The `test_correlation_stringency_new_cases` function directly calls `stats.pearsonr(data['stringency_index'], data['new_cases'])`. While the initial imputation handles NaNs for these columns, it's good practice to ensure no NaNs are present in the specific columns for the correlation, as `pearsonr` will fail if there are. The ground truth includes a `.dropna()` on the relevant columns.
3.  **T-test - 'high_death_rate_country' derivation:**
    *   `latest_data = data.sort_values('date').groupby('country').tail(1)`: This is prone to error. `groupby('country').tail(1)` might not always give the *latest* date if the data isn't sorted globally first, or if there are multiple entries for the same country on the latest date and `tail(1)` just picks one arbitrarily. The robust way is `groupby('country').last()` after sorting by date, as done in the ground truth.
    *   `death_rate = latest_data['total_deaths'] / latest_data['population']`: This does not handle potential division by zero (`population` being 0) which would lead to `inf` values, or `NaN` if population is `NaN`. The ground truth explicitly handles `inf` and `NaN` values before calculating the percentile.
    *   `latest_data['high_death_rate_country'] = (death_rate >= threshold).astype(int)`: This creates the column only in `latest_data`. The subsequent merge `merged_data = pd.merge(data, latest_data[['country', 'high_death_rate_country']], on='country', how='left')` is problematic because `high_death_rate_country` is defined per *country* based on *latest data*, not per *row* in the original `data` dataframe. The t-test should operate on the `median_age` of *countries*, not individual data points after merging back to the full dataset, which might contain multiple entries per country. The task states ""based on whether a country's 'total_deaths' / 'population' is above the 75th percentile of this ratio across all countries (use latest entry per country)"", and then ""Test if the 'median_age' is significantly different between 'high_death_rate_country' and others."" This implies grouping countries into these two categories *first*, and then comparing `median_age` *per country*. The generated code uses `merged_data`, meaning `high_death_rate_country` is effectively assigned to every row for a given country, then `median_age` is pulled from all these rows. This is not what the task implies. The t-test should be on the `median_age` from `latest_data` (the ground truth correctly does this using `latest_country_data`).
    *   Missing `equal_var=False` for t-test: The generated code does use `equal_var=False` which is good practice for independent t-tests, assuming unequal variances (Welch's t-test).

**Code Quality and Completeness Issues:**

1.  **Imports:** `statsmodels.api as sm` and `statsmodels.formula.api as ols` are imported but not used. This suggests unnecessary imports. The task specified to include `statsmodels`, but since it wasn't used for the requested tests (ANOVA was done with `scipy.stats.f_oneway`), it's an unnecessary import for this specific solution.
2.  **Output Formatting:** The output is very minimal. It prints the null hypothesis and p-value, but doesn't print the actual F-statistic, correlation coefficient, or t-statistic, which are helpful for understanding the results. The alpha level (0.05) is hardcoded repeatedly instead of being defined as a constant, which is a good practice for maintainability.
3.  **Robustness:** The code lacks checks for sufficient data. For example, if there's only one or zero continents present after filtering for ANOVA, `f_oneway` will throw an error. Similarly, for the t-test, if one of the groups (`high_death_rate_median_age` or `others_median_age`) is empty, `ttest_ind` will raise an error. The ground truth includes such checks.
4.  **Clarity in Imputation:** While `df[column] = df[column].fillna(df[column].median())` works, the ground truth's `inplace=True` or a re-assignment within the loop is functionally equivalent. The ground truth also explicitly checks `if col in df.columns` and if there are any `isnull().any()`, which are good defensive programming practices.

Overall, the generated code outlines the correct statistical tests but has several implementation details wrong, especially in the data preparation steps for the t-test and the robustness checks for all tests.",llama-4-maverick,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    df = pd.read_csv('clash_royale_cards.csv')

    # Select numerical columns
    numerical_cols = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
    df_numerical = df[numerical_cols]

    # Calculate pairwise correlation matrix
    corr_matrix = df_numerical.corr()

    # Visualize correlation matrix using heatmap
    plt.figure(figsize=(10, 8))
    sn...",9,"The generated code correctly performs all the required steps: loading the CSV, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it as a heatmap with annotations and a diverging colormap. The use of a `main` function is good practice.

The only minor area for improvement compared to the ground truth is the basic `square=True` argument in `sns.heatmap` instead of the more detailed `fmt="".2f"", linewidths=.5` for better visual presentation of the annotations and cell separations. However, this is a minor aesthetic point and doesn't detract from the core functionality. The ground truth also included robust error handling, which the generated code lacks, but the prompt didn't explicitly ask for it.",llama-4-maverick,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

def main():
    try:
        # Load the dataset
        data = pd.read_csv('clash_royale_cards.csv')

        # Group the data by rarity and calculate averages
        averages = data.groupby('rarity')[['elixirCost', 'Win Rate', 'Usage']].mean().reset_index()

        # Display the calculated averages
        print(averages)

    except FileNotFoundError:
        print(""The 'clash_royale_cards.csv' file was not found in the current directory."")
    except Exception...",9,"The generated code correctly solves the task. It properly loads the CSV, groups by `rarity`, calculates the mean of `elixirCost`, `Win Rate`, and `Usage`, and displays the results. The inclusion of a `main` function and error handling (FileNotFoundError, general Exception) is good practice and enhances robustness, which is a strong point. The use of `.reset_index()` after `mean()` is a stylistic difference from the ground truth. While `reset_index()` makes 'rarity' a regular column rather than an index, the ground truth's output (rarity as index) is also perfectly acceptable and directly shows the grouped results. Both outputs are valid ways to display the information. For this specific task's output requirement (""Display these calculated average values for each rarity""), either is fine. The ground truth's output is slightly more concise in its presentation for this specific task.",llama-4-maverick,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Create a new binary target column
df['is_high_win_rate'] = (df['Win Rate'] > 55).astype(int)

# Select the required features and target
X = df[['elixirCost', 'Usage', 'rarity']]
y = df['is_high_win_rate']

# Apply one-hot encoding to the categorical 'rarity' column
encoder = OneHotEncoder(handle_unknow...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, creates the `is_high_win_rate` target column, selects the specified features, applies one-hot encoding to the `rarity` column, and splits the data into training and testing sets with the correct parameters. The use of `OneHotEncoder` is also a valid approach for one-hot encoding, even though `pd.get_dummies` might be simpler for this specific case.

One minor area for improvement, specific to `OneHotEncoder`, is handling the index resetting when concatenating `X` with the encoded features. If the original `X` DataFrame had a non-default index (e.g., from a subset of a larger DataFrame), `pd.concat` without `reset_index(drop=True)` before or after concatenation might lead to misaligned rows. In this specific scenario, as `X` is directly a slice from `df`, the indices are likely aligned, but it's a good practice to be aware of when using `OneHotEncoder` with `pd.concat`. The ground truth's use of `pd.get_dummies` handles this more seamlessly as it operates directly on the DataFrame. However, given the task did not specify complex indexing, this is a minor theoretical point. Another negligible point for improvement is adding a `try-except` block for `FileNotFoundError` as seen in the Ground Truth for robustness. Overall, the code is very good and functional.",llama-4-maverick,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

def main():
    # Load the dataset
    data = pd.read_csv('iris.csv')

    # Define features X and target y
    X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
    y = data['Species']

    # Encode the categorical target variable into numerical l...",9,"The generated code is very good and correctly addresses almost all requirements of the task.

**Strong points:**
- All necessary imports are present.
- Data loading is correct.
- Features `X` and target `y` are correctly defined as specified.
- The categorical `Species` target variable is correctly encoded using `LabelEncoder`.
- Data splitting uses the specified `test_size=0.3` and `random_state=42`.
- A KNN classifier is initialized with `n_neighbors=3` and trained correctly.
- Predictions are made, and accuracy is reported.
- The code is encapsulated in a `main` function and executed with `if __name__ == ""__main__"":`, which is good practice.

**Minor Improvements:**
- The ground truth code includes `stratify=y_encoded` in `train_test_split`. While not explicitly stated in the prompt, it's a best practice for classification tasks with potentially imbalanced classes to ensure an even distribution of target classes in training and testing sets. Adding this would make the solution more robust.
- The prompt mentioned including error handling for the CSV file `try-except`, which the generated code does not have. This is a minor omission but was present in the ground truth.",llama-4-maverick,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
def load_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load data: {str(e)}"")

# Perform independent two-sample t-test
def perform_t_test(data):
    # Filter data for the two species
    versicolor = data[data['Species'] == 'Iris-versicolor']['PetalLengthCm']
    virginica = data[data['Species'] == 'Iris-virginica']['PetalLe...",7,"The generated code correctly performs the t-test and interprets the results, addressing most of the task requirements. It demonstrates good code organization by using functions for loading data and performing the test.

However, there are a few areas for improvement to reach a higher score:

1.  **Hypotheses Statement Clarity:** While the null and alternative hypotheses are printed, they are less formal and explicit than the ground truth. The ground truth clearly states the hypotheses using statistical notation (e.g., μ_versicolor = μ_virginica) and provides a more detailed explanation of what 'statistically significant difference' implies in context. The generated code's statements are a bit generic.

2.  **Missing Specific Conclusion:** The interpretation part only states ""Reject the null hypothesis"" or ""Fail to reject the null hypothesis."" It lacks the follow-up conclusion comparing the mean PetalLengthCm between the species, as present in the ground truth (e.g., ""There is a statistically significant difference...""). This is a crucial part of interpreting statistical tests.

3.  **Error Handling:** The `load_data` function has a generic `except Exception as e:` which is less specific than `FileNotFoundError`. While it catches errors, the ground truth's `exit()` after an error is also a more decisive way to prevent further execution with missing data.

4.  **Printing Format:** The ground truth uses f-strings with formatting (e.g., `:.4f`) for t-statistic and p-value, which makes the output cleaner and more professional. The generated code prints raw float values.

5.  **Direct Imports:** Importing `ttest_ind` directly from `scipy.stats` is fine, but the ground truth's `from scipy import stats` and then `stats.ttest_ind` might be considered slightly more conventional when using multiple functions from a sub-module. This is a minor stylistic point.

Overall, it's a functional solution but could benefit from more detailed output and stricter adherence to hypothesis statement conventions.",llama-4-maverick,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    # Load the dataset
    iris_df = pd.read_csv('iris.csv')

    # Define numerical features and species column
    numerical_features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
    species_column = 'Species'

    # Create a figure with multiple subplots
    fig, axes = plt.subplots(nrows=len(numerical_features), figsize=(8, 6*len(numerical_features)))

    # Iterate over n...",9,"The generated code correctly loads the dataset and generates box plots for each numerical feature, grouped by species, with appropriate titles. This satisfies all the core requirements of the task. The use of `subplots` to create a single figure with multiple plots is an efficient and clean approach.

**Strong points:**
- **Correctness:** The code correctly performs all requested operations.
- **Efficiency:** Using `plt.subplots` and iterating through `axes` is a good way to organize and display multiple plots.
- **Readability:** The code is well-structured and easy to understand.
- **Proper Imports:** All necessary libraries are imported.

**Areas for minor improvement:**
- **Error Handling:** The ground truth code includes `try-except` blocks for `FileNotFoundError` and general exceptions when loading the CSV. While not explicitly asked for, robust code often includes such error handling. The generated code assumes the file will always be present.
- **Plotting Individual Figures vs. Subplots:** The prompt implicitly asks for ""a set of 4 box plots"" and ""Each box plot should have a title."" The ground truth produces 4 *separate* figures, each with its own title. The generated code produces *one* figure with 4 subplots. While both are valid interpretations of ""a set of 4 box plots,"" the ground truth's approach might be slightly closer to the literal interpretation if the intent was standalone plots. However, `subplots` is often preferred for presentation. This is a minor nuance.
- **Clarity of Axis Labels:** The ground truth code explicitly sets `xlabel` and `ylabel` for each plot. While `sns.boxplot` sometimes infers these, explicitly setting them ensures clarity. This was implicitly handled by Seaborn for the y-axis, and the x-axis is correctly labeled by default due to `Species`.

Overall, this is a very strong submission that fulfills the task requirements effectively. The points for improvement are minor and relate more to robustness and explicit labeling than fundamental errors.",llama-4-maverick,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Select features and target
X = data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']]
y = data['Hea...",8,"The generated code correctly addresses almost all requirements of the task. It loads the data, selects the specified features and target, performs one-hot encoding on `Sex` and `ChestPainType`, splits the data, trains a Logistic Regression model, and reports accuracy.

**Strong Points:**
- Correctly uses `ColumnTransformer` and `Pipeline` for preprocessing and model training, which is good practice.
- Correctly identifies and preprocesses categorical features with `OneHotEncoder`.
- Correctly uses `train_test_split` with the specified `test_size` and `random_state`.
- Reports the accuracy score as requested.

**Areas for Improvement (Why not a 10):**
- **Unrequested Preprocessing:** The generated code includes `StandardScaler` for numerical features (`Age`, `RestingBP`, `Cholesterol`, `MaxHR`). While not incorrect for Logistic Regression (and often beneficial), the task explicitly did *not* request scaling for numerical features. The prompt only specified ""Preprocess categorical features: `Sex` (M, F) and `ChestPainType` (TA, ATA, NAP, ASY) using one-hot encoding."" Adding an unrequested preprocessing step, even if benign, deviates slightly from the exact instructions, leading to the score of 8 instead of 10.
- **`max_iter` for `LogisticRegression`**: The ground truth increased `max_iter` to 1000 for `LogisticRegression` to avoid potential convergence warnings, which is a good practice for robustness. The generated code uses the default `max_iter`, which might lead to a `ConvergenceWarning` depending on the dataset characteristics, though it might still produce a valid result within the default iterations. This is a minor point but shows a slight difference in robustness consideration.",llama-4-maverick,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
def load_dataset(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load dataset: {str(e)}"")

# Perform independent two-sample t-test
def perform_t_test(data, group_column, value_column, group1_value, group2_value):
    group1 = data[data[group_column] == group1_value][value_column]
    group2 = data[data[group_column] == group2_value][value_c...",8,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- Correctly loads the dataset using pandas.
- Accurately identifies males and females based on the `Sex` column.
- Performs an independent two-sample t-test on Cholesterol levels.
- Clearly states the null and alternative hypotheses.
- Prints the t-statistic and the p-value.
- Correctly interprets the p-value against the alpha = 0.05 significance level and states whether to reject the null hypothesis.
- Uses proper imports (`pandas`, `scipy.stats`).
- The code is structured into functions (`load_dataset`, `perform_t_test`, `main`), which improves readability and modularity.

**Areas for Improvement:**
- **Robustness in `load_dataset`:** While it includes error handling, it doesn't return an empty DataFrame or handle scenarios where `heart.csv` exists but is malformed, leading to potential downstream errors if the `main` function isn't modified to check for `None` more robustly before proceeding (though it does check `if data is not None`). The ground truth code handles `FileNotFoundError` explicitly and other exceptions generically with more informative print statements inside the function, and then returns.
- **`equal_var` parameter in `ttest_ind`:** The generated code uses `stats.ttest_ind(group1, group2)` which defaults to `equal_var=True` (Student's t-test). The ground truth code explicitly uses `equal_var=False` for Welch's t-test, which is generally more robust as it doesn't assume equal population variances and is often preferred unless equality of variances is known or confirmed. Although the prompt didn't explicitly specify *which* two-sample t-test variant, Welch's is a common and safer choice in practice when not otherwise specified. For a benchmark evaluating an ""expert"", considering this nuance would be beneficial.
- **Clarity of Output:** While the output includes the required information, the ground truth's output formatting is slightly more user-friendly with descriptive headers and consistent formatting (e.g., `:.4f` for numerical values).
- **Handling missing data:** The generated code doesn't explicitly handle `NaN` values in `Cholesterol` for the groups. While `ttest_ind` typically handles `NaN`s by ignoring them, explicitly calling `.dropna()` as in the ground truth ensures awareness and consistency, especially if the dataset had many missing values that could affect group sizes significantly.

Overall, the generated code is a good solution that fulfills the core requirements of the task. The suggested improvements are primarily about robustness, best practices, and output elegance, rather than critical functional errors.",llama-4-maverick,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

def calculate_iqr_bounds(series):
    q1 = series.quantile(0.25)
    q3 = series.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return lower_bound, upper_bound

def winsorize(series, lower_bound, upper_bound):
    return series.apply(lambda x: min(max(x, lower_bound), upper_bound))

def main():
    # Load dataset
    df = pd.read_csv('winequality-red.csv')

    # Identify potential outliers ...",7,"The generated code correctly performs the core tasks of identifying outliers using IQR, winsorizing them, and comparing statistics. This demonstrates a good understanding of the problem.

However, there are several areas for improvement:

1.  **Output Format and Clarity**: The output does not match the detailed, structured, and user-friendly output of the ground truth. For example, it doesn't explicitly print the calculated Q1, Q3, IQR, and bounds, which are helpful for understanding the outlier detection process. The ground truth provides more informative print statements throughout.
2.  **Dataset Copy for Winsorization**: The generated code modifies the DataFrame in place by adding new columns (`_winsorized`). While this isn't strictly incorrect, the task aimed for ""a new dataset version where these identified outliers are capped,"" implying a separate DataFrame or explicit use as a new version, as done in the ground truth with `df_winsorized = df.copy()`. The current approach of adding `_winsorized` columns essentially creates a new version *within* the original DataFrame, which slightly deviates from the spirit of ""a new dataset version.""
3.  **Error Handling**: The ground truth includes robust error handling for `FileNotFoundError`, which is crucial for real-world scripts. The generated code lacks this.
4.  **Winsorization Implementation**: The `winsorize` function uses `series.apply(lambda x: min(max(x, lower_bound), upper_bound))`. While functionally correct, using `np.where` as in the ground truth is generally more performant for large pandas Series/DataFrames due to vectorized operations.
5.  **Readability/Structure**: The ground truth breaks down the problem into numbered sections (1, 2, 3, 4) with clear headings, making it very easy to follow the logical flow and verify each step against the requirements. The generated code has functions but doesn't explicitly structure the output to match the task's requested steps as clearly.

Overall, it's a functional solution that addresses the technical requirements, but could be improved in terms of user experience (output clarity), robustness (error handling), and adherence to best practices (vectorization vs. `apply`).",llama-4-maverick,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def main():
    # Load the dataset
    df = pd.read_csv('heart.csv')

    # Create a grouped bar chart
    plt.figure(figsize=(10,6))
    sns.countplot(data=df, x='ChestPainType', hue='HeartDisease')
    plt.title('Count of HeartDisease by ChestPainType')
    plt.xlabel('Chest Pain Type')
    plt.ylabel('Count')
    plt.legend(title='Heart Disease')
    plt.show()

if __name__ == ""__main__"":
    main()
```",8,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It uses `seaborn.countplot` which is appropriate for this task, and includes a title, labeled axes, and a legend. All required imports are present.

Areas for improvement to reach a higher score:
1.  **Error Handling:** The code lacks error handling for `FileNotFoundError` or other potential issues when loading the CSV, which is present in the ground truth.
2.  **Robustness/Clarity for HeartDisease Labels:** The `hue` legend for `HeartDisease` simply shows '0' and '1'. The ground truth code maps these to more descriptive labels ('No Heart Disease', 'Heart Disease'), which improves readability.
3.  **ChestPainType Order:** The ground truth explicitly orders the `ChestPainType` categories, which ensures consistent plotting if the dataset order isn't guaranteed or if a specific presentation order is desired.
4.  **Legend Position:** While the legend is present, its position could be optimized (e.g., as in the ground truth for potential overlap with the plot).
5.  **Functionality:** Encapsulating the logic within a function that optionally takes a `file_path` argument, as in the ground truth, makes the code more reusable and testable. The current `main()` approach works, but the function approach is generally better practice for reusable code.",llama-4-maverick,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    try:
        data = pd.read_csv('spotify_2023.csv')
    except FileNotFoundError:
        print(""The file 'spotify_2023.csv' was not found."")
        return

    # Clean the 'in_shazam_charts' column
    original_len = len(data)
    data['in_shazam_charts'] = data['in_shazam_charts'].str.replace(',', '').astype(float, errors='coerce')
    nan_count = data['in_shazam_charts'].isna().sum()
    print(f""Count of NaNs in 'in_shazam...",9,"The generated code correctly addresses all requirements of the task. It loads the CSV, handles `FileNotFoundError`, correctly cleans the `in_shazam_charts` column by removing commas and converting to numeric (coercing errors), and correctly reports the count of NaNs. It then groups the data by `key` and `mode` and calculates the average `danceability_%` and `valence_%`.

**Strong Points:**
- Proper use of pandas for data loading, cleaning, and aggregation.
- Correct handling of `FileNotFoundError`.
- Accurate cleaning and conversion of `in_shazam_charts` using `str.replace(',', '')` and `astype(float, errors='coerce')`.
- Correctly reports the count of NaNs in `in_shazam_charts`.
- Correct grouping and calculation of means for the specified columns.
- Encapsulates the logic within a `main` function and uses `if __name__ == ""__main__"":`, which is good practice.

**Minor Area for Improvement:**
- While the solution is correct, the `astype(float, errors='coerce')` after `str.replace(',', '')` is generally okay, `pd.to_numeric` with `errors='coerce'` is often considered more robust and explicit for type conversion in pandas, as used in the ground truth. However, for this specific string format, `astype(float, errors='coerce')` works fine.
- The `original_len = len(data)` is declared but not used, which is a minor piece of dead code.",llama-4-maverick,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load dataset
df = pd.read_csv('spotify_2023.csv')

# Ensure streams column is numeric and drop rows with NaN in streams or predictor columns
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
predictor_columns = ['in_spotify_playlists', 'energy_%']
df = df.dropna(subset=['streams'] + predictor_c...",9,"The generated code successfully implements all the requirements of the task. It loads the dataset, correctly handles the `streams` column to be numeric and drops NaNs in relevant columns, selects the correct features and target, splits the data, trains a Linear Regression model, and reports the R2 and MSE scores.

**Strong points:**
- All required imports are present.
- Correct handling of `streams` column with `pd.to_numeric` and `errors='coerce'`.
- Correct application of `dropna` on the `streams` and predictor columns.
- Correct selection of features and target.
- Correct data splitting with `test_size=0.2` and `random_state=42`.
- Accurate training of the Linear Regression model.
- Correct calculation and printing of R-squared and MSE.

**Areas for minor improvement (not critical for functionality but good practice):**
- **Error Handling for File Not Found:** The ground truth includes a `try-except` block for `FileNotFoundError` during CSV loading, which is robust. The generated code assumes the file exists, which could lead to a program crash if the file is missing. While the prompt implied the file would be there, a production-ready solution would include this.
- **Output Formatting:** The ground truth provides slightly more verbose and clearly labeled print statements (e.g., ""Dataset loaded successfully."", ""Features (X) shape:"", etc.), which can be helpful for debugging and understanding the code's execution flow. The generated code's output is concise but lacks these informative messages. This is a stylistic preference and doesn't impact correctness.",llama-4-maverick,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    df = pd.read_csv('spotify_2023.csv')

    # Combine released_year, released_month, and released_day into release_date
    df['release_date'] = pd.to_datetime({
        'year': df['released_year'],
        'month': df['released_month'],
        'day': df['released_day']
    }, errors='coerce')

    # Create a new feature: days_since_release
    fixed_date = pd.to_datetime('2023-12-31')
    df['days_since_release'] = (fixed_date...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correct Data Loading:** It correctly loads the `spotify_2023.csv` dataset.
- **Correct `release_date` Creation:** It uses `pd.to_datetime` with a dictionary of year, month, and day columns, which is an efficient and clean way to create the datetime column.
- **Error Handling for Dates:** It correctly uses `errors='coerce'` to handle invalid date compositions, setting them to NaT, as required.
- **Correct `days_since_release` Calculation:** It accurately calculates `days_since_release` by subtracting `release_date` from `2023-12-31` and extracting `dt.days`.
- **Correct Display:** It displays the requested columns (`track_name`, `release_date`, `days_since_release`) for the first 5 tracks.
- **Good Structure:** Encapsulating the logic within a `main` function and using `if __name__ == ""__main__"":` is good practice for modularity.

**Areas for Minor Improvement (for a perfect 10):**
- **Integer Type for `days_since_release` with NaNs:** The task specified that `days_since_release` should be an *integer*. While `dt.days` returns integers, if `release_date` is `NaT`, `dt.days` will result in `NaN`. When `NaN`s are present in a column, pandas defaults to a float type unless an explicit nullable integer type (like `'Int64'`) is used. The ground truth correctly uses `.astype('Int64')` to ensure it remains an integer type even with `NaN`s. The generated code currently leaves it as a float if NaNs are present in the `release_date` column.
- **Missing File Error Handling:** The prompt specified ""Handle potential errors during conversion by setting invalid dates to NaT"", which was done. However, robust code would also include `try-except` blocks for the file loading itself (`FileNotFoundError`), similar to the ground truth. While not explicitly asked for *error handling for file loading*, it's a good practice and demonstrated in the ground truth for a complete solution.

Overall, it's a very good solution that functionally meets the core requirements. The minor point about the nullable integer type is a subtle detail that distinguishes a perfect solution in a data science context.",llama-4-maverick,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

def main():
    try:
        # Load the dataset
        data = pd.read_csv('youtube_trendings.csv')

        # Clean 'Video views' and 'Likes' columns
        for column in ['Video views', 'Likes']:
            data[column] = data[column].str.replace(',', '')
            data[column] = pd.to_numeric(data[column], errors='coerce').fillna(0)

        # Fill missing 'Category' values with 'Unknown'
        data['Category'] = data['Category'].fillna('Unknown')

       ...",8,"The generated code correctly addresses most of the task requirements.

**Strong Points:**
- **Correct Loading:** Successfully loads the CSV.
- **Correct Cleaning (Video Views & Likes):** The `str.replace(',', '')` combined with `pd.to_numeric(errors='coerce').fillna(0)` correctly implements the cleaning and handling of NaNs for 'Video views' and 'Likes'. The loop through the columns is an elegant way to handle this for multiple columns.
- **Correct Category Handling:** `fillna('Unknown')` for 'Category' is accurate.
- **Correct Grouping and Aggregation:** `groupby('Category')[['Video views', 'Likes']].sum()` is the correct approach to calculate the sums per category.
- **Error Handling:** Includes basic `try-except` blocks for `FileNotFoundError`, `EmptyDataError`, and `ParserError`, which is good practice.
- **Modularity:** Encapsulating the logic within a `main()` function and using `if __name__ == ""__main__"":` is good Python practice.

**Areas for Improvement (Why it's not a 10):**
- **Implicit Column Existence Assumption:** The generated code implicitly assumes that 'Video views', 'Likes', and 'Category' columns will always exist in the DataFrame. If one of these columns were missing (e.g., due to a typo in the dataset or a malformed file), the code would raise a `KeyError` (e.g., `KeyError: ""['Video views', 'Likes'] not in index""` during the `groupby` or `KeyError` from `df[column]` assignment), rather than gracefully handling it or printing a warning as the ground truth does. The ground truth explicitly checks for column existence with `if 'Column Name' in df.columns:` before proceeding.
- **Display Format:** The task asked to ""calculate and *display*"" the sum. The generated code prints `grouped_data` which includes the 'Category' column as an index or column. The ground truth explicitly prints a descriptive header ""Sum of Video Views and Likes per Category:"" which is a minor but effective enhancement for clarity of output, and `category_summary` is displayed, which effectively serves the same purpose as `grouped_data`. The `.reset_index()` in the generated code is not strictly necessary unless the goal is to have 'Category' as a regular column for further processing, but it doesn't harm the output for display.
- **Small edge case/robustness:** The `data[column].str.replace(',', '')` might fail if `data[column]` itself contains non-string types that haven't been coerced to string yet, as pandas `str` accessor works only on string-like Series. The Ground Truth uses `.astype(str)` before `.str.replace()` for robustness, ensuring that all values are treated as strings before attempting the replacement. While `pd.read_csv` often infers numeric columns that might be read as objects due to commas, if they are already strings, it wouldn't be an issue, but it's a difference in robustness. For this specific problem dataset scenario, it's unlikely to cause an issue, but for a general solution, `astype(str)` is safer.",llama-4-maverick,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy import stats

def clean_likes(likes):
    try:
        return float(str(likes).replace(',', ''))
    except ValueError:
        return float('nan')

def main():
    # Load dataset
    data = pd.read_csv('youtube_trendings.csv')

    # Clean 'Likes' column
    data['Likes'] = data['Likes'].apply(clean_likes).fillna(0)

    # Fill missing 'Category' values with 'Unknown'
    data['Category'] = data['Category'].fillna('Unknown')

    # Create groups for 'Mus...",7,"The generated code largely addresses the requirements but has a few areas for improvement, primarily in its handling of the `Likes` column cleaning and the t-test itself.

**Strengths:**
*   **Correct Imports:** Uses `pandas` and `scipy.stats` as required.
*   **Dataset Loading:** Correctly loads the CSV.
*   **Category NaN Handling:** Fills missing 'Category' values with 'Unknown'.
*   **Hypotheses Stated:** Clearly states the null and alternative hypotheses.
*   **T-test Execution:** Correctly performs the `ttest_ind`.
*   **Output:** Prints t-statistic and p-value.
*   **Interpretation:** Correctly interprets the p-value against the alpha level.
*   **Structure:** Uses a `main` function and `if __name__ == ""__main__"":` which is good practice.

**Areas for Improvement:**
*   **`Likes` Cleaning Robustness (Major):**
    *   The `clean_likes` function uses `float(str(likes).replace(',', ''))`. This is less robust than `pd.to_numeric(..., errors='coerce')` for large datasets and potential performance.
    *   Crucially, the `clean_likes` function returns `float('nan')` for `ValueError`. While `fillna(0)` is used after `apply`, the `pd.to_numeric` with `errors='coerce'` approach in the ground truth is more idiomatic pandas for this exact task, efficiently handling non-numeric strings across the entire series. The current implementation iterates row by row, which is not ideal for performance.
*   **T-test `equal_var` Parameter (Minor):** The generated code uses `stats.ttest_ind(music_likes, sports_likes)`, which defaults `equal_var=True`. In real-world data science, assuming equal variances is often risky. The ground truth explicitly sets `equal_var=False` (Welch's t-test), which is generally more appropriate unless there's strong evidence for equal variances. While not explicitly stated as a requirement, it's a best practice in statistical testing.
*   **Error Handling for File Not Found:** The generated code does not include a `try-except` block for `FileNotFoundError` when loading the CSV, which the ground truth does. This makes the code less robust for deployment.
*   **Intermediate Print Statements:** The problem statement asks for ""Clearly state the null and alternative hypotheses"" and ""Print the calculated t-statistic and the p-value."" While the generated code does this, the ground truth includes more verbose print statements (e.g., ""Dataset loaded successfully."", ""Likes column cleaned..."") which are helpful for tracing execution and confirming steps, but not strictly required.
*   **Creating a new Column for Cleaned Likes:** The ground truth creates `Likes_cleaned` to keep the original `Likes` column intact. The generated code overwrites the `Likes` column, which is acceptable if the original column isn't needed, but creating a new one is sometimes preferred for data integrity.

Overall, it's a good attempt, but the `Likes` cleaning method and the choice of t-test (implicitly assuming equal variances) miss some best practices and robustness aspects from the expert evaluation perspective.",llama-4-maverick,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd
import numpy as np

def main():
    # Load the dataset
    try:
        df = pd.read_csv('youtube_trendings.csv')
    except FileNotFoundError:
        print(""The file 'youtube_trendings.csv' was not found."")
        return

    # Clean 'Likes' and 'Dislikes' columns
    for col in ['Likes', 'Dislikes']:
        df[col] = df[col].astype(str).str.replace(',', '')
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

    # Create new columns
    df['Tot...",9,"The generated code correctly addresses all the requirements of the task. It loads the dataset, cleans the 'Likes' and 'Dislikes' columns (handling commas, numeric conversion, and filling NaNs with 0), creates the `TotalEngagement` and `LikeDislikeRatio` columns, and displays the specified columns. The use of a loop for cleaning 'Likes' and 'Dislikes' is an improvement over the ground truth's repetitive code. The code is well-structured within a `main` function and includes basic error handling for `FileNotFoundError`. The only minor point of improvement could be the explicit casting of `Likes` and `Dislikes` to `float` after cleaning, as the ground truth does, to ensure consistency, though `pd.to_numeric` typically handles this. Overall, it's a very good and efficient solution.",llama-4-maverick,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

def clean_price(price):
    try:
        return pd.to_numeric(price.str.replace('USD ', '').str.replace(',', ''))
    except AttributeError:
        price = str(price).replace('USD ', '').replace(',', '')
        return pd.to_numeric(price, errors='coerce')

def clean_screen_size(size):
    try:
        return pd.to_numeric(size.str.replace(' inches', ''))
    except AttributeError:
        size = str(size).replace(' inches', '')
        return pd.to_numeric(size, ...",8,"The generated code correctly addresses almost all aspects of the task. It loads the CSV, cleans both the 'Launched Price (USA)' and 'Screen Size' columns, handles conversion errors by setting to NaN, groups the data by 'Company Name' and 'Launched Year', and calculates the average of the cleaned columns. The use of functions for cleaning is a good modular approach.

However, there are a couple of minor points for improvement:
1.  **Redundant `try-except` in `clean_price` and `clean_screen_size`:** The `try-except AttributeError` inside the cleaning functions is not strictly necessary when applying the function to a pandas Series, as `pd.to_numeric` with `errors='coerce'` will handle non-numeric values correctly after `str.replace` attempts. The `errors='coerce'` should directly be applied. The `df[column].str.replace` already handles the string operations efficiently. Explicitly converting individual elements to string and then back to numeric in a custom function is less idiomatic pandas than using vectorized string methods and `pd.to_numeric` on the Series directly.
2.  **Implicit NaNs:** While `errors='coerce'` handles NaNs, the generated code doesn't explicitly mention or demonstrate the count of NaNs introduced, which was part of the ground truth's detailed output for verification. This isn't a functional error but a slightly less complete diagnostic output.
3.  **Naming of cleaned columns:** The ground truth created new columns for the cleaned data (`Cleaned Price (USD)`, `Cleaned Screen Size (inches)`), leaving the original columns intact (though modified in the generated code). The generated code modifies the original columns in place, which is acceptable but sometimes less desirable if the original data needs to be preserved for other operations.

Overall, it's a very good solution that successfully accomplishes the core requirements.",llama-4-maverick,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

def clean_camera(camera):
    match = re.search(r'\d+', str(camera))
    return int(match.group()) if match else 0

def main():
    # Load the dataset
    df = pd.read_csv('mobiles_2025.csv')

    # Apply the cleaning function to camera columns
    df['Cleaned Front Camera'] = df['Front Camera'].apply(clean_camera)
    df['Cleaned Back Camera'] = df['Back Camera'].apply(clean_camera)

    # Create a new feature: Total Camera MP
    df['Total Camera MP'] =...",9,"The generated code correctly performs all the specified tasks: loading the dataset, defining and applying the camera cleaning function (which correctly extracts the first numerical value or defaults to 0), creating the `Total Camera MP` feature, and displaying the required columns for the first 10 rows. The imports are correct, and the code structure is logical with a `main` function.

One minor area for improvement is handling `FileNotFoundError` as seen in the ground truth, which makes the script more robust. However, considering the direct task requirements, it's a very good solution. The provided `clean_camera` function implicitly handles `NaN` values by converting them to strings, where `re.search` will correctly return `None`, leading to a 0 output, which matches the required behavior.",llama-4-maverick,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# Load dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean data
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')
df['Screen Size'] = pd.to_numeric(df['Screen Size'].str.replace...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs the specified cleaning operations on the `RAM`, `Battery Capacity`, `Screen Size`, and `Launched Price (USA)` columns, handles `NaN` values by dropping rows where they occur in these specific columns, defines features and target, splits the data, trains a Linear Regression model, and evaluates it by reporting R-squared and MAE. The imports are correct and the code structure is logical and easy to follow.

One minor area for improvement, although not strictly required by the prompt, is that the code doesn't explicitly convert the columns to string type before applying `str.replace()`. While in many cases pandas handles this implicitly, explicitly converting to `astype(str)` for robustness against mixed types is a good practice. Also, the ground truth includes error handling for `FileNotFoundError` and checks for empty DataFrame after cleaning, which makes it more robust, but these weren't specified as requirements so their absence doesn't penalize the core task performance. The ground truth also explicitly trims whitespace after cleaning which is a good practice. Overall, a very good solution.",llama-4-maverick,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

def main():
    # Load the Titanic dataset
    titanic_df = pd.read_csv('titanic.csv')

    # Drop rows with missing values in relevant columns
    titanic_df = titanic_df.dropna(subset=['Pclass', 'Survived', 'Age', 'Fare'])

    # Chi-square test: Pclass vs Survived
    contingency_table = pd.crosstab(titanic_df['Pclass'], titanic_df['Survived'])
    chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_t...",6,"The generated code correctly performs the three requested statistical tests (Chi-square, T-test, ANOVA) and calculates effect sizes for each. It also interprets the results based on p-values and an alpha level of 0.05.

However, there are several areas for improvement:

1.  **Hardcoded Alpha:** The alpha level (0.05) is hardcoded in the `if` conditions for p-value comparisons. While this works, it's better practice to define `alpha = 0.05` at the beginning of the script as a variable, making it easily configurable.
2.  **Data Cleaning:** The generated code performs `dropna` on `['Pclass', 'Survived', 'Age', 'Fare']` upfront. While this makes the data cleaner for subsequent analyses, it might exclude more data than necessary if individual tests only require specific columns (e.g., Chi-square doesn't strictly need 'Age' or 'Fare' to be present). The ground truth code handles `dropna` defensively *before* each test,
    `df_age_cleaned = df.dropna(subset=['Age'])` for example.
3.  **Error Handling (File Not Found):** The ground truth includes robust error handling for `FileNotFoundError` when loading the CSV, which is a good practice for user-facing scripts. The generated code lacks this.
4.  **Effect Size Calculation:**
    *   **Cramer's V:** The formula used `phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))` and `min((kcorr - 1), (rcorr - 1))` is for *corrected* Cramer's V, which is more complex than the standard formula often used for a simpler interpretation or when dealing with small sample sizes/imbalanced marginals. While not technically wrong, the provided formula in the prompt for Cramer's V was `np.sqrt(chi2_stat / (n * min_dim))`, which is simpler and more directly aligns with typical introductory statistics contexts. The ground truth stuck with standard Cramer's V (`np.sqrt(chi2_stat / (n * min_dim))`).
    *   **Cohen's d:** The calculation is correct, but it doesn't consider the results of Levene's test for variance equality, which the ground truth code performs. If variances are unequal, Welch's t-test (`equal_var=False`) should be used, and the pooled standard deviation calculation for Cohen's d would also need adjustment or a different effect size measure might be more appropriate.
    *   **Eta-squared:** The calculation is correct.
5.  **Assumptions and Warnings:** The ground truth includes checks for statistical assumptions (e.g., Levene's test for t-test and ANOVA) and prints warnings if assumptions are violated. This contextual information is crucial for proper statistical interpretation. The generated code lacks these checks and warnings.
6.  **Readability/Output:** The output is concise but less informative than the ground truth. The ground truth provides more context, including explicit hypotheses, degrees of freedom, and more detailed conclusions that tie back to rejecting/failing to reject the null hypothesis.

To improve, add more robust error handling, include assumption checks (like Levene's test), consider using the standard Cramer's V unless explicitly asked for the corrected one, and enhance the output clarity with explicit hypotheses and more detailed conclusions.",llama-4-maverick,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    imdb_data = pd.read_csv('imdb_top_250.csv')

    # Group data by 'Year' and calculate average 'Rating' and 'Runtime'
    yearly_avg = imdb_data.groupby('Year')[['Rating', 'Runtime']].mean().reset_index()

    # Create a line plot for average 'Rating' over 'Year'
    plt.figure(figsize=(10,6))
    plt.plot(yearly_avg['Year'], yearly_avg['Rating'], marker='o')
    plt.title('Average Rating Trend O...",7,"The generated code correctly loads the data, groups by year, calculates the required averages, and generates a line plot with a title and labeled axes as requested. This addresses the core requirements of the task.

However, there are a few areas for improvement:
- **Robustness**: The ground truth includes essential data cleaning steps for 'Year' and 'Runtime' columns (e.g., handling non-numeric characters, ' min' suffix, and converting to the correct data types). The generated code assumes these columns are already in a clean, numerical format, which might lead to errors if the CSV data isn't perfectly formatted. This is a common issue with real-world datasets and a robust solution should account for it.
- **Error Handling**: The generated code lacks error handling for `FileNotFoundError` or `KeyError`, which are present in the ground truth and are good practice for production-ready code.
- **Plotting Aesthetics**: While it includes a grid, the plot could benefit from better aesthetics like `plt.tight_layout()` and `plt.xticks(rotation=45)` for large datasets, as seen in the ground truth.
- **Clarity and Readability**: The ground truth encapsulates the logic in a function with docstrings and type hints, which improves code organization and maintainability.

Overall, it's a functional solution that meets the basic requirements, but lacks the robustness, error handling, and some best practices found in the ground truth.",llama-4-maverick,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create two groups of movies based on the `Year` column
before_1990 = df[df['Year'] < 1990]['Rating']
after_1990 = df[df['Year'] >= 1990]['Rating']

# State the null and alternative hypotheses
null_hypothesis = ""The mean Rating of movies released before 1990 is equal to the mean Rating of movies released in or after 1990.""
alternative_hypothesis = ""The mean Rating of movies released b...",7,"The generated code correctly performs data loading, group creation, t-test execution, and interpretation, fulfilling most of the task requirements.

**Strong points:**
*   Correctly loads the dataset using `pd.read_csv`.
*   Correctly creates the two groups based on the `Year` column and extracts `Rating`.
*   Correctly performs the independent two-sample t-test using `scipy.stats.ttest_ind`.
*   Correctly prints the t-statistic and p-value.
*   Correctly interprets the p-value against `alpha = 0.05` and states the conclusion.

**Areas for improvement:**
*   **Hypotheses statement:** While the hypotheses are printed, they are not stated as formally and completely as in the ground truth, which explicitly mentions ""statistically significant difference"" as a core part of the alternative hypothesis. The ground truth states both the null and alternative more clearly and completely.
*   **Robustness in data loading/type conversion:** The code assumes the 'Year' and 'Rating' columns are directly numeric and that the file will always be found. The ground truth includes error handling for `FileNotFoundError` and explicit type conversion with `pd.to_numeric` and `errors='coerce'`, followed by `dropna`. This makes the ground truth more robust to potential data quality issues or missing files, which is good practice in real-world data science tasks.
*   **Clarity in output:** The ground truth adds more descriptive print statements (e.g., ""--- Hypotheses ---"", ""--- T-Test Results ---"", ""--- Interpretation ---"") which makes the output much clearer and easier to follow, aligning with the ""Clearly state"" and ""Print"" requirements for specific sections.
*   **Absence of `equal_var` parameter:** The `ttest_ind` function by default assumes `equal_var=True`. While this often works, explicitly stating `equal_var=True` or `equal_var=False` makes the code's assumption clear. The ground truth mentions this explicitly.

Overall, the code is functional and addresses the core statistical challenge. However, the lack of robustness in data loading and less
formal/complete phrasing of hypotheses prevents it from reaching a higher score.",llama-4-maverick,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    df = pd.read_csv('imdb_top_250.csv')

    # Create a new column 'Decade'
    df['Decade'] = (df['Year'] // 10) * 10
    df['Decade'] = df['Decade'].apply(lambda x: f'{x}s')

    # Display the required columns for the first 10 movies
    print(df[['Title', 'Year', 'Decade']].head(10))

if __name__ == ""__main__"":
    main()
```",9,"The generated code correctly performs all the required tasks: loading the dataset, creating the `Decade` column by categorizing the `Year`, and displaying the specified columns for the first 10 movies. The logic for calculating the decade is concise and correct.

Improvements:
1.  **Error Handling**: The ground truth includes robust error handling for `FileNotFoundError` and `KeyError`, which is good practice for production-level code. The generated code lacks this.
2.  **Modularity**: The ground truth uses functions (`perform_feature_engineering`, `main`) to organize the code, making it more modular, reusable, and testable. The generated code puts all logic directly into `main`. While acceptable for such a small script, modularity is generally preferred.
3.  **Clarity of Display**: The `to_string(index=False)` in the ground truth makes the output cleaner by removing the DataFrame index. The generated code's `print(df[['Title', 'Year', 'Decade']].head(10))` includes the index, which is a minor aesthetic difference but can be improved.",llama-4-maverick,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

def main():
    # Load the dataset
    penguins_df = pd.read_csv('penguins.csv')

    # Impute missing values in numerical columns with their mean
    numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
    penguins_df[numerical_cols] = penguins_df[numerical_cols].fillna(penguins_df[numerical_cols].mean())

    # Impute missing values in the 'sex' column with its mode
    penguins_df['sex'] = penguins_df['sex'].fillna(pengui...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, imputes numerical columns with their respective means and the 'sex' column with its mode, groups the data as required, and calculates the average `body_mass_g`.

Strong points:
- Correctly loads the CSV file.
- Correctly identifies and imputes missing values in the specified numerical columns using `mean()`.
- Correctly identifies and imputes missing values in the `sex` column using `mode().iloc[0]`.
- Correctly groups the data by 'species' and 'sex' and calculates the mean of 'body_mass_g'.
- Uses idiomatic pandas operations for imputation (e.g., `fillna` on a selection of columns), which is concise and efficient.
- Encapsulates the logic within a `main()` function and uses the `if __name__ == ""__main__"":` block, which is good practice.

Areas for minor improvement (not critical enough to deduct significant points):
- The `reset_index()` on `grouped_df` changes the output format from a Series to a DataFrame. The task only asked to ""display the average `body_mass_g` ... for each group"", which typically implies the Series output from `groupby().mean()`. While valid, it's a slight deviation from the most direct interpretation of ""display the average ... for each group"" which is typically the Series output similar to the ground truth.
- Error handling (e.g., `FileNotFoundError`) is absent, which the ground truth included. While the prompt didn't explicitly ask for error handling, it's good practice in robust code. For a benchmark assessing core task completion, this might be considered a minor omission.",llama-4-maverick,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
def load_penguins_data(file_name):
    try:
        data = pd.read_csv(file_name)
        return data
    except Exception as e:
        print(f""Failed to load data: {e}"")
        return None

# Main function to perform t-test
def main():
    # Load data
    penguins_data = load_penguins_data('penguins.csv')
    if penguins_data is None:
        return

    # Filter data for 'Adelie' and 'Gentoo' species
    filtered_data ...",8,"The generated code correctly addresses most of the requirements. It loads the data, filters it, handles NAs, states the hypotheses, performs the t-test, and interprets the results correctly based on the specified alpha.

**Strengths:**
*   **Correct Filtering and NA Handling:** The filtering for species and subsequent `dropna` on the relevant columns is correctly implemented.
*   **Correct T-test Application:** `scipy.stats.ttest_ind` is used appropriately.
*   **Clear Hypothesis Statements:** The null and alternative hypotheses are clearly stated.
*   **Correct Interpretation:** The interpretation of the p-value against alpha is accurate.
*   **Good Structure:** Using a `main` function and a `load_penguins_data` function is a good practice for code organization.

**Areas for Improvement (Why not a 9 or 10):**
*   **Error Handling for Empty Groups:** The ground truth code includes checks to ensure that after filtering and NA removal, there are enough data points (at least 2) for each group to perform a meaningful t-test. The generated code does not include this check. If one of the species groups ends up empty or with only one data point, `ttest_ind` might raise an error or produce unreliable results.
*   **`equal_var` parameter in `ttest_ind`:** The ground truth code explicitly mentions and uses `equal_var=False` (Welch's t-test), which is often a more robust choice when variances are unknown or unequal. The generated code uses the default `equal_var=True`. While the problem didn't explicitly state which one to use, in statistical tests, being precise about assumptions (or being robust by assuming unequal variances) is important. Given the context of a ""expert code evaluator,"" this detail could be considered.
*   **Print Formatting:** The t-statistic and p-value are printed without formatting (`f""t-statistic: {t_statistic}, p-value: {p_value}""`). The ground truth uses `:.4f` for better readability and consistency.
*   **Robustness in Data Loading:** The `load_penguins_data` function handles `Exception as e` but doesn't specifically check for `FileNotFoundError`. While general `Exception` catches it, being more specific is often better. The ground truth also includes an `exit()` call on failure, which might be overkill for a function but ensures the script doesn't proceed without data.",llama-4-maverick,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('penguins.csv')

# Select features X and target y
X = df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = df['species']

# Drop rows with missing values in selected columns
df_clean = df.dropna(subset=['cul...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, selects the specified features and target, handles missing values, encodes the target variable, splits the data, trains a Logistic Regression model with `max_iter=1000`, and reports the accuracy.

Minor points for improvement:
1.  **Handling NA rows**: The current implementation drops NA values from `df` *before* the `X` and `y` are definitively assigned. The `X` and `y` are initially assigned from the *original* `df`, and then reassigned from `df_clean`. This is slightly redundant. A cleaner way would be to create `X` and `y` *after* the `dropna` operation, ensuring consistency and avoiding potential pitfalls if the initial `X` and `y` were used elsewhere. The current approach is functionally correct due to reassignment but could be more elegant.
2.  **Robustness**: The ground truth includes error handling for `FileNotFoundError` and checks for missing columns, which makes it more robust. The generated code assumes the file exists and all columns are present. While not explicitly asked for in the prompt, it significantly improves real-world usability.
3.  **Clarity of variable assignment**: The code reassigns `X` and `y` after dropping NAs (`X = df_clean[...]`, `y = df_clean[...]`). This is logically correct but slightly less streamlined than creating `X` and `y` directly from the cleaned DataFrame, as demonstrated in the ground truth (`X = df_cleaned[feature_columns]`, `y = df_cleaned[target_column]`).

Overall, it's a very good solution that successfully accomplishes the task.",llama-4-maverick,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    penguins_df = pd.read_csv('penguins.csv')

    # Drop rows where `body_mass_g` or `species` is NA
    penguins_df = penguins_df.dropna(subset=['body_mass_g', 'species'])

    # For each `species`, calculate the average `body_mass_g`
    avg_body_mass = penguins_df.groupby('species')['body_mass_g'].mean().reset_index()

    # Create a bar plot showing the average `body_mass_...",9,"The generated code successfully completes all aspects of the task: it loads the data, correctly handles NA values for the specified columns, calculates the average body mass per species, and generates a well-labeled bar plot. The use of `dropna(subset=[...])` is correct for the NA handling. The `groupby().mean().reset_index()` pattern is standard and effective for the aggregation. The plot is properly titled and has labeled axes.

Minor improvements could include:
- Adding error handling for `FileNotFoundError` when loading the CSV, similar to the ground truth.
- Although not strictly required by the prompt, adding `plt.tight_layout()` can sometimes prevent labels from overlapping plot elements, especially with more complex plots.
- The use of `main()` function and `if __name__ == ""__main__"":` is good practice for script organization.",llama-4-maverick,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def main():
    # Load the dataset
    penguins_df = pd.read_csv('penguins.csv')

    # Drop rows where specified columns are NA
    penguins_df = penguins_df.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

    # Create a new feature 'culmen_area'
    penguins_df['culmen_area'] = penguins_df['culmen_length_mm'] * penguins_df['culmen_depth_mm']

    # Calculate the Pear...",9,"The generated code correctly addresses all requirements of the task. It loads the dataset, handles NA values in the specified columns by dropping rows, creates the `culmen_area` feature, calculates the Pearson correlation matrix for the required columns, and visualizes it as an annotated heatmap with the correct title.

**Strong points:**
- **Correctness:** The code correctly performs all the specified feature engineering and visualization steps.
- **Clarity:** The code is straightforward and easy to understand.
- **Conciseness:** It achieves the task efficiently with minimal lines of code.
- **Pandas and Seaborn usage:** Correctly utilizes `dropna`, arithmetic operations for new feature creation, `corr()`, `heatmap`, `plt.figure`, `plt.title`, and `plt.show`.

**Areas for minor improvement (to reach a perfect 10):**
- **Error Handling:** The ground truth includes `try-except` for `FileNotFoundError` and checks for empty DataFrames after dropping NAs. While not explicitly requested in the prompt, robust data science code often includes such error handling, which the generated code lacks.
- **`square=True` in heatmap:** While `square=True` can be useful, the prompt only requested 'annotated' and 'title'. `linewidths=.5` (as in ground truth) can sometimes make the plot more visually appealing by adding borders, but `square=True` is also a valid aesthetic choice.
- **`fmt="".2f""` for annotation:** The ground truth specifies `fmt="".2f""` for the heatmap annotation, which ensures consistent decimal places. The generated code defaults to a floating-point format that might show more decimals than visually necessary. This is a minor stylistic point.

Overall, the generated code is a solid and correct solution to the problem, demonstrating strong understanding of pandas and seaborn for data manipulation and visualization.",llama-4-maverick,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
import numpy as np

def load_and_prepare_data(csv_file):
    # Load the dataset
    penguins_df = pd.read_csv(csv_file)

    # Select features and target
    features_df = penguins_df[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
    target = penguins_df['species']

    # Impute NA values for numerical features with their mean
    numerical_features = ['culmen_length_mm',...",8,"The generated code correctly performs all the required steps: loading the data, selecting features, imputing numerical NAs with the mean, imputing and mapping the 'sex' column, and one-hot encoding 'island'. It also prints the shape and head of the processed features as requested.

**Strong points:**
*   Correctly loads the CSV and selects specified features.
*   Accurately imputes numerical NAs with the mean.
*   Correctly imputes 'sex' NA values with 'Unknown' and maps 'MALE', 'FEMALE', and 'Unknown' to 0, 1, and -1, respectively.
*   Properly applies one-hot encoding to the 'island' column using `pd.get_dummies`.
*   Includes the required print statements for shape and head.
*   Encapsulates the logic in a function `load_and_prepare_data`, which is good practice.

**Areas for improvement:**
*   **SettingWithCopyWarning:** The line `features_df[numerical_features] = features_df[numerical_features].fillna(features_df[numerical_features].mean())` might trigger a `SettingWithCopyWarning`. It's generally safer to work on a copy of the slice or use `.loc` to avoid this. For example, `features_df = penguins_df[features_to_select].copy()` initially, then `features_df.loc[:, numerical_features] = features_df[numerical_features].fillna(...)`.
*   **Error Handling:** The ground truth code includes basic error handling for `FileNotFoundError`. The generated code does not, which could lead to a crash if the CSV is missing.
*   **Clarity on Target:** While the task asked for code to ""prepare the `penguins.csv` dataset for a classification task"" and mentioned `species` is the target, the output (shape and head) is specifically for the features matrix. The target variable `target` is returned but not explicitly printed or further processed or confirmed. This is a minor point, as the primary request was for *features* processing.
*   **Redundant Import:** `from sklearn.preprocessing import OneHotEncoder` is imported but not used, as `pd.get_dummies` is used instead. This is minor but unnecessary.",llama-4-maverick,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

def main():
    # Create a DataFrame with the given data
    data = {
        'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['C...",9,"The generated code correctly creates the DataFrame, calculates total spending per customer, identifies the top 3 customers by spending, and calculates the total quantity sold per category. All imports are handled properly, and the code is well-structured within a `main` function.

**Strong points:**
- Correctly implements all required calculations.
- Uses pandas functions effectively (`groupby`, `sum`, `sort_values`, `head`, `reset_index`).
- Clear and concise code.

**Areas for minor improvement:**
- The task asked to ""Identify and print the 'CustomerID' and their total spending for the top 3 customers"", and the ground truth iterated through the result to print it in a more human-readable ""CustomerID: CUST001, Total Spending: $...."" format. The generated code prints the DataFrame directly, which is correct but less granular in presentation than the ground truth's specific f-string output. This is a minor difference in output formatting rather than correctness.
- The intermediate column `Total_Spending` is created for calculating customer spending; the ground truth used `LineTotal`, which is semantically similar but could have been used consistently if the output of `Total_Spending` was not required for the final output. This isn't a flaw, just a stylistic choice.",llama-4-maverick,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# Load data
df = pd.read_csv('titanic.csv')

# Preprocess data
df.drop(['Name', 'Ticket', 'Cabin'], ...",6,"The generated code addresses most of the requirements but has several key deficiencies compared to the ground truth, primarily around data preprocessing and the robustness of the solution.

**Strengths:**
*   Correctly loads data and performs basic missing value imputation.
*   Splits data into training and testing sets.
*   Trains a Logistic Regression model.
*   Calculates and prints common evaluation metrics (accuracy, precision, recall, F1, ROC-AUC).
*   Displays feature coefficients.
*   Generates a confusion matrix (though not visualized as a plot in the same way as ground truth).
*   Plots an ROC curve.
*   Performs cross-validation.

**Weaknesses and Areas for Improvement:**

1.  **Preprocessing (Critical):**
    *   **Categorical Encoding:** Uses `LabelEncoder` for 'Sex' and 'Embarked'. This is problematic for features with more than two categories (like 'Embarked' and 'Pclass' in the ground truth, which was correctly identified as categorical), as `LabelEncoder` assigns arbitrary numerical values, implying an ordinal relationship that doesn't exist, which can mislead the model. The ground truth correctly uses `OneHotEncoder`.
    *   **Feature Scaling:** Only scales 'Age' and 'Fare', missing 'Pclass' if it were to be treated numerically and not one-hot encoded, and potentially generated features. The ground truth correctly integrates scaling within a `ColumnTransformer` and `Pipeline` for robustness.
    *   **No Pclass Preprocessing:** The `Pclass` column is completely ignored in the preprocessing steps, neither encoded nor scaled, which is a significant oversight.
    *   **Hardcoding Preprocessing:** The preprocessing steps are applied directly to the X dataframe without using `Pipelines` and `ColumnTransformer`. This makes the code less robust, harder to maintain, and prone to data leakage if not handled carefully (e.g., applying `fit_transform` on the entire dataset instead of `fit` on train and `transform` on both train/test). The ground truth's use of `Pipeline` is a much better practice.
    *   **Feature Engineering:** Missing the 'FamilySize' feature engineering, which is a common and often impactful step in Titanic datasets.
    *   **Dropped Columns:** Does not drop `PassengerId`, which is generally irrelevant.

2.  **Code Structure and Robustness:**
    *   **Lack of Pipelines:** Does not use `sklearn.pipeline.Pipeline` or `sklearn.compose.ColumnTransformer`. This is a major drawback for production-ready code, as it separates preprocessing from modeling, making deployment harder and increasing the risk of inconsistent preprocessing or data leakage. The ground truth excels here.
    *   **Confusion Matrix Display:** While it prints the confusion matrix array, it doesn't *visualize* it as a heatmap using `ConfusionMatrixDisplay` like the ground truth does, which was implied by the ""visualization"" requirement.
    *   **Interpretation of Coefficients:** The interpretation of feature coefficients is present but generic. The ground truth provides a more detailed explanation and correctly extracts feature names after one-hot encoding, which is crucial for interpretability. The generated code's coefficient display simply uses the original column names, which is incorrect for encoded features.

3.  **Completeness:**
    *   The cross-validation only prints the scores and their average, but doesn't compute or display the standard deviation or multiple metrics as done in the ground truth, which provides a more comprehensive view of model stability.
    *   Missing `try-except` block for file loading, although this is a minor point.

Overall, the generated code provides a functional solution, but its preprocessing approach is flawed and limits its potential for handling diverse feature types correctly. The lack of `Pipeline` and `ColumnTransformer` significantly reduces its code quality and robustness compared to best practices.",llama-4-maverick,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']), 
        'Ticker': ['TECHCO']*8, 
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0], 
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5], 
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0], 
        'Close': [152.0, 151.0, 153.5, 155...",9,"The generated code is almost perfect, correctly addressing all requirements of the task.

**Strong Points:**
- **Correctness:** The code successfully creates the DataFrame, sets 'Date' as the index, calculates the 3-day SMA for 'Close' (handled NaNs correctly), and calculates the percentage change in 'Volume' (handled NaNs correctly).
- **Code Quality:** Imports are correct, and the code is structured logically. It uses efficient pandas functions (`rolling().mean()`, `pct_change()`).
- **Completeness:** All aspects of the task, including the specific data provided, DataFrame creation, index setting, two calculations, and printing, are handled.

**Area for Improvement (Minor):**
- **Percentage formatting:** The task asked for ""percentage change in 'Volume'"", and `pct_change()` naturally returns decimal values (e.g., 0.2 for 20%). The generated code multiplies this by 100 (`df['Volume'].pct_change() * 100`). While this makes the value look like a percentage (e.g., 20.0), it's not standard practice to multiply by 100 unless explicitly requested for display purposes, as `pct_change()` itself is the percentage change. If the goal was to display it as a string with a '%' sign, a different approach would be needed. For numerical analysis, the decimal form is typically preferred. The ground truth code correctly keeps it in decimal format, which is more conventional for calculations involving percentage change. This is a very minor point and doesn't affect the core functionality, but it's a slight deviation from typical data science practice unless display formatting is explicitly required.",llama-4-maverick,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create a pandas DataFrame with student performance data
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88...",9,"The generated code correctly performs all the required operations: DataFrame creation, new column calculation, grouped average calculation, and conditional filtering. The imports are correct, and the code structure is logical.

The only minor point of improvement is the output format for the last requirement. The ground truth code iterates and prints each student ID on a new line, which is slightly more readable for a list of IDs than `tolist()` output. However, `tolist()` is perfectly valid and achieves the same result. The ground truth also includes prints of intermediate DataFrames, which wasn't explicitly asked for but can be useful for debugging/understanding - the generated code correctly focuses only on the requested final outputs.",llama-4-maverick,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

# Create the DataFrame
df = pd.DataFrame(data...",9,"The generated code correctly defines the DataFrame, filters it according to the specified conditions, and accurately calculates the average 'ProjectsCompleted' for the filtered subset. The code is well-structured and uses appropriate pandas operations.

The only minor area for improvement is the lack of an explicit check for an empty `filtered_df` before calculating `mean()`. While `mean()` on an empty Series returns `NaN`, which is technically correct, robustness is often improved by explicitly handling cases where no data matches the criteria. The ground truth code includes this robustness check, which is a good practice for production-level code. However, for the given task and a data science benchmark, this is a very minor omission.",llama-4-maverick,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Calculate conversion rates for Desktop and Mobile users
desktop_conversion_rate = (df[df['DeviceType']...",9,"The generated code correctly defines the DataFrame and accurately calculates the conversion rates for both Desktop and Mobile users. It also prints the results clearly labeled, fulfilling all prompt requirements. The code is concise and efficient.

One minor area for improvement, though not strictly required by the prompt given the specific dataset, would be to add handling for cases where there might be no users for a particular `DeviceType` (e.g., if `df[df['DeviceType'] == 'Desktop']` results in an empty DataFrame, `.mean()` on an empty Series would return `NaN`). While this isn't an issue with the provided `data`, robust code often accounts for such edge cases, as demonstrated in the ground truth's `if not ... .empty:` checks. However, for the given problem, the generated solution is perfectly functional and correct.",llama-4-maverick,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

# Create DataFrame
data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}
df = pd.DataFrame(data)

# Calculate and print average AirQualityIndex for each uniqu...",10,"The generated code is correct, complete, and of high quality. It correctly defines the DataFrame as specified, accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"", and prints the output clearly, satisfying all task requirements. The use of `.reset_index()` before iterating is a common and clear way to handle the grouped result, although iterating directly over the Series created by `groupby(...).mean()` as in the ground truth is also perfectly valid. Both approaches yield the correct and desired output format.",llama-4-maverick,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"```python
import pandas as pd

# Define the DataFrame representing social media post engagement
data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create a new featur...",9,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"" feature, filters for ""Instagram"" posts, and accurately calculates their average ""EngagementScore"". All requirements of the task are met. The code is clean, well-structured, and easy to understand.

The only minor point of improvement is the formatting of the printed output: the ground truth code formats the average score to two decimal places (e.g., `:.2f`). While not strictly required by the prompt, it's good practice for numerical outputs. However, this doesn't detract from the correctness or quality of the core logic.",llama-4-maverick,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",10,"The generated code is correct, complete, and follows good coding practices. It successfully defines the DataFrame, calculates the ""LengthOfStay"" column, filters for patients with ""Flu"" diagnosis, and accurately calculates and prints their average ""LengthOfStay"". The code is concise and directly addresses all parts of the task. The use of `.dt.days` is appropriate for converting timedelta to days. The f-string for printing the result is also good.",llama-4-maverick,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}
df = pd.DataFrame(data)

# Fil...",10,"The generated code is excellent. It correctly defines the DataFrame as specified, accurately filters the DataFrame to include only ""Sci-Fi"" movies, and then correctly calculates and prints the average rating for these movies. The code is clean, well-structured, and directly addresses all parts of the prompt. No improvements are needed for correctness or completeness. The output format is also good, matching the ground truth's level of formatting.",llama-4-maverick,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

# Create the DataFrame
data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020,2021,2019,2022,2023,2020],
    ""PriceUSD"": [25000,22000,18000,30000,60000,24000],
    ""Mileage"": [30000,20000,45000,15000,5000,35000]
}
df = pd.DataFrame(data)

# Filter the DataFrame to include only Toyota cars
toyota_df ...",10,"The generated code is correct, complete, and of excellent quality. It accurately defines the DataFrame, correctly filters for ""Toyota"" cars, and precisely calculates and prints their average mileage. The code is concise and follows best practices for pandas operations. The variable names are clear, and the f-string for printing is well-formatted.",llama-4-maverick,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63,...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat. All requirements of the task are met. The code is clean, readable, and follows good practices for DataFrame manipulation.

The only minor area for improvement, which is not strictly required by the prompt but present in the ground truth, would be:
1.  Adding a check for an empty `fire_pokemon_df` before calculating the mean, to prevent potential division by zero errors or NaN propagation if there were no Fire-type Pokemon in the original data (though not the case here).
2.  Formatting the output print statement to potentially include more decimals or better phrasing, similar to the ground truth's f-string with `:.2f`.

However, given the prompt's explicit requirements, the generated code fulfills them perfectly. The score is docked by 1 point due to the missing edge case handling for an empty filtered DataFrame, which is a good practice for robustness in real-world scenarios.",llama-4-maverick,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load data
df = pd.read_csv('titanic.csv')

# Preprocess data
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], labels=['0-12', '13-18', '19-35', '36-60', '61+'])
df['Survived'] = df['Survived'].map({0: 'No', 1: 'Yes'})

# Multi-panel figure for survival rates by demographic groups
fig, axs = plt.subplots(1, 3, figsize=(18, 6))
sns.countplot(data=df, x='Pclass', hue='...",6,"The generated code addresses most of the task requirements but falls short in quality and sophistication compared to the ground truth.

**Strengths:**
- Correctly uses `pandas`, `matplotlib`, `seaborn`, and `plotly.express` for visualizations.
- Attempts to create all requested plot types: multi-panel, heatmap, violin, interactive scatter, and a dashboard.
- Includes basic data preprocessing (binning Age, mapping Survived).

**Weaknesses and Areas for Improvement:**

1.  **Multi-panel figure (Requirement 1):**
    *   The prompt asked for ""survival rates by different demographic groups"" (mean survival). The generated code uses `sns.countplot` showing raw counts of survivors/non-survivors. While it conveys some information, it's not a direct representation of ""rates"" and doesn't align with the requirement for ""survival rates"". A bar plot of `mean()` survival would be more appropriate.
    *   The age binning used a fixed `[0, 100]` for the last bin, which might not be `np.inf` and could miss ages above 100 if they existed (though unlikely for Titanic data, it's less robust).

2.  **Heatmap (Requirement 2):**
    *   The heatmap calculation for `survival_rate` using `apply(lambda x: (x == 'Yes').mean())` is correct for obtaining the rates.
    *   However, plotting `survival_rate.to_frame()` is incorrect. `survival_rate` is already a pivot table with `Pclass` as index and `Sex` as columns, which is the correct format for the heatmap. Converting it to a single column frame will result in a malformed heatmap.

3.  **Interactive Scatter Plot (Requirement 4):**
    *   The scatter plot correctly uses `px.scatter` with `Age`, `Fare`, `color='Pclass'`, and `symbol='Survived'`.
    *   The ground truth also added `hover_name='Name'` and `hover_data` for a richer interactive experience, which is missing here.

4.  **Dashboard (Requirement 5):**
    *   The requirement was to ""Design a comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights.""
    *   The generated dashboard contains only 4 plots and lacks any annotations or textual insights, which was a crucial part of the ""storytelling"" aspect. Simpler `plt.subplots` was used instead of `gridspec` for more flexible layouts.

5.  **Data Preprocessing:**
    *   Does not handle missing `Age` values explicitly (it will default to `NaN` if `NaN`s are present before `pd.cut`). The ground truth handles this by filling with the median.
    *   No handling for `Fare` NaNs.
    *   The `Survived` column is mapped to 'No'/'Yes' which is fine, but the Class ('Pclass') is just converted to string without specific categorical mapping or ordering.

6.  **Readability/Best Practices:**
    *   Lack of comments to explain design choices.
    *   Some plot titles and axis labels are present, but not as detailed or consistent as in the ground truth.
    *   No general styling setup (like `sns.set_style`) or `rcParams` for consistent figure quality across plots.

The code provides a functional attempt but misses the nuances of data storytelling and robust visualization requested in the prompt, especially concerning plotting ""rates"" instead of counts and the lack of textual insights in the dashboard. The heatmap error is also a significant functional flaw.",llama-4-maverick,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

# Define the data for the DataFrame
data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10]...",9,"The generated code correctly defines the DataFrame, filters for the 'Dystopian' genre, and accurately calculates the average stock for that genre, fulfilling all requirements of the task. The code is clean, well-structured, and uses appropriate pandas operations. The primary reason it didn't receive a perfect 10 is the lack of an explicit check for an empty filtered DataFrame before calculating the mean, which is a good practice for robustness, although in this specific test case, it's not strictly necessary as Dystopian books are present. The ground truth includes this robustness check, which is a minor enhancement.",llama-4-maverick,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
from scipy import stats
import numpy as np

def load_data(file_name):
    return pd.read_csv(file_name)

def perform_t_tests(data):
    features = data.columns.drop('Potability')
    alpha = 0.05
    results = {}
    for feature in features:
        potable = data[data['Potability'] == 1][feature]
        non_potable = data[data['Potability'] == 0][feature]
        t_stat, p_value = stats.ttest_ind(potable, non_potable, equal_var=False)
        cohen_d = (potable.me...",5,"The generated code attempts to address all parts of the task but has significant issues in correctness and completeness, particularly regarding data preprocessing and the implementation of statistical tests.

**Primary Reasons for Failure/Critical Issues:**

1.  **Missing Value Handling:** The code completely ignores the requirement to handle missing values. The `water_potability.csv` dataset is known to have missing values (e.g., pH, Sulfate, Trihalomethanes). Failing to handle these will lead to incorrect statistical results or errors, especially with `dropna()` implicitly performed by `ttest_ind` on series with NaNs, which might not be desirable for every context without explicit imputation. The ground truth explicitly addresses this by imputing with the mean.

2.  **Cohen's d Calculation:** The `cohen_d` calculation in `perform_t_tests` is incorrect. It uses a simplified formula `(m1 - m2) / sqrt((s1^2 + s2^2) / 2)`, which is often referred to as Cohen's d for independent samples with equal variances. However, the t-test itself uses `equal_var=False` (Welch's t-test), which implies unequal variances. For consistency and better statistical practice with Welch's t-test, Cohen's d should typically be calculated using the pooled standard deviation, which accounts for potentially unequal variances, or using a specific formula for Cohen's d with unequal variances. The ground truth correctly implements Cohen's d using the pooled standard deviation formula compatible with standard practice for comparing two means.

3.  **Shapiro-Wilk Test Limitations:** The Shapiro-Wilk test has a sample size limitation (typically up to 5000 samples). The generated code performs it on `data[feature]` without considering the possibility of larger datasets, which could lead to errors or unreliable results if the dataset grows. The ground truth correctly identifies and addresses this limitation, suggesting alternative tests for large samples.

4.  **No Interpretation of Results in `main`:** While the functions return results, the `main` function merely prints the raw statistics and a boolean for significance. It lacks the rich, human-readable interpretation of the results (e.g., ""Reject the null hypothesis,"" ""data is NOT normally distributed,"" ""There is a statistically significant difference"") that was explicitly requested and provided by the ground truth. This significantly reduces the utility and clarity of the output.

5.  **Lack of Error Handling for File Loading:** The `load_data` function does not include any error handling for `FileNotFoundError`, which was explicitly part of the ground truth's robustness.

6.  **Redundant `alpha` Definitions:** The `alpha` variable is defined within each function (`perform_t_tests`, `perform_chi_square_test`, `perform_normality_tests`) instead of being a single, configurable top-level constant, reducing code maintainability and adherence to good practice.

**Summary of Differences from Ground Truth and Requirements:**

*   **Missing Values:** Ground truth imputes; generated code ignores. (Critical flaw)
*   **Cohen's d:** Ground truth uses pooled standard deviation (correct for the context of independent samples); generated code uses an incorrect simplified formula for unequal variances. (Correctness flaw)
*   **Shapiro-Wilk Sample Size:** Ground truth handles limitation; generated code does not. (Completeness/Robustness flaw)
*   **Result Interpretation:** Ground truth provides detailed interpretations; generated code provides none. (Completeness flaw)
*   **Error Handling:** Ground truth includes file loading error handling; generated code does not. (Robustness flaw)
*   **Code Structure:** Ground truth uses a more script-like approach with clear sequential steps and print statements for each stage; generated code uses functions but sacrifices the detailed, step-by-step reporting. Both are valid structures, but the ground truth's output is more informative for a benchmark.",llama-4-maverick,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# Load dataset
df = pd.read_csv('w...",6,"The generated code addresses most of the task requirements but has several crucial omissions and inaccuracies that prevent it from being a strong solution.

**Strong Points:**
- It correctly performs data loading and a basic missing value imputation (though not the specified median imputation).
- It performs feature scaling and train-test split.
- It trains multiple classification algorithms (Logistic Regression, Random Forest, SVM).
- It calculates the required evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC).
- It attempts to visualize confusion matrices and ROC curves.
- It performs k-fold cross-validation.
- It compares model performances and selects a ""best"" model based on ROC-AUC.
- It includes logic for displaying feature importance for Random Forest.

**Areas for Improvement / Critical Missing/Incorrect Points:**

1.  **Missing Value Handling (Critical):** The ground truth specifies handling missing values using `SimpleImputer(strategy='median')` after the train-test split to prevent data leakage. The generated code uses `df.fillna(df.mean(), inplace=True)` *before* train-test split, which is a common and significant data leakage error. Also, it uses mean imputation, not median.
2.  **Data Leakage in Preprocessing:** `StandardScaler` is fitted on the entire `X` (`X_scaled = scaler.fit_transform(X)`), *then* the train-test split happens. This causes data leakage from the test set into the training of the scaler. The scaler should be fitted only on the training data (`X_train`) and then used to transform both `X_train` and `X_test`. `X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, ...)` is incorrect. It should be `X_train, X_test, y_train, y_test = train_test_split(X, y, ...)` and then `scaler.fit_transform(X_train)` and `scaler.transform(X_test)`.
3.  **Cross-Validation Preprocessing (Critical):** The `cross_val_score` function in the generated code uses `model.fit(X_train, y_train)` and `cross_val_score(model, X_train, y_train, cv=5)`. This is incorrect because:
    *   It's fitting and scoring *only on the training set*, not the full dataset. Cross-validation is typically performed on the entire dataset `X`, `y` or at least the `X_scaled` dataset (`X_train` in this case *after proper scaling*).
    *   Crucially, `cross_val_score` `model` is the classifier, but the preprocessing (imputation and scaling) needs to be *part of the cross-validation loop* or handled by a `Pipeline` to prevent data leakage. The ground truth correctly uses a `Pipeline` for its cross-validation. The generated code's CV results are thus unreliable.
4.  **Confusion Matrix Visualization:** The generated code uses `plt.imshow` which is a less user-friendly way to display a confusion matrix compared to `seaborn.heatmap` or `sklearn.metrics.ConfusionMatrixDisplay` (as used in the ground truth), especially regarding displaying actual numbers and labels.
5.  **ROC Curve Visualization:** While it plots ROC curves, `RocCurveDisplay.from_estimator` is a more modern and convenient way provided by scikit-learn (as in the ground truth).
6.  **Feature Importance for Logistic Regression and SVM:** The generated code only provides feature importance for Random Forest. It does not handle Logistic Regression (coefficients) and incorrectly states that SVM does not have importance (though direct extraction is harder, it should acknowledge this or suggest alternatives like the ground truth does). The ground truth correctly addresses both LR coefficients and SVM.
7.  **Implicit Assumptions/Lack of Robustness:** The code assumes the file exists without error handling.
8.  **Output of Results:** The `print` statements for results are a bit scattered, whereas the ground truth uses a `pd.DataFrame` for a cleaner summary of all metrics.",llama-4-maverick,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('water_potability.csv')

# Handle missing values
df = df.fillna(df.mean())

# Create new features
df['pH_Chloramines_ratio'] = df['pH'] / df['Chloramines']
df['Hardness_Solids_ratio'] = df['Hardness'] / df['TDS']
df['Sulfate_Chloride_ratio'] = df...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being excellent.

**Strengths:**
*   **Correct Imports:** All necessary libraries are imported.
*   **Basic Feature Engineering:** Creates some ratio, interaction, and polynomial features.
*   **Correlation Analysis:** Calculates and visualizes the correlation matrix, and identifies top 5 correlated features.
*   **Feature Selection:** Implements both SelectKBest and RFE with a RandomForestClassifier.
*   **Visualizations:** Includes correlation heatmap, feature importance plot, and pairwise scatter plots.
*   **Comparison of Feature Sets:** Compares overlap between the selected feature sets.

**Areas for Improvement/Missing Aspects:**

1.  **Missing Value Handling:** The generated code uses `df = df.fillna(df.mean())` which is okay, but it should explicitly mention which columns usually contain NaNs (pH, Sulfate, Trihalomethanes) as in the ground truth. Also, the ground truth copies the dataframe before engineering features, which is good practice.

2.  **Feature Engineering Quality/Completeness:**
    *   **Ratios:** The generated code creates `df['Hardness_Solids_ratio'] = df['Hardness'] / df['TDS']` and `df['Sulfate_Chloride_ratio'] = df['Sulfate'] / df['Chloride']`. The problem statement references `Hardness/Solids` and gives `pH/Chloramines`, `Hardness/Solids` etc., implying specific column names. The dataset `water_potability.csv` has a column named `Solids` not `TDS`, and no `Chloride` column. This indicates a lack of understanding or verification of the actual dataset columns. The Ground Truth uses correct column names (`Solids`, `Trihalomethanes`, `Organic_carbon`, etc.) and creates more diverse and meaningful ratios.
    *   **Interaction Terms:** Only two interaction terms are created (`pH_Sulfate_interaction`, `TDS_Hardness_interaction`). The Ground Truth creates more diverse interaction terms. Again, `TDS` is incorrect.
    *   **Polynomial Features:** Only two polynomial features are created (`pH_squared`, `TDS_squared`). The ground truth uses `PolynomialFeatures` from `sklearn.preprocessing`, which is the correct and more robust way to generate polynomial features for multiple columns, including cross-terms (e.g., `pH*Sulfate` for degree 2). The prompt specifically asked for polynomial features, plural.

3.  **Robustness in Ratio Calculation:** The generated code does not include an `epsilon` to prevent division by zero in ratio calculations, which the ground truth does.

4.  **Correlation Analysis:**
    *   The generated code calculates `corr_with_potability = X.apply(lambda x: x.corr(df['Potability']))`. This is an unusual way to calculate correlations with the target. The standard and more efficient way is to combine `X` and `y` then calculate `.corr()` on the combined DataFrame and extract the 'Potability' column, as demonstrated by the ground truth.
    *   The ground truth also correctly handles excluding 'Potability' itself from the top correlated features.

5.  **Feature Selection:**
    *   The `SelectKBest` and `RFE` target `k=5` features. While this is valid, the problem statement says ""select optimal features,"" which often implies more than just 5. The ground truth sets it to 15, which is a more reasonable number to work with for a benchmark.
    *   **Scaling:** The generated code does not scale the features (`X`) before applying `SelectKBest` and `RFE`. This is a critical omission for `SelectKBest` (especially with statistical tests like `f_classif`) and generally good practice for many machine learning algorithms like RFE's underlying estimator. The ground truth correctly performs `StandardScaler`.

6.  **Visualizations:**
    *   **Feature Importance Plot:** The `x` axis labels are overlapping in the generated plot. The ground truth sorts importances and takes the top 25, making the plot much more readable and informative for a large number of features.
    *   **Pairwise Scatter Plots:** The ground truth sets `hue='Potability'` and uses `diag_kind='kde'` for better visualization, and adjusts the title position. The generated code also has an issue with `TDS` instead of `Solids`, which would lead to an error if `TDS` wasn't present.
    *   The plots in the generated code are basic; the ground truth adds more details like `sns.set_style`, specific `figsize`, `annot=False` for heatmaps to avoid clutter with many features, rotation of x-axis ticks for readability, `tight_layout()`, etc., which define ""advanced visualizations"".

7.  **Comparison of Feature Sets:** While it prints the overlap, the ground truth provides a more comprehensive comparison (overlap, unique to each, union) with clear print statements.

8.  **Structure and Comments:** The Ground Truth is very well-structured with clear comments for each section, making it much easier to understand the flow and intent of the code. The generated code lacks this level of detail.",llama-4-maverick,LLM_judge_config
