task_id,difficulty,category,dataset_reference,generated_code,evaluation_score,evaluation_details,model_config,evaluator_config
E1,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('winequality-red.csv')

# 1) List all column names and identify the data type of each column
column_names = df.columns
column_dtypes = df.dtypes
column_info = df.info()

print(""Column Names and Data Types:"")
for col in column_names:
    print(f""{col}: {column_dtypes[col]}"")

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
descr...",7,"The generated code correctly addresses most of the requirements.

**Strong Points:**
- Properly loads the CSV using pandas.
- Correctly uses `df.select_dtypes` for identifying numerical and categorical/object columns, which is a very efficient and clean way to do it.
- Correctly uses `df.describe()` for numerical statistics.
- Correctly reports total rows and columns using `df.shape`.

**Areas for Improvement:**
- **Column Data Type Identification (Requirement 1):** While `df.dtypes` and printing them works, the ""Identified Type: Numerical/Categorical/etc."" as requested in the task description is not explicitly provided. The ground truth code iterates and prints a more distinct classification (Numerical, Categorical/Ordinal). The generated code just prints the pandas dtype.
- **Categorical/Ordinal Identification (Requirement 4):** The task asks to ""Identify any columns that appear to be categorical or ordinal."" The `winequality-red.csv` dataset contains only numerical columns. The ground truth code explicitly recognizes this and provides a note about the 'quality' column often being treated as ordinal despite being numerical. The generated code's output for this section would simply be an empty list, which is technically correct but doesn't provide the expected nuance for this specific dataset as the ground truth does.
- **Error Handling:** No error handling (e.g., `FileNotFoundError`) is included. While not explicitly asked to handle errors, robust code often includes it, as seen in the ground truth.
- **`df.info()` usage:** `df.info()` is called but its output is not captured or displayed in the final prints, making that line slightly redundant.",mistral-small-24b-instruct,LLM_judge_config
E10,easy,Exploratory Data Analysis (EDA),video_games_sales.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
data = pd.read_csv('video_games_sales.csv')
print(data.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(data.shape)

# 3) List all column names and their data types
print(data.dtypes)

# 4) Display the top 10 games by Global_Sales
top_games = data.sort_values(by='Global_Sales', ascending=False).head(10)
print(top_games[['Name', 'Global_Sales']])

# 5) Show the unique values in the Platform an...",7,"The generated code correctly performs most of the requested tasks.

**Strengths:**
- Correctly loads the dataset and displays the first 10 rows.
- Correctly shows the shape of the dataset.
- Correctly identifies column names and data types (though `df.info()` from ground truth is often preferred for a more comprehensive summary).
- Correctly displays the top 10 games by Global_Sales, selecting relevant columns.
- Correctly shows unique values for 'Platform' and 'Genre'.
- Correctly calculates and displays summary statistics for sales columns.
- Correctly identifies missing values.

**Areas for Improvement:**
- **Robustness (File Not Found):** The generated code lacks error handling for `FileNotFoundError` when `pd.read_csv()` is called. The ground truth includes a `try-except` block, making it more robust.
- **Clarity of Output:** The ground truth code includes print statements to label each section, making the output much clearer and easier to follow, which is crucial for EDA. The generated code presents raw outputs without context.
- **Completeness of `df.info()` vs `df.dtypes`:** While `df.dtypes` fulfills the literal request ""List all column names and their data types,"" `df.info()` provides more comprehensive information, including non-null counts and memory usage, commonly used in EDA.
- **Displaying Top Games:** While the generated code correctly identifies top games, the ground truth explicitly selects `['Name', 'Platform', 'Genre', 'Publisher', 'Global_Sales']` which provides more relevant context for ""top games"" than just `['Name', 'Global_Sales']`.
- **Conditional Checks for Column Existence:** The ground truth includes checks like `if 'Global_Sales' in df.columns:` before attempting operations. While less critical for this specific task where column names are known, it's a good practice for more flexible code.

Overall, it's a functional solution but could benefit from more robust error handling and improved output formatting for better user experience.",mistral-small-24b-instruct,LLM_judge_config
E11,easy,Data Cleaning and Preprocessing,video_games_sales.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('video_games_sales.csv')

# Handle missing values in the Year column by filling them with the median year
df['Year'].fillna(df['Year'].median(), inplace=True)

# Remove games with missing Publisher information
df.dropna(subset=['Publisher'], inplace=True)

# Create a new column 'Years_Since_Release'
df['Years_Since_Release'] = 2023 - df['Year']

# Remove duplicate games based on Name and Platform
df.drop_duplicates(subset=['Name'...",7,"The generated code correctly addresses most of the requirements: loading data, handling missing 'Year' (though with a minor issue), removing missing 'Publisher', creating 'Years_Since_Release', removing duplicates, filtering by 'Global_Sales', and displaying the result.

However, there are a few areas for improvement that prevent a higher score:
1.  **Robustness of 'Year' handling**: The ground truth explicitly converts 'Year' to numeric with `errors='coerce'` before calculating the median and filling NaNs. This handles potential non-numeric entries in the 'Year' column gracefully. The generated code directly calls `median()` and `fillna()` on the column, which might fail if 'Year' contains non-numeric strings from the start (e.g., '1990s').
2.  **Robustness of 'Global_Sales' filtering**: Similar to 'Year', 'Global_Sales' might contain non-numeric values. The ground truth explicitly converts it to numeric and handles potential NaNs introduced by this conversion. The generated code assumes 'Global_Sales' is already numeric, which could lead to errors if it's not.
3.  **Clarity/Readability**: While not explicitly asked, adding `print` statements to show intermediate `df.shape` changes (as in ground truth) is good practice for data processing tasks, making the flow clearer and easier to debug.
4.  **Error Handling**: The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, which is crucial for robust code in real-world scenarios. The generated code lacks this.

Overall, it's a good attempt that covers the core logic but lacks some of the robustness and best practices of a production-ready data cleaning script.",mistral-small-24b-instruct,LLM_judge_config
E12,easy,Data Visualization,video_games_sales.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load the dataset
data = pd.read_csv('video_games_sales.csv')

# 2) Create a bar chart showing the top 10 gaming platforms by total global sales
top_10_platforms = data.groupby('Platform')['Global_Sales'].sum().sort_values(ascending=False).head(10)
plt.figure(figsize=(10, 6))
top_10_platforms.plot(kind='bar')
plt.title('Top 10 Gaming Platforms by Total Global Sales')
plt.xlabel('Platform')
plt.ylabel('Total Global Sales')
plt.xti...",7,"The generated code addresses most of the task requirements and successfully creates the requested visualizations. The individual plots are well-formatted with titles and labels.

**Strengths:**
- Correctly loads the dataset.
- Generates all specified types of plots (bar, pie, histogram, scatter, line).
- Includes appropriate titles and labels for most plots.
- Uses `plt.tight_layout()` for better plot presentation.

**Areas for Improvement (Why it's not higher):**
- **Data Preprocessing/Robustness:** The code assumes 'Year_of_Release' is directly convertible to datetime and does no error handling for file loading or for non-numeric/missing sales/year data. The ground truth includes robust data cleaning for `Year_of_Release` and sales columns, which is crucial for real-world datasets and prevents potential crashes or incorrect plots from malformed data.
- **Line Plot `Year` Conversion:** The `pd.to_datetime(data['Year_of_Release'], format='%Y').dt.year` attempts to convert `Year_of_Release` to a datetime object. However, `Year_of_Release` might already contain non-numeric values or NaNs, which would cause an error or unexpected behavior. The Ground Truth code first converts to numeric, coercing errors, then drops NaNs, which is more robust.
- **Missing `plt.ylabel('')` for Pie Chart:** While it's common practice to remove the y-label for pie charts, the generated code doesn't explicitly mention why or handle it. It just happens to be missing.
- **Minor Enhancements:** The ground truth includes minor enhancements like `plt.grid` for bar, histogram, and line plots, `xticks(rotation=45, ha='right')` for bar charts, and `range` for histograms (though `bins` is sufficient for many histograms). These are not critical failures but add to the clarity and professionalism of the plots.",mistral-small-24b-instruct,LLM_judge_config
E13,easy,Exploratory Data Analysis (EDA),diabetes.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
df = pd.read_csv('diabetes.csv')
print(df.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(df.shape)

# 3) List all column names and their data types
print(df.dtypes)

# 4) Calculate the percentage of diabetic vs non-diabetic patients (Outcome column)
diabetic_count = df['Outcome'].value_counts()[1]
total_count = df['Outcome'].count()
diabetic_percentage = (diabetic_count / total_count) * 100...",7,"The generated code correctly addresses most of the requirements but lacks some robustness and completeness compared to the ground truth.

**Strengths:**
*   **Correctness:**
    *   Successfully loads the dataset and displays the first 10 rows.
    *   Correctly shows the shape of the dataset.
    *   Correctly lists column names and data types (though `df.info()` is often preferred for more detail).
    *   Calculates the percentage of diabetic vs non-diabetic patients accurately.
    *   Correctly calculates missing values.
    *   Provides basic summary statistics.
    *   Identifies zero values.

**Areas for Improvement:**
*   **Robustness/Error Handling:** The ground truth includes `try-except` blocks for file loading, which is good practice. The generated code assumes the file exists.
*   **Clarity of Output:** The ground truth adds descriptive print statements and separators (`--- 1. ...`, `=====`) which significantly improve the readability of the output for exploratory analysis. The generated code just prints the raw results.
*   **Percentage Calculation (Refinement):** While correct, the ground truth's use of `value_counts(normalize=True)` is more concise and directly calculates percentages for all outcomes, which is generally better practice for multi-class outcomes or simply a cleaner way to get both counts and percentages in one go. The generated code also assumes `Outcome` is binary (0/1) and explicitly calculates for 0 and 1.
*   **Data Types Display:** Using `df.dtypes` is fine, but `df.info()` as used in the ground truth provides more comprehensive information, including non-null counts, which is crucial for quick data quality checks.
*   **Specific Zero Value Check:** The ground truth specifically focuses on columns where zero values are *suspicious* (e.g., BloodPressure, BMI) rather than all columns (e.g., Pregnancies can legitimately be zero). The generated code's `(df == 0).sum()` identifies zero values across *all* columns, which includes columns like `Pregnancies` and `Outcome` where 0 is a valid value, not necessarily missing data. This makes the interpretation of the results less clear. The ground truth's approach is more nuanced and aligns better with typical EDA for this specific dataset.
*   **`numpy` import:** While not strictly used directly, it's often a good practice to import `numpy` when working with `pandas` for numerical operations, as `numpy` provides core array operations and types that `pandas` builds upon. The ground truth includes this.",mistral-small-24b-instruct,LLM_judge_config
E14,easy,Data Cleaning and Preprocessing,diabetes.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
import numpy as np

# 1) Load the dataset
data = pd.read_csv('diabetes.csv')

# 2) Identify columns with zero values that represent missing data
columns_with_zero_values = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# 3) Replace zero values in these columns with NaN
data[columns_with_zero_values] = data[columns_with_zero_values].replace(0, np.nan)

# 4) Handle missing values by filling them with t...",6,"The generated code addresses most of the requirements but has critical flaws that prevent it from being production-ready or fully correct for the task.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly identifies and replaces zeros with NaN in specified columns.
*   Correctly fills NaN values with the median of respective columns.
*   Handles duplicate rows.
*   Creates the 'BMI\_Category' column using a logical function.
*   Applies `StandardScaler`.

**Areas for Improvement / Critical Flaws:**

1.  **Scaling Numerical Features (Critical Flaw):** The code attempts to scale *all* numerical columns, including the target variable 'Outcome' and potentially the newly created 'BMI\_Category' if it were numerical (though it's not in this case due to the pandas cut). This is incorrect. The 'Outcome' column should typically *not* be scaled, as it's the target. The ground truth correctly identifies and excludes the 'Outcome' column from scaling. This is a significant issue for a data preprocessing task.
2.  **Robustness / Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. This makes the code less robust in real-world scenarios.
3.  **BMI Categorization Logic:** While the `categorize_bmi` function works, `pd.cut` (as used in the ground truth) is more idiomatic, efficient, and less prone to manual errors for this type of binning task, especially as the number of categories grows.
4.  **Display Requirements:** The generated code displays the shape and head, but without descriptive print statements as requested (""Display the cleaned dataset's shape and first 5 rows"").
5.  **Information Loss/Clarity:** The ground truth includes print statements that provide valuable information to the user (e.g., how many duplicates were removed, which columns were scaled). This improves the user experience and understanding of the code's execution.
6.  **Redundancy in `replace(0, np.nan)`:** While functional, applying `replace(0, np.nan)` to a subset of columns directly using `df[columns_with_zero_values]` is less explicit and potentially less robust than iterating if one of the `columns_with_zero_values` doesn't exist in the DataFrame (though for this specific dataset and problem, it's less likely to be an issue). The ground truth's iterative approach with checks is slightly more defensive.",mistral-small-24b-instruct,LLM_judge_config
E15,easy,Data Visualization,diabetes.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
data = pd.read_csv('diabetes.csv')

# 2) Create a correlation heatmap
plt.figure(figsize=(12, 8))
corr = data.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

# 3) Generate histograms for all numerical features
data.hist(bins=15, figsize=(15, 10), layout=(3, 3))
plt.suptitle('Histogram of Numerical Features')
plt.show()

# 4)...",8,"The generated code correctly addresses most of the task requirements and produces the specified visualizations.

**Strong points:**
*   All required types of visualizations (heatmap, histograms, box plots, pie chart, scatter plots, pair plot) are generated.
*   Proper use of `matplotlib.pyplot` and `seaborn` for plotting.
*   Appropriate titles are included for most plots.
*   The use of `figsize` is good for better visualization.
*   The pair plot correctly uses `hue` for the 'Outcome' variable.

**Areas for improvement:**
*   **Missing labels/clarity:**
    *   The pie chart `labels=outcome_counts.index` will show ""0"" and ""1"", which is less descriptive than the ground truth's ""Non-Diabetic (0)"" and ""Diabetic (1)"".
    *   The scatter plots' legends should be more descriptive (e.g., ""Non-Diabetic"", ""Diabetic"") instead of just ""0"" and ""1"".
    *   Box plot x-axis label could be more descriptive (e.g., ""Diabetes Outcome (0: Non-Diabetic, 1: Diabetic)"").
*   **Correlation Heatmap:** The `data.corr()` includes the 'Outcome' column in the correlation calculation. While not explicitly wrong, the ground truth specifically targets ""relationships between all numerical variables"" *excluding* the `Outcome` for this plot, which is a common approach as Outcome is often considered a target variable to predict, not a feature whose correlation defines relationships among features. The ground truth also explicitly sets `xtick` and `ytick` rotations for better readability.
*   **Histograms:** The `data.hist()` method is quick but offers less control over individual plot aesthetics (like `kde=True` or specific colors/bins per plot) compared to iterating and using `sns.histplot`. The overall `suptitle` is good, but individual plot titles are missing.
*   **Box plots:** The loop `for i, column in enumerate(data.select_dtypes(include=['float64', 'int64']).columns, 1):` includes 'Outcome' itself in the box plots, which doesn't make sense as 'Outcome' is the grouping variable. The ground truth correctly identifies `numerical_cols = df.columns.drop('Outcome')` for these plots.
*   **Error Handling for File Loading:** The ground truth includes `try-except FileNotFoundError` for a more robust loading process; the generated code assumes the file will always be present.
*   **General Aesthetics/Readability:** While not a critical flaw, setting a `sns.set_style()` as in the ground truth can enhance overall plot aesthetics consistency. The ground truth also uses `plt.tight_layout()` more consistently which helps prevent overlap.
*   **Pair Plot features:** The task specified ""key features (Glucose, BMI, Age, Outcome)"". The generated code correctly uses `vars=['Glucose', 'BMI', 'Age']` and `hue='Outcome'`, which fully covers the requirement. The ground truth also explicitly listed `key_features = ['Glucose', 'BMI', 'Age', 'Outcome']` and passed `df[key_features]` to `sns.pairplot`. Both approaches arrive at the same visual.",mistral-small-24b-instruct,LLM_judge_config
E16,easy,Exploratory Data Analysis (EDA),world-happiness-report-2021.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 10 rows
data = pd.read_csv('world-happiness-report-2021.csv')
print(data.head(10))

# 2) Show the shape of the dataset (rows and columns)
print(data.shape)

# 3) List all column names and their data types
print(data.dtypes)

# 4) Display the unique regional indicators and count of countries per region
region_counts = data['Regional indicator'].value_counts()
print(region_counts)

# 5) Show the top 10 happiest countries ba...",7,"The generated code correctly addresses most of the tasks. However, it lacks robustness and user-friendliness compared to the ground truth.

**Areas for Improvement:**
- **Error Handling (File Not Found):** The generated code does not include any error handling (e.g., `try-except` block) for `FileNotFoundError` when reading the CSV. If the file is missing, the script will crash. The ground truth gracefully handles this.
- **Output Clarity/Formatting:** The ground truth code includes clear print statements (e.g., ""1. First 10 rows of the dataset:"") and separators (e.g., ""=""*70) which greatly improve the readability and understanding of the output for each task. The generated code simply prints the output without any descriptive text.
- **Column Name Discrepancy:** For task 6, the generated code uses 'Log GDP per capita', whereas the ground truth correctly identified 'Logged GDP per capita' (which is the actual column name in the dataset). This would lead to a `KeyError` if 'Log GDP per capita' doesn't exist, or incorrect results if it does but isn't the intended column. The ground truth also has a more robust way of checking for existing columns before describing them.
- **`df.info()` vs. `df.dtypes`:** For task 3, `df.info()` from the ground truth provides a more comprehensive summary (including non-null counts and memory usage) which is generally more useful for EDA than just `df.dtypes`.
- **Robustness for Missing Columns:** The ground truth includes checks (`if 'Regional indicator' in df.columns:`) before attempting to access columns, which makes the code more robust against variations in the dataset structure. The generated code assumes all columns will always exist.

While the core logic for most tasks is correct, the lack of error handling, output formatting, and column name robustness prevents it from being a top-tier solution for an expert setting.",mistral-small-24b-instruct,LLM_judge_config
E17,easy,Data Cleaning and Preprocessing,world-happiness-report-2021.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
import numpy as np

# Load the dataset
data = pd.read_csv('world-happiness-report-2021.csv')

# Handling missing values
for column in data.select_dtypes(include=[float, int]).columns:
    data[column].fillna(data[column].mean(), inplace=True)

for column in data.select_dtypes(include=[object]).columns:
    data[column].fillna(data[column].mode()[0], inplace=True)

# Create new derived features
data['Happ...",8,"The generated code correctly addresses most of the task requirements, demonstrating a good understanding of data preprocessing steps.

**Strong points:**
*   Correctly loads the dataset.
*   Handles missing numerical and categorical values appropriately using `mean` and `mode`.
*   Successfully creates 'GDP_Category' using `pd.qcut` and 'Well_being_Score' by averaging the specified columns.
*   Implements IQR method for outlier removal on 'Ladder score'.
*   Correctly standardizes numerical features using `StandardScaler`.
*   Encodes 'Regional indicator' using `LabelEncoder`.
*   Displays the shape and head of the cleaned dataset.

**Areas for improvement:**
*   **Happiness_Category Implementation:** The generated code uses `pd.cut` with `bins=[0, 4, 6.5, 10]` and labels `['Low', 'Medium', 'High']`. While this is a plausible approach, the ground truth uses `np.select` with specific conditions for `>6.5` (High), `4-6.5` (Medium), `<4` (Low). The `pd.cut` implementation effectively handles this, but it implies closed intervals, which might be slightly different interpretation for the exact boundary points compared to the `np.select` (e.g., `Ladder score=4` would be 'Medium' in generated code but might be in the 'Low' bin if the bin is `(0, 4]`). The prompt specifies `>6.5`, `4-6.5`, `<4`, which `np.select` handles more precisely for boundary conditions. For this specific problem, `pd.cut` is acceptable but `np.select` aligns more closely with the explicit conditions given.
*   **Error Handling for File Loading:** The generated code lacks robust error handling for `FileNotFoundError` when loading the CSV, which the ground truth gracefully handles with a `try-except` block. This is a good practice for production-ready code.
*   **Verbosity/Readability:** While not strictly affecting correctness, the ground truth includes more comments and `print` statements during execution, which can be helpful for debugging and understanding the flow, especially in a benchmark scenario where intermediate steps are being evaluated.
*   **`select_dtypes` for scaling**: It's good that all numerical are selected, but after outlier removal and new features creation, the list of numerical features might include newly created categorical features or features already meant for encoding. However, in this specific case, `Happiness_Category` and `GDP_Category` are objects (due to labels) not numerical, and `Regional indicator` is separately encoded, so the `select_dtypes(include=[float, int]).columns` logic is correct to select only numerical columns for scaling.

Overall, the solution is highly functional and meets the core requirements of the task. The minor points are mostly about robustness and adherence to precise boundary conditions for categorical feature creation.",mistral-small-24b-instruct,LLM_judge_config
E18,easy,Data Visualization,world-happiness-report-2021.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import folium
from folium.plugins import MarkerCluster

# Load the dataset
data = pd.read_csv('world-happiness-report-2021.csv')

# Create a world map visualization showing happiness scores by country
world_map = px.choropleth(data,
                          locations='Country name',
                          locationmode='country names',
                          color='Ladder score',...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a strong solution.

**Strengths:**
*   **Dataset Loading:** Correctly loads the CSV file.
*   **World Map:** Implements the world map using `plotly.express` effectively.
*   **Scatter Plots:** Correctly generates the two requested scatter plots.
*   **Box Plots:** Generates the box plots, although there's a minor column name issue.
*   **General Structure:** The code is structured well with separate sections for each visualization.

**Areas for Improvement/Weaknesses:**

1.  **Column Name Inconsistency/Errors:**
    *   The `Regional indicator` column is misspelled as `Reginal indicator` in the bar chart and box plot sections, leading to a `KeyError` or an empty plot. This is a critical logical error.
    *   The radar chart section uses an arbitrary list of categories that don't directly correspond to the dataset's columns or the typical factors (e.g., `'Life Ladder'`, `'Positive affect'` are not the *factors* the task implied for the radar chart, which should compare happiness components). The task implies using the factors *contributing* to happiness, not the 'Ladder score' itself as a factor for the profile.

2.  **Radar Chart Implementation:**
    *   The radar chart is incorrectly implemented as a standard `matplotlib` line plot on a `plt.subplots` object instead of a polar plot (`polar=True`). This fundamentally fails to create a radar chart.
    *   It also lacks normalization of the values, which is crucial for a meaningful radar chart comparison across different scales.
    *   The chosen `categories` for the radar chart are inconsistent and don't make sense for a ""happiness profile"" where contributing factors should be compared.

3.  **Bar Chart:**
    *   The bar chart is created but uses `data.groupby('Reginal indicator').mean()['Ladder score'].plot(kind='barh')`, which relies on pandas' internal plotting function. While it works, explicitly using `seaborn.barplot` (as in the ground truth) often provides more control and consistency with other `seaborn` plots. More importantly, the `Reginal indicator` typo makes this fail.

4.  **Correlation Heatmap:**
    *   The heatmap plots `data.corr()` which includes all numerical columns, even derived ones like 'Standard error of ladder score', 'upperwhisker', 'lowerwhisker', etc., which aren't typically considered ""happiness factors"" for a correlation analysis. The ground truth correctly selects only the major influencing factors.

5.  **Imports:** Includes `folium` and `MarkerCluster` but doesn't use them, which is unnecessary.

6.  **Missing `numpy` Import:** Although not strictly needed for this solution, `numpy` is a standard import for data science tasks and often comes in handy.

7.  **Plot Formatting:** While titles and labels are included, some formatting (e.g., `plt.tight_layout()`, explicit figure sizing, legend placement for scatter plots) could be improved for better presentation, though this is a minor point.

The most significant issues are the spelling errors leading to `KeyError` and the completely incorrect implementation of the radar chart.",mistral-small-24b-instruct,LLM_judge_config
E19,easy,Exploratory Data Analysis (EDA),synthetic_covid19_data.csv,"```python
import pandas as pd

# Load the dataset and parse the 'date' column as datetime objects
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Display the first 10 rows
print(data.head(10))

# Show the shape of the dataset
print(data.shape)

# List all column names and their data types
print(data.dtypes)

# Calculate and display the total sum of 'total_cases' and 'total_deaths' grouped by 'continent'
total_summary = data.groupby('continent')[['total_cases', 'total_de...",7,"The generated code addresses most of the task requirements but misses some finer points relating to robustness and comprehensive output formatting.

**Strengths:**
*   **Correctness:** Most operations are logically correct and produce the expected results for a typical dataset.
*   **Efficiency:** Uses concise pandas operations for grouping, sorting, and aggregation.
*   **Completeness:** Covers nearly all requested EDA steps.

**Areas for Improvement:**

1.  **Error Handling (Missing):** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which is present in the ground truth and good practice.
2.  **Output Readability:** While the ground truth uses `print` statements with descriptive headers (e.g., ""--- First 10 rows of the dataset ---""), the generated code just prints the dataframes/series directly without labels, making the output less intuitive for a user.
3.  **`df.info()` vs `df.dtypes`:** The ground truth uses `df.info()` for ""List all column names and their data types,"" which also provides non-null counts and memory usage, offering a more comprehensive overview than just `df.dtypes`. This is a subtle but important difference for EDA.
4.  **Handling Latest Date for Top Locations (Slightly different logic):** The generated code's `latest_data = data.sort_values(by='date').groupby('location').tail(1)` works, but the ground truth `drop_duplicates(subset=['location'], keep='last')` after sorting by `location` and `date` is arguably slightly more explicit and robust for ensuring *the* latest record per location, especially if there were multiple entries for the same date for a location (though unlikely here). The `nlargest` method is good for finding top cases, but the GT explicitly ensures the `total_cases` are numeric using `pd.to_numeric` before sorting, which is a robustness improvement missing in the generated code.
5.  **Summary Statistics Detail:** The generated code uses `.describe()`, which is good but provides more statistics than strictly requested (it includes count, 25%, 50%, 75% quartiles). The ground truth code explicitly calculates only 'mean', 'median', 'min', 'max', 'std', which directly matches the prompt. Also, the ground truth explicitly converts `new_cases` and `new_deaths` to numeric and drops NaNs before calculating stats, enhancing robustness.
6.  **Edge Cases/Robustness:** The ground truth code includes checks for column existence (`if 'continent' in df.columns...`) before performing operations, which makes it more robust to potentially missing columns in a real-world dataset. This is absent in the generated code.",mistral-small-24b-instruct,LLM_judge_config
E2,easy,Exploratory Data Analysis (EDA),winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# 1) Calculate and report the average (mean) 'quality' of the red wines
average_quality = data['quality'].mean()
print(f""Average quality of the red wines: {average_quality}"")

# 2) Group the wines by their 'quality' score and calculate the average 'alcohol' content for each quality group
average_alcohol_by_quality = data.groupby('quality')['alcohol']....",9,"The generated code correctly addresses all three main requirements of the task: calculating the mean quality, grouping by quality to find average alcohol, and visualizing this relationship with a bar chart.

**Strong points:**
*   Correctly uses pandas for data loading and manipulation (`read_csv`, `mean`, `groupby`).
*   Successfully calculates and prints the average quality.
*   Correctly groups by 'quality' and calculates the mean 'alcohol'.
*   Generates a bar chart using seaborn, as requested, showing the correct relationship between 'quality' and average 'alcohol'.
*   Includes essential imports (`pandas`, `matplotlib.pyplot`, `seaborn`).
*   Sets appropriate labels and a title for the chart.

**Areas for minor improvement:**
*   **Robustness:** The generated code lacks error handling for `FileNotFoundError` if the CSV file isn't present, which the ground truth includes. While not strictly a core data analysis step, it's good practice for production-ready code.
*   **Readability/Presentation (Minor):** The ground truth provides more detailed formatting for the printed average quality (`.2f`) and adds more visual enhancements to the plot (e.g., `sns.set_style`, data labels on bars, `plt.tight_layout`). While the generated code's plot is functional and correct, these additions improve user experience and clarity.
*   **Variable Use in Plotting:** The generated code uses `average_alcohol_by_quality.index` and `average_alcohol_by_quality.values` for plotting. While correct, if `average_alcohol_by_quality` was `reset_index()` as in the ground truth, it would be a DataFrame, and plotting directly with `x='quality', y='alcohol', data=average_alcohol_by_quality` would be slightly more explicit and maintainable, especially if other columns were desired from the grouped data. However, the current approach is perfectly valid for a Series.",mistral-small-24b-instruct,LLM_judge_config
E20,easy,Data Cleaning and Preprocessing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Handle missing values
data['hosp_patients'].fillna(0, inplace=True)
data['icu_patients'].fillna(0, inplace=True)
data['positive_rate'].fillna(data['positive_rate'].mean(), inplace=True)
data['reproduction_rate'].fillna(data['reproduction_rate'].mean(), inplace=True)

# Create new columns
data['mortality_rate'] = (data['total_deaths'] / data['total_cases']...",8,"The generated code correctly performs all the required data cleaning and preprocessing steps.

**Strong points:**
*   **Correctness:** It successfully loads the data, parses the 'date' column, handles missing values as specified, creates both new rate columns with appropriate handling for division by zero (NaNs/infs to 0), and filters the data for the year 2021.
*   **Conciseness:** The code is quite concise and directly addresses the requirements.
*   **Use of Pandas features:** Correctly uses `fillna`, `replace`, and `dt.year` for filtering.

**Areas for improvement (to achieve a higher score):**
*   **Robustness for potential non-numeric data:** The ground truth includes `pd.to_numeric(..., errors='coerce')` for columns like `positive_rate`, `reproduction_rate`, `total_deaths`, `total_cases`, `people_fully_vaccinated`, and `population`. While Pandas often infers data types correctly on load, explicitly converting these columns to numeric (with `errors='coerce'` to turn unparseable values into NaN) before calculations and mean imputation makes the code more robust against unexpected non-numeric data in these columns. The current code assumes these columns are already numeric, which might cause errors if the synthetic dataset has non-numeric entries in those columns.
*   **Clarity in output:** The ground truth separates the shape and head print statements with a newline and adds descriptive text (""Shape of the cleaned dataset (records from 2021):"", ""First 5 rows of the cleaned dataset (records from 2021) with new columns:""). This improves the readability of the output.
*   **File handling:** The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is good practice for robust code, though not explicitly asked for by the prompt.
*   **Minor variable naming:** The ground truth used `df` and `df_2021` to differentiate between the original and filtered dataframe, which can sometimes aid clarity, though `data` is acceptable. The `copy()` method used in the ground truth when filtering (`df[...].copy()`) is also a good practice to avoid `SettingWithCopyWarning` in subsequent operations on the filtered dataframe, even if not strictly necessary for this specific problem (as no further modifications are made after filtering).",mistral-small-24b-instruct,LLM_judge_config
E21,easy,Data Visualization,synthetic_covid19_data.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the dataset and parse the 'date' column
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# 2. Create a time series line plot of 'new_cases' for the location 'India' over the entire period
india_data = data[data['location'] == 'India']
plt.figure(figsize=(10, 6))
plt.plot(india_data['date'], india_data['new_cases'], marker='o')
plt.title('New COVID-19 Cases in India')
plt.xlabel('...",7,"The generated code correctly addresses most of the plotting requirements.
1. Chart types: All requested chart types (time series line plot, bar chart, scatter plot, histogram, box plot) are generated.
2. Data loading and parsing: The `parse_dates` argument in `pd.read_csv` handles the date parsing correctly.
3. Specific plots:
    - Time series for India's `new_cases` is present.
    - Bar chart for `total_deaths` by `continent` is present.
    - Scatter plot for `gdp_per_capita` vs `total_vaccinations` using the latest data is present.
    - Histogram for `reproduction_rate` is present.
    - Box plot for `stringency_index` by `continent` is present.

However, there are several areas for improvement which prevent a higher score:
- **Bar chart for total deaths:** The generated code sums `total_deaths` directly, which is incorrect for a cumulative metric. The ground truth correctly uses the latest `total_deaths` value per country before summing by continent, which is a more accurate representation.
- **Scatter plot:** While it correctly uses `latest_data`, it doesn't handle `NaN` values for `gdp_per_capita` or `total_vaccinations`, which can lead to warnings or incorrect plots if those columns contain `NaN`s. The ground truth uses `dropna` for robustness.
- **Missing `latest_data` for bar chart:** Similar to the scatter plot, for cumulative metrics like `total_deaths`, it's best practice to use the latest recorded value per location before aggregating. The submitted code simply sums all `total_deaths` entries per continent.
- **Aesthetics & Robustness:**
    - `sns.set_theme(style=""whitegrid"")` from the ground truth can enhance plot aesthetics globally.
    - Error handling for `FileNotFoundError` (as in the ground truth) is a good practice for robust code.
    - The use of `sns.lineplot`, `sns.histplot`, `sns.scatterplot`, `sns.boxplot` directly from Seaborn often provides better default aesthetics and options compared to basic matplotlib functions used by the generated code for some plots. For example, `plt.plot` doesn't automatically handle `NaN`s as gracefully as `sns.lineplot`.
    - Plot titles and labels are basic but sufficient, but some specific enhancements (e.g., log scales for scatter plot axes, explicit `grid(True)`) are missing compared to the ground truth.
    - `plt.xticks(rotation=45)` is used, which is good, but `plt.grid` and other fine-tuning are less present.",mistral-small-24b-instruct,LLM_judge_config
E22,easy,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.dtypes)

# Ensure that the 'elixirCost', 'Win Rate', and 'Usage' are recognized as numerical columns
df['elixirCost'] = pd.to_numeric(df['elixirCost'], errors='coerce')
df['Win Rate'] = pd.to_numeric(df['Win Rate'], errors='coerce')
df['Usage'] = pd.to_numeric(df['Usage'], errors='coerce')

#...",7,"The generated code correctly loads the dataset, lists column data types, calculates descriptive statistics for the specified numerical columns, and reports the total number of rows. It also includes an important step of converting columns to numeric types using `pd.to_numeric` with `errors='coerce'`, which is a good practice for robust data handling.

However, there are several areas for improvement:
- **Output Clarity and Formatting:** The ground truth code provides much clearer and more descriptive print statements, making the output more readable and user-friendly. For example, explicitly listing column names, providing clear section headers, and formatting the total number of cards output.
- **Error Handling:** The generated code lacks basic error handling for `FileNotFoundError` or other exceptions during file loading. This makes the code less robust for real-world scenarios.
- **Specific Statistics vs. `describe()`:** While `describe()` is useful, the prompt specifically asked for ""mean, median, standard deviation, min, max"". `describe()` provides these but also includes 'count', '25%', '50%', and '75%' percentiles, which are not explicitly requested. The ground truth's use of `.agg()` to select only the requested statistics is more precise to the prompt's requirements.
- **Redundant `pd.to_numeric` calls:** While good for robustness, in the context of this specific output, the ground truth assumes the columns are already in a state where `describe` can operate on them. If they *weren't* numeric, `describe` would only work on the numeric ones by default. For cases where they might be objects (e.g., strings with numbers), the `to_numeric` helps, but the ground truth avoids redundant explicit conversions, relying on Pandas' type inference or assuming data types are generally correct.
- **Completeness of ""List all column names"":** The prompt asked to ""List all column names and identify the data type of each column."" The generated code only prints `df.dtypes`, which identifies data types and implicitly lists column names, but doesn't explicitly *list* them as the ground truth does (e.g., using a loop).",mistral-small-24b-instruct,LLM_judge_config
E23,easy,Data Manipulation,clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Filter the dataset
filtered_df = df[df['elixirCost'] <= 3]

# Select the 'Card' and 'Win Rate' columns
selected_df = filtered_df[['Card', 'Win Rate']]

# Sort the resulting data by 'Win Rate' in descending order
sorted_df = selected_df.sort_values(by='Win Rate', ascending=False)

# Display the resulting DataFrame
print(sorted_df)
```",9,"The generated code correctly loads the CSV, filters by `elixirCost <= 3`, selects the specified columns, and sorts by `Win Rate` in descending order. It correctly addresses all functional requirements of the task. The code is concise and readable. The main point of minor distinction from the ground truth is the lack of robust error handling (e.g., `FileNotFoundError`, missing column checks) and the absence of clear configuration variables, which could make the code more maintainable for larger projects or more robust in production environments. However, for a simple data science benchmark task, these are often secondary concerns unless explicitly requested.",mistral-small-24b-instruct,LLM_judge_config
E24,easy,Data Visualization,clash_royale_cards.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv('clash_royale_cards.csv')

plt.figure(figsize=(10, 6))
plt.hist(data['elixirCost'], bins=10, edgecolor='k', alpha=0.7)
plt.title('Distribution of Elixir Cost')
plt.xlabel('Elixir Cost')
plt.ylabel('Frequency')
plt.show()
```",8,"The generated code correctly loads the CSV, generates a histogram for `elixirCost`, and includes a title and labeled axes. This fulfills the core requirements of the task. The use of `matplotlib.pyplot.hist` is appropriate.

Areas for improvement:
1.  **Robustness**: The ground truth includes error handling for `FileNotFoundError` and checks if the `elixirCost` column exists. The generated code lacks these checks, making it less robust if the file is missing or the column name is incorrect.
2.  **Binning**: While `bins=10` is given, `elixirCost` is discrete and typically ranges from 1 to 10. `bins=range(1, 12)` as used in the ground truth often provides a more accurate and clear representation for discrete integer values, ensuring each integer cost has its own bar.
3.  **Visualization Enhancement**: The ground truth uses `seaborn.histplot` which often produces more aesthetically pleasing plots by default and offers more convenient options (`kde=False`). It also adds a grid which improves readability. While not strictly required by the prompt, these are good practices for data visualization.
4.  **Labeling**: The Y-axis label ""Frequency"" is correct, but ""Number of Cards"" as used in the ground truth might be slightly more intuitive for this specific context.",mistral-small-24b-instruct,LLM_judge_config
E25,easy,Exploratory Data Analysis (EDA),iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# 1) List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.dtypes)

# 2) Calculate and report basic descriptive statistics for numerical columns
numerical_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
for column in numerical_columns:
    print(f""\nDescriptive Statistics for {column}:"")
    print(df[column].describe())

# 3) Report...",7,"The generated code correctly addresses most of the task requirements. It successfully loads the dataset, identifies data types, calculates descriptive statistics for the specified numerical columns, and reports unique species counts.

However, there are several areas for improvement:
- **Robustness (File Loading):** The ground truth includes a `try-except` block for robust file loading, which is crucial for real-world applications where the file might be missing or corrupted. The generated code lacks this.
- **Column Name Display:** The task explicitly asks to ""List all column names"". While `df.dtypes` indirectly shows column names, `df.columns.tolist()` or `df.columns` provides a direct list which is more explicit.
- **Descriptive Statistics Presentation:** The generated code uses a loop to print `describe()` for each numerical column separately, which can be verbose. The ground truth uses `df[existing_numerical_columns].agg(['mean', 'median', 'std', 'min', 'max'])` or `describe()` on a slice of the DataFrame, providing a more concise and readable summary table for all numerical columns at once. While `describe()` of a specific column is acceptable, the aggregated approach is often preferred for multiple columns.
- **Error Handling for Missing Columns:** The ground truth checks if the specified numerical columns and the 'Species' column actually exist in the DataFrame before attempting operations. The generated code assumes their presence, which could lead to `KeyError` if a column name is misspelled or missing.
- **Output Clarity:** The ground truth adds more descriptive print statements and separators (`print(""-"" * 40 + ""\n"")`) to clearly delineate different sections of the output, making it more user-friendly.",mistral-small-24b-instruct,LLM_judge_config
E26,easy,Data Manipulation,iris.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('iris.csv')

# Filter the dataset for 'Iris-setosa'
filtered_df = df[df['Species'] == 'Iris-setosa']

# Select and display the 'SepalLengthCm' and 'SepalWidthCm' columns
selected_columns = filtered_df[['SepalLengthCm', 'SepalWidthCm']]
print(selected_columns)
```",8,"The generated code correctly performs all requested operations: loading the CSV, filtering by 'Species' == 'Iris-setosa', and selecting/displaying the specified columns. It uses proper pandas syntax and includes the necessary import. The code is concise and directly addresses the prompt.

Areas for minor improvement (which are present in the ground truth but not explicitly requested by the prompt):
1.  **Error Handling**: The generated code lacks explicit error handling for situations like `FileNotFoundError` (if 'iris.csv' doesn't exist) or `KeyError` (if expected columns are missing), which is present in the ground truth and generally good practice for robust code.
2.  **Docstrings/Comments**: While the variable names are clear, a few inline comments or a docstring could enhance long-term readability for complex scripts, though for this simple task, it's less critical.
3.  **File Path Definition**: Defining the `DATA_FILE` path as a constant, as in the ground truth, can make it easier to modify if the file location changes.

Despite these minor points, the code fully satisfies the core requirements of the task.",mistral-small-24b-instruct,LLM_judge_config
E27,easy,Data Visualization,iris.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('iris.csv')

# Create scatter plot
plt.scatter(df['SepalLengthCm'], df['SepalWidthCm'], c=df['Species'].map({'setosa': 'red', 'versicolor': 'blue', 'virginica': 'green'}), marker='o')

# Add title and labels
plt.title('Sepal Length vs Sepal Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')

# Show plot
plt.legend(['Setosa', 'Versicolor', 'Virginica'])
plt.show()
```",6,"The generated code correctly loads the dataset and creates a scatter plot with a title and axis labels. However, it falls short in the critical requirement of coloring points by species effectively and providing a proper legend.

**Strengths:**
*   Correctly imports `pandas` and `matplotlib.pyplot`.
*   Successfully loads `iris.csv`.
*   Correctly uses `SepalLengthCm` for the x-axis and `SepalWidthCm` for the y-axis.
*   Includes a title and axis labels.

**Areas for Improvement:**
*   **Coloring by Species:** The current approach of manually mapping species to colors (`c=df['Species'].map(...)`) is brittle and assumes predefined knowledge of species names and their desired colors. It also doesn't automatically handle potential new species or misspellings. A more robust way to color by species for a scatter plot is often achieved by iterating through unique species as shown in the ground truth, or using `seaborn` which handles categorical coloring more elegantly. More importantly, using `c` and `label` in `plt.scatter` directly with unique values for `c` can be tricky to get a proper legend without iterating.
*   **Legend:** The `plt.legend(['Setosa', 'Versicolor', 'Virginica'])` call is problematic. It provides a text-based legend without associating the colors in the legend with the actual colored points on the plot. To generate a correct legend for different colored groups in `matplotlib`, each `scatter` call usually needs a `label` argument, and then `plt.legend()` is called without arguments to automatically collect these labels. Since all points are plotted in a single `plt.scatter` call here, generating an accurate legend for distinct species with their corresponding colors is not achieved. The ground truth addresses this by making separate `scatter` calls for each species.",mistral-small-24b-instruct,LLM_judge_config
E28,easy,Exploratory Data Analysis (EDA),heart.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('heart.csv')

# List all column names and identify the data type of each column
print(""Column Names and Data Types:"")
print(df.dtypes)

# Calculate and report basic descriptive statistics for numerical columns
numerical_columns = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR']
descriptive_stats = df[numerical_columns].describe()
print(""\nDescriptive Statistics for Numerical Columns:"")
print(descriptive_stats)

# Report the count of ...",7,"The generated code correctly loads the CSV, reports data types, calculates descriptive statistics for the specified numerical columns, and provides value counts for 'HeartDisease'. However, it lacks robust error handling mechanisms, particularly for `FileNotFoundError` when loading the CSV, which is present in the ground truth. It also doesn't explicitly print column names as a separate list, only data types, and the descriptive statistics generated by `.describe()` are complete but not specifically filtered to only mean, median, std, min, and max as requested, though `median` is covered by the '50%' quantile. The ground truth explicitly renames '50%' to 'median' for clarity. The ground truth also includes checks for column existence before performing operations, which adds to its robustness. These minor points impact the overall completeness and robustness compared to the ground truth.",mistral-small-24b-instruct,LLM_judge_config
E29,easy,Data Manipulation,heart.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('heart.csv')

# Filter the dataset
filtered_data = data[(data['Age'] > 50) & (data['HeartDisease'] == 1)]

# Select and display the specified columns
columns_to_display = ['Age', 'Sex', 'ChestPainType', 'Cholesterol']
display_data = filtered_data[columns_to_display]

print(display_data)
```",9,"The generated code correctly implements all the primary requirements of the task:
1. Loads the `heart.csv` dataset.
2. Filters the data based on `Age` > 50 AND `HeartDisease` == 1.
3. Selects and displays the specified columns (`Age`, `Sex`, `ChestPainType`, `Cholesterol`).
4. Includes the necessary `pandas` import.

The code is concise and directly addresses the problem.

Areas for minor improvement (to reach a 10):
- **Error Handling**: The ground truth includes `try-except` blocks for `FileNotFoundError`, `KeyError`, and general exceptions, which is crucial for robust code in real-world scenarios. The generated code lacks this.
- **Function Encapsulation**: The ground truth encapsulates the logic within a function (`analyze_heart_data`), making the code reusable and modular. While not explicitly asked for, it's generally good practice for code that performs a specific task.
- **Comments/Docstrings**: While not explicitly requested, good code often includes comments or docstrings explaining the different steps, especially in a function context.

Overall, it's a very good direct solution to the given prompt. The missing error handling is the main reason for not giving a perfect score.",mistral-small-24b-instruct,LLM_judge_config
E3,easy,Data Filtering,winequality-red.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Filter wines based on the given conditions
filtered_wines = data[(data['alcohol'] > 12) & (data['pH'] < 3.3)]

# Identify the wine with the highest quality score
highest_quality_wine = filtered_wines.loc[filtered_wines['quality'].idxmax()]

# Display all features of the wine with the highest quality score
print(highest_quality_wine)
```",8,"The generated code correctly implements the core logic required by the task: filtering based on two conditions, finding the maximum quality wine in the filtered subset, and displaying its features. It uses appropriate pandas functions (`read_csv`, boolean indexing, `idxmax`, `loc`).

Areas for improvement:
1.  **Error Handling:** The generated code lacks any error handling (e.g., `FileNotFoundError` if the CSV doesn't exist, or `KeyError` if columns are missing). The ground truth includes `try-except` blocks which is a good practice for robust code.
2.  **Edge Cases:** It doesn't explicitly handle the case where `filtered_wines` might be empty (i.e., no wines meet the criteria). In such a scenario, `idxmax()` on an empty Series would raise an error. The ground truth checks `if filtered_wines.empty:`.
3.  **Readability/Structure:** While functional, encapsulating the logic within a function (as in the ground truth) improves reusability and modularity, especially for larger scripts or benchmarks.
4.  **Output Clarity:** The `print(highest_quality_wine)` will display a pandas Series. The ground truth uses `to_string()` for potentially better multi-line display of all features without truncation, though the default `print` is often sufficient. The ground truth also provides more intermediate print statements which can be helpful for debugging or understanding the flow in a benchmark scenario.

Overall, it's a good effort that gets the job done but could be more robust and user-friendly with added error handling and edge case considerations.",mistral-small-24b-instruct,LLM_judge_config
E30,easy,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('heart.csv')

# Generate a histogram for the 'Age' column
plt.figure(figsize=(10, 6))
sns.histplot(df['Age'], bins=20, kde=True)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Generate a bar chart for the 'Sex' column
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Sex')
plt.title('Distribution of Sex')
plt.xlabel('Sex')
plt.ylabe...",8,"The generated code correctly addresses the core requirements of loading the dataset, generating a histogram for 'Age', and a bar chart for 'Sex', with titles and labeled axes. The imports are proper, and the code structure is clean and easy to understand.

Areas for improvement to achieve a higher score include:
1.  **Robustness**: The ground truth includes error handling for `FileNotFoundError` and `KeyError`, which is good practice for data science scripts dealing with external files. The generated code lacks this.
2.  **Mapping 'Sex' values**: The `heart.csv` dataset typically uses 0 and 1 for 'Sex'. The ground truth explicitly maps these to 'Female' and 'Male' for better readability on the bar chart. While `sns.countplot` will technically plot 0 and 1 directly, mapping them improves the clarity of the visualization for the end-user.
3.  **Plot Customization**: The ground truth utilizes more detailed plot customizations (e.g., `fontsize`, `grid`, `tight_layout`, `color`, `palette`) that enhance the visual appeal and readability of the plots. While not strictly required by the prompt's basic criteria, these demonstrate better plotting practices.",mistral-small-24b-instruct,LLM_judge_config
E31,easy,Exploratory Data Analysis (EDA),spotify_2023.csv,"```python
import pandas as pd

# 1) Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 2) List all column names and identify the data type of each
print(df.dtypes)

# 3) Convert the `streams` column to a numeric data type, handling potential errors
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')

# 4) Report any NaNs in the `streams` column
print(df['streams'].isna().sum())

# 5) Calculate and report basic descriptive statistics for specified columns
for column in ['streams',...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
*   Correctly loads the CSV.
*   Correctly identifies column names and data types using `df.dtypes`.
*   Successfully converts the `streams` column to numeric, handling errors with `errors='coerce'`.
*   Correctly counts and reports NaNs in the `streams` column.
*   Calculates and prints descriptive statistics for the specified numerical columns using `describe()`.
*   Correctly counts and reports the total number of unique artists.
*   Properly uses pandas for all data manipulations.

**Areas for Improvement (Why not a 10):**
*   **Error Handling for File Loading:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is robust practice. The generated code assumes the file will always be present.
*   **Reporting Descriptive Statistics:** While `describe()` is called, the ground truth specifically requests mean, median, std, min, max (which `describe()` provides, but the ground truth explicitly filters for `loc[['mean', '50%', 'std', 'min', 'max']]`). The generated code prints the full `describe()` output, which is acceptable but slightly less precise to the exact request for ""basic descriptive statistics (mean, median, std, min, max)"".
*   **Clarity of Output:** The output for descriptive statistics in the generated code is a bit verbose (printing `describe()` for each column separately in a loop). The ground truth consolidates this into a single, well-formatted table, which is generally preferred for readability when comparing multiple columns' statistics.
*   **Initial `streams` dtype reporting:** The ground truth explicitly shows the original and new dtypes for the `streams` column; the generated code only implicitly shows the new dtype. While `df.dtypes` at the beginning shows the original, it's not explicitly tied to the conversion step.
*   **Informational Print Statements:** The ground truth includes more descriptive print statements (e.g., ""--- Column Names and Data Types ---""), making the output more user-friendly and structured. The generated code's output is more direct but less explanatory.",mistral-small-24b-instruct,LLM_judge_config
E32,easy,Data Manipulation,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('spotify_2023.csv')

# Filter for songs released in 2023
filtered_data = data[data['released_year'] == 2023]

# Select the required columns
selected_data = filtered_data[['track_name', 'artist(s)_name', 'streams']]

# Convert the `streams` column to numeric
selected_data['streams'] = pd.to_numeric(selected_data['streams'], errors='coerce')

# Sort the data by the `streams` column in descending order
sorted_data = selected_data....",8,"The generated code correctly addresses all requirements outlined in the task. It loads the CSV, filters by `released_year`, selects the specified columns, correctly converts the `streams` column to numeric with error coercion, and sorts the data in descending order.

**Strong Points:**
- All required steps are implemented.
- Uses `pandas` effectively for data manipulation.
- Correctly handles potential non-numeric values in the `streams` column using `errors='coerce'`.
- Efficient and concise.

**Areas for Improvement:**
- The generated code performs the `pd.to_numeric` conversion *after* filtering and selecting columns. While this works, it's generally better practice to ensure data types are correct for critical columns *before* filtering or sorting, especially if the filtering or sorting criteria depend on that column's type. In this specific case, `released_year` is an integer and `streams` is only used for sorting *after* filtering, so the order doesn't cause functional issues. However, if `streams` were part of the filtering logic, doing `to_numeric` earlier would be essential.
- The `SettingWithCopyWarning` could potentially arise for `selected_data['streams'] = pd.to_numeric(...)` if `selected_data` is a view rather than a copy. While often harmless in simple cases, explicitly using `.loc` for assignment after selection can prevent this and make the intent clearer (e.g., `selected_data.loc[:, 'streams'] = pd.to_numeric(...)`). The ground truth code handles this by performing the `to_numeric` operation on the original DataFrame before any slicing, which is a robust way to avoid this warning.
- Adding basic error handling for `FileNotFoundError` as seen in the ground truth would make the code more robust, though it wasn't an explicit requirement of the prompt beyond ""assume the CSV file is in the current directory.""",mistral-small-24b-instruct,LLM_judge_config
E33,easy,Data Visualization,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('spotify_2023.csv')

# Generate a histogram for the 'bpm' column
plt.figure(figsize=(10, 6))
plt.hist(data['bpm'], bins=30, edgecolor='k', alpha=0.7)
plt.title('Distribution of BPM')
plt.xlabel('BPM')
plt.ylabel('Frequency')
plt.show()

# Create a scatter plot visualizing the relationship between 'danceability_%' and 'energy_%'
plt.figure(figsize=(10, 6))
plt.scatter(data['danceability_%'], data[...",9,"The generated code successfully loads the dataset, generates the required histogram and scatter plot with proper titles and axis labels. It demonstrates correct usage of pandas and matplotlib.pyplot for the given task.

Strong points:
- Correctly loads the CSV file.
- Generates a histogram for `bpm` with a title and labeled axes.
- Creates a scatter plot for `danceability_%` vs `energy_%` with a title and labeled axes.
- Uses appropriate imports.

Areas for minor improvement (not critical for this task but good practice):
- The `plt.show()` calls after each plot will display them sequentially. For a single script producing multiple plots, it's often better to call `plt.show()` only once at the very end after all figures have been created, so they can be displayed simultaneously.
- While the axis labels are good, adding units or more descriptive text (e.g., 'Distribution of BPM in Spotify Tracks' instead of just 'Distribution of BPM') can enhance clarity.
- Error handling for `FileNotFoundError` or `KeyError` (if a column is missing) as seen in the ground truth is a good practice for robust code, though not explicitly requested in the prompt.",mistral-small-24b-instruct,LLM_judge_config
E34,easy,Exploratory Data Analysis (EDA),youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('youtube_trending.csv')

# List all column names and their inferred data types
print(data.dtypes)

# Clean the 'Video views', 'Likes', and 'Dislikes' columns
clean_columns = ['Video views', 'Likes', 'Dislikes']
for column in clean_columns:
    data[column] = data[column].str.replace(',', '').astype(float, errors='ignore')

# Fill NaNs with 0
data[clean_columns] = data[clean_columns].fillna(0)

# Calculate and report basic descr...",6,"The generated code addresses most of the requirements but has some significant issues.

**Correctness:**
- **File Name:** The generated code attempts to load `youtube_trending.csv` instead of `youtube_trendings.csv` (missing 's'). This will lead to a `FileNotFoundError`. The ground truth includes robust error handling for this.
- **`astype(float, errors='ignore')`:** This is problematic. `errors='ignore'` will simply return the original object if a conversion fails, it will not coerce to `NaN`. The correct way to handle this is `pd.to_numeric(..., errors='coerce')`, as shown in the ground truth. This is a critical error in handling the numeric conversion and NaN generation.
- **Order of operations for NaNs:** The removal of commas, conversion to float, and filling NaNs for the numeric columns are partially incorrect. The `astype(float, errors='ignore')` won't produce NaNs correctly, and thus the `fillna(0)` might not work as intended for invalid values. The ground truth correctly uses `pd.to_numeric(errors='coerce')` to create NaNs for unconvertible values *before* filling them.

**Code Quality:**
- **Clarity:** It's reasonably clear.
- **Robustness:** Lacks error handling for file not found.
- **Output:** The output format for column names and dtypes is `print(data.dtypes)`, which is less informative than `df.info()` used in the ground truth for displaying dtypes alongside non-null counts.

**Completeness:**
- **Load CSV:** Yes, but with the wrong filename.
- **List column names/dtypes:** Yes, but using `data.dtypes` which is less comprehensive than `df.info()`.
- **Clean numeric columns:** Attempts to, but the `astype(float, errors='ignore')` mechanism is flawed for this task.
- **Descriptive statistics:** Yes, correctly uses `.describe()`.
- **Fill 'Category' NaNs:** Yes.
- **Unique 'Category' count:** Yes.

To improve, the code needs to correct the `astype` to `pd.to_numeric` with `errors='coerce'`, ensure the correct filename is used, and ideally add file-not-found error handling.",mistral-small-24b-instruct,LLM_judge_config
E35,easy,Data Manipulation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean and convert the 'Video views' column
df['Video views'] = df['Video views'].str.replace(',', '').replace('', 'NaN').apply(pd.to_numeric, errors='coerce').fillna(0)

# Filter dataset
filtered_df = df[df['Video views'] > 50000000]

# Select and display the 'Video' and cleaned 'Video views'
result_df = filtered_df[['Video', 'Video views']]

# Sort by 'Video views' in descending order
result_df_sorted...",9,"The generated code correctly addresses almost all requirements of the task. It loads the dataset, cleans the 'Video views' column by removing commas, converting to numeric, and filling NaNs with 0. It then filters the data, selects the specified columns, and sorts them correctly. The main strength is the conciseness of the cleaning step.

One minor point of improvement could be the chaining of string operations and numeric conversion. The use of `.replace('', 'NaN')` is a bit unusual and might not be strictly necessary if `pd.to_numeric` with `errors='coerce'` is used, which correctly handles non-numeric strings as NaN. However, for an empty string after comma removal, it might be an attempt to explicitly make it a NaN before `pd.to_numeric` would. The `astype(str)` is implicitly handled when `str.replace` is called on a Series that might contain numbers (if `pd.read_csv` infers numbers for some rows of 'Video views'). In this case, `df['Video views'].str.replace(',', '')` would work directly if the column is initially string-like. Nevertheless, the outcome is correct. The ground truth's explicit `astype(str)` is slightly more robust if the column could be mixed types initially.

Another minor point is the lack of a `try-except` block for file loading, which is good practice for robustness, as shown in the ground truth. However, the prompt did not explicitly ask for error handling for file not found, so it's not a critical omission that would significantly lower the score.",mistral-small-24b-instruct,LLM_judge_config
E36,easy,Data Visualization,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('youtube_trendings.csv')

# Clean the 'Likes' and 'Video views' columns
data['Likes'] = pd.to_numeric(data['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
data['Video views'] = pd.to_numeric(data['Video views'].str.replace(',', ''), errors='coerce').fillna(0)

# Create the scatter plot
plt.scatter(data['Video views'], data['Likes'])

# Add title and labels
plt.title('Video Views vs. Lik...",9,"The generated code correctly loads the dataset, cleans the specified columns by removing commas, converting to numeric, and filling NaNs with 0. It also correctly creates a scatter plot with the specified title and axis labels. The code is concise and directly addresses all requirements of the task.

One minor improvement could be to include error handling for the `pd.read_csv` call (e.g., using a try-except block for `FileNotFoundError`), which would make the code more robust, as in the ground truth. However, the task prompt did not explicitly ask for error handling for file loading, but implied it by saying ""assume the CSV file is in the current directory"". The lack of error handling for file IO is a common omission in rapid data analysis scripts and not a critical functional error based on the prompt's explicit requirements for data processing and plotting.",mistral-small-24b-instruct,LLM_judge_config
E37,easy,Exploratory Data Analysis (EDA),mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'Mobile Weight' column
df['Mobile Weight'] = pd.to_numeric(df['Mobile Weight'].str.replace('g', ''), errors='coerce')

# Clean the 'RAM' column
df['RAM'] = pd.to_numeric(df['RAM'].str.replace('GB', ''), errors='coerce')

# Clean the 'Battery Capacity' column
df['Battery Capacity'] = pd.to_numeric(df['Battery Capacity'].str.replace('mAh', ''), errors='coerce')

# Clean the 'Launched Price (USA)' co...",8,"The generated code correctly addresses all the requirements of the task. It loads the CSV, cleans the specified columns (`Mobile Weight`, `RAM`, `Battery Capacity`, `Launched Price (USA)`) by removing units and special characters, and converts them to numeric types, handling errors by coercing to NaN. It then correctly lists the data types, calculates `describe()` statistics for the cleaned columns, and reports the count of unique values in the `Company Name` column.

**Strong points:**
- All cleaning steps are correctly implemented, including handling spaces and multiple replacements for 'Launched Price (USA)'.
- `errors='coerce'` is used for all `pd.to_numeric` conversions, which is crucial for handling non-convertible values gracefully.
- All required outputs (dtypes, descriptive statistics, unique count) are produced.

**Areas for improvement (minor):**
- While `df[col].describe()` provides mean, median (as 50%), std, min, and max, the prompt specifically requested *mean, median, std, min, max* to be reported. A more precise output tailored to these specific metrics (e.g., using `df[col].mean()`, `df[col].median()`, etc. or filtering the `.describe()` output) would have been slightly better alignment with the prompt's explicit list. However, `describe()` does contain all these, so it's a very minor point.
- The prompt specified ""Provide only the Python code without explanations,"" and the generated code includes comments before each cleaning step (`# Clean the ... column`). Removing these would make it perfectly align with the ""no explanations"" constraint, though these are benign and not extensive.
- No explicit `numpy` import, but it's not strictly necessary for this task as `pandas` handles the numeric operations.
- The use of `str.strip()` after `replace` operations could be beneficial to handle leading/trailing spaces in the raw data, although the current code passes the test case. For instance, `df['Mobile Weight'].str.replace('g', '').str.strip()` would be more robust.",mistral-small-24b-instruct,LLM_judge_config
E38,easy,Data Manipulation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# Clean the 'Launched Price (India)' column
data['Launched Price (India)'] = data['Launched Price (India)'].str.replace('INR ', '').str.replace(',', '').replace('[^0-9]', '', regex=True).astype(float, errors='ignore')

# Clean the 'RAM' column
data['RAM'] = data['RAM'].str.replace('GB', '').replace('[^0-9]', '', regex=True).astype(float, errors='ignore')

# Filter the dataset
filtered_data = data[(data['Com...",9,"The generated code correctly performs all the required steps: loading the data, cleaning both specified columns (with correct error handling for `Launched Price (India)` and `RAM`), filtering by company and price, selecting the correct columns, and sorting the final output. The use of `astype(float, errors='ignore')` for cleaning correctly handles conversion errors by setting them to NaN, which aligns with the requirement. The regex `[^0-9]` in the `replace` calls after cleaning the prefix/suffix is an interesting addition, seemingly aiming to remove other non-numeric characters, which is generally a good robustness measure, but potentially unnecessary given the `astype(float, errors='ignore')`. One minor point is the `reset_index(drop=True)` which was not explicitly requested but doesn't harm the output. The ground truth includes more robust error handling for file loading, but the prompt only requested the core data manipulation logic. Overall, a very good solution.",mistral-small-24b-instruct,LLM_judge_config
E39,easy,Data Visualization,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# Clean the `RAM` column
data['RAM'] = pd.to_numeric(data['RAM'].str.replace('GB', ''), errors='coerce')

# Clean the `Battery Capacity` column
data['Battery Capacity'] = pd.to_numeric(data['Battery Capacity'].str.replace('mAh', ''), errors='coerce')

# Create a scatter plot
plt.figure(figsize=(10, 6))
scatter_plot = sns.scatterplot(data=data, x='RAM', y...",9,"The generated code successfully loads the dataset, cleans the 'RAM' and 'Battery Capacity' columns by removing units and converting to numeric, handles conversion errors, and generates a scatter plot. It correctly colors the points by 'Company Name' and includes a title and axis labels, satisfying the core requirements of the task.

Strengths:
- Correctly loads the CSV.
- Accurately cleans both 'RAM' and 'Battery Capacity' columns using `str.replace` and `pd.to_numeric` with `errors='coerce'`, demonstrating proper error handling for non-numeric conversions.
- Generates a scatter plot with the correct x and y axes.
- Uses `hue='Company Name'` to color points as required.
- Includes a title and correct axis labels.
- Uses `seaborn` for plotting, which is good practice for data visualization.

Areas for minor improvement (preventing a perfect 10):
- The generated code performs the cleaning in-place (`data['RAM'] = ...`), which modifies the original columns. While this works for the task, creating new cleaned columns (e.g., `RAM_cleaned`) as shown in the ground truth is often safer practice, especially if the original, uncleaned data might be needed later or if chaining operations.
- The `plt.legend(title='Company Name')` call is redundant after `hue='Company Name'` in `sns.scatterplot` as seaborn automatically adds the legend with the correct title. It doesn't cause harm but isn't strictly necessary.
- Does not include error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth. While not explicitly asked for in the prompt's requirements, robust code would include it. However, the prompt did say ""assume the CSV file is in the current directory"", so this isn't a strict failure.

Overall, it's a very good and correct solution to the problem.",mistral-small-24b-instruct,LLM_judge_config
E4,easy,Exploratory Data Analysis (EDA),titanic.csv,"```python
import pandas as pd

# 1) Load the dataset and display the first 5 rows
df = pd.read_csv('titanic.csv')
print(df.head())

# 2) Show the shape of the dataset (rows and columns)
print(df.shape)

# 3) List all column names and their data types
print(df.dtypes)

# 4) Calculate the survival rate (percentage of passengers who survived)
survival_rate = df['Survived'].mean() * 100
print(f'Survival Rate: {survival_rate:.2f}%')

# 5) Show the count of missing values for each column
print(df.isnu...",8,"The generated code correctly addresses almost all requirements of the task.

Strong points:
- **Correctness**:
    - Loads the dataset successfully.
    - `df.head()`, `df.shape`, `df.isnull().sum()` are all correctly used for their respective purposes.
    - Calculating survival rate using `df['Survived'].mean() * 100` is a very concise and correct way to get the percentage of survivors, as `mean()` of a binary (0/1) column directly gives the proportion of 1s.
- **Completeness**: All five requirements are met.
- **Code Quality**: Proper import (`pandas as pd`). The code is clean and concise.

Areas for improvement to reach a perfect score:
- **Output Clarity/Formatting**: The ground truth includes print statements that explicitly state what each output block represents (e.g., ""1. First 5 rows of the dataset:""). The generated code simply prints the output without any descriptive text. While functional, adding these labels improves readability and user experience significantly for an EDA script.
- **Error Handling**: The ground truth includes a `try-except FileNotFoundError` block for loading the CSV, which is robust practice. The generated code assumes the file exists, which is acceptable for a direct benchmark but less robust for production use.
- **Data Types Display**: The prompt asked to ""List all column names and their data types"". `df.dtypes` correctly lists the column names and their data types, so this is technically met. However, `df.info()` from the ground truth provides a more comprehensive summary that also includes non-null counts and memory usage, which is generally more useful for EDA insights into data quality. For this specific prompt, `df.dtypes` is sufficient.",mistral-small-24b-instruct,LLM_judge_config
E40,easy,Exploratory Data Analysis (EDA),imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 1. List all column names and their data types
print(df.info())

# 2. Calculate and report basic descriptive statistics for the numerical columns Year, Rating, and Runtime
numerical_columns = ['Year', 'Rating', 'Runtime']
descriptive_stats = df[numerical_columns].agg(['mean', 'median', 'std', 'min', 'max'])
print(descriptive_stats)

# 3. Report the total number of movies in the dataset
total_movies = df.shap...",6,"The generated code correctly addresses most parts of the task but has significant limitations and potential issues compared to the ground truth.

**Strengths:**
*   Properly loads the CSV using pandas.
*   Correctly identifies the `df.info()` method for displaying column names and data types.
*   Uses `df.shape[0]` to accurately report the total number of movies.
*   Attempts to calculate descriptive statistics for the specified numerical columns using `agg()`, which is a valid approach.

**Areas for Improvement / Limitations:**
*   **Robustness (Error Handling):** The generated code lacks any error handling for `FileNotFoundError` when loading the CSV. The ground truth includes a `try-except` block to gracefully handle this common issue, which is crucial for production-ready code.
*   **`Runtime` Column Preprocessing:** This is the most critical deficiency. The `Runtime` column in the `imdb_top_250.csv` dataset is typically in a string format (e.g., ""142 min""). The generated code directly tries to calculate statistics on it without any preprocessing, which will likely raise an error because it's not a numeric type. The ground truth explicitly handles this by removing "" min"" and converting it to numeric.
*   **Descriptive Statistics Output:** While `df[numerical_columns].agg(['mean', 'median', 'std', 'min', 'max'])` *attempts* to get the stats, if 'Runtime' is not processed, it will fail. Even if `Year` and `Rating` are numeric, `agg` will produce a different output format than `.describe().loc[...]`, and will put the statistics as rows, not columns, which is a less common and harder-to-read format for these specific stats. The task implied a standard descriptive statistics report. The ground truth's use of `.describe().loc[['mean', '50%', 'std', 'min', 'max']]` provides a much cleaner, more standard, and directly comparable output for the requested statistics.
*   **Print Statements for `df.info()`:** `print(df.info())` is redundant; `df.info()` itself prints the information and returns `None`. While not a major error, it's slightly less clean if you're trying to only print the info once.
*   **Clarity for Descriptive Statistics:** The print statement for descriptive statistics is simply `print(descriptive_stats)`. The ground truth includes a header `--- Descriptive Statistics for Numerical Columns ---` for better readability and context.

**Reasoning for Score:**
The score of 6 reflects that the code partially solves the problem, but it fails on a core aspect of the problem (preprocessing `Runtime` to calculate statistics) and lacks basic robustness. The lack of `FileNotFoundError` handling makes it fragile, and the inability to handle the `Runtime` column as typically formatted makes it incomplete for the specified numerical columns.",mistral-small-24b-instruct,LLM_judge_config
E41,easy,Data Manipulation,imdb_top_250.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Filter movies with a Rating greater than or equal to 9.0
filtered_df = df[df['Rating'] >= 9.0]

# Select the Title and Rating columns
selected_df = filtered_df[['Title', 'Rating']]

# Sort the resulting data by Rating in descending order
sorted_df = selected_df.sort_values(by='Rating', ascending=False)

# Display the sorted data
print(sorted_df)
```",8,"The generated code correctly implements all the functional requirements of the task: loading the CSV, filtering by rating, selecting specific columns, and sorting. The imports are proper, and the code structure is logical and readable.

Areas for improvement (preventing a higher score):
1.  **Error Handling:** The generated code lacks robust error handling mechanisms (e.g., `FileNotFoundError`, `KeyError` for missing columns), which are present in the ground truth. In a real-world scenario, this is crucial for making the script resilient.
2.  **Clarity/Comments:** While the code is straightforward, adding a few inline comments or more descriptive variable names (like the ground truth's `high_rated_movies` vs. `filtered_df`) could marginally improve clarity for complex operations or multiple users.
3.  **Dataset Path:** The ground truth defines `DATASET_PATH` as a constant, which is a good practice for maintainability. The generated code directly embeds the string.

Overall, it's a very good solution that fully addresses the core logical requirements of the prompt. The missing error handling is the primary reason for not giving a perfect score.",mistral-small-24b-instruct,LLM_judge_config
E42,easy,Data Visualization,imdb_top_250.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Create a histogram for the Rating column
plt.figure(figsize=(10, 6))
sns.histplot(data['Rating'], kde=True)
plt.title('Distribution of Movie Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

# Create a scatter plot for Rating vs Runtime
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Runtime', y='Rating', data=data)
plt.title('Relations...",7,"The generated code correctly addresses the core plotting requirements: loading the dataset, creating a histogram for 'Rating', and a scatter plot for 'Rating' vs 'Runtime', with appropriate titles and labels. The use of seaborn and matplotlib is correct.

However, there are two significant areas for improvement, which prevent a higher score:
1.  **Robustness to `FileNotFoundError`**: The code does not handle the case where `imdb_top_250.csv` is not found, which is a common issue and is handled in the ground truth. This makes the code less robust for real-world use.
2.  **Handling of `Runtime` column**: The `Runtime` column in the `imdb_top_250.csv` dataset typically contains strings like ""120 min"". The generated code directly uses this column for plotting without parsing or converting it to a numeric type. This will likely result in a `TypeError` or incorrect plot behavior as `matplotlib` or `seaborn` cannot directly plot string values as numerical axes. The ground truth code correctly preprocesses this column. This is a critical oversight given it's a data visualization task and assumes clean numeric data for `Runtime`.

Strong points include:
*   Correct use of `pandas` for loading.
*   Correct application of `sns.histplot` and `sns.scatterplot`.
*   Proper plotting with titles and axis labels.
*   Sensible `figsize` settings.",mistral-small-24b-instruct,LLM_judge_config
E43,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Report the number of missing values (NA) for each column
missing_values = penguins.isna().sum()
print(""Number of missing values in each column:\n"", missing_values)

# Drop rows with missing values in the specified numerical columns
penguins_cleaned = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# Calculate and report basic descriptive statistics for ...",7,"The generated code correctly performs most of the requested tasks.

**Strengths:**
- Correctly loads the CSV using pandas.
- Accurately reports missing values per column (`isna().sum()`).
- Correctly filters rows based on NAs in the four specified numerical columns using `dropna(subset=...)`.
- Calculates descriptive statistics for the numerical columns on the filtered data using `.describe()`, which conveniently provides mean, std, min, max, and other quartiles.
- Correctly counts unique values for 'species' and 'island' columns using `nunique()`.
- Includes proper import for pandas.

**Areas for Improvement (Why not a higher score):**
1.  **Reporting specific statistics:** While `.describe()` provides mean, std, min, and max, it also provides count, 25%, 50% (median), and 75% quartiles. The request was specifically for ""mean, median, standard deviation, min, max"". The `.describe()` method outputs more than requested and doesn't explicitly label the median as such (it's the 50% percentile). The ground truth used `.agg(['mean', 'median', 'std', 'min', 'max'])` which is more precise in fulfilling the specific request for *only* those statistics.
2.  **Robustness (File Handling):** The generated code does not include any error handling for `FileNotFoundError` or other potential issues when loading the CSV, which the ground truth does. This makes it less robust for real-world scenarios.
3.  **Clarity of Output:** The ground truth provides clearer print statements separating different sections of the output and explaining what is being reported, which enhances readability and user understanding. For example, explicitly stating ""--- Missing Values Report ---"" or ""--- Descriptive Statistics for Filtered Numerical Columns ---"".
4.  **Reporting of original/filtered rows:** The ground truth also reports the number of rows before and after filtering, which provides valuable context about the impact of the `dropna` operation. The generated code does not include this.

Overall, it's a good attempt that addresses the core requirements technically, but lacks some robustness, precision in output, and user-friendly reporting that the ground truth demonstrates.",mistral-small-24b-instruct,LLM_judge_config
E44,easy,Data Manipulation,penguins.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('penguins.csv')

# Filter the dataset
filtered_df = df[(df['species'] == 'Adelie') & (df['island'] == 'Torgersen')]

# Select the required columns
selected_df = filtered_df[['culmen_length_mm', 'culmen_depth_mm']]

# Drop rows with missing values
cleaned_df = selected_df.dropna()

# Display the first 5 rows
print(cleaned_df.head())
```",9,"The generated code correctly implements all the requirements of the task. It properly loads the CSV, filters by species and island, selects the specified columns, drops NaNs, and displays the first 5 rows. The code is structured well and uses appropriate pandas functions.

The only minor point for improvement, which prevents a perfect score, is in the `dropna()` call. While `selected_df.dropna()` will work correctly here because `selected_df` *only* contains `culmen_length_mm` and `culmen_depth_mm`, specifying `subset=['culmen_length_mm', 'culmen_depth_mm']` as in the ground truth is more explicit and robust, especially if the `selected_df` were to contain other columns by mistake in a larger pipeline. However, given the clear selection of only those two columns just before, `dropna()` implicitly achieves the desired outcome. The ground truth also includes robust error handling for file loading, which the generated code omits. While not explicitly requested in the prompt, it's a good practice.",mistral-small-24b-instruct,LLM_judge_config
E45,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('penguins.csv')

# Filter out rows with missing values in specified columns
data_cleaned = data.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'species'])

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data_cleaned, x='culmen_length_mm', y='culmen_depth_mm', hue='species', palette='viridis')

# Add title and labels
plt.title('Scatter Plot ...",9,"The generated code successfully addresses almost all requirements of the task. It correctly loads the dataset, filters out missing values for the specified columns, creates a scatter plot of `culmen_length_mm` versus `culmen_depth_mm` colored by `species`, and includes a title and axis labels. Imports are proper.

The only minor point of improvement is that the ground truth wrapped the plotting logic in a function and included error handling for `FileNotFoundError`, which is good practice for robust code but not explicitly requested as a requirement for *this specific task* (which focused on the plotting logic itself). The generated code's more direct approach is perfectly acceptable given the prompt's simplicity. The use of `palette='viridis'` in the generated code is a nice touch for color diversity, even if not explicitly requested.",mistral-small-24b-instruct,LLM_judge_config
E46,easy,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Step 1: Read the dataset
data = pd.read_csv('penguins.csv')

# Step 2: Fill any NA values with the mean of their respective columns
data['culmen_length_mm'].fillna(data['culmen_length_mm'].mean(), inplace=True)
data['culmen_depth_mm'].fillna(data['culmen_depth_mm'].mean(), inplace=True)

# Step 3: Calculate and print the mean, median, and standard deviation
culmen_length_mm = data['culmen_length_mm']
culmen_depth_mm = data['culmen_depth_mm']

mean_length = culmen...",8,"The generated code correctly addresses all the requirements of the task: loading the dataset, filling NA values in the specified numerical columns with their respective means, calculating and printing the mean, median, and standard deviation for these cleaned columns, and finally, printing the total number of rows and columns.

**Strengths:**
*   **Correctness:** All calculations and data manipulations are performed correctly.
*   **Clarity:** The code is straightforward and easy to understand.
*   **Completeness:** All four steps outlined in the task are addressed.
*   **Efficiency:** The `inplace=True` argument for `fillna` is used effectively.

**Areas for Improvement (to achieve a higher score):**
*   **Robustness:** The code assumes the `penguins.csv` file always exists. Adding a `try-except` block for `FileNotFoundError` during `pd.read_csv` would make it more robust, similar to the ground truth.
*   **Generalization/Readability:** While the task specified only two columns, hardcoding the column names multiple times (for NA filling and for stat calculation/printing) makes the code less flexible. Using a loop over a list of `NUMERICAL_COLUMNS` (as seen in the ground truth) would be more scalable and cleaner, especially for the statistics printing part, avoiding repetitive `print` statements.
*   **Output Formatting:** The output for the statistics is on a single line, which can be harder to read compared to the multi-line, formatted output in the ground truth. Providing more descriptive labels and formatting (e.g., using f-strings with precision specifiers like `:.2f`) would improve readability.
*   **Error Handling for Columns:** The code assumes `culmen_length_mm` and `culmen_depth_mm` columns always exist. In a real-world scenario, checking for column existence before attempting operations on them would add robustness.",mistral-small-24b-instruct,LLM_judge_config
E47,easy,Data Visualization,penguins.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Drop rows where `flipper_length_mm` is NA
data = data.dropna(subset=['flipper_length_mm'])

# 3) Create a histogram of the `flipper_length_mm` column
plt.figure(figsize=(10, 6))
sns.histplot(data['flipper_length_mm'], kde=False, bins=30)
plt.title('Distribution of Flipper Length (mm)')
plt.xlabel('Flipper Length (mm)')
plt.ylabel('Frequency')
plt.sho...",9,"The generated code successfully addresses all the requirements of the task. It correctly loads the data, handles NA values in `flipper_length_mm`, and generates both the histogram and the bar chart with appropriate titles and axis labels. The use of seaborn's `histplot` and `countplot` is efficient and good practice.

**Strong points:**
*   Correctly imports necessary libraries (`pandas`, `matplotlib.pyplot`, `seaborn`).
*   Loads the dataset as specified.
*   Correctly drops rows with NA values in the `flipper_length_mm` column.
*   Generates a histogram for `flipper_length_mm` with the correct title and x-axis label. It uses `sns.histplot` effectively.
*   Generates a bar chart for `island` counts with the correct title and axis labels. It uses `sns.countplot` efficiently.
*   Includes `plt.show()` after each plot, which is good practice.

**Minor areas for improvement:**
*   The ground truth includes error handling for `FileNotFoundError` when loading the CSV, which makes the code more robust. The generated code lacks this. This is a common practice in production-ready code.
*   The ground truth adds more detailed plot enhancements like `fontsize` for labels and titles, `plt.tight_layout()`, and grid lines, which improve readability and professionalism. While not strictly required by the prompt, they are good additions for visualization tasks.
*   The y-axis label for the histogram in the generated code is 'Frequency', while typically for `sns.histplot` without `stat='density'`, it's 'Count' (as in the ground truth). Although 'Frequency' is not entirely wrong, 'Count' is more precise for the default behavior.",mistral-small-24b-instruct,LLM_judge_config
E48,easy,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Filter the data to include only penguins from the 'Torgersen' island
torgersen_penguins = data[data['island'] == 'Torgersen']

# 3) Count and print the number of penguins belonging to each species
species_counts = torgersen_penguins['species'].value_counts()
print(species_counts)
```",9,"The generated code correctly performs all the required tasks: loading the data, filtering by 'Torgersen' island, and counting the species. The code is concise and directly addresses the problem statement. The use of `value_counts()` is appropriate for counting species.

**Strong Points:**
*   **Correctness:** The code produces the correct output for the given task.
*   **Clarity:** The code is easy to understand and follows a logical flow matching the task steps.
*   **Conciseness:** It achieves the goal with minimal, effective lines of code.

**Areas for Improvement (minor):**
*   **Error Handling:** The ground truth includes robust error handling for `FileNotFoundError` and checks for column existence (`island`, `species`). While not strictly required by the prompt's simplicity, a production-ready or more robust solution would benefit from such checks. The generated code would crash if `penguins.csv` is missing or if the 'island'/'species' columns are not found.
*   **Printing:** The prompt asks to ""count and print the number of penguins belonging to each species,"" which the code does by directly printing `species_counts`. The ground truth adds more descriptive print statements, which can be helpful for user experience but aren't strictly necessary for correctness.

Overall, it's a very good solution to the given task, lacking only the robust error handling and descriptive print statements that the ground truth exemplifies for ""expert"" level code.",mistral-small-24b-instruct,LLM_judge_config
E49,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

print(""Shape of the DataFrame:"", df.shape)
print(""\nData ...",10,"The generated code is excellent. It correctly defines the pandas DataFrame as specified, precisely prints its shape, and accurately lists the data types of all columns. The code is concise and directly addresses all requirements of the task. Imports are correctly handled, and the structure is clean and readable. There are no missing components or incorrect implementations. The use of f-strings or direct concatenation for print statements is a minor stylistic difference from the ground truth but functionally equivalent and perfectly acceptable.",mistral-small-24b-instruct,LLM_judge_config
E5,easy,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('titanic.csv')

# 1. Bar chart showing the count of survivors vs non-survivors
survival_counts = data['Survived'].value_counts()
survival_counts.plot(kind='bar', title='Survivors vs Non-Survivors', color=['blue', 'orange'])
plt.xlabel('Survived')
plt.ylabel('Count')
plt.xticks([0, 1], ['Non-Survivors', 'Survivors'])
plt.show()

# 2. Bar chart showing the distribution of passenger classes (Pclass)...",8,"The generated code successfully addresses all the core requirements of the task. It creates four distinct plots with relevant data, proper titles, and axis labels. It also correctly handles missing age values by filling them with the median.

Good points:
- All four plots are generated correctly.
- Missing age values are handled appropriately using the median.
- Plots have titles and axis labels.
- Uses standard libraries (pandas, matplotlib).

Areas for improvement to reach a higher score:
- **X-axis labels for Pclass and Gender:** While functional, the `xticks` for 'Pclass' (`plt.xticks(range(1, 4))`) and 'Gender' (`plt.xticks([0, 1], ['Female', 'Male'])`) are brittle. If the order of `value_counts()` changes or if there are fewer classes/genders, this could lead to incorrect labels. It's generally safer and more robust to use the index of the `value_counts()` series for labels, similar to how the ground truth code automatically maps. For 'Pclass', `pclass_counts.index.astype(str)` would be more robust. For 'Gender', directly letting `plot()` handle them (it usually infers from the index) or explicitly mapping via `gender_counts.index` as labels would be better. The ground truth code maps the numerical `Survived` values to descriptive labels, which is very clear. The generated code manually maps `Survived` values [0,1] to ""Non-Survivors"" and ""Survivors"", which is also correct but could be more robust if the values map to `value_counts()` directly without hardcoding indices.
- **Plotting best practices:** The ground truth code includes `plt.tight_layout()` and `plt.grid()` for better aesthetics and readability, which are good practices. These were omitted in the generated code.
- **File Handling:** The ground truth code includes robust file handling with a `try-except` block for `FileNotFoundError`, which is a good practice for deployed code. The generated code assumes the file exists.
- **Figure and Axis Management:** The generated code continuously calls `plt.show()` after each plot. While this meets the prompt's requirement of showing each plot, more advanced usage often involves creating figures and axes objects explicitly (`fig, ax = plt.subplots()`) to manage them, especially if you wanted to save plots or arrange multiple plots within one figure. For this basic task, it's acceptable.
- **Docstrings/Comments:** Adding comments for each section or more descriptive variable names could improve readability, though for this relatively short task, it's not a major drawback.",mistral-small-24b-instruct,LLM_judge_config
E50,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {
    'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008'],
    'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science'],
    'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89],
    'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93],
    'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96]
}

df = pd.DataFrame(data)

# Calculate and print the average 'Midterm_Score' ...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It correctly creates the DataFrame, calculates and prints the average midterm and final scores, and accurately identifies the subject with the highest average final score. The code quality is good with proper imports and a clear structure.

The only minor area for improvement, which is not critical for correctness but adds to presentation, is the lack of formatting of scores in the print statements (e.g., to two decimal places). The ground truth code formats the output to two decimal places, which is generally good practice for numerical results like averages. This is a very minor aesthetic point and doesn't impact the functionality or correctness.

Specifically:
- **Correctness:** All calculations and identifications are correct.
- **Code Quality:** Imports are correct, variable names are descriptive, and the code flow is logical.
- **Completeness:** All three parts of the task (average midterm, average final, subject with highest average final) are addressed.",mistral-small-24b-instruct,LLM_judge_config
E51,easy,Time Series Analysis,,"```python
import pandas as pd

# Data for the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08']),
    'Ticker': ['TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO', 'TECHCO'],
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5],
    'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0],
    'Close': [152.0, 151.0, 153.5, 155.0, 154.5, 156.0],
    'Volume': [100000...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, sets 'Date' as the index, calculates the 'Daily_Range', identifies the date with the highest 'Close' price, and prints both the date and the updated DataFrame. The imports are correct, and the code is well-structured and readable.

One minor point of improvement is the extraction of `max_close_date`. While `df.loc[df['Close'].idxmax()].name` works perfectly and robustly, `df['Close'].idxmax()` alone would directly give the `Timestamp` object, which is slightly more direct if only the index value is needed.

The ground truth's `strftime('%Y-%m-%d')` for printing `date_highest_close` provides a more consistently formatted date string, which might be a slight enhancement in presentation but not functionality. The generated code prints the `Timestamp` object directly, which is also acceptable.",mistral-small-24b-instruct,LLM_judge_config
E52,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST...",6,"The generated code correctly creates the DataFrame, calculates the 'Total_Revenue' column, and prints the first 5 rows of the DataFrame. However, there's a critical flaw in how it identifies the product sold the most in terms of quantity.

The line `product_sold_most = df['Product'][df['Quantity'].idxmax()]` only finds the product associated with the *single row* that has the maximum 'Quantity'. This is incorrect for determining the *total* quantity sold for a product across all transactions. For example, if 'Mouse' is sold twice with 'Quantity' 1 each (total 2), and 'Webcam' is sold once with 'Quantity' 3 (total 3), the current code would incorrectly identify 'Webcam' as best-selling even if 'Mouse' has a higher cumulative quantity. The correct approach, as shown in the ground truth, involves grouping by 'Product' and summing 'Quantity' to find the overall best-selling product.

To improve, the code needs to aggregate quantities by product before finding the maximum.",mistral-small-24b-instruct,LLM_judge_config
E53,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# 1) Calculate and print the average ""SessionDurationMinutes""
average_session_duration = df[""SessionDurationMinutes""].mean()
pr...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the average session duration, and precisely counts users by device type using standard pandas functions. The code is clean, concise, and directly addresses all requirements of the task. The output format is also clear and useful.",mistral-small-24b-instruct,LLM_judge_config
E54,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

df = pd.DataFrame(data)

max_temperature = df['TemperatureCelsius'].max()
min_humidity = df['HumidityPe...",9,"The generated code is highly correct and complete, successfully addressing all requirements of the task. It accurately defines the DataFrame and then correctly finds and prints the maximum ""TemperatureCelsius"" and minimum ""HumidityPercent"".

The only minor area for improvement, preventing a perfect 10, is the output string formatting. The ground truth code included units (°C and %) in its print statements, which is a good practice for clarity in data presentation. The generated code's print statements are correct in their values but omit these units.

Overall, an excellent solution.",mistral-small-24b-instruct,LLM_judge_config
E55,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90],
    ""Shares"": [20, 15, 50, 30, 10],
    ""Comments"": [10, 5, 25, 15, 8]
}
df = pd.DataFrame(data)

# 1) Print the total number of ""Likes"" across all posts
total_likes = df['Likes'].sum()
print(f""Total number of Likes: {total_lik...",10,"The generated code is correct, complete, and of excellent quality. It accurately defines the DataFrame, correctly calculates the total number of likes, and precisely identifies the PostType with the highest average shares. The code is well-structured, uses clear variable names, and follows standard pandas operations. It fully addresses all requirements of the task. The print statements are clear and informative.",mistral-small-24b-instruct,LLM_judge_config
E56,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

# Define the DataFrame
data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male""...",9,"The generated code is highly correct and complete. It successfully defines the DataFrame, calculates the average age, and counts patients by gender, fulfilling all task requirements. The code is well-structured and uses appropriate pandas methods.

The only minor area for improvement is in the printing statements. While functional, the ground truth's printing of the average age includes "".2f"" for formatting to two decimal places, which is good practice for numerical results like averages. Additionally, the ground truth's print statements are slightly more descriptive (e.g., ""Average Age of all patients:"" vs. ""Average Age:""). These are cosmetic differences and do not affect the correctness or core functionality.",mistral-small-24b-instruct,LLM_judge_config
E57,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000]
}
df = pd.DataFrame(data)

# 1) Print the ""Title"" of the movie with the highest ""Rating""
highest_rating_titl...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies and prints the title of the movie with the highest rating using `idxmax()`, which is a concise and efficient method for this task. It also correctly calculates and prints the average year. The code is clean, readable, and directly addresses all parts of the prompt. There are no missing imports or structural issues. It perfectly fulfills all the requirements of the task.",mistral-small-24b-instruct,LLM_judge_config
E58,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# Print the total number of cars in the dataset
print(""Total number ...",9,"The generated code successfully defines the DataFrame, correctly calculates and prints the total number of cars, and the average ""PriceUSD"". It addresses all requirements of the task.

**Strong points:**
*   **Correctness:** Both calculations (`len(df)` for total cars and `df['PriceUSD'].mean()` for average price) are accurate and produce the expected results.
*   **Completeness:** All aspects of the task are covered.
*   **Conciseness:** The code is clean and to the point.

**Areas for minor improvement (to reach a perfect 10):**
*   **Output Formatting:** The ground truth uses f-strings with specific formatting (e.g., `:.2f` for currency), which is generally good practice for numerical outputs like money. While not strictly required by the prompt, it significantly improves readability and user experience.
*   **Clarity of variable names (minor):** The ground truth uses `total_cars` and `average_price_usd` variables, which can sometimes make the code slightly more readable, especially in larger scripts, although for this small task, direct printing is perfectly acceptable.
*   **Comments:** While not always necessary for very simple lines, the ground truth's comments explaining `df.shape[0]` and `.mean()` add value, especially for beginners or quick understanding.

Overall, the generated code is highly functional and fulfills the task effectively. The identified improvements are minor stylistic and best-practice suggestions rather than critical flaws.",mistral-small-24b-instruct,LLM_judge_config
E59,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying""],
    ""HP"": [45, 60, 80, 39, 58, 78],
    ""Attack"": [49, 62, 82, 52, 64, 84],
    ""Defense"": [49, 63, 83, 43, 58, 78]
}

df = pd.DataFrame(data)

# 1) Print the name of the Pokemon with the highest ""HP...",10,"The generated code is correct, complete, and follows good code quality practices. It accurately defines the DataFrame and correctly identifies and prints the names of the Pokemon with the highest HP and lowest Defense using efficient pandas methods (`idxmax()` and `idxmin()`). The structure is clear, and all task requirements are met.",mistral-small-24b-instruct,LLM_judge_config
E6,easy,Data Cleaning,titanic.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('titanic.csv')

# Identify columns with missing values
missing_values = df.isnull().sum()

# Fill missing Age values with the median age
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing Embarked values with the most frequent port
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Drop the Cabin column
df.drop(columns=['Cabin'], inplace=True)

# Create a new column 'Title' extracted from the Name co...",7,"The generated code correctly performs most of the required data cleaning steps. It loads the data, identifies missing values, fills 'Age' with the median, fills 'Embarked' with the mode, and drops the 'Cabin' column as requested. It also extracts a 'Title' column and displays the final shape and head.

However, there are a few areas for improvement:
1.  **Title Extraction Robustness**: The `lambda` function for title extraction `x.split(',')[1].split('.')[0].strip()` is less robust than using regex as in the ground truth. It might fail for names that don't strictly follow the ""Last, Title. First"" pattern or if there are unexpected commas or periods. The ground truth's regex approach is more flexible and resilient.
2.  **Implicit Requirement for displaying missing values**: The task asked to ""Identify columns with missing values"", and while `missing_values = df.isnull().sum()` does this internally, it doesn't *display* them to the user as explicitly as the ground truth (which optionally printed it). While not a strict failure, it's less user-friendly.
3.  **No error handling for file not found**: The ground truth includes a `try-except` block for `FileNotFoundError`, which is good practice for robust code. The generated code lacks this.
4.  **No title standardization**: The ground truth demonstrated a good practice of standardizing less common titles (e.g., 'Mlle' to 'Miss', 'Mrs' to 'Mme', 'Capt' to 'Rare'). While not explicitly stated as a hard requirement in the prompt, it significantly improves the utility of the 'Title' feature and is a common part of feature engineering for this dataset.

Overall, it's a solid attempt that gets most of the core requirements right but could be improved in terms of robustness, user feedback, and advanced feature engineering.",mistral-small-24b-instruct,LLM_judge_config
E60,easy,Exploratory Data Analysis (EDA),,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy""],
    ""Stock"": [12, 8, 15, 7, 20],
    ""Price"": [10.99, 9.50, 12.00, 8.75, 15.25]
}

df = pd.DataFrame(data)

# 1) Print the ""...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately identifies the title of the book with the highest price, and precisely calculates the total stock. The code is concise, efficient, and demonstrates a clear understanding of pandas operations. It directly addresses all requirements of the task.",mistral-small-24b-instruct,LLM_judge_config
E7,easy,Exploratory Data Analysis (EDA),water_potability.csv,"```python
import pandas as pd

# Load the dataset
data = pd.read_csv('water_potability.csv')

# Display the first 10 rows
print(data.head(10))

# Show the shape of the dataset
print(data.shape)

# List all column names and their data types
print(data.dtypes)

# Calculate the percentage of potable vs non-potable water samples
potable_count = data['Potability'].sum()
non_potable_count = len(data) - potable_count
total_count = len(data)
potable_percentage = (potable_count / total_count) * 100
non_p...",7,"The generated code correctly performs most of the requested tasks. It loads the dataset, displays the first 10 rows, shows the shape, lists column data types, calculates potability percentages, displays missing values, and shows summary statistics.

However, there are several areas for improvement:
- **Presentation and Clarity:** The ground truth code uses `print` statements to clearly label each section, making the output much easier to understand. The generated code lacks these descriptive headers, making the output harder to parse.
- **Robustness:** The ground truth code includes error handling for `FileNotFoundError` when loading the CSV, which is a good practice for real-world applications. The generated code does not.
- **Detailed `dtypes` output:** The ground truth uses `df.info()` for displaying column names and data types, which provides a more comprehensive overview including non-null counts, which is beneficial for EDA. The generated code uses `df.dtypes` which is correct but less informative than `info()`.
- **Potability calculation:** The generated code's calculation for `non_potable_count` assumes all non-potable samples are '0' and total samples is `len(data)`, ignoring potential NaN values or other unexpected values in the 'Potability' column. The `value_counts()` method used in the ground truth is more robust as it accounts for all unique values and is less prone to misinterpretation if the column contains values other than 0 or 1.
- **Missing values display:** The ground truth code intelligently filters to only show columns with missing values and adds a message if no missing values are found, which is more user-friendly. The generated code simply prints all columns, even those with zero missing values.",mistral-small-24b-instruct,LLM_judge_config
E8,easy,Data Cleaning,water_potability.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Identify columns with missing values
missing_values = df.Isnull().sum()

# Fill missing values using appropriate strategies
for column in df.columns:
    if df[column].dtype in ['int64', 'float64']:
        df[column].fillna(df[column].mean(), inplace=True)

# Check for and remove any duplicate rows if they exist
df.drop_duplicates(inplace=True)

# Verify that all values are within re...",6,"The generated code addresses most of the requirements but has several significant issues and omissions that prevent it from being highly rated.

**Correctness:**
*   **Case Sensitivity (Critical Error):** `df.Isnull().sum()` should be `df.isnull().sum()`. Python is case-sensitive, and this typo will cause a `TypeError`. This is a fundamental correctness issue.
*   **Before/After Stats:** The ""before cleaning"" statistics are printed *after* the cleaning operations (missing value imputation, duplicate removal, range filtering) have already occurred. This makes the ""before"" statistics inaccurate.
*   **Range Verification Logic:** The generated code applies range filters by replacing the dataframe with the filtered rows. While this is valid, it uses hardcoded upper bounds for `Hardness`, `Solids`, `Chloramines`, `Sulfate`, `Conductivity`, `Organic_carbon`, `Trihalomethanes`, and `Turbidity`, which might not be universally ""reasonable"" without context. The ground truth code correctly identifies that most of these parameters simply cannot be negative but might not have a strict upper limit without domain knowledge, or handles pH as the only strict upper AND lower bound. However, hardcoding upper bounds is fine for a demonstration exercise, but it leads to significant data loss if these bounds are too restrictive.
*   **Missing Value Identification:** The initial `missing_values = df.Isnull().sum()` suffers from the case sensitivity error mentioned above. Even if corrected, it identifies all missing values *before* printing the ""before stats"" which are displayed *after* cleaning.

**Completeness:**
*   **Error Handling for File Loading:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, unlike the ground truth.
*   **Before/After Comparison Detail:** While it attempts a before/after summary, it only shows missing values and shape. The ground truth provides more detailed insights into what was actually done (e.g., how many duplicates removed, how many rows removed due to range issues).
*   **No specific print statements for actions:** The ground truth code provides print statements for each step (e.g., ""Filling missing values in 'column' with its mean,"" ""Removed X duplicate rows,"" ""Removed Y rows due to out-of-reasonable-range values""). The generated code is silent about these actions.
*   **Verification of no missing values after cleaning:** The ground truth explicitly verifies and prints ""No missing values remaining"" if successful. The generated code just prints the `isnull().sum()` result which for most columns should be 0.

**Code Quality:**
*   **Variable Naming:** `missing_values` is used for ""before cleaning"" but then `missing_values_after` for ""after cleaning"". The `df.Isnull().sum()` error is a major quality flaw.
*   **Structure:** The structure is somewhat jumbled due to the incorrect placement of the ""before cleaning"" summary prints. Cleaning actions occur, then the ""before"" summary, then the ""after"" summary. This is confusing and incorrect.
*   **Efficiency:** The iterative range filtering `df = df[...]` for each column in `reasonable_ranges` can be less efficient than a single chained boolean indexing operation if all conditions are known upfront.
*   **Use of `numpy`:** `numpy` is imported but not explicitly used in the generated code, whereas the ground truth uses `np.number` for selecting numerical columns.

To improve, the student should:
1.  **Fix the `Isnull()` typo immediately.**
2.  **Restructure the code** to correctly place the before-cleaning summary *before* any cleaning operations.
3.  Add print statements to clearly indicate what cleaning steps are being performed and their impact (e.g., ""X duplicates found and removed"").
4.  Consider adding `try-except` for file loading.
5.  Refine the range check to be more robust or clearly state assumptions for the upper bounds.",mistral-small-24b-instruct,LLM_judge_config
E9,easy,Data Visualization,water_potability.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('water_potability.csv')

# 1) Bar chart for potable vs non-potable water samples
plt.figure(figsize=(8, 6))
potable_counts = data['Potability'].value_counts()
potable_counts.plot(kind='bar')
plt.title('Distribution of Potable vs Non-Potable Water Samples')
plt.xlabel('Potability')
plt.ylabel('Count')
plt.xticks([0, 1], ['Non-Potable', 'Potable'])
plt.show()

# 2) Histograms ...",7,"The generated code correctly addresses all four visualization requirements and uses the specified libraries (pandas, matplotlib, seaborn). Imports are proper, and the structure is clear, with each visualization created sequentially.

However, there are several areas for improvement:

1.  **Readability/Best Practices for Bar Chart:** While `potable_counts.plot(kind='bar')` works, using `sns.countplot` (as in the ground truth) is more idiomatic for visualizing counts of categorical variables in Seaborn, and often produces more aesthetically pleasing and informative plots with less manual configuration. The `plt.show()` call directly after the bar chart is also missing for the histograms and boxplot, making the plots appear separately instead of being shown in sequence.
2.  **Missing Value Handling:** The generated code does not explicitly handle missing values for the histograms (`data['ph']`, `data['Hardness']`, `data['Chloramines']`). While `matplotlib.pyplot.hist` will implicitly drop NaNs, it's good practice to demonstrate awareness, especially for plots involving distributions where NaNs can significantly alter the appearance or interpretation if not handled. The ground truth explicitly uses `.dropna()` for clarity, which is a good practice.
3.  **Plot Formatting and Detail:**
    *   **Titles and Labels:** While titles and basic labels are present, they lack the enhanced readability (e.g., font sizes, explicit labels for ticks) seen in the ground truth, particularly for the bar chart and boxplot where the `Potability` axis labels are generic. Ground truth provides more descriptive labels like ""(0: Not Potable, 1: Potable)"".
    *   **Histograms:** The ground truth uses `sns.histplot` with `kde=True`, which adds a Kernel Density Estimate line, providing a smoother representation of the distribution. The generated code uses `plt.hist`, which is more basic.
    *   **Aesthetics:** The ground truth sets a consistent `sns.set_style(""whitegrid"")` and uses more varied and potentially more appealing color palettes (e.g., `viridis`, `coolwarm`, `skyblue`). The generated code uses basic colors and doesn't set a style, which can lead to less polished plots.
    *   **Grids:** The ground truth adds grids to most plots, enhancing readability. The generated code largely omits them.
    *   **`plt.tight_layout()` and `plt.show()`:** The generated code uses `plt.tight_layout()` for the histograms, but not consistently for all plots, and `plt.show()` is only called for the very first plot and then at the end of the histogram block, which means the boxplot and heatmap will be shown together after the first two. Each `plt.show()` should ideally follow each distinct plot generation (or group of subplots) to display them sequentially as distinct figures.

Overall, the code is functional and creates the requested visualizations, but it could be significantly improved in terms of robustness (missing value handling, potentially file not found error), aesthetic quality, and adherence to best practices for data visualization.",mistral-small-24b-instruct,LLM_judge_config
H1,hard,Feature Engineering,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Load the dataset
data = pd.read_csv('winequality-red.csv', sep=';')

# Feature Engineering
data['density_residual'] = data['density'] - data['residual sugar']
data['alcohol_acidity_ratio'] = data['alcohol'] / data['volatile a...",6,"The generated code attempts to address all aspects of the prompt but has several critical issues that prevent it from being a strong solution.

**Issues Breakdown:**

1.  **Feature Engineering - Meaningfulness & Handling:**
    *   `density_residual`: `density` vs `residual sugar` is an interesting idea but `residual sugar` is in g/L and `density` is g/cm^3. Subtracting them directly without considering units or what that difference truly represents makes it less 'meaningful' in a physical sense.
    *   `alcohol_acidity_ratio`: `alcohol` (in %v/v) divided by `volatile acidity` (g/L) again involves different units, potentially leading to less intuitive interpretations.
    *   `citric_acid_titratable_acidity_ratio`: `citric acid` (g/L) and `total sulfur dioxide` (mg/L or g/L depending on dataset description, usually mg/L). Division is also problematic due to units. More importantly, using `total sulfur dioxide` as a ""total acidity"" proxy is fundamentally incorrect. Titratable acidity (often `fixed acidity` + `volatile acidity`) would be the correct denominator.
    *   **Crucially, the code doesn't handle potential division by zero (e.g., if `volatile acidity` or `total sulfur dioxide` are zero)**, which is a common issue with ratios. The ground truth code correctly uses `EPSILON`.
    *   **Polynomial Features:** While `PolynomialFeatures` is a valid engineering technique, the prompt asks for ""at least three new meaningful features"". The polynomial features generated are numerous and primarily interaction terms of degree 2. While they *could* be meaningful, they are implicitly generated rather than explicitly thought out domain-wise like the initial three. More significantly, the code then treats these expanded features as base features for RFE/SelectKBest *without* scaling them. This can lead to very poor performance for models sensitive to feature scales (like RandomForestRegressor internally, though less so than linear models).

2.  **Feature Selection - Application and Interpretation:**
    *   **RFE on unscaled data:** RFE with RandomForest is less sensitive to scaling than, say, SVC or linear models, but it's still generally good practice to scale features. More importantly, applying RFE (and SelectKBest) directly on the *polynomial expansion of the entire dataset* creates an explosion of features (e.g., 200+ features from the original 11). `n_features_to_select=7` from such a large space is a tiny fraction and might not yield the most interpretable or robust results without prior dimension reduction or careful selection among the polynomial terms. The prompt asks for 5-7 important features, implying from the *original* or a reasonable engineered set.
    *   **RFE Ranking:** The code correctly prints `rfe_ranking` but doesn't explicitly link the rank to the feature name beyond the selected ones. `rfe.ranking_` gives the rank for *all* features, where 1 means selected. The ground truth extracts importances from the *final* RFE model trained on the selected features, which is a better way to show feature importance after selection.
    *   **`f_regression` issues:** `f_regression` assumes a linear relationship. Applying it to unscaled polynomial features can be problematic.
    *   **Comparison and Contrast:** The code simply prints the selected features and their rankings/scores. It does *not* provide any qualitative comparison or contrast, which is a key requirement of the prompt (""Compare and contrast the feature sets selected by each method""). The ground truth provides detailed textual explanation.

3.  **Displaying Results / Code Structure:**
    *   The `print` statements provide basic output, but they are not as well-formatted or informative as the ground truth (e.g., not printing friendly DataFrames).
    *   The initial `data = pd.read_csv('winequality-red.csv', sep=';')` is good for datasets that might use semicolons, but the standard `winequality-red.csv` typically uses commas. Assuming comma separator is safer, or adding a note about checking the separator.
    *   The visualization part is commented out and appears separate from the main code. While visualizations are good practice, the prompt specifically asks for *Python code without explanations* for the core task. Including visualizations outside the prompt's specified format is extra but not directly requested within the primary submission. If it was to be included, it should be integrated into the main script. Also, the plots show importance for *all* features, not specifically the *selected* ones or how they rank *within the selected set*. The RFE plot plots `rf.feature_importances_` which are from the `RandomForestRegressor` trained on all features *before* RFE. This is not the importance of features *selected by RFE*.

**Summary of missing/weak points:**

*   Lack of robustness for feature engineering (division by zero, less meaningful unit conversions).
*   Over-reliance on `PolynomialFeatures` without proper scaling or contextualizing the large feature space for selection.
*   Incomplete comparison and contrast of feature selection methods (only prints lists, no textual analysis).
*   Suboptimal display of importance/ranking for RFE (not from the final selected set).
*   Potential for `f_regression` performance issues with unscaled, expanded features.
*   The separate visualization code would not typically be included in a ""Python code without explanations"" request unless explicitly mentioned.

To improve, the student should:
1.  Re-evaluate the meaning and units of engineered features, ensuring robustness (e.g., epsilon for division).
2.  Perform feature scaling *before* applying RFE/SelectKBest, especially if using `PolynomialFeatures`.
3.  Provide a clear textual comparison and contrast of the two feature selection methods, discussing their underlying principles and why different features might be selected.
4.  Ensure that RFE's importance scores are derived from the model trained on the *selected* features, not the full feature set.
5.  Consider the size of the feature space when using `PolynomialFeatures` and how that impacts the `n_features_to_select` parameter.",mistral-small-24b-instruct,LLM_judge_config
H10,hard,Advanced Machine Learning and Ensemble Methods,video_games_sales.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline...",7,"The generated code addresses most of the requirements but has several issues that prevent it from being a full solution matching the implied quality of the ground truth.

**Strong Points:**
*   **Comprehensive Task Coverage:** The code attempts to implement most of the required tasks: loading, preprocessing, multiple ensemble methods, cross-validation, hyperparameter tuning, voting ensemble, stacking ensemble, evaluation metrics, and residual plots.
*   **Pipeline Usage:** Correctly uses `Pipeline` for preprocessing and model fitting, which is good practice.
*   **Clear Structure:** The code is generally well-structured and flows logically through the steps.
*   **Use of `ColumnTransformer`:** Proper handling of numerical and categorical features using `ColumnTransformer` and appropriate imputers/encoders.

**Areas for Improvement / Missing Features:**

1.  **Missing `MaxAbsScaler` Import:** The `MaxAbsScaler` is used in `numeric_transformer` but not imported. This would cause a `NameError`. (Critical Bug)
2.  **No `OneHotEncoder` Import:** `OneHotEncoder` is used but not imported. This would cause a `NameError`. (Critical Bug)
3.  **Incorrect Feature Importance Plotting:**
    *   The `plot_feature_importances` function attempts to get feature names from `model.named_steps['preprocessor'].transformers_[0][2]`, which is the `numeric_features` list. This is incorrect for models fitted *after* `OneHotEncoder` because `feature_importances_` will be an array corresponding to *all* preprocessed features (including the one-hot encoded ones), not just the original numerical ones.
    *   It does not account for the `ColumnTransformer` output where categorical features are one-hot encoded, leading to a mismatch between `importances` length and `feature_names` length if not handled properly.
    *   It lacks the sophisticated handling found in the ground truth to correctly map importances back to original and one-hot encoded feature names. This is a significant functional limitation in visualizing feature importances accurately for models using one-hot encoded features.
4.  **Inconsistent Cross-Validation:**
    *   Individual models are evaluated using `cross_val_score` on `X_train` only, giving a single average MAE. The ground truth performs a more granular, custom 5-fold CV to evaluate MAE, MSE, RMSE, and R2 for *each fold*, and then averages these, providing a more robust measure and the necessary `y_pred` for follow-up analysis like residual plots per fold.
    *   Voting and Stacking ensembles are fitted on `X_train`/`y_train` and then evaluated only on `X_test`, not with cross-validation. The ground truth meticulously evaluates these ensembles using the same 5-fold CV framework as individual models, which is crucial for fair comparison and robust evaluation.
5.  **Hyperparameter Tuning Scope:** The `param_grid` for hyperparameter tuning is hardcoded for `RandomForestRegressor`. If `GradientBoosting`, `XGBoost`, or `AdaBoost` were chosen as the best individual model, the tuning would apply irrelevant parameters or fail. The ground truth dynamically builds the `param_grid` based on the identified `best_model_name`.
6.  **`use_label_encoder` and `eval_metric` for XGBoost:** While `use_label_encoder` is deprecated, explicitly setting `eval_metric='mae'` for an `XGBRegressor` is generally only meaningful when *fitting* the model (e.g., in `XGBoost.fit()`) for early stopping validation, not just when initializing the model; it doesn't change how `predict` works. The ground truth simply initializes `XGBRegressor` without these extra parameters, which is sufficient given the evaluation via `sklearn.metrics`.
7.  **Minor Redundancies/Inefficiencies:**
    *   `X_train`, `X_test`, `y_train`, `y_test` are created, but then `cross_val_score` is used on `X_train` for individual models. This creates a data split that's not consistently used across all evaluations (e.g., voting and stacking are fitted on `X_train` and evaluated on `X_test`, but individual models use `X_train` for CV, then later evaluated on `X_test` too). The ground truth processes all evaluation (individual, voting, stacking) within a consistent KFold cross-validation loop over the whole dataset.
    *   Dropping `Global_Sales` `dropna(subset=['Global_Sales'])` is unnecessary if it's the target variable and assumed to be complete.
8.  **Output and Reporting:** The output is less structured and verbose compared to the ground truth, which prints detailed cross-validation results for each fold and a summary table.

The code provides a functional framework for the task, but the issues with imports, feature importance plotting, and inconsistent/less robust cross-validation (especially for ensemble methods and tuning) are significant shortcomings when compared to a production-ready solution or the detailed requirements.",mistral-small-24b-instruct,LLM_judge_config
H11,hard,Advanced Statistical Modeling and Time Series Analysis,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.api import OLS
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import Ridge, Lasso
import pymc3 as pm
import ruok.offline as offline
from sklearn.linear_model import LogisticRegressionCV
from lifelines import KaplanMeierFitter
from sklearn.model_selecti...",3,"The generated code attempts to address all requirements but falls short in implementation for almost every point, leading to multiple errors or incorrect logic, particularly in the later sections. The use of PyMC3 is incorrect, change point detection uses a non-standard library and method, and survival analysis variables ('Lifespan', 'Event_Observed') are not defined. The multivariate regression and polynomial regression setup is fundamentally flawed, especially regarding categorical features and the model inputs. Time series cross-validation is attempted but incorrectly applied for the models. Many sections would not run or would produce incorrect results if they did.

Here's a breakdown of critical issues:

1.  **Data Loading and Aggregation (Minor Issues):** The line `data['Year'] = pd.DatetimeIndex(data['Year'] + '-01-01').year` is incorrect. The 'Year' column in the dataset is `Year_of_Release`, not `Year`. Also, the way `pd.DatetimeIndex` is used implies `data['Year']` is an integer, but then it's converted to string with `'-01-01'`, which is inconsistent. The ground truth correctly uses `Year_of_Release` and converts it to datetime index for time series.

2.  **ARIMA Modeling (Incomplete/Incorrect):**
    *   `ARIMA(sales_by_year['Global_Sales'], order=(5,1,0))` is a hardcoded order and doesn't implement `auto_arima` as suggested by best practices for forecasting or allow for automatic order selection.
    *   The forecasting plot `plt.plot(pd.Series(forecast, index=pd.date_range(start=sales_by_year['Year'].max()+1, periods=5, freq='Y')), label='Forecast')` is incorrect because `sales_by_year['Year']` is an integer column, not a datetime index. The ground truth correctly converts the index to datetime first.
    *   **No confidence/prediction intervals for ARIMA forecast.**

3.  **Seasonal Decomposition (Fundamental Error):** `seasonal_decompose` requires a DatetimeIndex, but `sales_by_year` does not have one. The ground truth correctly converts the index to datetime.

4.  **Multivariate Regression (Major Errors):**
    *   `data['Genre_Interaction'] = data['Genre'] + '_' + data['Platform']` and `data['Publisher_Interaction'] = data['Genre'] + '_' + data['Publisher']` merely concatenate strings; they do not create proper interaction terms for numerical regression. These categorical columns need to be one-hot encoded, and then interaction terms should be created using `X1 * X2` for numerical interactions or by multiplying dummy variables.
    *   The `OLS.from_formula` model expects the formula to include actual column names derived from one-hot encoding, not just the original columns concatenated. It would likely fail due to string categorical variables not being handled correctly or trying to perform arithmetic on strings.
    *   **No confidence/prediction intervals for the multivariate regression model.**

5.  **Polynomial Regression with Regularization (Major Errors):**
    *   `X_poly = poly.fit_transform(data[['Year', 'Genre', 'Platform', 'Publisher']])` cannot work. `PolynomialFeatures` is for numerical features. Passing categorical 'Genre', 'Platform', 'Publisher' will raise an error (or silently fail if they are converted to numbers, which isn't the intention). Categorical features need to be one-hot encoded *before* any polynomial transformation, if a polynomial effect on their dummy representation is desired, or typically they are not included in the polynomial transformation directly.
    *   Scaling `X_poly` containing categorical features also makes no sense if those are not properly numeric.
    *   No evaluation metric (RMSE, R-squared) is calculated or printed for these models, and no cross-validation is performed *specifically* for them in this section, despite being a core requirement.
    *   **No confidence/prediction intervals for polynomial regression models.**

6.  **Bayesian Regression with PyMC3 (Critical Errors):**
    *   `import ruok.offline as offline` is an incorrect import and will cause an `ModuleNotFoundError`. It seems like a typo, maybe attempting to import `ruptures` and `arviz`, but the code doesn't use `arviz` to summarize the trace.
    *   `pm.Model() as model:` is initialized, but `n_features` is not defined, leading to a `NameError`.
    *   `mu = alpha + beta * X_scaled` is an incorrect linear model for `X_scaled` which often contains multiple features, likely requiring a dot product `X_scaled @ beta`.
    *   The model doesn't account for categorical variables correctly within PyMC3. They would need to be passed as indices or used to create appropriate effects.
    *   `return_inferencedata=False` is outdated syntax for PyMC3 v4+.
    *   **No confidence/prediction intervals (or credible intervals) extracted and printed for the Bayesian model.**

7.  **Change Point Detection (Critical Errors):**
    *   It uses `ruok.offline.change_point_detection`, which is likely an incorrect or non-standard library import (`ruok` is not a common library for this). The correct library is `ruptures`.
    *   `sigma=adfuller(data['Global_Sales'])[1]` passes a p-value as sigma, which is not the correct parameter for change point detection. `ruptures` expects the standard deviation of noise if provided, not a p-value.
    *   The result is stored in `result2` but never used/printed correctly (e.g., to identify change points on a plot or list years).

8.  **Survival Analysis (Critical Errors):**
    *   `kmf.fit(x='Lifespan', event_observed='Event_Observed', data=data)` expects 'Lifespan' and 'Event_Observed' columns to exist in the `data` DataFrame, but these columns are *not defined* or created anywhere in the code. This will cause a `KeyError`.
    *   The definition of ""game longevity"" and the required steps for creating these columns are completely missing. The ground truth correctly defines how `Longevity_Years` is derived.
    *   **Cox Proportional Hazards model is entirely missing.**

9.  **Confidence/Prediction Intervals (Critically Incomplete):**
    *   The code only shows `result.conf_int()` and `result.get_prediction()` for the `OLS` model (which is already flawed).
    *   It *doesn't* create intervals for ARIMA, polynomial regression (Ridge/Lasso), or Bayesian regression, which were specifically requested.

10. **Time Series Cross-Validation (Incorrect Application):**
    *   `model = OLS(y_train, X_train)` is attempting to fit an OLS model where `X_train` is `sales_by_year` (which contains both Year and Global_Sales from `sales_by_year.iloc[train_index]`), not just the features. This setup is incorrect. `OLS(y, X)` expects `X` to be the feature matrix, not the entire DataFrame.
    *   It only applies TS-CV to the `OLS` model (and incorrectly). It *doesn't* apply it to ARIMA or the polynomial/regularized regression models, which were specified in the prompt.
    *   The `print` statement for RMSE won't work correctly due to the `OLS` model definition issue.

The code is far from functional and demonstrates significant misunderstandings of how to implement several statistical and machine learning concepts.",mistral-small-24b-instruct,LLM_judge_config
H12,hard,Advanced Clustering and Market Segmentation,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, davies_bouldin_score
from consensusclustering import ConsensusClustering, evaluate_consensus
from scipy.stats import chi2_contingency
from mlxtend.frequent_patt...",3,"The generated code attempts to address various requirements but falls short in correctness, completeness, and code quality significantly.

**Critical Issues and Missing Elements:**

1.  **Fundamental Preprocessing Errors:**
    *   `Regional_Sales_Distribution`: Calculated as `sum(axis=1)` then divided by `df['Global_Sales']`. This results in `Regional_Sales_Distribution` being a single column with the sum of regional sales, not a distribution (proportions). It also incorrectly recalculates `Global_Sales` by summing regional sales, which might not align with the original `Global_Sales` column if it exists in the CSV, leading to data inconsistencies. The ground truth correctly calculates individual proportional sales for each region.
    *   `Market_Position`: The ground truth calculates rank *within genre*, which is crucial for market positioning. The generated code computes a simple rank, which is less meaningful for the specified task.
    *   `Publisher_Portfolio_Size`: The ground truth correctly calculates the number of *unique games* per publisher. The generated code uses `value_counts()` on the `Publisher` column, which essentially gives the *number of times* a publisher appears, not their portfolio size (unique games).
    *   **Feature Selection for Clustering:** The generated code only selects numerical columns directly for clustering, ignoring categorical features like 'Platform', 'Genre', 'Publisher'. This is a major omission as these are critical for market segmentation and should be one-hot encoded or handled appropriately. The ground truth correctly uses `ColumnTransformer` and `OneHotEncoder`.
    *   **Missing Value Handling:** The code has no explicit handling for missing values, which is present and necessary in the `video_games_sales.csv` dataset, leading to potential crashes or incorrect results.

2.  **Clustering Implementation and Tuning:**
    *   **Fixed `n_clusters`:** The K-Means, Hierarchical, and GMM algorithms are initialized with a fixed `n_clusters=5`. The task explicitly asks for determining the optimal number of clusters using silhouette analysis, elbow method, and gap statistic, which is not integrated into the main clustering models. The ground truth correctly determines and uses an optimal `k`.
    *   **DBSCAN Parameter Tuning:** DBSCAN requires careful parameter tuning (`eps`, `min_samples`). The generated code hardcodes these values without any attempt to determine them systematically, as seen in the ground truth's attempt to use k-distance graphs. This makes the DBSCAN result highly arbitrary.

3.  **Dimensionality Reduction and Visualization:**
    *   The generated code *applies* PCA, t-SNE, and UMAP but **does not visualize** the results. The task explicitly asks for visualization.

4.  **Optimal Cluster Determination:**
    *   **Incomplete Methods:** Only silhouette and elbow methods are mentioned. Gap statistic is missing.
    *   **No Application:** The results of the silhouette and elbow analysis (i.e., `silhouette_scores` and `elbow_values` lists) are calculated but **not used to determine or apply an optimal `k`** to the clustering models. They are simply computed and left unused. No plotting of these results is done either, hindering the ""determination"" aspect.

5.  **Consensus Clustering:**
    *   **External Library Assumption:** It imports `ConsensusClustering, evaluate_consensus` from a seemingly external non-standard library (`consensusclustering`), which is not part of common `scikit-learn` or `scipy` distributions and would require installation, making the code non-self-contained and hard to run. The ground truth implements a basic form of consensus clustering using co-occurrence matrices, which is a common and transparent technique without needing external obscure libraries.
    *   `consensus.get_profiles()`: This is highly speculative and would require specific implementation within the assumed external library. The ground truth clearly defines how profiles are generated from the `df_profile`.

6.  **Cluster Profiling and Statistical Significance:**
    *   `cluster_profiles = consensus.get_profiles()`: Reliance on an assumed external library function.
    *   **`chi2_p_values`:** Only `chi2_contingency` is used, and it's applied in a way that seems problematic for profiling multi-cluster data (`chi2_contingency(cluster_profiles[cluster])` would likely be an error as `cluster_profiles[cluster]` should be a single cluster's data, not a contingency table). ANOVA for numerical features is completely missing.
    *   **Missing Details:** The code only calculates p-values for statistical tests but does not print interpretable profiles (mean/mode values per cluster) as requested by the task.

7.  **Anomaly Detection:**
    *   Used `LocalOutlierFactor` (LOF), which is a valid algorithm. However, `anomalies = cluster_df[lof.negative_outlier_factor_ < -1]` is a very simplistic threshold. The ground truth uses `IsolationForest` which provides a clear ""is\_outlier"" score, and correctly prints the number of detected anomalies and examples, which is missing here.

8.  **Association Rule Mining:**
    *   **Incorrect Basket Creation:** `basket = pd.get_dummies(df[['Genre', 'Genre_Portfolio_Size']])` is fundamentally flawed. `Genre_Portfolio_Size` is not a typical categorical feature for ARM. ARM requires items to be distinct, typically one-hot encoded product attributes or categories. The ground truth correctly uses relevant categorical features and consensus clusters.

9.  **Recommendation System:**
    *   `recommendations = { game: consensus.predict(np.array(game).reshape(1, -1))[0] for game in cluster_data }`: This approach is nonsensical. `cluster_data` contains numerical features, not game names. `consensus.predict` takes processed data for one game, not the `game` variable which would be a numerical array here. It attempts to map *raw data points* to cluster labels, not to recommend games given a `game_name`. This deviates completely from the task's requirement (recommend based on clustering results).

10. **Validation Metrics:**
    *   `ari_scores`: While `adjusted_rand_score` is a valid external validation metric, it requires *ground truth labels* to compare the clustering results against. Here, it compares each algorithm's labels against the `consensus.labels_`. This can be a form of internal consistency check but is not a true ""external validation"" if no external labels exist. The task asks for both internal and external validation. Internal metrics like Silhouette, Davies-Bouldin, Calinski-Harabasz are directly calculated in the ground truth.

**Code Quality and Completeness:**

*   **Imports:** Includes an import for `consensusclustering` which is non-standard.
*   **Structure:** Lacks clear separation of steps, comments, and print statements found in the ground truth code, making it hard to follow the execution flow and verify results.
*   **Readability:** Many code blocks are non-descriptive due to lack of comments and proper variable naming conventions in some places.
*   **Completeness:** Misses several specific requirements explicitly stated in the prompt, such as gap statistic, specific types of statistical significance tests (ANOVA), and a proper recommendation system implementation. Also lacks comprehensive plotting for optimal K determination and dimensionality reduction.

In summary, the generated code makes multiple fundamental errors in data preprocessing and feature engineering, relies on problematic assumptions (like the `consensusclustering` library), and often implements required steps superficially or incorrectly, failing to deliver on the specified advanced analytical techniques.",mistral-small-24b-instruct,LLM_judge_config
H13,hard,Advanced Machine Learning and Ensemble Methods,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, KFold, RandomizedSearchCV
from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusio...",5,"The generated code attempts to address many aspects of the prompt but falls short in correctness, completeness, and overall quality, especially when compared to the comprehensive ground truth.

**Major Criticisms:**

1.  **Preprocessing and Feature Engineering (Weak):**
    *   The prompt asked for ""comprehensive feature engineering"". The generated code only uses `StandardScaler` and `OneHotEncoder`, which is basic. It misses handling `0` values as NaNs, `SimpleImputer`, and `PolynomialFeatures` as specified by the ground truth, which are critical for this dataset.
    *   The `ColumnTransformer` is used for `numeric_features` and `categorical_features`, but the `diabetes.csv` dataset typically contains only numerical features, making `OneHotEncoder` on `categorical_features` redundant and potentially incorrect if it blindly selects columns. The ground truth correctly identifies and handles missing values (0s) in specific numeric columns and applies polynomial features.

2.  **Bayesian Optimization (Incorrect Implementation):**
    *   The `objective` function for Optuna uses `X` and `y` directly for `cross_val_score` without applying the `preprocessor`. This means the optimization is happening on raw, unscaled data, which is incorrect given that preprocessing was defined.
    *   The `best_model` from Optuna is an `XGBClassifier`, but it's not explicitly used to update the other ensembles or tested further.
    *   The prompt explicitly asked for `XGBoost` for optimization and the GT used it, but the generated code's implementation of the objective doesn't guarantee the best model is used in nested CV.

3.  **Nested Cross-Validation (Fundamentally Flawed):**
    *   The `nested_cv` function is incorrect. Inside the outer loop, it tries to `set_params` on the `model` with `best_params` which are hardcoded `100` for `n_estimators` and `0.1` for `learning_rate`. This completely bypasses the Optuna-tuned parameters and defeats the purpose of nested CV. Nested CV should involve hyperparameter tuning (like using Optuna or GridSearchCV) *within* each inner fold, not simply taking globally best parameters. The ground truth correctly integrates the Optuna-tuned parameters from an initial train-validation split and then uses those tuned parameters in the actual CV loop.
    *   The function also expects `X` as a numpy array, but `X_preprocessed` is created as a dense array, while `train_idx` and `val_idx` are masks. Slicing with `X[train_idx]` would return a numpy array of objects if `X` wasn't `X_preprocessed` already, or not work if `X` is a DataFrame.

4.  **Custom Ensemble with Dynamic Weight Assignment (Placeholder/Incorrect):**
    *   The `custom_ensemble` function is implemented but `weights` are simply ""Assumed weights"" (`[0.6, 0.4, 0.0, 0.0, 0.0]`). There is no dynamic assignment based on performance, which was a core requirement. The ground truth provides a proper example of dynamic weight assignment based on validation AUCs.

5.  **Model Calibration (Incomplete/Incorrect Usage):**
    *   The generated code fits `best_model` on `X_preprocessed` and `y` and then directly calculates `calibration_curve` on these same predictions. This is an optimistic evaluation, as calibration should ideally be done on out-of-sample predictions or on a separate calibration set, similar to what the ground truth attempts (even if simplified).
    *   It plots `CalibrationDisplay` directly without explicitly showing Brier scores, which is a key metric for calibration as seen in the ground truth.

6.  **SHAP Values (Basic/Incomplete):**
    *   It uses `shap.TreeExplainer` and `shap.summary_plot`, but the `shap_values` is `shap_values[1]` which assumes a binary classification where the `shap_values` is a list of two arrays (for class 0 and class 1). While often correct, it's less robust than general access like `explainer.shap_values(X_test_shap)` and it misses other critical SHAP plots specified (dependence plots, force plots).

7.  **Ensemble Diversity Analysis (Very Basic):**
    *   The `diversity_metrics` concept is mentioned, but the code `diversity_metrics[model] = accuracy_score(y, predictions)` merely calculates accuracy for each model. It doesn't actually analyze diversity (e.g., correlation between model predictions), which is a key part of the prompt. The ground truth calculates and visualizes a correlation matrix of OOF predictions, which is the standard way to assess diversity.

8.  **CatBoost Handling (Incorrect):**
    *   The `CatBoostClassifier` declaration and usage do not specify `cat_features`, which is one of its primary advantages. While the preprocessor might handle categorical features differently, the ground truth correctly shows how CatBoost would be used with `cat_features` if they existed in the raw data, and then clarifies why it's not needed after numeric-only preprocessing.

9.  **Overall Structure and Readability:**
    *   The code is a single block rather than a structured approach with functions like the ground truth. This makes it harder to follow, debug, and logically separate concerns.
    *   Many imports are unused (e.g., `GridSearchCV`, `RandomizedSearchCV`, `resample`, `KNeighborsClassifier`, `KFold` used incorrectly, `CalibrationDisplay` imported twice).

In summary, while the code identifies many of the required components, their implementation is often simplistic, flawed, or deviates from the prompt's implied advanced requirements. It would not produce reliable or well-evaluated results for a real-world application.",mistral-small-24b-instruct,LLM_judge_config
H14,hard,Advanced Statistical Modeling and Bayesian Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from lifelines import KaplanMeierFitter
from statsmodels.formula.api import mixedlm
from BayesianLogisticRegression import BayesianLogisticRegression  # Placeholder for hypothetical library
import seaborn as sns
import matplotlib.pyplot as plt

# Load and preprocess the dataset
df = pd.read_csv('diabetes.csv')
imputer = S...",3,"The generated code attempts to address many aspects of the prompt but fails significantly in implementation details, making it largely incorrect and incomplete for the given task.

**Critical Issues:**

1.  **Bayesian Model Implementation (`pymc3`):**
    *   **Hierarchical Model:** The `pymc3` hierarchical model is fundamentally incorrect. It tries to model `scaled_df` (the entire dataframe including features and target) with a single intercept `a_1` and weights `b` of shape `df.shape[1]`. It then incorrectly links `y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=df['Outcome'])`. This is not a hierarchical model, nor is it a logistic regression, and `pm.math.dot(scaled_df, b)` performs a matrix multiplication of the entire dataframe with the beta coefficients, which is not how linear models are typically constructed in PyMC3 for features. It completely misses the ""group effects"" and specific feature handling.
    *   **Gaussian Process:** The GP model is also incorrectly specified. `pm.gp.kernels.Matern52(mu, sigma=1)` is wrong; `mu` should be `X` (input space), not a parameter. `gps = pm.GP('gps', mu, cov, X=scaled_df.values.T)` is also incorrect. `mu` should be the mean function, not a random variable. The observation `y_obs = pm.Normal('y_obs', mu=gps.mean_y(0), sigma=sd, observed=scaled_df)` is fundamentally problematic as `mean_y(0)` is not how GP predictions are typically observed, and `observed=scaled_df` implies observing the entire dataframe, not just the target variable.
    *   **Prior Specification:** The prompt explicitly asks for ""informative priors based on medical knowledge."" The generated code uses `pm.Normal('a_1', mu=0, sigma=1)` and `pm.Normal('b', mu=0, sigma=1, shape=df.shape[1])`, which are uninformative and do not reflect specific medical knowledge.
    *   **Logistic Regression:** The task specifically asks for Bayesian *logistic* regression. The defined `pymc3` model uses `pm.Normal` for `y_obs`, implying a continuous outcome, not a binary one as `Outcome` is.
    *   **Mixed-Effects in PyMC3:** The `pymc3` part does not implement mixed-effects models at all, despite being a core requirement.

2.  **Missing/Incorrect Libraries and Placeholders:**
    *   `BayesianLogisticRegression` is a placeholder, as is `IV2SLS` from `econometricmodels`. This renders those sections non-functional without external, specific implementations. The ground truth provides functional PyMC3 code for Bayesian logistic regression and `statsmodels` for IV.
    *   `KaplanMeierFitter` is good for basic survival plots but the prompt asked for ""survival analysis techniques treating diabetes onset as time-to-event data (simulate time component)"". It attempts to plot, but the `df['Time']` and `df['Event']` columns don't exist in the input data and are not simulated or explained how they would be obtained. This section is incomplete and non-functional.
    *   The `mixedlm` call `model = mixedlm('Outcome ~ L1 + L2 + ...', scaled_df, groups=scaled_df['Group'])` is a placeholder. `L1`, `L2`, and `Group` columns do not exist. The ground truth demonstrates how to *create* a group variable.

3.  **Causal Inference:**
    *   **Propensity Score Matching:** While it uses `LogisticRegression` to calculate propensities and `NearestNeighbors` for matching, the matching logic is faulty: `matches = nn.kneighbors(prop, n_neighbors=1)` finds matches for *all* data points, not specifically for the treated group matched to the control group. It doesn't then proceed to calculate the Average Treatment Effect (ATT) by comparing outcomes in the matched sample, which is the core purpose of PSM.
    *   **Instrumental Variables:** Relies on a placeholder library. The prompt asked for ""propensity score matching, instrumental variables,"" implying either or both. The ground truth provides a 2SLS approach using `statsmodels`.

4.  **Model Comparison, Credible Intervals, PPC:**
    *   While `az.compare`, `az.summary`, `az.plot_posterior`, and `az.plot_ppc` are mentioned, they are called in a generic way without specific interpretation or linkage to the *correctly* implemented (or rather, *not* correctly implemented) models. `az.compare({'model1': trace, 'model2': trace_2})` assumes `trace_2` exists and is a meaningful comparison, which it doesn't from the preceding code.

5.  **Completeness & Correctness (General):**
    *   **Data Preprocessing:** The imputation is simpler (`mean` vs. `median` for `cols_with_zeros_as_nan`) and the scaling is done on the entire imputed dataframe, not just features. The crucial step of identifying and handling zeros as NaNs in specific columns (Glucose, BP, etc.) is missed entirely.
    *   **Simulated Time Component:** The prompt explicitly asks to ""simulate time component"" for survival analysis, which is not done; it implicitly assumes `Time` and `Event` columns exist.
    *   **Variable Names:** The repeated use of `L1`, `L2`, `...` and `Group` is a significant weakness, as these variables are never defined from the `diabetes.csv` dataset. The ground truth explicitly shows how to create a `group` variable.

In summary, much of the provided code is either partially implemented placeholders, conceptually incorrect in its statistical modeling, or assumes data structures not present or created by the code itself. It does not provide a runnable or statistically sound solution to the complex requirements.",mistral-small-24b-instruct,LLM_judge_config
H15,hard,Advanced Clustering and Patient Segmentation,diabetes.csv,"Please note that Diagnostics clusters and inferring results should be done with heir domain experts`s help. There are multiple approaches one can take - as such I have followed one approach; embeddings as health care data is complex and high dimensional, with fist approach founded on specification of projective non-linearity based in manifold structures (explicit mapping temporal specialization not given in dataset).

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing im...",6,"The generated code attempts to address many of the prompt's requirements but falls short in several critical areas, particularly regarding the depth and correctness of implementation for advanced concepts.

**Strengths:**
- **Identifies key libraries:** Uses `sklearn` for clustering, `umap` for dimensionality reduction, `tensorflow.keras` for autoencoders, and `mlxtend` for association rules.
- **Attempts broad coverage:** Covers most of the requested steps: preprocessing, multiple clustering algorithms, multiple dimensionality reduction techniques, optimal parameter determination (K-Means), consensus clustering, phenotyping, anomaly detection, and association rules.
- **Includes basic visualization:** Plots DR results with cluster labels.

**Weaknesses and Areas for Improvement:**

1.  **Correctness and Robustness (Major Issues):**
    *   **Feature Engineering:** The ""Waist_Circumference"" and ""High_Blood_Pressure"" engineering is based on non-existent or incorrect column names (`Waist_hip_ratio`) and simplified/incorrect thresholds, potentially leading to misleading features. Also, the `data.get('Waist_hip_ratio', 0)` always defaults to 0 and `astype(int)` directly on a boolean `(data['Waist_Circumference'] >= 0.88)` after it's been set to something derived from a nonexistent column is problematic.
    *   **Missing Value Handling:** The code uses `data.dropna()` which aggressively removes rows with *any* NaN. For the `diabetes.csv` dataset, 0s in `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI` are actually missing values, not true zeros, and should be imputed, not dropped. The ground truth correctly identifies and imputes these. This `dropna()` choice significantly reduces the dataset size and can bias results.
    *   **Autoencoder:** The autoencoder uses `binary_crossentropy` loss, which is inappropriate for continuous, scaled numerical data without a binary output. `mse` (mean squared error) would be correct for reconstruction. `encoding_dim = 14` when `n_components_ae=2` was requested for visualization is also a mismatch.
    *   **Optimal Clustering Parameters:** Only `OptimalK` for K-Means (implicitly silhouette or gap statistic) is used. The prompt specifically asked for silhouette, gap statistic, and elbow method **with statistical validation** for *all algorithms* requiring 'k'. The generated code only picks one k and applies it to K-Means, ignoring parameter optimization for GMM, Hierarchical, and Spectral clustering entirely, fixing them to `n_clusters=3` without justification. DBSCAN/OPTICS parameter tuning (eps, min_samples) is also missing.
    *   **Consensus Clustering:** The `consensus_km` library is a good attempt, but the specific `ConsensusClustering` implementation used seems to be from a non-standard library or a custom one not readily available. The ground truth implements a robust and widely accepted co-occurrence matrix approach to consensus clustering, which is more fundamental. The generated code also sets `n_clusters=3` for consensus without determining it optimally.
    *   **Patient Phenotype Profiles:** Only `data.groupby(consensus_labels).mean()` is performed. The prompt asked for ""detailed patient phenotype profiles for each cluster with statistical significance testing."" This includes descriptive statistics (mean, std, count) for *all relevant features* and rigorous statistical tests (ANOVA/Kruskal-Wallis for continuous, Chi-squared for categorical) to identify distinguishing features between clusters. This is a significant omission.
    *   **Anomaly Detection:** While `IsolationForest` is used, the prompt also requested `OneClassSVM` and `LocalOutlierFactor`, which are missing.
    *   **Time-Series Clustering:** The code correctly notes that the dataset is cross-sectional but doesn't provide the conceptual outline or typical approaches as requested in the prompt.
    *   **Association Rule Mining:** `mlxtend.frequent_patterns.apriori` requires a one-hot encoded boolean DataFrame. The generated code passes `X` directly, which is the original non-discretized, non-boolean feature matrix, and `apriori` will fail or produce meaningless results. Features need to be discretized and one-hot encoded for this to work correctly.

2.  **Code Quality and Structure:**
    *   **Imports:** `from keras.models import Model` should be `from tensorflow.keras.models import Model`.
    *   **Magic Numbers:** Many parameters like `n_clusters=3`, `eps=0.5`, `min_samples=5`, `contamination=0.1`, `min_support=0.05`, `min_confidence=0.5` are hardcoded without justification or optimization.
    *   **Clarity:** The code is functional but could benefit from more comments, especially for complex operations or non-standard library usage.
    *   **Missing Libraries:** `gap_statistic` and `consensus_km` are not standard libraries and would require additional `pip install` commands, which should be commented or handled. (The ground truth correctly adds comments for `umap-learn`, `tensorflow`, `mlxtend` where needed).
    *   **Repetitive Plotting:** The plotting section at the end is very basic and could be more informative, showing clustering with cluster labels on the DR plots.

3.  **Completeness:**
    *   **Optimal Parameter Determination:** Fails to implement for most algorithms and statistical validation.
    *   **Consensus Clustering:** Uses a non-standard library.
    *   **Patient Phenotyping:** Lacks statistical significance testing and detailed profiles.
    *   **Anomaly Detection:** Only one algorithm out of three requested is implemented.
    *   **Time-Series Clustering:** Lacks the conceptual outline.
    *   **Risk Stratification:** The implementation is superficial; it just assigns the consensus cluster labels as risk, without linking back to clinical interpretation based on phenotype.

Overall, the generated code provides a skeletal solution that hits many keywords but lacks the depth, correctness, and robust implementation expected for an ""advanced"" analysis, especially concerning data preprocessing, parameter optimization, detailed phenotyping, and the proper use of association rules.",mistral-small-24b-instruct,LLM_judge_config
H16,hard,Advanced Machine Learning and Ensemble Methods,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import VotingRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
import lightgbm as ...",5,"The generated code attempts to address many aspects of the prompt but fails significantly in correctness and completeness for several advanced requirements.

**Correctness Issues:**
1.  **Preprocessing:** The preprocessing pipeline is defined but never integrated into the models (e.g., as part of `Pipeline` objects for ensemble methods). This means the models are trained on raw, un-transformed features, leading to incorrect results.
2.  **Voting Regressor `voting` parameter:** The `VotingRegressor` in `scikit-learn` for regression tasks does not have a `voting` parameter ('hard' or 'soft'). Instead, it defaults to averaging. Weights can be applied for weighted averaging. The use of `voting='hard'` and `voting='soft'` is incorrect.
3.  **`AdaBoostRegressor` base estimator:** The `base_estimator` for `AdaBoostRegressor` should be a regressor (e.g., `DecisionTreeRegressor`), not `GradientBoostingRegressor` (which is already an ensemble method).
4.  **Bayesian Optimization with Optuna:** The Optuna objective function uses `X_train` and `X_test` from the initial `train_test_split`. This is a basic train/test split tuning, not a proper cross-validation within the objective function as required for robust hyperparameter tuning. The prompt specifically asked for nested cross-validation, and Optuna's use here doesn't fit that.
5.  **Custom Ensemble with Dynamic Weight Assignment:** The `DynamicWeightedEnsemble` is incorrectly implemented.
    *   It trains models directly on `X, y` (or `X_train, y_train` in the code's context) for every `predict` call or within its `fit` method. This creates severe data leakage. Dynamic weights should be based on out-of-fold (OOF) predictions, not on predictions from models trained on the same data they are evaluated on.
    *   The weight update logic `_update_weights` is highly problematic; `error = np.mean((predictions - np.mean(predictions, axis=1)) ** 2, axis=0)` is not a standard or correct way to determine model error for weighting individual models based on their performance, especially without knowing the true `y` values. It looks like it's trying to calculate variance around the ensemble's mean prediction, not individual model error against the true target.
6.  **Blending and Multi-level Stacking:** Blending is implicitly attempting to be done through a `StackingRegressor` with a different name, but true blending involves a dedicated validation set for meta-model training, which is not clearly implemented. The prompt asked for ""multi-level stacking"" but the code only shows a single `StackingRegressor`.
7.  **Model Calibration:** `CalibratedClassifierCV` is for classification problems. For regression, model calibration usually involves analyzing residuals or using methods like quantile regression. Using `CalibratedClassifierCV` for a `GradientBoostingRegressor` is fundamentally incorrect.
8.  **SHAP and LIME:**
    *   SHAP `explainer` and `shap_values` are initialized, but no visualization or actual interpretation step (e.g., `shap.summary_plot`, `shap.force_plot`) is shown.
    *   LIME is imported but not initialized or used at all.

**Completeness Issues:**
1.  **Comprehensive Preprocessing with Advanced Feature Engineering:** The preprocessing is basic (imputation, scaling, one-hot encoding). ""Advanced feature engineering"" beyond this is missing.
2.  **Nested Cross-Validation:** This crucial requirement is completely absent. The code only performs a simple `cross_val_score` at the very end for a single XGBoost model.
3.  **Uncertainty Quantification:** This is mentioned in the prompt but not implemented at all.
4.  **Ensemble Diversity Analysis/Model Combination Optimization:** The code comments indicate ""implementation depends on the specific criteria"" and then largely skips this, which was a specific requirement. While it attempts a ""dynamic weight assignment"" (incorrectly, as noted), it doesn't cover general diversity analysis (e.g., correlation matrix of predictions) or optimization.
5.  **Proper Imports/Code Quality:** `matplotlib.pyplot` and `seaborn` are missing, even though `plt` and `sns` are used implicitly in the task description. The code structure is a bit fragmented, and `GridSearchCV` is imported but never used. There's also `sklearn.utils.resample` imported unnecessarily.

Overall, while the code touches on many ensemble methods, the implementation details for the more advanced requirements (nested CV, correct OOF for custom ensembles, proper calibration, comprehensive interpretation, diversity analysis, correct application of `VotingRegressor`) are either incorrect or missing entirely.",mistral-small-24b-instruct,LLM_judge_config
H17,hard,Advanced Statistical Modeling and Causal Inference,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import pymc3 as pm
import arviz as az
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.sandbox.regression as smr
import statsmodels.tools
import pyreadr
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from statsmodels.stats.outlier_tests import ma
from sklearn.cross_decomposition import PLSRegression
import statsmodels.stats.mediation as med
from statsmod...",1,"The generated code comprehensively fails to address the requirements of the task. It demonstrates a fundamental misunderstanding of several advanced statistical and causal inference concepts, particularly in their implementation using the specified libraries (PyMC3, statsmodels).

Here's a detailed breakdown of the critical issues:

**1. Correctness & Implementation Failures:**

*   **Data Loading and Preprocessing:** The code loads `world-happiness-report-2021.csv` but only performs `data.dropna()`. This is severely insufficient for ""comprehensive missing value handling"" as required, which typically involves imputation strategies, feature scaling, and handling categorical variables (e.g., Regional Indicator) properly. The Ground Truth uses `SimpleImputer`, `StandardScaler`, `OneHotEncoder`, and `ColumnTransformer` which are standard and necessary.
*   **Hierarchical Bayesian Model:**
    *   The `pm.Categorical` for `Regional Indicator` is incorrectly used. It's attempting to use the categorical variable directly as an index without mapping it to numerical IDs first and then using `p=np.array(data['Regional Indicator'].value_counts())` makes no sense in this context. Random effects in PyMC3 require integer indices.
    *   The `b` parameter with `shape=data.shape[0]` seems to imply a random intercept for each *data point* rather than each country, which is not typical for hierarchical models unless `b` is specified with an index for a grouping variable.
    *   The model structure `mu = a[...] + b` is highly unusual and likely incorrect for a standard hierarchical model for means.
    *   There's no mention of `return_inferencedata=True` when `pm.sample` (which is needed for ArviZ functions later).
*   **Structural Equation Models (SEM):**
    *   The generated code uses `statsmodels.stats.mediation.Mediation` which is for *mediation analysis*, not a full SEM framework. While partial mediation can be conceptualized within SEM, the task explicitly asks for ""structural equation models to analyze causal pathways,"" implying a more comprehensive SEM library or a set of interconnected models. The provided code does not build a proper SEM.
    *   The `formula=""y ~ x+m""` and `mediator=""m""` are placeholders and not adapted to the specific columns of the happiness dataset.
*   **Instrumental Variables (IV):**
    *   `data['iv'] = data['GDP per capita']` just copies the endogenous variable to the instrument variable slot. This is fundamentally incorrect. An instrumental variable must be distinct from the endogenous variable and satisfy exclusion and relevance criteria. The Ground Truth simulates a valid instrument for demonstration.
    *   The `sm.IV2SLS` usage is malformed; it expects `exog` and `endog` for the first-stage and second-stage regressions, not just the single `GDP per capita`.
*   **Propensity Score Matching (PSM):**
    *   `psmatching_data` is undefined.
    *   The formula `Treated ~ GDP + Social + Psychological` uses undefined column names (`GDP`, `Social`, `Psychological`) that are not present in the dataset.
    *   `ps_matching = np.array([model.predict(data) for model in [propensity_model]])` is syntactically problematic and conceptually incomplete for PSM (which requires matching, balance checks, and ATE estimation).
*   **Bayesian Regression with Informative Priors:**
    *   `sigma1` is undefined.
    *   `mu_a` is undefined.
    *   The likelihood `y_obs = pm.Normal('y_obs', mu=a, sigma=1, observed=data['Happiness Score'])` incorrectly uses `a` (which is `shape=data.shape[0]`) as the mean directly, implying a random shift per data point, not a regression model with predictors. It entirely omits the `Logged GDP per capita` or other predictors.
    *   `return_inferenced_data=False` will prevent subsequent ArviZ calls from working correctly.
*   **Gaussian Processes (GP):**
    *   `GPRegressor`, `pymg`, `ConstantKernel`, `RBF` are not standard PyMC3 or statsmodels components for GPs. PyMC3 has `pm.gp`. The Ground Truth correctly uses `pm.gp`.
    *   The `fit` method implies a scikit-learn like GP, not a Bayesian GP inferred with a probabilistic programming language.
*   **Mediation Analysis:**
    *   Again, uses `statsmodels.stats.mediation.Mediation` for ""Mediation Analysis,"" which is correct for that specific sub-task, but the task implies a link to SEM and direct/indirect effects calculation (which the Ground Truth does from multiple PyMC3 regressions). The formula `Happiness Score ~ GDP + Social + Psychological` has undefined column names.
*   **Difference-in-Differences (DiD):**
    *   The simulation of a time component is completely absent.
    *   `data_test` and `data_control` are undefined.
    *   `did_model.detected=data)` is syntactically incorrect.
    *   `diffs.polinear(x='econ vary')` is also syntactically incorrect and conceptually nonsensical.
    *   There is no regression model for DiD, which is typically an interaction term in an OLS regression.
*   **Bayesian Model Comparison:**
    *   `bayes_factor = sm.stat_compare(trace)` is completely wrong. `sm` (statsmodels) does not have a `stat_compare` function for PyMC3 traces to calculate Bayes factors directly. Bayes factors are complex to compute and typically require specific methods (e.g., thermodynamic integration, bridge sampling, or approximation using Savage-Dickey density ratio), not a direct function call like this. WAIC and LOO-CV can be done with `arviz`, but Bayes Factors often need more specialized PyMC3 methods or separate calculations. The Ground Truth correctly states that Bayes Factors are not directly computed but WAIC/LOO are shown.
*   **Credible Intervals and Posterior Predictive Checks:**
    *   `az.hdi(trace)` is correct for HDI calculation, but `trace` might not be `InferenceData` if `return_inferencedata=False` was used in `pm.sample`.
    *   `ppc = pm.sample_posterior_predictive(trace, var_names=['Happiness Score'])` is okay, but `var_names=['Happiness Score']` would only work if a variable was named `'Happiness Score'` *within* the PyMC3 model and not just the observed data. It's typically the name of the observed stochastic variable (e.g., `'y_obs'` or `'happiness_likelihood'`).
*   **Sensitivity Analysis:**
    *   `sm.common.poisson_sensitivity(data)` is entirely out of context. Poisson sensitivity is used for Poisson regression diagnostics, not general sensitivity analysis for Bayesian models by varying priors. The Ground Truth correctly demonstrates sensitivity analysis by re-running a model with different priors and comparing posteriors.

**2. Code Quality Issues:**

*   **Imports:** Many unnecessary or incorrect imports: `statsmodels.sandbox.regression`, `statsmodels.tools`, `pyreadr`, `RandomForestClassifier`, `train_test_split`, `ma`, `PLSRegression`, `FactorAnalysis`. These are not used or are used incorrectly.
*   **Undefined Variables:** Repeatedly uses undefined variables (e.g., `psmatching_data`, `sigma1`, `mu_a` in the second Bayesian regression, `data_test`, `data_control`).
*   **Syntax Errors/Bad Logic:** Numerous syntax errors or highly unconventional programming constructs (e.g., `did_model.detected=data)`, `diffs.polinear`).
*   **Clarity and Readability:** The code often lacks comments and variable names are sometimes generic, making it harder to follow without deeper context.
*   **Reproducibility:** No random seed set globally at the start, which is crucial for reproducibility of `pm.sample`.

**3. Completeness:**

*   **Comprehensive Missing Value Handling:** Severely lacking.
*   **Hierarchical Bayesian Models:** Fails to implement correctly.
*   **Structural Equation Models (SEM):** Misinterprets the requirement as simple mediation; does not build a full SEM.
*   **Causal Inference (IV & PSM):** Fails on both IV (incorrect instrument) and PSM (incomplete, undefined variables).
*   **Bayesian Regression with Informative Priors:** Incorrect model structure and undefined variables.
*   **Gaussian Processes:** Uses incorrect library and implementation.
*   **Mediation Analysis:** Partial implementation, uses a statsmodels function that isn't the full ""causal pathway"" requested.
*   **Difference-in-Differences:** Completely broken; no simulation or correct analytical framework.
*   **Bayesian Model Comparison:** Fails at Bayes factors, and WAIC/LOO would likely fail due to incorrect PyMC3 trace objects.
*   **Credible Intervals, PPC, Sensitivity Analysis:** Partially implements these but relies on correct prior model steps which are largely flawed. Sensitivity analysis is fundamentally wrong.

In essence, the generated code seems to have used keywords from the prompt to import libraries and define function calls without understanding their proper application or the underlying statistical concepts. It's far from a workable solution for advanced statistical modeling and causal inference.",mistral-small-24b-instruct,LLM_judge_config
H18,hard,Advanced Clustering and Country Segmentation,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.manifold import TSNE
from umap import UMAP
import hdbscan
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_classification
import ...",3,"The generated code attempts to address many parts of the prompt but fails significantly in correctness, completeness, and code quality.

**Correctness:**
*   **Data Preprocessing:** The `df.dropna()` call removes all rows with any missing values, which can drastically reduce the dataset size (e.g., if 'Regional indicator' has NAs). The ground truth intelligently imputes missing values. The generated code also drops 'Country name' and 'Regional indicator' for scaling, then attempts to `pd.concat` them back, which is problematic as the index might not align correctly.
*   **Feature Engineering:** The `Happiness_Profile` creation as a simple mean of several factors is a very basic form of feature engineering compared to the ground truth's interaction terms and ratios. The `Regional_Characteristic` mapping is hardcoded and incomplete, lacking many regions present in the actual data.
*   **Dimensionality Reduction:** While TSNE and UMAP are used, there's no autoencoder implementation. The manifold learning (Isomap) is also missing. The output of DR methods (`tsne_results`, `umap_results`) isn't integrated back into the DataFrame or used for visualization as requested.
*   **Clustering:** Spectral Clustering is initialized but its label is not passed to the `consensus_clustering` function. The `consensus_clustering` function provided is a placeholder and seems to call a non-existent `consensus_clustering` function directly inside it, making it non-functional. It doesn't implement the co-association matrix approach.
*   **Optimal Clustering Parameters:** The `silhouette_analysis` only applies to K-Means and doesn't cover the full scope of parameter optimization (gap statistic, stability analysis) requested. It also doesn't visualize anything.
*   **Detailed Country Profiles:** The `detailed_country_profiles` function expects a 'Cluster' column, which is never added to the main `df`. It also lacks statistical significance testing and effect size analysis.
*   **Anomaly Detection:** `make_classification` is imported but unused. The `anomaly_countries` are stored but not used for analysis or visualization.
*   **Network Analysis:** The approach to create a network graph is extremely inefficient (nested loops) and unlikely to scale. `G.add_edge(row[2:], ...)` is incorrect, it should add `Country name` as node. The thresholding for similarity is very loose (any similarity > 0) unless it's for `cosine_similarity([row[2:]], [other_row[2:]])[0][0]`. The node names being passed are row slices, not country names.
*   **Association Rule Mining:** `apriori(df, ...)` will fail because `df` contains numerical and string data, not a one-hot encoded boolean DataFrame. The data preparation for ARM (binning) is entirely missing.
*   **Recommendation System:** The recommendation system is very basic, lacking ""most similar countries within the same cluster"". The `labels` variable needed for `target_cluster = labels[df['Country name'] == target_country][0]` is undefined and the approach is flawed.
*   **Policy Insights:** This section is completely missing, apart from a vague `geopolitical_insights` dictionary that doesn't do anything.

**Completeness:**
*   **Feature Engineering:** Lacks advanced techniques like interaction terms.
*   **Clustering Algorithms:** While listed, the HDBSCAN and Spectral implementations are basic; optimal parameter tuning for DBSCAN/HDBSCAN is missing.
*   **Dimensionality Reduction:** Autoencoders and general manifold learning (beyond UMAP/t-SNE) are missing. No visualization of DR results with clusters.
*   **Optimal Parameter Tuning:** Gap statistic and stability analysis are completely missing.
*   **Consensus Clustering:** The provided implementation is fundamentally incorrect and incomplete.
*   **Detailed Country Profiles:** Lacks statistical significance testing and effect size analysis.
*   **Policy Insights:** Completely missing.
*   **Geopolitical Validation:** Not present.

**Code Quality:**
*   **Imports:** Includes unnecessary imports (`make_classification`, `MLPRegressor`, `train_test_split`).
*   **Structure:** No class-based structure, which makes the code harder to manage, replicate, and extend compared to the ground truth. Functions are defined but not called, or called with incorrect parameters.
*   **Readability/Maintainability:** Hardcoded `n_clusters=5` for all clustering algorithms, which is not optimal. Lots of repeated `df.drop(columns=...)` calls.
*   **Error Handling:** Lacks any error handling for edge cases (e.g., no clusters found by DBSCAN).
*   **Comments:** Sparse comments, making it difficult to understand the intent of certain sections.

Overall, the generated code attempts to address many requirements by including relevant library calls, but the implementation details are often incorrect or too simplified to meet the specific demands of the prompt, especially for the ""advanced"" aspects. Many sections would directly lead to errors if executed.",mistral-small-24b-instruct,LLM_judge_config
H19,hard,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')

# Parse 'date', filter for 'Germany', and set 'date' as index
data['date'] = pd.to_datetime(data['date'])
germany_data = data[data['country'] == 'Germany'].set_index('date')

# Handle missing values
germany_data = germany_...",6,"The generated code correctly implements the core SARIMA forecasting task, including data loading, filtering, splitting, model training, forecasting, and RMSE evaluation. However, there are several key areas where it deviates from the requirements or best practices, leading to a lower score:

1.  **Incorrect Seasonal Order Handling:** The `auto_arima` call sets `seasonal=False`, meaning it will *not* search for seasonal orders. Yet, the `SARIMAX` model then explicitly sets `seasonal_order=(1, 1, 1, 7)`. This is a contradiction and means the crucial seasonal component (which is a key requirement, especially for 'new_cases' with `s=7`) is not properly determined by `auto_arima` or correctly used. The ground truth correctly uses `m=7` and `seasonal=True` in `auto_arima`.
2.  **Missing Value Handling:**
    *   It drops rows with `new_cases` NaNs using `dropna(subset=['new_cases'])`. While this handles the target, it's generally not advisable to drop rows in time series data if imputation (ffill/bfill/interpolate) is possible, as it creates gaps.
    *   For exogenous variables, it only uses `ffill`. The ground truth includes `bfill` as a second step to handle NaNs at the beginning of the series, and a more robust check for remaining NaNs with interpolation.
3.  **`country` vs. `location`:** The code uses `data['country'] == 'Germany'` to filter, whereas the dataset has a `location` column for this purpose. This will cause a `KeyError` or an empty DataFrame if the dataset schema is strictly followed, making the code non-functional as-is for the specified dataset.
4.  **Error Handling:** The generated code lacks robust error handling for file not found, insufficient data for splitting, or other common issues, which the ground truth incorporates.
5.  **Small Details/Best Practices:**
    *   The `auto_arima` call should ideally specify `m=7` (seasonal period) and allow `auto_arima` to determine `d` and `D` if `seasonal=True`.
    *   The indexing alignment for `forecast` could be more explicit (though `forecast` from `statsmodels` typically maintains the index).
    *   No handling for potential negative forecasts for `new_cases`, which is a logical constraint.
    *   The `SARIMAX` model parameters `enforce_stationarity` and `enforce_invertibility` were not set, which are good practice to manage when `d` or `D` are determined by auto_arima.

To improve, the generated code needs to properly configure `auto_arima` for seasonal order detection, implement more comprehensive missing value handling, correct the column name for filtering, and add basic error handling.",mistral-small-24b-instruct,LLM_judge_config
H2,hard,Statistical Testing,winequality-red.csv,"```python
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import MultiComparison

# Load the dataset
data = pd.read_csv('winequality-red.csv', sep=';')

# Define quality groups
data['quality_group'] = pd.cut(data['quality'], bins=[0, 4, 6, 7, 10],
                               labels=['low', 'medium', 'high'])

# Function to check normality
...",3,"The generated code has several critical issues that make it largely incorrect and incomplete for the given task.

1.  **Incorrect Quality Group Definition:** The `data['quality_group'] = pd.cut(data['quality'], bins=[0, 4, 6, 7, 10], labels=['low', 'medium', 'high'])` is incorrect. The prompt specified 'low quality' wines (score <= 4), 'medium quality' (score 5-6), and 'high quality' (score >= 7). The `pd.cut` bins [0, 4, 6, 7, 10] create intervals (0,4], (4,6], (6,7] and (7,10], which are not right. Specifically, `(6,7]` is assigned to ""high"", meaning only quality 7 is `high`, which contradicts `>=7`. Quality 8, 9, 10 would be unassigned. The labels also don't match the number of bins. This is a fundamental error in categorizing the data.

2.  **Flawed Normality Check:** The `check_normality` function `check_normality(group[column])` attempts to check normality on the *entire* column, not on the individual quality groups ('low', 'medium', 'high'). Statistical tests like ANOVA and Kruskal-Wallis compare *groups*, so normality needs to be checked *within each group*. This leads to incorrect test selection.

3.  **No Homogeneity of Variance Check for ANOVA:** The ground truth code correctly identifies that for ANOVA, homogeneity of variances (Levene's test) is also an assumption. The generated code completely ignores this, only checking for normality, which is insufficient for proper ANOVA application.

4.  **Incorrect Kruskal-Wallis Post-Hoc:** The prompt explicitly asks for ""Dunn's test"" for Kruskal-Wallis post-hoc. The code attempts `stats.newman_keuls`, which is a post-hoc test for ANOVA, not Kruskal-Wallis. This is a significant error in statistical methodology. Dunn's test or pairwise Mann-Whitney U with Bonferroni/FDR correction are appropriate for Kruskal-Wallis.

5.  **`statsmodels.formula.api.ols` Usage:** While `ols` can be used for ANOVA, it's not directly doing the `f_oneway` and then Tukey's as requested. The integration of `MultiComparison` with `ols` is not as straightforward for a direct post-hoc application following manual `f_oneway` decision logic. The ground truth separates the primary test from the post-hoc, which is a clearer workflow.

6.  **Incomplete/Incorrect Post-Hoc Implementation:**
    *   The `result.print_report()` for Tukey's works, but the way it's called doesn't allow for storing or processing the results easily for summarization.
    *   The Newman-Keuls usage is incorrect for Kruskal-Wallis and seems to be called without proper `data` and `group` arguments. It's likely to fail or produce nonsensical output.
    *   There's no visible output capture or summarization of the post-hoc results as required by the prompt (""Summarize which features significantly differ between groups"").

7.  **Minor Data Loading Error:** `pd.read_csv('winequality-red.csv', sep=';')` - The `winequality-red.csv` typically uses commas as separators, not semicolons. While this might be a variance in datasets, it's a common mistake if not explicitly mentioned.

8.  **Missing Error Handling:** No `try-except` block for file loading, which is good practice.

9.  **Overall Structure:** The code is functional in parts but lacks the clear separation of logic for statistical testing (deciding between ANOVA/Kruskal-Wallis based on assumptions and handling post-hoc tests correctly for each) found in the ground truth.

In summary, the generated code fails on fundamental aspects of statistical correctness (quality grouping, normality/homogeneity assumptions, post-hoc test selection) and completeness (summarization of post-hoc results).",mistral-small-24b-instruct,LLM_judge_config
H20,hard,Classification,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix
import xgboost as xgb
import shap
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])

# Determine 90th percentile for 'new_deaths' (where new_deaths > 0)
data['new_deaths_nonzero'] = da...",6,"The generated code addresses most of the task requirements but has several significant issues that prevent it from being a robust and entirely correct solution.

**Key Strengths:**
*   **Data Loading and Parsing:** Correctly loads data and parses the 'date' column.
*   **Feature Engineering (Rolling/Lag):** Implements rolling means/stds and lag features as requested.
*   **Target Variable Definition:** Correctly attempts to define 'high_daily_death_increase' based on the 90th percentile of non-zero 'new_deaths', though there is a subtle logical flaw in the sorting/merging approach.
*   **XGBoost Training & Hyperparameter Tuning:** Uses GridSearchCV for XGBoost with the specified parameters.
*   **Evaluation Metrics:** Calculates ROC-AUC and F1-score.
*   **SHAP Implementation:** Attempts to use SHAP for feature importance.

**Areas for Improvement / Critical Issues:**

1.  **Target Variable Definition Issue:**
    *   The line `data = data.sort_values(by=['location', 'date']).dropna(subset=['new_deaths_nonzero'])` before calculating percentiles will **remove rows** where `new_deaths` (and thus `new_deaths_nonzero`) is zero or NaN. This is incorrect. The target definition specifically states ""excluding days with zero new deaths"" for the *percentile calculation*, not for the entire dataset used for the target variable itself. The ground truth correctly handles this by using `apply(lambda x: x if x > 0 else np.nan)` and then `dropna()` on the percentile calculation *only*, or `quantile(0.90)` which ignores NaN by default.
    *   This incorrect `dropna()` will lead to data loss and an incorrect target variable for many days. Also, it sorts and drops, then merges back, potentially re-ordering or losing data. The target variable should be `(data['new_deaths'] > data['new_deaths_90th_percentile']) & (data['new_deaths'] > 0)`. The `(data['new_deaths'] > 0)` is critical to ensure that a day with zero deaths isn't incorrectly flagged as a high increase if its location's 90th percentile also happens to be zero (which is unlikely but possible for locations with very few deaths). The generated code just has `data['high_daily_death_increase'] = data['new_deaths'] > data['new_deaths_90th_percentile']`.

2.  **Missing Value Handling:**
    *   `data = data.fillna(0)` is too broad. While the prompt allows filling with 0, a blanket fillna(0) for *all* columns is problematic. For example, `people_fully_vaccinated` or `population` might have NaNs. Also, rolling and lag features should ideally be filled *per group* (location) first, then globally, to preserve time series integrity. `ffill` then `fillna(0)` is a better approach for time-series features.
    *   The `people_fully_vaccinated` and `population` columns are used directly in `features` without being combined into a ratio as specified in the prompt (`'people_fully_vaccinated' / 'population'`). This is a direct missing requirement.

3.  **Preprocessing (Scaling and Encoding):**
    *   The scaling `data[features] = scaler.fit_transform(data[features])` happens *before* one-hot encoding categorical features and including them, and *before* the train-test split. This is a severe form of data leakage because the scaler learns from the test set data distributions. Preprocessing steps (scaling and encoding) must be part of a `Pipeline` or applied *after* the train-test split, using `fit_transform` on train and `transform` on test.
    *   The one-hot encoding `continent_encoded_df = pd.DataFrame(continent_encoded, columns=encoder.get_feature_names_out(['continent']))` assumes the original DataFrame index aligns after the `dropna` operation, which it likely won't. This can lead to misaligned rows if the index is not reset consistently, or if rows were dropped. The `ColumnTransformer` approach (as in ground truth) is much safer and canonical.

4.  **Time-Based Split and Data Leakage:**
    *   The time-based split is implemented incorrectly. `data.loc[data['time'] <= data.groupby('location')['time'].quantile(0.8)]` can lead to data leakage if locations have different start/end dates. The robust way is to sort the *entire* dataset by global date, then split by a fixed percentage (e.g., last 20% of rows). The current method picks an 80th percentile *per location*, meaning the test set might contain dates earlier than some train set dates from other locations.
    *   Also, the split drops 'location', 'date', 'time' BEFORE the data is passed to the ColumnTransformer / Pipeline. This is okay if the pipeline is set up correctly, but the current implementation creates `X_train` and `X_test` then scales them directly, which means the OHE features aren't handled correctly in the scaling part. Again, a `Pipeline` is the robust solution.
    *   The `features` list used for scaling and for creating `X` and `y` doesn't include the one-hot encoded continent features or the `people_fully_vaccinated_ratio`. It uses `people_fully_vaccinated` and `population` as separate features which is not what was requested.

5.  **SHAP Feature Names:**
    *   `top_10_features_names = [features[i] for i in top_10_features]` will only pick names from the base `features` list and won't correctly identify the names of the one-hot encoded `continent` features if they are among the top 10. The `shap.summary_plot` call then passes these potentially incorrect names. The ground truth correctly retrieves all processed feature names.

6.  **Pipeline Usage:**
    *   The prompt implies a workflow suitable for `sklearn.pipeline.Pipeline` and `ColumnTransformer` for robust preprocessing. The generated code implements these steps manually which is prone to errors, especially regarding data leakage and feature mapping.

7.  **Minor Issues:**
    *   The F1-score evaluation should ideally provide a `classification_report` for more detailed metrics (precision, recall, support).
    *   No `warnings.filterwarnings('ignore')` to suppress typical `use_label_encoder=False` warnings.

Overall, the code has the right structure for the task but suffers from critical data leakage issues during preprocessing and an incorrect time-based split, which would invalidate the evaluation results. Additionally, the target variable definition and some feature selections are not precisely aligned with the prompt.",mistral-small-24b-instruct,LLM_judge_config
H21,hard,Statistical Modeling,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Load data
data = pd.read_csv('synthetic_covid19_data.csv')

# Parse 'date' and calculate 'vaccination_percentage'
data['date'] = pd.to_datetime(data['date'])
data['vaccination_percentage'] = data['people_fully_vaccinated'] / data['population']

# Handle missing values using median imputation grouped by 'location'
for column in ['reproduction_rate', 'stringency_index', 'vaccinati...",3,"The generated code attempts to address the task but fails significantly in several key areas, particularly regarding the mixed-effects model specification and the handling of missing values.

**Correctness (1/4):**
*   **Missing Value Handling:** The `ffill().bfill()` strategy for `population` and `people_fully_vaccinated` is missing entirely from the generated code, which is crucial for correct `vaccination_percentage` calculation. The median imputation applied for `reproduction_rate`, `stringency_index`, and `vaccination_percentage` is too simplistic and doesn't consider the time-series nature or grouping by location as effectively as interpolation. Furthermore, the `raise ValueError` if NaNs persist is overly aggressive and may stop execution unnecessarily.
*   **Vaccination Percentage:** It calculates `vaccination_percentage` without clipping at 100%, which is a practical consideration for a percentage and is done in the ground truth.
*   **Mixed Model Specification:** This is the most critical failure.
    *   The `groups=data[""location""]` is incorrect. The task specifically requires ""random intercepts for 'location' nested within 'continent'"". This means the grouping variable should be a unique ID for each (continent, location) combination, not just `location`. The ground truth creates `continent_location_id` for this.
    *   The `re_formula=""~location""` is also incorrect and redundant/misguided given the `groups` specification. For a simple random intercept, `re_formula=""1""` (or not specifying it, as ""1"" is often default) is used. Specifying `~location` here attempts a random slope for `location`, which doesn't make sense as `location` is the grouping variable itself. This is a fundamental misunderstanding of mixed-model syntax in `statsmodels`.
    *   The `time_agg=['continent', 'location']` argument is not a standard argument for `mixedlm` and will cause an error. This suggests a lack of understanding of the `statsmodels.formula.api.mixedlm` function.
*   **Reporting Random Effects:** `result.random_results.re_fe.summarstuitregs_summary_in_covariates()` is syntactically incorrect and clearly made up. There's no such method. The variance components are typically accessed via `result.cov_re` or visible in `result.summary()`.

**Code Quality (2/3):**
*   **Imports:** Proper imports are present.
*   **Structure:** The code has a basic structure (load, process, model, report).
*   **Readability:** Variable names are okay.
*   **Error Handling:** Missing `try-except` for file loading. The `ValueError` for persistent NaNs is not ideal.
*   **Warnings:** No warning suppression, which can make output noisy for mixed models.

**Completeness (0/3):**
*   **Interpretation of Fixed Effects:** The task asked for interpretation (""Interpret the significance and direction of these effects""). The generated code only prints the summary table and then completely omits any interpretive text.
*   **Random Effects Variance:** Failed to correctly report the variance components and interpret them.
*   **Model Assumptions Discussion:** While it mentions assumptions, it's very generic and doesn't clearly differentiate how they *could* be checked, nor does it mention the specific assumptions for *mixed* models (e.g., normality of random effects). It also uses a strange phrase ""Residuals and I-long-normal-distance attribute should look linear.""
*   **Interaction Term:** The interaction term `stringency_index * vaccination_percentage` is in the formula, but the interpretation part doesn't specifically address how to interpret an interaction term. The ground truth provides a detailed breakdown of interaction interpretation.

Overall, the generated code has severe functional errors regarding model specification and random effects reporting, and it lacks critical interpretive and completeness aspects requested in the prompt.",mistral-small-24b-instruct,LLM_judge_config
H22,hard,Clustering,clash_royale_cards.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# 1) Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# 2) Select the features `elixirCost`, `Win Rate`, and `Usage`
features = data[['elixirCost', 'Win Rate', 'Usage']]

# 3) Standardize these features using StandardScaler
scaler = Stan...",8,"The generated code correctly addresses most of the requirements and demonstrates a good understanding of the task.

**Strong Points:**
- **Correct Imports:** All necessary libraries are imported.
- **Data Loading and Feature Selection:** Data loading and feature selection are handled correctly.
- **Standardization:** Features are properly standardized using `StandardScaler`.
- **K-Choice Method:** Both Elbow Method (WCSS) and Silhouette scores are calculated and plotted, which is excellent for justifying K.
- **K-Means Application:** K-Means is applied correctly with a chosen K.
- **PCA Dimensionality Reduction:** PCA is performed to reduce dimensions to 2 components.
- **Visualization:** A scatter plot of PCA components colored by cluster labels is generated, fulfilling the visualization requirement.

**Areas for Improvement (Why not a 10):**
- **Error Handling:** The generated code lacks error handling for `FileNotFoundError` when loading the CSV, which was present in the ground truth. This is a crucial robustness check in real-world scenarios.
- **Optimal K Justification/Automation:** While it plots both WCSS and Silhouette, the choice of `optimal_k = 4` is hardcoded. Although the prompt stated *trying* values and *you can use* the methods, it implies that the output should *justify* the choice. A comment explaining *why* 4 was chosen based on the plots (e.g., ""based on the elbow point/peak silhouette score around K=4"") would make it more robust. The ground truth explicitly states the choice is based on observation and suggests adjustment.
- **Clarity in Plots:** The ground truth code improved the clarity of the plots by adding specific x-ticks for K values, more descriptive titles, and a larger figure size for the Elbow/Silhouette plots in the generated code. The generated code also has an issue with the silhouette plot's x-axis (`range(2, 8)[1:]`) which should be `range(3, 8)` to align with the K values, as silhouette score is defined for k >= 2.
- **`n_init` in KMeans:** In `sklearn` versions 1.2 and later, `n_init` defaults to 'auto' which runs 10 initializations. However, explicitly setting `n_init=10` (as in the ground truth) is good practice for reproducibility and to avoid potential `FutureWarning` messages in older versions or ensure backward compatibility. The generated code doesn't specify `n_init`.",mistral-small-24b-instruct,LLM_judge_config
H23,hard,Regression,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Select features and target
X = data[['elixirCost', 'Usage', 'rarity']]
y = data['Win Rate']

# Preprocess the ...",9,"The generated code correctly addresses all requirements of the task. It loads the data, selects the specified features and target, correctly preprocesses the 'rarity' column using one-hot encoding (and a `ColumnTransformer` which is a robust way to do so), splits the data, trains a regression model (Linear Regression, as allowed), and evaluates it using R-squared and MSE. The use of `Pipeline` and `ColumnTransformer` demonstrates good practice for managing preprocessing steps.

The only minor difference, not necessarily a flaw, is that the `OneHotEncoder` by default does not use `drop_first=True`. While this is often fine for linear models, including `drop_first=True` was explicitly done in the ground truth to avoid multicollinearity, which is a good practice. However, the problem statement did not explicitly require `drop_first=True`.

Overall, the code is very good, well-structured, and complete as per the task description.",mistral-small-24b-instruct,LLM_judge_config
H24,hard,Hypothesis Testing,clash_royale_cards.csv,"```python
import pandas as pd
import scipy.stats as stats

# 1) Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# 2) Identify the unique categories in the `rarity` column
unique_rarities = data['rarity'].unique()

# 3) Perform an ANOVA test
anova_result = stats.f_oneway(*[group['Win Rate'] for name, group in data.groupby('rarity')])

# 4) State the null and alternative hypotheses
null_hypothesis = ""There is no significant difference in mean `Win Rate` across different `rarity` cat...",6,"The generated code correctly performs the ANOVA test and interprets the result. It correctly states the hypotheses and identifies unique rarity categories. However, it lacks robustness and essential data cleaning steps that are critical for real-world data analysis, such as handling `FileNotFoundError` or non-numeric 'Win Rate' values. It also doesn't check for empty dataframes or groups with insufficient observations. The output for hypotheses and conclusions could be more explicitly formatted.

Specific areas for improvement:
- **Error Handling and Robustness:** The ground truth code includes `try-except` blocks for file loading and checks for empty DataFrames or groups with insufficient data, which is crucial for robust code. The generated code assumes perfect data and file availability.
- **Data Cleaning (`Win Rate`):** The `Win Rate` column might not always be numeric. The ground truth explicitly converts it using `pd.to_numeric` with `errors='coerce'` and handles NaNs. The generated code relies on 'Win Rate' being perfectly numeric by default, which is a common pitfall.
- **Hypothesis Formatting:** While the hypotheses are correctly stated, printing them separately from the F-statistic and p-value, perhaps with a clear heading, would improve readability as done in the ground truth.
- **Intermediate Checks:** The ground truth includes checks like `if len(rarity_categories) < 2` or `if len(group_win_rates) >= 2`, which are important for preventing errors or misleading results in edge cases where there aren't enough groups or observations per group for ANOVA. The generated code does not include these checks.",mistral-small-24b-instruct,LLM_judge_config
H25,hard,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features and target
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm...",9,"The generated code correctly addresses almost all requirements of the task.

**Strong Points:**
- **Correctness:** The code correctly loads the dataset, defines features and target, encodes the target, splits the data, trains the specified models (Logistic Regression, SVC, Random Forest), and calculates the required performance metrics (accuracy, macro-averaged precision, recall, F1-score).
- **Code Quality:** The code is well-structured and uses appropriate imports and variable names. It effectively uses a dictionary to manage models and iterate through them for training and evaluation.
- **Completeness:** It successfully identifies and prints the best model based on the highest F1-score. All specified models and metrics are included. The Logistic Regression model correctly includes `max_iter=200`, `SVC` includes `kernel='linear'` and `random_state=42`, and `RandomForestClassifier` includes `n_estimators=100` and `random_state=42`.

**Areas for Improvement (Minor):**
- **Data Splitting:** The ground truth includes `stratify=y_encoded` in the `train_test_split` function. While not explicitly mentioned in the prompt, using `stratify` is good practice for classification tasks to ensure that the class proportions are maintained in both training and testing sets, especially with potentially imbalanced datasets. The generated code omits `stratify`, which is a minor oversight but could lead to slightly different or less representative splits for the `Species` column.
- **Error Handling:** The ground truth includes a `try-except` block for `FileNotFoundError` when loading the CSV, which is robust programming practice. The generated code directly calls `pd.read_csv`, which would raise an error if the file is missing. This wasn't explicitly asked but is a good addition.
- **Output Formatting:** While the output for individual model metrics is good, the prompt specifically asked to ""print which model performed best based on the highest F1-score (macro average)"" and the ground truth provides a more explicit print statement for this, including the best F1-score value. The generated code prints just the model name. This is a very minor point.",mistral-small-24b-instruct,LLM_judge_config
H26,hard,Clustering,iris.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Load the dataset
data = pd.read_csv('iris.csv')

# Select the features
features = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Standardize these features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)
...",8,"The generated code successfully completes most of the requirements of the task. It correctly loads the dataset, selects and standardizes features, applies K-Means with the specified parameters, performs PCA, and generates two scatter plots.

However, there are a few areas for improvement:
1.  **Plot Titles:** The specified task explicitly asked for two separate plots with specific titles: ""PCA of Iris Dataset: Points Colored by K-Means Cluster Labels"" and ""PCA of Iris Dataset: Points Colored by True Species Labels"". The generated code uses ""K-Means Clustering"" and ""True Species Labels"", which are less descriptive.
2.  **Separate Plots vs. Subplots:** The task implies two *separate* plots (as shown by `plt.figure()` and `plt.show()` twice in the ground truth). The generated code uses `plt.subplot(1, 2, 1)` and `plt.subplot(1, 2, 2)`, combining them into a single figure with two subplots. While functionally similar, it doesn't strictly adhere to the implied separation of plots.
3.  **Color Bar:** The task implicitly expects color bars for the plots, especially since the ground truth code used them for better clarity. While not explicitly stated as a *requirement*, it's a common and good practice for plots where color represents different categories. The generated code omits them.
4.  **`pd.read_csv` and `FileNotFoundError`:** The ground truth includes a robust `try-except` block for `FileNotFoundError`. While not strictly required by the prompt, it's good practice for production-ready code.

Overall, the core functionality is correct, but minor stylistic and completeness aspects related to the plotting fall short of the ground truth's excellence.",mistral-small-24b-instruct,LLM_judge_config
H27,hard,Feature Engineering,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

# 1) Load the dataset
data = pd.read_csv('iris.csv')

# 2) Create two new features: PetalArea and SepalArea
data['PetalArea'] = data['PetalLengthCm'] * data['PetalWidthCm']
data['SepalArea'] = data['SepalLengthCm'] * data['SepalWidthCm']

# 3) Define features X
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLen...",10,"The generated code correctly addresses all requirements of the task. It loads the dataset, creates the specified new features, defines `X` with both original and new features, encodes the target `y`, trains a Gradient Boosting Classifier with the correct parameters, performs 5-fold cross-validation, and reports the mean accuracy. The imports are proper, and the code structure is clear and follows the problem's steps. It is a complete and correct solution.",mistral-small-24b-instruct,LLM_judge_config
H28,hard,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the dataset
df = pd.read_csv('heart.csv')

# Define features and target
features = ['Age', 'Sex', 'ChestPainTy...",9,"The generated code correctly implements all specified requirements, demonstrating a good understanding of preprocessing techniques (one-hot encoding, scaling) and model training/evaluation using a pipeline.

Strong points:
- Correctly loads data and defines X and y.
- Accurately identifies and separates categorical and numerical features for preprocessing.
- Uses `ColumnTransformer` and `Pipeline` effectively for streamlined preprocessing and model training, which is best practice to prevent data leakage and ensure clean code.
- Correctly performs train-test split with the specified `test_size` and `random_state`.
- Trains a `RandomForestClassifier` with the specified `n_estimators` and `random_state`.
- Evaluates the model using all requested metrics (accuracy, macro precision, macro recall, macro F1-score, and ROC-AUC).

Area for minor improvement:
- The `roc_auc_score` calculation in the generated code uses `y_pred` instead of `y_proba`. For ROC-AUC, especially in binary classification, it's conventional and more accurate to use predicted probabilities (`model.predict_proba(X_test)[:, 1]`) rather than hard predictions (`y_pred`). While `multi_class='ovr'` is used, a single binary score, it ideally should be based on probabilities. The ground truth code correctly uses `y_proba[:, 1]`. This is a minor point as `y_pred` can technically be used in some cases for ROC-AUC, but probabilities are preferred for more informative scores.
- The `OneHotEncoder` argument `drop='first'` was added, which was not explicitly requested or implied by the prompt. While sometimes useful to avoid multicollinearity, it's a deviation from a direct one-hot encoding without specifying an `drop` strategy, and `handle_unknown='ignore'` used in the ground truth is generally safer when new categories might appear in test data.

Overall, it's a very robust and well-structured solution.",mistral-small-24b-instruct,LLM_judge_config
H29,hard,Clustering,heart.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('heart.csv')

# Selecting numerical features for clustering
features = data[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']]

# Standardizing the selected features
scaler = StandardScaler()
standardized_features = scaler.fit...",9,"The generated code is very good and largely addresses all requirements of the task.

**Strong Points:**
*   Correctly loads the dataset and selects the specified features.
*   Correctly standardizes the features using `StandardScaler`.
*   Implements both the Elbow method (WCSS) and Silhouette scores for determining K in the specified range (2-5).
*   Applies K-Means clustering with the chosen K (`chosen_k = 3`), `n_init='auto'`, and `random_state=42`.
*   Performs PCA to reduce dimensionality to 2 components.
*   Creates a scatter plot of the principal components, colored by cluster labels, with a title.
*   Includes all necessary imports.

**Areas for Minor Improvement:**
*   **Error Handling for File Loading:** The ground truth code includes basic error handling (`try-except`) for `FileNotFoundError` when loading `heart.csv`. The generated code assumes the file exists, which is acceptable but less robust in a real-world scenario.
*   **Plotting Silhouette Scores:** The `K_range` for plotting silhouette scores is `K_range[1:]`, but the `wcss` plot uses `K_range`. While `silhouette_score` can't be calculated for K=1, it's good practice to align the x-axis for both plots or make it clear which K values they represent. The ground truth starts `k_range` from 2 ensuring all values in `k_range` are suitable for `silhouette_score` calculation, then plots all `silhouette_scores` against `k_range`. In the generated code, `K_range` is `[2, 3, 4, 5]` and `silhouette_scores` will have 4 elements, but `K_range[1:]` is `[3, 4, 5]`. The ground truth handles this by ensuring `k_range` only contains values 2-5, which correctly aligns with the list of silhouette scores. This is a minor plotting detail, but it could lead to misinterpretation if not handled carefully. (Looking closer, `silhouette_scores` will have 4 elements, but 1 was appended when k=2. `K_range[1:]` yields `[3,4,5]`. So the x-axis labels are slightly off, as the first point for `silhouette_scores` (calculated for K=2) would be plotted at K=3 on the generated code's graph). The ground truth correctly aligns the x-axis of the silhouette plot with the correct K values.

These are minor points and don't detract significantly from the overall correctness and completeness of the solution.",mistral-small-24b-instruct,LLM_judge_config
H3,hard,Machine Learning,winequality-red.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Load data
data = pd.read_csv('winequality-red.csv')

# Create binary target variable
data['quality'] = data['quality'].apply(lambda x: 'good' if x >= 7 else 'bad')

# Preprocess data
data = data.dropna()  # Handle miss...",7,"The generated code correctly addresses most of the requirements but has a few areas for improvement.

**Strong Points:**
*   Correctly loads the dataset.
*   Correctly uses `StandardScaler` for feature scaling.
*   Correctly splits data into train/test sets using `train_test_split`.
*   Correctly trains a `LogisticRegression` model.
*   Correctly evaluates the model using accuracy, F1-score, and classification report.
*   Attempts to report feature importance using coefficients.

**Areas for Improvement:**
*   **Binary Target Variable Creation:** The generated code defines 'good' and 'bad' as strings (`'good'` and `'bad'`) which limits the direct use of `f1_score` without specifying `average='weighted'` or `average='binary'` (if the target variable is mapped to 0/1). It's generally better practice for binary classification in `sklearn` to have numerical labels (0 and 1). The Ground Truth uses `astype(int)` which is more robust for `sklearn` metrics.
*   **Target Variable Handling after Binarization:** The generated code reassigns the `quality` column to the string labels and then tries to drop it. When creating a new binary target, it's often more explicit to create a *new* column for the binary target and then drop the *original* `quality` column before separating `X` and `y`. This prevents potential confusion or errors if the `quality` column is used elsewhere.
*   **Scaling Order:** The scaling happens *before* the train/test split (`X_scaled = scaler.fit_transform(X)`), which leads to data leakage from the test set into the training set's scaling parameters. `StandardScaler` should be fitted *only* on the training data and then *transformed* on both training and test data separately (`scaler.fit(X_train)` then `scaler.transform(X_train)` and `scaler.transform(X_test)`). The Ground Truth demonstrates the correct approach.
*   **Feature Importance:** While the code correctly extracts coefficients, `X_scaled` is an array and does not retain column names. When creating `coefficients = pd.Series(model.coef_[0], index=X.columns)`, `X.columns` refers to the columns of the *unscaled* `X`, and `model.coef_[0]` refers to coefficients of the *scaled* data. This will work if the column order is preserved, but it's better practice to either convert `X_train_scaled` back to a DataFrame with column names or explicitly ensure the index is correct for interpretation. The Ground Truth makes `X_train_scaled_df` and `X_test_scaled_df` to maintain this structure which aids in interpretation.
*   **Robustness (File Not Found):** The ground truth includes a `try-except` block for file loading, which is good practice. The generated code assumes the file exists.
*   **Missing Values:** The `data.dropna()` immediately after loading is a valid way to handle missing values, but for this specific dataset, there are typically none. The Ground Truth includes a check (`if X.isnull().sum().sum() > 0:`) before handling, which is more informative for the user.

Overall, it's a good attempt, but the data leakage during scaling and the slightly less robust binary target creation are key areas that prevent a higher score.",mistral-small-24b-instruct,LLM_judge_config
H30,hard,Statistical Modeling,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
import numpy as np

# 1) Load the dataset
data = pd.read_csv('heart.csv')

# 2) Create a box plot
plt.figure(figsize=(8, 6))
sns.boxplot(x='HeartDisease', y='Oldpeak', data=data)
plt.title('Distribution of Oldpeak by HeartDisease')
plt.show()

# 3) Perform logistic regression
X = data[['Oldpeak']]
y = data['HeartDisease']
model = LogisticRegression()
model.fit(X...",7,"The generated code correctly addresses most parts of the task.

**Strong points:**
*   Correctly loads the dataset.
*   Correctly creates the box plot (although the labels are less descriptive than ideal).
*   Correctly performs logistic regression and extracts the coefficient and odds ratio.
*   Correctly discretizes `Oldpeak` using `pd.qcut` into three bins.
*   Correctly calculates the proportion of `HeartDisease=1` within each bin.

**Areas for improvement to achieve a higher score:**

1.  **Interpretation of Odds Ratio:** The prompt explicitly asked to ""interpret its meaning in the context of odds ratios."" The generated code outputs the coefficient and the odds ratio but lacks any textual interpretation, which was a specific requirement. This is a significant omission for a data science task.
2.  **Visualization Enhancement:** For the box plot, the labels for the x-axis are `0` and `1` which are not very descriptive. Adding meaningful labels (e.g., 'No Heart Disease', 'Heart Disease') as done in the ground truth would improve readability. The box plot is displayed but not explicitly given a title.
3.  **Robustness/Error Handling:** The ground truth includes `try-except` for file loading and checks for missing values, making the code more robust. The generated code assumes the file exists and has no missing values in relevant columns, which is a common but less robust practice.
4.  **Completeness of Output:** While the proportions are printed, visualizing them (e.g., with a bar plot) as often done in analysis, and as implicitly suggested by the overall analytical nature of the task could have enhanced the solution. The prompt didn't explicitly ask for a plot for the proportions, but it did for the box plot, so consistency in demonstration would be good.
5.  **Print Statements:** The ground truth code includes more descriptive print statements to guide the user through the execution steps, which is good practice for a benchmark solution.

Overall, the core functionalities are implemented correctly, but the lack of interpretation and robustness, along with minor presentation issues, prevents a higher score.",mistral-small-24b-instruct,LLM_judge_config
H31,hard,Classification,spotify_2023.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix

# 1) Load data
data = pd.read_csv('spotify_2023.csv')

# 2) Convert `streams` to numeric (coerce errors to NaN). Drop rows where `streams` is NaN.
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')
data = data.dr...",7,"The generated code correctly implements most of the requirements. It loads the data, correctly handles the `streams` column conversion and NaN dropping, creates the `is_popular` target, selects numerical features, and performs one-hot encoding for categorical features. The model training and evaluation using F1-score (macro), ROC AUC score, and confusion matrix are also correctly implemented.

However, there is a critical bug in how the one-hot encoded features are handled when combining them with the original features. The `pd.concat` operation `features_data = pd.concat([data[features], encoded_categorical_features_df], axis=1)` is problematic because `data[features]` retains the original index of the `data` DataFrame, whereas `encoded_categorical_features_df` has a new, default integer index (0, 1, 2...). If the original `data` DataFrame had any rows dropped (due to `streams` being NaN), its index would no longer be a contiguous range from 0 to N-1. When `concat` tries to align these DataFrames by index (which is its default behavior for `axis=1`), it will misalign or create many NaNs if the indices don't match perfectly. This creates incorrect feature sets for training.

To fix this, `data[features]` should either be reset in its index before concatenation, or (even better) `pd.get_dummies` should be applied directly on the entire `X` dataframe after feature selection and before the train/test split, ensuring the one-hot encoding aligns correctly with the other features.

Additionally, the code could be improved by using `stratify=y` in `train_test_split` to ensure the class distribution of `is_popular` is maintained in both training and testing sets, which is good practice for imbalanced datasets common in classification.

Minor points of improvement:
- No error handling for file loading, unlike the ground truth.
- No `random_state` set for `LogisticRegression`, which is good practice for reproducibility.",mistral-small-24b-instruct,LLM_judge_config
H32,hard,Clustering,spotify_2023.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1) Load data
data = pd.read_csv('spotify_2023.csv')

# 2) Select audio features and drop rows with any NaNs
features = ['danceability_%', 'energy_%', 'valence_%', 'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%', 'bpm']
data_selected = data[features].dropna()

# 3) Standardize these features...",9,"The generated code is largely correct and addresses almost all requirements of the task.

**Strong points:**
*   **Correct Feature Selection and NaN Handling:** It correctly identifies and selects the specified audio features, and handles NaNs by dropping rows.
*   **Correct Standardization:** Uses `StandardScaler` as required.
*   **Correct K-Means Application:** Applies K-Means with the specified K=4, `random_state=42`, and `n_init='auto'`.
*   **Correct PCA Application:** Reduces data to 2 components using PCA with `random_state=42`.
*   **Correct PCA Plot Coloration:** The plot correctly colors points by their cluster labels, although the implementation differs slightly from the ground truth (iterating through clusters vs. using `c` argument in `plt.scatter`). Both achieve the desired visual.
*   **Correct Mean Calculation:** Accurately calculates and displays the mean of original (unstandardized) features per cluster.
*   **Proper Imports:** All necessary libraries are imported.
*   **Good Code Structure:** The code is logically structured, following the steps outlined in the task.

**Minor Area for Improvement (preventing a perfect 10):**
*   **Plot Title Consistency:** The task specifically asked for the title ""K-Means Clusters (K=4) on PCA-Reduced Spotify Audio Features"" or a similar descriptive one. The generated code uses ""K-Means Clustering of Spotify Audio Features"", which is close but not exactly what was implied in the ground truth. This is a very minor aesthetic point but technically a deviation from the specific instruction implicitly given through the ground truth's detailed plot title.

Overall, this is a very strong submission that successfully completes the task.",mistral-small-24b-instruct,LLM_judge_config
H33,hard,Trend Analysis,spotify_2023.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# 1) Load data
data = pd.read_csv('spotify_2023.csv')

# 2) Convert `streams` to numeric (coerce errors to NaN)
data['streams'] = pd.to_numeric(data['streams'], errors='coerce')

# 3) Filter for songs with `streams` greater than 10,000,000 and where `released_year` is between 2010 and 2023, inclusive. Drop rows with NaN `streams` after conversion.
filtered_data = data[(data['streams'] > 10_000_000) & (data['released_year'] >= 2010) &...",9,"The generated code correctly implements all the requirements of the task. It loads the data, converts 'streams' to numeric, filters the data based on streams and released year, groups by year, calculates the average audio features, and generates three separate line plots with appropriate titles and labels.

**Strong points:**
*   **Correctness:** All filtering, data type conversion, aggregation, and plotting steps are correctly implemented.
*   **Completeness:** Addresses all specified requirements in the prompt.
*   **Code Quality:** The code is well-structured and follows a logical flow mirroring the task's steps. Imports are correct.
*   **Concise Filtering:** The single-line filtering for `streams` and `released_year` while dropping NaNs is efficient.

**Areas for minor improvement (to reach a 10):**
*   **Error Handling:** The ground truth includes basic error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robustness. The generated code lacks this.
*   **Plot Aesthetics:** While functional, the plots could benefit from minor aesthetic enhancements (e.g., `plt.xticks` rotation like in the ground truth for better readability of year labels, especially if more years were present; adding a grid).
*   **Readability of grouping/aggregation:** The generated code groups the data and then calculates the mean for each feature individually. While correct, the ground truth's approach using `yearly_avg_features = df_filtered.groupby('released_year')[audio_features].mean()` is slightly more concise and scalable if more features were to be analyzed.",mistral-small-24b-instruct,LLM_judge_config
H34,hard,Regression,youtube_trendings.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import OneHotEncoder

# 1) Load the dataset
data = pd.read_csv('youtube_trendings.csv')

# 2) For `Likes`, `Dislikes`, and `Video views`: remove commas, convert to numeric, and fill resulting NaNs with 0
data['Likes'] = data['Likes'].str.replace(',', '').astype(float).fillna(0)
dat...",8,"The generated code correctly addresses almost all requirements of the task.

**Strong points:**
- Correctly loads the dataset.
- Successfully cleans `Likes`, `Dislikes`, and `Video views` by removing commas, converting to numeric, and filling NaNs with 0.
- Correctly extracts the year from the `published` column and uses it as a numeric feature.
- Handles missing values in `Category` by filling with 'Unknown' and then applies one-hot encoding correctly using `OneHotEncoder`.
- Defines `X` and `y` using the specified columns and the one-hot encoded features.
- Correctly splits the data into training and testing sets with the specified `test_size` and `random_state`.
- Successfully trains a `LinearRegression` model.
- Accurately evaluates the model using R-squared and MAE.
- Includes all necessary imports.

**Areas for improvement (preventing a higher score):**
- **Robustness in data cleaning:** The generated code directly applies `str.replace(',', '')` and `astype(float)`. While this works for the given dataset, if a column like 'Likes' or 'Dislikes' already contains non-numeric values (e.g., ""N/A"" before the comma removal), `astype(float)` would raise an error. The ground truth uses `pd.to_numeric(..., errors='coerce')` which is more robust as it converts non-convertible values to `NaN`, allowing for `fillna(0)`. This is a subtle but important difference in error handling for real-world data.
- **Handling of `published` column NaNs**: The `pd.to_datetime(data['published']).dt.year` line will produce NaT (Not a Time) for unparseable dates, and then attempting `.dt.year` on NaT will result in NaN. The code doesn't explicitly `fillna(0)` for the `published` year as it does for (Likes, Dislikes, Video views). If there were unparseable dates in the `published` column, these NaNs would propagate and potentially cause issues later (e.g., if the model or subsequent operations don't handle NaNs gracefully, though many sklearn models do when fed pandas DataFrames directly). The ground truth explicitly handles this.
- **Column Dtypes after cleaning**: The ground truth explicitly casts the cleaned numeric columns to `int` (`astype(int)`). While `float` is also numeric and works for regression, `int` might be more appropriate semantically for counts like `Likes` and `Dislikes` if they are indeed whole numbers. This is a minor point.
- **Error handling for file loading**: The generated code assumes the CSV file exists. In a production environment, adding a `try-except` block for `FileNotFoundError` as seen in the ground truth is good practice.",mistral-small-24b-instruct,LLM_judge_config
H35,hard,Trend Analysis,youtube_trendings.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('youtube_trendings.csv')

# Clean the 'Video views' column
data['Video views'] = data['Video views'].str.replace(',', '').astype(float).fillna(0)

# Ensure 'published' column is treated as numeric
data['published'] = pd.to_datetime(data['published'])
data['year'] = data['published'].dt.year

# Group the data by year
grouped_data = data.groupby('year')

# Calculate metrics
average_views_per_year =...",8,"The generated code successfully addresses most of the task requirements.

**Strong Points:**
- **Data Loading:** Correctly loads the `youtube_trendings.csv` dataset.
- **Video Views Cleaning:** Accurately removes commas, converts to float, and fills NaNs with 0, which is a good approach for numerical data.
- **Published Year Handling:** Correctly converts a 'published' column (presumably 'publishedAt' as per common dataset structures) to a datetime object and extracts the year, treating it as numeric.
- **Metrics Calculation:** Correctly calculates both the average 'Video views' and the count of unique 'Video title' (assuming 'Video title' maps to 'Video' in the ground truth for unique video identification) per year.
- **Plotting:** Creates two subplots with appropriate data, titles, and axis labels as requested. `plt.tight_layout()` is used for good presentation.

**Areas for Improvement (to achieve a higher score):**
- **Column Name Discrepancy:** The task specifically mentioned `published` (year) as the column to be treated as numeric, while the ground truth uses `publishedAt` to extract the year. The generated code uses `data['published'] = pd.to_datetime(data['published'])`. If the `published` column *is* the full date string, then this is fine. If `published` was intended to be just the year, it should explicitly extract the year from `publishedAt` or a similar full date column. Given the dataset structure, `publishedAt` is more likely the full date column. The code should explicitly use `data['publishedAt']` if that's the source for year extraction, or clarify which column `published` refers to.
- **Robustness:** The code assumes the `published` column (`publishedAt` in ground truth) exists and is in a convertible format. Adding error handling for `pd.read_csv` and explicit column checks would improve robustness, similar to the ground truth.
- **Clarity of Column Names:** The task specified ""count of unique videos (count of unique `Video` titles)"". The generated code uses `'Video title'`. This is a minor point, but consistency with the task's specified column name (`Video`) would be ideal if the actual dataset uses `Video`.
- **Minor Plotting Enhancements:** While functional, the plots could benefit from minor touches like markers, line styles, or potentially more descriptive axis labels if the exact column names were ambiguous. Setting x-axis ticks to be just the years could improve readability for some datasets.

Overall, the code is very close to a perfect solution and demonstrates a strong understanding of Python, Pandas, and Matplotlib for data analysis.",mistral-small-24b-instruct,LLM_judge_config
H36,hard,Text Feature EDA,youtube_trendings.csv,"```python
import pandas as pd
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords

# Download stopwords if not already available
nltk.download('stopwords')

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Create a new feature `title_word_count`
df['title_word_count'] = df['Video'].apply(lambda x: len(x.split()))

# Cleaning and filling NaNs in the `Video View` column
df['Video views'] = df['Video views'].str.replace(',', '').astype(float).filln...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being fully correct and robust.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly calculates `title_word_count`.
*   Correctly cleans `Video views` by removing commas, converting to numeric, and filling NaNs.
*   Calculates and prints the Pearson correlation.
*   Attempts to filter by year and identify most common words.
*   Includes all necessary imports.

**Areas for Improvement/Corrections:**

1.  **Handling `FileNotFoundError`**: The generated code does not include error handling for `FileNotFoundError` when loading the CSV, which is a good practice for robustness.
2.  **`Video` column type handling**: The ground truth explicitly converts the 'Video' column to string using `astype(str)` before calculating word counts. While `apply(lambda x: len(x.split()))` might work for most cases, explicitly ensuring string type prevents potential errors if there are non-string entries in the 'Video' column. The generated code implicitly relies on `x.split()` to work.
3.  **`Video views` cleaning**:
    *   `df['Video views'] = df['Video views'].str.replace(',', '').astype(float).fillna(0)`: This line, while compact, can be problematic. If `str.replace(',', '')` results in empty strings, `astype(float)` will raise an error. The ground truth wisely uses `pd.to_numeric(errors='coerce')` which gracefully handles unparseable values by converting them to `NaN`, which are then correctly filled with 0. This is more robust.
4.  **Filtering by `Published` year**:
    *   `df_filtered = df[df['Published'].str.slice(-4).isin(['2023', '2024'])]`: This approach assumes the 'Published' column string format consistently ends with the year (e.g., ""YYYY-MM-DD"" or similar). This is brittle. The ground truth converts the 'published' column to proper datetime objects (`pd.to_datetime`) and then uses the `.dt.year` accessor, which is a much more robust and idiomatic way to handle date-based filtering in pandas.
5.  **Word processing for common words**:
    *   `word_list = df_filtered['Video'].str.lower().str.findall(r'\b\w+\b')`: Uses `findall` which is good for word extraction.
    *   `words = [word for word in words if word not in stop_words and word.strip(r'.""\'?!,')]`: The `word.strip(r'.""\'?!,')` part is incorrect for punctuation removal. `strip()` removes characters *from the beginning and end* of the string. It does not remove punctuation *within* a word (e.g., ""don't"" would become ""don't"" not ""dont""). The `re.sub(r'[^\w\s]', '', title)` approach in the ground truth is more effective for removing all punctuation. Also, `word.isalpha()` in the ground truth ensures only actual words (no numbers or empty strings) are considered, which is missing here.

In summary, while the generated code gets close, it introduces more brittle assumptions and less robust data manipulation techniques (especially in date handling and `Video views` cleaning) compared to the ground truth. The word processing isn't as thorough as required for effective text analysis.",mistral-small-24b-instruct,LLM_judge_config
H37,hard,Classification,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder

# 1) Load data
data = pd.read_csv('mobiles_2025.csv')

# 2) Clean 'Launched Price (USA)'
data['Launched Price (USA)'] = data['Launched Price (USA)'].str.replace('USD ', '').str.replace(',', '').str.extract(r'(\d+)').astype(float, errors='ignore'...",6,"The generated code addresses most of the requirements but has several issues that affect its correctness and robustness, particularly around data cleaning and NaN handling, leading to potential data misalignment and incorrect results.

**Specific Issues:**

1.  **`Launched Price (USA)` cleaning and `is_expensive_usa` generation:**
    *   The use of `.str.extract(r'(\d+)')` for `Launched Price (USA)` is problematic. It only extracts integers, discarding decimal cents if any (though `mobiles_2025.csv` seems to have only integer prices, it's not robust for all cases). The ground truth correctly uses `pd.to_numeric` directly after removing non-numeric characters.
    *   Crucially, the generated code creates `is_expensive_usa` using `(data['Launched Price (USA)'] > 1000).astype(int)`. This will assign `0` or `1` even if the price conversion failed and resulted in `NaN`. The task explicitly states: ""Drop rows where `is_expensive_usa` is NaN."" This means `is_expensive_usa` should be `NaN` if the original price was unparseable. The generated code incorrectly converts `NaN` prices to `0` for `is_expensive_usa`, thus violating the requirement to drop rows with unparseable prices. The `dropna(subset=['is_expensive_usa'])` is then applied but won't drop anything related to unparseable prices under this logic.

2.  **Camera Feature Cleaning (`extract_first_number`):**
    *   The `extract_first_number` function is oversimplified. It assumes the *first* space-separated token is the MP value. For example, ""Dual 12MP"" or ""Rear 50MP, 10MP"" would be incorrectly parsed to `12.0` or `50.0` respectively, if it was intended to handle multiple camera values. The task specifies ""extract the first numerical MP value"", which the ground truth accomplishes using `re.search(r'(\d+(\.\d+)?)\s*MP')`, explicitly looking for `MP` units. The generated code's approach could fail if the string format deviates slightly.
    *   It also doesn't handle `NaN` values gracefully; `float(value.split()[0]) if value else 0` will fail if `value` is `NaN` because `NaN` is not `None` or an empty string, and `NaN.split()` will raise an error. The `elif value is np.nan: return 0` (or similar) would be needed, or `df.apply` should be used after converting to string to prevent NaNs from raising errors directly. The ground truth handles this effectively by checking `pd.isna(camera_str)`.

3.  **One-Hot Encoding (`OneHotEncoder`):**
    *   The generated code creates `encoded_df` but does not properly integrate it into the `data` DataFrame before `X` is defined. Instead, it concatenates it side-by-side: `X = pd.concat([data[features], encoded_df], axis=1)`. This is problematic because `OneHotEncoder` by default uses **integer indexing** for its output, and `data[features]` retains the original DataFrame index. If `data` had any rows dropped *before* one-hot encoding (e.g., in the price cleaning step), the indices of `data` and `encoded_df` would no longer align, leading to misaligned features and incorrect data. The ground truth approaches this by joining encoded features with a copy of the main dataframe (`df_encoded = pd.get_dummies(df_cleaned, ...)`) ensuring index alignment.

4.  **NaN Handling (`X = X.dropna().reset_index(drop=True)` and `y = y[X.index]`):**
    *   This is the most critical flaw. After `X` is formed by concatenating `data[features]` and `encoded_df` (which has integer index 0...N-1), `X.dropna()` will drop rows from *this concatenated DataFrame*. When `X.dropna()` is called, it creates a new DataFrame `X` with a potentially different (shorter) index.
    *   Then, `y = y[X.index]` attempts to select `y` values based on the *new* index of `X`. Since `encoded_df` had a fresh 0-based index and `data[features]` had the original, likely non-contiguous index, `X` itself will have a non-meaningful index after concatenation. Dropping NaNs from `X` after this concatenation and then re-indexing `y` based on `X`'s *new* index will lead to a complete mismatch between features and target. The target `y` (which correctly retains the original index from `data`) will be completely out of sync with the rows that remain in `X` after `dropna`. This would result in training the model on completely scrambled feature-target pairs, making the results meaningless.
    *   The ground truth addresses this by keeping `X` and `y` aligned throughout by working on a single `df_cleaned` DataFrame, and only dropping NaNs from `X` and `y` *together* using `pd.concat([X, y], axis=1).dropna()`, which correctly removes rows where NaNs exist in *either* `X` or `y` while preserving their alignment.

5.  **Output:** The ground truth includes sensible print statements for progress and shapes, which are good practice for understanding data flow. The generated code lacks these.

**Recommendations for Improvement:**

*   **Robust Price Cleaning:** Use `pd.to_numeric` with `errors='coerce'` as done in the ground truth for `Launched Price (USA)`. Define `is_expensive_usa` such that `NaN` prices result in `NaN` target values, then drop those rows.
*   **Robust Camera Feature Extraction:** Adopt the regex-based `extract_mp` function from the ground truth; it's much more robust and handles `NaN` values and various string formats correctly.
*   **Correct One-Hot Encoding and Index Alignment:** Perform one-hot encoding directly on your working DataFrame (`data`) using `pd.get_dummies()`. This implicitly handles index alignment correctly. Avoid concatenating DataFrames with misaligned indices as done in the generated code.
*   **Correct NaN Handling for X and y:** Combine `X` and `y` for final `dropna` if you decide to separate them early, or ensure all cleaning steps (especially those that drop rows) are applied to the `data` DataFrame *before* `X` and `y` are extracted. The ground truth's `pd.concat([X, y], axis=1).dropna()` is a good pattern.
*   **Add Informative Print Statements:** Include print statements to show progress, data shapes, and important intermediate steps.",mistral-small-24b-instruct,LLM_judge_config
H38,hard,Price Analysis,mobiles_2025.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Define a function to clean price columns
def clean_price(price):
    try:
        price = price.replace('PKR', '').replace('INR', '').replace('CNY', '').replace('USD', '').replace('AED', '').replace(',', '')
        return pd.to_numeric(price, errors='coerce')
    except:
        return np.nan

# Apply the cleaning function to the specified columns
price_columns = [
    'Launched Price (P...",9,"The generated code is largely correct and addresses most of the requirements.

**Strong Points:**
*   Correctly loads the dataset.
*   The `clean_price` function correctly removes currency symbols and commas and converts to numeric. Using `pd.to_numeric(errors='coerce')` is an efficient way to handle conversion errors, largely similar to the ground truth method of returning `np.nan`.
*   Correctly identifies and applies the cleaning function to all specified price columns.
*   Calculates `Average Global Price` by taking the mean across the specified columns.
*   Correctly identifies the top 5 models using `nlargest`.
*   Displays the 'Model Name' and 'Average Global Price' for the top 5 models.

**Areas for Minor Improvement:**
*   **NaN Handling in Average Calculation:** The `df[price_columns].mean(axis=1)` implicitly handles `NaN` values by default using `skipna=True`. While this works, explicitly adding `skipna=True` as in the ground truth code would make the intention clearer and more robust, especially if pandas' default behavior were to change in a future version.
*   **Edge Case for `clean_price` on `NaN` Input:** The ground truth `clean_price` function explicitly checks `if pd.isna(price_str): return np.nan`. While `price.replace()` might throw an `AttributeError` if `price` is already `NaN` (which would then be caught by the `try-except` block), adding a direct `pd.isna` check can prevent unnecessary try-except overhead for already NaN values and make the function's behavior more explicit. However, in practice, `df.apply` on a column with `NaN`s will pass `NaN` as input, and if the first operation (like `.replace()`) is called on it, an `AttributeError` will occur, leading to the `except` block and returning `np.nan`. So, it works, but the explicit check is slightly cleaner.
*   **File Not Found Handling:** The generated code lacks explicit `try-except` block for `pd.read_csv`, which is good practice for robustness in real-world scenarios, as implemented in the ground truth. This is a minor omission but a good practice.
*   **`price.replace()` vs `str(price).replace()`:** If `price` column had mixed types (e.g., some numeric, some string), `price.replace()` might fail. The ground truth explicitly converts to string `str(price_str)` inside the `try` block, which is safer. However, for a CSV column, `pd.read_csv` typically ingests these as objects (strings) which then causes `price.replace()` to work as intended. The `pd.to_numeric(errors='coerce')` in the generated code is good enough to handle non-string inputs that aren't purely numeric.

Overall, the generated code is very close to perfect and fulfills the task requirements effectively. The minor points are mostly about robustness and explicit clarity rather than fundamental correctness errors.",mistral-small-24b-instruct,LLM_judge_config
H39,hard,Comparative Analysis,mobiles_2025.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# 1) Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2) Clean the columns
def clean_column(col, unit):
    return col.str.replace(unit, '').astype('float', errors='ignore')

df['RAM'] = clean_column(df['RAM'], ' GB')
df['Battery Capacity'] = clean_column(df['Battery Capacity'], ' mAh')
df['Launched Price (USA)'] = clean_column(df['Launched Price (USA)'], 'USD ').str.replace(',', '').astype('float', errors=...",7,"The generated code correctly addresses most of the task requirements, including loading the data, filtering for Apple products, grouping by year, and generating box plots for the specified metrics.

However, there are several areas for improvement:
- **Cleaning `RAM` and `Battery Capacity`**: The `clean_column` function uses `astype('float', errors='ignore')`. While this might seem intuitive, `errors='ignore'` returns the original array, not converting problematic values to NaN. `pd.to_numeric(errors='coerce')` is the correct method for ensuring non-numeric values become NaNs, which is crucial for the subsequent `dropna`. The `Generated Code` also assumes the units directly follow the numbers (e.g., ' GB'), which might not always be the case (e.g., 'GB' without a space), making the split less robust than a simple `replace`.
- **Cleaning `Launched Price (USA)`**: The `str.replace(',', '').astype('float', errors='ignore')` part for price cleaning has the same `errors='ignore'` issue. Furthermore, it assumes 'USD ' is always present and at the start. If 'USD' is inside the string, or if there's no 'USD', the regex in the ground truth is more robust.
- **Handling NaNs during cleaning**: The `dropna` call based on `['RAM', 'Battery Capacity', 'Launched Price (USA)']` is correct *after* the conversion attempt. However, if the `astype('float', errors='ignore')` fails to convert to actual floats and leaves strings, `dropna` won't work as intended on those columns because they aren't `NaN` but still non-numeric strings. This is a critical logical flaw stemming from the cleaning method.
- **Plotting**: The generated code uses `vert=False` for horizontal box plots, while the prompt implied vertical (typical distribution plots). While not explicitly forbidden, it's a stylistic choice that differs from the common expectation and the ground truth. Also, the plots are saved as images but `plt.show()` is still called in the loop, which will open many windows. It's generally better to either save *or* show, not both, unless explicitly required. The titles and labels are present, but the y-axis label for RAM and Battery Capacity is just the metric name (`RAM`, `Battery Capacity`), not including the units (`RAM (GB)`, `Battery Capacity (mAh)`), which was a specific requirement.
- **No `Launched Year` cleaning**: The `Launched Year` column is used for grouping but isn't explicitly cleaned to ensure it's numeric and handling potential NaNs, as done in the ground truth. This could lead to errors if `Launched Year` is not clean.
- **Code style**: While imports are correct, the lack of `seaborn` (used in ground truth to set a better plot style) means the plots might look less aesthetically pleasing. No font sizes, specific colors, or gridlines were set using `matplotlib` properties.",mistral-small-24b-instruct,LLM_judge_config
H4,hard,Advanced Machine Learning,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score, learning_curve, validation_curve
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.impute import SimpleImputer, IterativeImputer
from sklea...",4,"The generated code attempts to address many aspects of the prompt but falls short in several critical areas, particularly regarding correctness, completeness, and adherence to advanced techniques.

**Critical Issues (Why the low score):**

*   **Incorrect Hyperparameter Tuning Application:** The generated code initializes `GridSearchCV` instances (e.g., `grid_rf = GridSearchCV(...)`) but *never calls `.fit()` on them*. This means `grid_rf.best_estimator_` (and for all other models) will raise an `AttributeError` or contain default/uninitialized estimators. This is a fundamental flaw, as none of the base models or ensembles will actually be tuned. The subsequent pipelines and ensembles will be built with untuned models, rendering the tuning step effectively useless.
*   **Missing LIME Implementation:** The prompt specifically requested LIME for model interpretability, but it is entirely absent from the generated code. Only SHAP is included.
*   **Incomplete Feature Engineering:**
    *   The `NameFeatureEngineer` in the generated code uses a hardcoded `title_mapping` and then immediately re-uses the original numeric `Name` column for `simpleimputer` and `onehot` through the `text_transformer`. This is incorrect. The `Name` column should be dropped after `Title` is extracted, and `Title` (or other name-derived features) should be put through a categorical/numeric pipeline.
    *   The generated code uses separate, sequential custom transformers (`AgeImputer`, `FareImputer`, `CabinImputer`) which are not integrated into the main `ColumnTransformer` or pipeline. This is less robust and efficient than integrating imputation into `ColumnTransformer` (as in the ground truth) or handling it more holistically. The `AgeImputer` also incorrectly applies `fillna` with `apply` which might not work as intended within a pipeline.
    *   ""Text mining from names"" is very basic, only extracting a 'Title' and mapping it to numbers. No advanced text features like name length or rarity of title are generated.
*   **Incorrect Feature Selection Application:** The feature selection steps (e.g., `selector_rf`, `selector_rfe`, `selector_kbest`) are initialized but *never used* to transform the data or compare model performance with selected features, which was a specific requirement (""compare their impact"").
*   **Model Evaluation Incompleteness:**
    *   The `plot_dependence` from `shap` is imported but not used.
    *   In statistical significance tests, `ttest_ind` is used to compare `predict_proba` outputs directly, which is not the standard way to compare model performance. The prompt asked for statistical significance tests *between models* using their cross-validation scores (typically a paired t-test or Wilcoxon signed-rank test), which is more appropriate for comparing algorithms trained on the same data splits. The generated code calls `ttest_ind` on the raw probabilities, which is not meaningful for comparing models.

**Other Areas for Improvement:**

*   **Imports:** While many imports are present, `lime` is missing.
*   **Code Structure/Clarity:** The placement of some sections (e.g., `pipeline` creation and evaluation after feature selection) is a bit illogical given that feature selection results are not integrated.
*   **Advanced Imputation Strategy:** `IterativeImputer` is imported but not used. `SimpleImputer` is used for numeric columns, contradicting the prompt's request for ""multiple imputation strategies"".
*   **Polynomial Features:** `PolynomialFeatures` is created (`poly = PolynomialFeatures(...)`) but never integrated into the `ColumnTransformer` or main pipeline, meaning it's not actually applied. It's passed to `make_pipeline` as a separate step after `preprocessor`, but this setup is not standard for `ColumnTransformer` integration.
*   **Hyperparameter Search Space:** The `GridSearchCV` parameter grids are very small (e.g., only 2 values for `n_estimators`, 2 for `max_depth`), which is more indicative of a basic grid search rather than a comprehensive or randomized search for ""tuning multiple base models"". RandomizedSearchCV was also requested explicitly, but only GridSearchCV is used.
*   **Model Instantiation in Ensembles:** The `stacking` and `voting` classifiers are instantiated *before* the GridSearchCV instances are fitted. This means `grid_rf.best_estimator_` (and others) will error out or contain default estimtors, making the ensembles untuned. The fitting of GridSearchCV should happen first, then the ensembles are defined using the *best found estimators*.",mistral-small-24b-instruct,LLM_judge_config
H40,hard,Regression,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1) Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# 2) Define features X using the Year and Runtime columns
X = data[['Year', 'Runtime']]

# 3) Define the target y using the Rating column
y = data['Rating']

# 4) Split the data into training (80%) and testing (20%) sets, using random_state=42
X_...",7,"The generated code correctly implements the core requirements of loading data, defining features and target, splitting data, training a Linear Regression model, and evaluating it with R-squared and MAE. All specified imports are present, and the code structure is logical.

However, the major drawback is the lack of robustness in data handling. The `imdb_top_250.csv` dataset typically has 'Runtime' in a string format (e.g., '142 min') and might have other columns or values that are not directly numeric.

Specifically, the generated code is missing:
1.  **Error Handling for File Loading:** It assumes the file exists and will throw a `FileNotFoundError` if not, which is not handled gracefully.
2.  **Robust Data Preprocessing:** The 'Runtime' column needs to be cleaned (e.g., stripping ' min' and converting to numeric) before it can be used as a feature. Without this, the model will fail or yield incorrect results if 'Runtime' is not purely numeric. The 'Year' column might also benefit from `pd.to_numeric` with `errors='coerce'`.
3.  **Handling Missing Values:** If any `NaN` values are introduced during the conversion (e.g., if 'Runtime' has non-numeric entries or if 'Year'/'Rating' are invalid), the `LinearRegression` model might yield a `ValueError` or inconsistent results. The ground truth explicitly handles dropping rows with NaN values.

While the core logic is sound, the lack of these preprocessing steps makes the code fragile and potentially non-functional with the real-world dataset as it often appears online. Adding these would significantly improve its reliability and score.",mistral-small-24b-instruct,LLM_judge_config
H41,hard,Clustering,imdb_top_250.csv,"```python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Select the features for clustering
features = data[['Rating', 'Runtime']]

# Standardize these features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=3, n_init='auto', random_state=42)
data['Cluster'] = kmean...",6,"The generated code correctly performs data loading, feature selection, standardization, and K-Means clustering with the specified parameters (`n_clusters=3`, `n_init='auto'`, `random_state=42`). It also calculates and prints the mean rating and runtime for each cluster.

However, there are several issues, primarily regarding the scatter plot and data handling:

1.  **Incorrect X-axis in Plot**: The task explicitly asks for ""original (non-standardized) `Runtime` (x-axis) vs. `Rating` (y-axis)"". The generated code plots `Rating` on the x-axis and `Runtime` on the y-axis, which is the reverse of the requirement.
2.  **Plotting Method**: The generated code uses a `for` loop to plot each cluster separately, which is less efficient and generally not preferred compared to using `plt.scatter(x, y, c=labels, ...)` where `c` directly maps to cluster labels. This also makes it harder to get a proper color bar.
3.  **Missing Data Cleaning/Handling**: The ground truth includes robust data cleaning for the `Runtime` column (converting '142 min' to numeric) and handling missing values in both `Rating` and `Runtime`. The generated code assumes the columns are perfectly numeric and clean, which is often not the case with real-world datasets like 'imdb_top_250.csv'. This would lead to errors or incorrect results if the 'Runtime' column contains ' min' strings or if there are NaNs.
4.  **Plot Labels/Title**: While it has a title and labels, the labels are swapped due to the incorrect axis plotting. The `cmap` and `s` parameters for scatter points are also missing, which could improve visualization.
5.  **Output Format for Means**: While correct, the ground truth uses `groupby().mean()` which is more concise and idiomatic Pandas for aggregating by groups.

These deficiencies, especially the critical plotting error and lack of data robustness, prevent a higher score.",mistral-small-24b-instruct,LLM_judge_config
H42,hard,Text Analysis,imdb_top_250.csv,"```python
import pandas as pd
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk
from scipy.stats import pearsonr

# Ensure NLTK stopwords resource is available
nltk.download('stopwords')
nltk.download('punkt')

# Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# Create a new feature called `title_length`
df['title_length'] = df['Title'].apply(len)

# Calculate Pearson correlation coefficient
correlation_coef...",6,"The generated code addresses most of the requirements but has several issues that prevent it from achieving a higher score.

**Correctness:**
- **Pearson Correlation:** The correlation calculation `pearsonr(df['title_length'], df['Rating'])` will likely fail or produce incorrect results if `df['Rating']` is not purely numeric or contains non-finite values (like NaNs). The ground truth correctly handles this by converting to numeric and dropping NaNs.
- **Punctuation Removal and Tokenization:** The line `df['processed_title'] = df['Title'].apply(lambda x: word_tokenize(x.lower().translate(str.maketrans('', '', string.punctuation))))` is problematic. `string.punctuation` removes punctuation, but `word_tokenize` might re-interpret some combined words (e.g., ""U.S.A"", ""rock'n'roll"") incorrectly after punctuation removal if not handled carefully, or struggle with remaining non-alphanumeric characters that aren't purely ""punctuation"" as per `string.punctuation`. The prompt specified removing characters that are *not alphanumeric or whitespace*, which `string.punctuation` does not fully cover (e.g., symbols like ©, ™, etc.). The ground truth's regex approach is more robust. Also, `word_tokenize` is applied *after* punctuation removal, which typically happens *before* or during tokenization, or `word_tokenize` is expected to handle it. A more robust approach combines lowercasing, comprehensive non-alphanumeric removal, and then tokenization.

**Completeness:**
- The prompt explicitly asks to print the correlation with 4 decimal places, which the generated code does not enforce (though it might coincidentally print that way).
- The prompt asks to print ""10 most common words along with their frequencies"" with a specific format or structure. The generated output for common words is just `word: freq` without a header which makes it less clear.

**Code Quality:**
- The NLTK downloads are placed directly, which is fine, but the ground truth's `try-except LookupError` block is a more graceful way to handle cases where resources might already be present, avoiding unnecessary downloads and potential warnings.
- The use of `string.punctuation` for removing punctuation is less flexible than regular expressions as seen in the ground truth, especially for the prompt's specific requirement (""characters that are not alphanumeric or whitespace"").
- The problem description asked specifically for `string` import (which was correctly included), but the ground truth correctly noted that `re` is often better for robust punctuation removal, which the generated code does not use.

Overall, the code provides a basic solution, but lacks robustness in data handling (correlation) and text preprocessing (punctuation removal/tokenization), which are key aspects of the task.",mistral-small-24b-instruct,LLM_judge_config
H43,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Load the dataset
data = pd.read_csv('penguins.csv')

# Define features and target
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper...",6,"The generated code attempts to use pipelines effectively, which is a good practice. However, it fails on several critical points that make it incorrect and incomplete for the given task.

**Incorrectness & Incompleteness:**

1.  **Redundant and Incorrect `preprocessor` Pipeline:** The code defines `preprocessor` using `ColumnTransformer`, which is appropriate for applying different transformers to different columns. However, it then wraps this `ColumnTransformer` *again* in a `Pipeline` named `preprocessor`, and adds `('cat_encoder', OneHotEncoder(handle_unknown='ignore'))`. This is fundamentally incorrect because `ColumnTransformer` already handles the transformations (including one-hot encoding if `OneHotEncoder` were placed correctly within the `categorical_transformer` pipeline). The `OneHotEncoder` should be part of the `categorical_transformer` within the `ColumnTransformer`, not a separate step *after* the `ColumnTransformer` which is supposed to output transformed features ready for the classifier. As a result, the categorical features will not be correctly one-hot encoded and combined with numerical features before classification.
    *   **Correct approach:** `OneHotEncoder` should be the final step inside the `categorical_transformer` pipeline defined for the `ColumnTransformer`.

2.  **Handling `island` NA after mode imputation:** The prompt specifically mentions: ""if a row still has NA in `island` after mode imputation, e.g. if all were NA, it can be dropped, or a placeholder like 'Unknown' can be used if mode is NA itself. For this task, if mode imputation results in NA for island, drop those rows"". The `SimpleImputer(strategy='most_frequent')` will only impute existing NAs. If the `mode()` itself is NA (meaning all values in `island` were NA), `SimpleImputer` would typically raise an error or produce an unexpected result, and it certainly won't *drop* rows. The ground truth correctly handles this edge case by checking if `island` column is entirely NA and dropping rows accordingly. The generated code completely misses this specific requirement for `island` column, which is a significant omission.

3.  **Missing `species` NA handling:** The ground truth includes a crucial step to handle rows where the `species` (target variable) is missing. The generated code does not address this, assuming `y` will be complete. If `species` has NAs, `LabelEncoder` or subsequent steps may fail or produce incorrect results.

4.  **No `drop_first` for `OneHotEncoder`:** While not explicitly requested, `drop_first=True` is often used in one-hot encoding to avoid multicollinearity. The prompt does not specify `True` or `False`, but the ground truth uses `False`. The generated code uses `handle_unknown='ignore'` but doesn't set `drop_first`, which means it defaults to `False`, so this particular point aligns with the ground truth's outcome for `pd.get_dummies` default, but it's not explicitly controlled like in the ground truth.

**Good Points:**

*   Uses `Pipeline` and `ColumnTransformer` which is a good architectural choice for preprocessing.
*   Correctly uses `SimpleImputer` for numerical (median) and categorical (most frequent).
*   Correctly uses `StandardScaler` for numerical features.
*   Correctly uses `LabelEncoder` for the target variable.
*   Correctly sets up `RandomForestClassifier` with the specified parameters.
*   Correctly uses `cross_val_score` for evaluation.
*   Proper imports are included.

The main reasons for the lower score are the incorrect pipeline structure for `OneHotEncoder` and the complete mishandling/omission of the specific NA dropping requirement for the `island` column, and missing the target NA handling.",mistral-small-24b-instruct,LLM_judge_config
H44,hard,Clustering,penguins.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Select the features
features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
data = data[features]

# 3) Drop any rows with missing values
data = data.dropna()

# 4) Standardize the features
scaler = Sta...",6,"The generated code correctly performs steps 1-7 for K-Means clustering and PCA visualization. It loads the data, selects features, handles missing values, standardizes, applies K-Means, applies PCA, and generates the first plot correctly.

However, the second plot (step 8) is incorrect. The issue lies in how `original_data` is handled for the second plot.
1. The code reloads `penguins.csv` for the second plot, which is inefficient and unnecessary.
2. When `original_data.dropna(subset=features)` is called, it drops rows based on the *original* state of `original_data`, not considering the rows that were *already* dropped in the `data` DataFrame used for standardization and PCA. This means that the `species_labels` collected for the second plot might not correspond row-wise to the `principal_components` and `cluster_labels` which were derived from a different set of rows after NA values were dropped from the *initial* feature selection. This leads to an `IndexError` if the number of rows doesn't match, or incorrect plotting if it happens to match but the mapping is off.

To fix this, the `species` column should be captured *before* dropping NAs from the features and then aligned with the final set of rows used for clustering and PCA, similar to how the ground truth code handles `original_species_labels`.

Additionally, the generated code uses `plt.scatter` directly without using `seaborn`, which is fine but less aesthetically pleasing for presentation plots. The `c=species_labels['species']` for coloring might lead to `TypeError` if `species` column contains non-numeric values as it expects numerical or a list of colors. It works if matplotlib's internal categorical mapping can handle it, but it's less robust than Seaborn's `hue` argument which automatically handles categorical data.",mistral-small-24b-instruct,LLM_judge_config
H45,hard,Feature Engineering,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Impute missing values
imputer_numeric...",6,"The generated code addresses most of the requirements but has several issues that reduce its score.

**Correctness:**
- **Imputation:** The imputation of numerical features is correct in principle, but `SimpleImputer` is re-instantiated and re-fit for each column, which is inefficient and not how it's typically used (`fit_transform` should be called once on the relevant columns). However, it does still achieve the desired imputation.
- **One-hot encoding:** The `ColumnTransformer` is used correctly for one-hot encoding `island` and `sex`.
- **Feature Names for Importance:** The way feature names are extracted from the `ColumnTransformer` is clever and mostly correct, but it leads to a subtle bug related to the order of `ColumnTransformer`'s internal stages for the numerical features compared to how `get_feature_names_out` works for simple numerical columns. The current code assumes numerical features are directly passed through, which is true, but `get_feature_names_out` with `numerical_features` won't work that way. It should be applied to the preprocessor's output. Wait, `preprocessor.transformers_[0][1].get_feature_names_out(numerical_features)` is correct, my mistake. This part is actually quite robust.
- **Box Plot Incorrectness:** This is a major issue. The box plots are incorrect.
    - `plt.subplot(1, 3, list(data['species'].unique()).index(species) + 1)`: This creates 3 subplots (assuming 3 species).
    - `boxplot_data = data[data['species'] == species]['flipper_length_mm']`: This correctly filters the data.
    - `sns.boxplot(x='sex', y=boxplot_data, data=data[data['species'] == species])`: Here, `y=boxplot_data` is incorrect. `sns.boxplot` expects `y` to be a column name within the `data` DataFrame. Passing a Series directly for `y` causes issues, especially when `x` is also specified and `data` is also provided. The plot that is generated is a single box plot for the entire `boxplot_data` with `sex` labels on the x-axis, but the y-axis values are not grouped correctly by sex. It should be `y='flipper_length_mm'`. This is a significant functional error for the visualization requirement.
    - The `plt.subplot` usage leads to incorrect visualization where all three species' plots are drawn on top of each other in the first subplot if run sequentially, or only the final one if `plt.show()` is called outside the loop. If `plt.show()` is called inside the loop, the subplots are not correctly arranged. The task asked for ""For each species, create box plots..."", which implies a separate plot for each species, or a single combined plot as in the ground truth. The current implementation tries to combine them but fails.

**Code Quality:**
- **Redundant Imputer Instantiation:** As mentioned, `imputer_numerical` and `imputer_categorical` are instantiated and fitted repeatedly. This is inefficient. Using a `Pipeline` or fitting once on all relevant columns would be better.
- **Unnecessary Imports:** `train_test_split` and `Pipeline` (though `Pipeline` could have been used to improve some parts) are imported but not used.
- **Readability:** The `ColumnTransformer` usage for feature engineering is good practice.

**Completeness:**
- The code loads the data, performs the specified imputations (though imperfectly for numerical columns), defines `X` and `y` correctly, trains the model, and prints feature importances with correct names.
- The visualization requirement is not met correctly due to the plotting error mentioned above. The task asked for ""For each `species`, create box plots to visualize the distribution of `flipper_length_mm` (using imputed data) grouped by the imputed `sex` column."" The provided plot attempts this but fails to render correctly per species, and the `y` axis parameter is used incorrectly. The ground truth's `sns.boxplot(data=df_processed, x='species', y='flipper_length_mm', hue='sex', palette='viridis')` is a much more concise and correct way to achieve the visualization.

The primary reason for a score of 6 is the significant bug in the plotting section, which fails to correctly visualize the data as required, and the less optimal imputation code. While the feature importance part is well-executed, the full task is not completed as intended.

To improve:
1.  **Correct the box plot generation:** Use `sns.boxplot` in a way that truly groups by sex for each species (e.g., passing `species` to `col` or using `hue` if all on one plot, or iterating and creating a new figure for each species as shown in the commented out section of the ground truth). The simplest fix to meet the requirement (similar to ground truth) would be `sns.boxplot(data=data, x='species', y='flipper_length_mm', hue='sex')`.
2.  **Optimize Imputation:** Apply `SimpleImputer` more efficiently, possibly within the `ColumnTransformer` or by fitting once on all numerical columns. However, the current code's approach to the `ColumnTransformer` already handles the imputation within it, so the lines `data['culmen_length_mm'] = imputer_numerical.fit_transform(data[['culmen_length_mm']])` etc. become redundant and potentially problematic if `ColumnTransformer` then re-imputes on potentially already imputed data (though in this case `num` transformer is set to `imputer_numerical`, which is a distinct object). The issue is that it's fitting `imputer_numerical` 4 times on single columns before the `ColumnTransformer` even runs. The `ColumnTransformer` would then use a *new* instance of `imputer_numerical` that is *not* fit on the original raw data's mean but rather on the means of the already imputed columns - but wait, the `ColumnTransformer`'s `imputer_numerical` would be a *new* instance. The `preprocessor` setup is good, and it *should* handle the imputation. The manual imputation lines before it are extraneous and potentially confusing. For example, if `preprocessor = ColumnTransformer(transformers=[('num', SimpleImputer(strategy='mean'), numerical_features), ('cat', OneHotEncoder(), categorical_features)])`, it would be much cleaner, and no manual `imputer_numerical.fit_transform` would be needed. This is a subtle but important point in data handling.Score: 6 Feedback: The generated code addresses most of the requirements but has several issues that reduce its score.

**Correctness:**
- **Imputation of numerical features:** The direct application of `imputer_numerical.fit_transform` for each numerical column independently is inefficient and atypical, but it does achieve the correct imputation for `data` before being passed to the `ColumnTransformer`. However, the `ColumnTransformer` then uses a *new* instance of `SimpleImputer` internally (because `imputer_numerical` was passed, not a fitted one), which would then re-impute on the *already imputed* data. This is problematic, as `SimpleImputer` would calculate means on data that has already had its NaNs replaced, leading to a potential double imputation or miscalculation if not handled carefully. The correct approach would be to let the `ColumnTransformer` handle all imputation, or to fit `SimpleImputer` once on all relevant columns or create new imputer instances for each column.
- **Box Plot Visualization:** This is the most significant functional error. The `sns.boxplot` call `sns.boxplot(x='sex', y=boxplot_data, data=data[data['species'] == species])` is incorrect. Specifically, `y=boxplot_data` (which is a Series) is not how `seaborn` expects a `y` argument when `data` is also provided. `y` should be a column name within the `data` DataFrame. This results in plots that do not correctly show the distribution of `flipper_length_mm` grouped by `sex` for each species. Additionally, the `plt.subplot` loop leads to overplotting if `plt.show()` is only called once at the end, or separate unmanaged plots if called inside the loop, rather than a single coherent figure of subplots or distinct figures as intended by the task description.

**Code Quality:**
- **Redundant Imputation:** The explicit `data['culmen_length_mm'] = imputer_numerical.fit_transform(...)` lines are redundant and potentially problematic given that the `ColumnTransformer` then re-applies an imputer to these same columns. The `ColumnTransformer` itself should be sufficient for handling feature transformations including imputation.
- **Unused Imports:** `train_test_split` and `Pipeline` are imported but not used.
- **Naming Redundancy:** `preprocessor.transformers_[0][1].get_feature_names_out(numerical_features)` is correct for getting numerical feature names, but it's a bit verbose.

**Completeness:**
- The code successfully loads the dataset, imputes the `island` NA values by dropping rows, and imputes `sex` with its mode.
- It correctly defines `X` using the four imputed numerical measurements, plus one-hot encoded `island` and imputed `sex`.
- It trains a `GradientBoostingClassifier` and extracts/prints feature importances with reasonably correct names.
- However, the visualization part is fundamentally flawed and does not correctly generate the required box plots for `flipper_length_mm` per `species` grouped by `sex`. This fails to meet the completeness of the visualization requirement.

The primary issues are the incorrect and inefficient imputation setup (even if it ends up replacing NaNs) and, critically, the incorrect implementation of the box plots which leads to non-functional visualization.

To achieve a higher score:
1.  **Correct the Box Plot:** The simplest way to fulfill the visualization requirement for ""flipper_length_mm by species grouped by sex"" using Seaborn would be to use `sns.boxplot(data=data, x='species', y='flipper_length_mm', hue='sex', palette='viridis')` on a single plot, as demonstrated in the ground truth. If separate plots for each species are preferred, each should be a new figure (`plt.figure()`) and `sns.boxplot(x='sex', y='flipper_length_mm', data=data[data['species'] == species])` should be called.
2.  **Streamline Imputation with ColumnTransformer:** Remove the manual `imputer_numerical.fit_transform` lines. Let `ColumnTransformer` handle all of the numerical imputation. Define the `ColumnTransformer` as:
    ```python
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', SimpleImputer(strategy='mean'), numerical_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # handle_unknown can be useful
        ])
    ```
    This way, `numerical_features` will be imputed *within* the `ColumnTransformer` pipeline correctly. Additionally, ensure `sex` is imputed *before* being used in `categorical_features` for the `ColumnTransformer's` `OneHotEncoder`, or use `SimpleImputer(strategy='most_frequent')` for `sex` within the `ColumnTransformer` too. The `ColumnTransformer` is powerful for this.
3.  **Error Handling for File Loading:** Add a `try-except FileNotFoundError` block for `pd.read_csv`, as demonstrated in the ground truth, for robustness.",mistral-small-24b-instruct,LLM_judge_config
H46,hard,Data Cleaning,penguins.csv,"Sure, here is the Python code to perform the advanced data cleaning as described:

```python
import pandas as pd
from sklearn.impute import KNNImputer

# Load the dataset
df = pd.read_csv('penguins.csv')

# Numerical columns for KNN imputation
num_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']

# Apply KNN imputation to numerical columns
imputer = KNNImputer(n_neighbors=3)
df[num_cols] = imputer.fit_transform(df[num_cols])

# Impute categorical column 'sex' wi...",9,"The generated code correctly addresses all the requirements of the task: it loads the dataset, performs KNN imputation on the specified numerical columns, imputes the 'sex' column with its mode, and prints the remaining NA values. The imports are correct, and the code structure is clean and follows the steps outlined in the task.

One minor improvement would be to include error handling for `FileNotFoundError` when loading the CSV, as demonstrated in the ground truth. While not explicitly asked for in the prompt's output format, it's good practice for robust code. Additionally, the ground truth includes more verbose print statements for better verification steps and user feedback, which could enhance the generated code's clarity for debugging or presentation purposes. However, the core imputation logic is sound and complete according to the prompt's requirements.",mistral-small-24b-instruct,LLM_judge_config
H47,hard,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1. Load the dataset
penguins = pd.read_csv('penguins.csv')

# 2. Filter the dataset to include only 'Adelie' and 'Gentoo' species
filtered_penguins = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]

# 3. Handle NA values in the `culmen_length_mm` column
filtered_penguins = filtered_penguins.dropna(subset=['culmen_length_mm'])

# 4. Perform an independent two-sample t-test
adelie_culmen_length = filtered_penguins[filtered...",9,"The generated code correctly performs all requested steps and provides a valid solution to the problem.

Strong points:
- **Correctness**: All steps (loading, filtering, NA handling, t-test, and interpretation) are performed accurately.
- **Conciseness**: The code is straightforward and directly addresses the requirements without unnecessary complexity.
- **Readability**: The variable names are clear, and the comments align with the task's numbered requirements, making it easy to follow.

Areas for minor improvement (to achieve a perfect 10):
- **Error Handling**: The code currently lacks error handling for `FileNotFoundError` if 'penguins.csv' is not present. While the prompt asks for ""only Python code"" and implies the CSV exists, robust production-ready code would include `try-except` blocks.
- **Function Encapsulation**: Wrapping the logic within a function (as done in the ground truth) would make the code more reusable and modular, adhering to good software engineering practices for benchmarks.
- **Magic Number**: The significance level `0.05` is hardcoded. Defining it as a variable (e.g., `alpha = 0.05`) at the top would improve readability and make it easier to change if needed.
- **Informative Prints**: While it prints the required values, adding more descriptive print statements (e.g., sample sizes, mean values of each group, or explanations for each step) would enhance the output, similar to the ground truth. This is more about user experience for the output than functional correctness.
- **`copy()` when filtering**: It's good practice to use `.copy()` when filtering DataFrames to avoid `SettingWithCopyWarning` in downstream operations (though not strictly necessary here since `dropna` is applied immediately). The ground truth includes this, which is a good habit.",mistral-small-24b-instruct,LLM_judge_config
H48,hard,Classification,penguins.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Define features and target
fe...",9,"The generated code successfully completes all tasks outlined in the problem description, including data loading, defining features, comprehensive preprocessing (median imputation for numeric, mode imputation for sex, one-hot encoding for categorical features, and label encoding for the target), stratified data splitting, training a RandomForestClassifier within a pipeline, making predictions, and evaluating the model with accuracy and a classification report.

Strong points:
- Correctly uses `Pipeline` and `ColumnTransformer` for robust preprocessing.
- Implements all specified imputation strategies (median for numeric, mode for 'sex').
- Correctly applies OneHotEncoder to 'island' and 'sex' and LabelEncoder to 'species'.
- Correctly performs a stratified train-test split with the specified `random_state`.
- Trains the RandomForestClassifier with the specified `n_estimators` and `random_state`.
- Prints accuracy and a full classification report with correct target names.

Areas for minor improvement (preventing a perfect 10):
- The `LabelEncoder` is applied to `data[target]` *before* the train-test split. While this works because the `ColumnTransformer` only operates on `X`, it's generally better practice to apply transformations that modify `y` (like Label Encoding) either:
    1. Directly to `y` *after* `X` and `y` are separated but *before* the `train_test_split`.
    2. Or, if applicable, making the target transformation part of a broader pipeline (though for simple label encoding, separate `X` and `y` handling is common).
    The ground truth code separates `X` and `y` first, then encodes `y`, which is a slightly cleaner separation of concerns. In the generated code, `X` (`data[features]`) still contains the original `species` column in `data` when `data[target]` is encoded, but since `X` is later redefined *without* that column, it doesn't cause a direct error. It's a point of minor confusion or potential for error in larger, more complex pipelines.
- No error handling for `FileNotFoundError` when loading the CSV, which the ground truth included. While not explicitly asked for, it's good practice.
- The print statements are minimal compared to the ground truth, which provided more verbose output about data shapes, head, and process steps. This might be seen as less 'user-friendly' for someone trying to follow the execution.",mistral-small-24b-instruct,LLM_judge_config
H49,hard,Statistical Analysis,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}

df = pd.DataFrame(data)

# Calculate Pearson correlation coefficients
corr_years_...",9,"The generated code correctly defines the DataFrame, calculates the specified Pearson correlation coefficients, prints them, and correctly identifies the feature with the stronger absolute correlation with ""PerformanceScore"".

**Strong Points:**
- **Correct DataFrame Creation:** The DataFrame `df` is created exactly as specified in the task.
- **Correct Correlation Calculation:** It correctly uses `df['PerformanceScore'].corr()` to calculate the Pearson correlation coefficients for both 'YearsExperience' and 'TrainingHours'. The `method='pearson'` argument is implicitly used by default in `corr()`, making its absence not an error.
- **Correct Printing of Correlations:** Both calculated correlation coefficients are printed clearly.
- **Correct Stronger Feature Determination:** It correctly uses `abs()` to compare the absolute correlation values and identify the stronger one.

**Areas for Minor Improvement (for a perfect 10):**
- **Tie-breaking for Stronger Feature:** The `else` block in the stronger feature determination only assigns `stronger_feature = 'TrainingHours'`. It doesn't explicitly handle the case where `abs(corr_years_experience)` is exactly equal to `abs(corr_training_hours)`. While not strictly an error for distinct values, a more robust solution would state that both have the same strength in that scenario (as the ground truth does).
- **Output Verbosity for Stronger Feature:** The ground truth provides more explanatory output for the stronger feature, including the absolute correlation values, which is helpful for verification. The generated code just states the feature name.
- **Formatting of Printed Correlations:** The ground truth formats the correlations to 4 decimal places, which is good practice for numerical outputs. The generated code prints them with full precision.

Overall, the generated code is highly functional and addresses almost all requirements accurately. The minor points are about output presentation and edge-case handling for a perfect user experience.",mistral-small-24b-instruct,LLM_judge_config
H5,hard,Advanced Statistical Modeling,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.utils import median_survival_times
from scipy.stats import chi2
from statsmodels.formula.api import glm
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import pymc3 as pm
import arviz as az

# Load the dataset
titanic = pd.read_csv('titanic.csv')

# Kaplan-Meier Survival Analysis
kmf = KaplanM...",3,"The generated code attempts to address several requirements but falls short in significant areas, leading to incorrect implementation and missing key components.

**Correctness:**
- **Survival Analysis (Kaplan-Meier & CoxPH):** The code assumes 'voyage' and 'survived' columns exist directly in the `titanic` DataFrame for `T` and `E`. The prompt explicitly states to treat voyage as time-to-event and survival as an event indicator. The ground truth correctly interprets this as setting T=1 for all (as no actual duration of voyage is given, just the completion of the voyage) and E as `1 - Survived` (death). The generated code's definition of `T = titanic['voyage']` is likely incorrect as 'voyage' is not a standard column in the Titanic dataset and its interpretation as a time-to-event variable is unclear. The variable `E = titanic['survived']` is also flipped if we consider 0 as event and 1 as censor, as is standard in `lifelines` (Survived=1 is censor, Survived=0 is event).
- **Advanced Statistical Tests (Multivariate/Interaction Effects):** It uses `statsmodels.formula.api.glm` for multivariate analysis, which is okay but doesn't explicitly look for interaction effects as requested. The reference to `sm.families.Binomial()` is also problematic as `sm` is not imported, and it should be `sm.families.Binomial()` after `import statsmodels.api as sm`.
- **Propensity Score Matching:** The entire PSM implementation is missing. The code trains a RandomForestClassifier on `X` and `y` but this has nothing to do with propensity score matching. It completely omits calculating propensity scores, matching, and checking for covariate balance or treatment effects.
- **Bootstrap Resampling:** The `bootstrap_resample` function is defined but not used for statistical inference (e.g., getting confidence intervals for model coefficients). It just concatenates resampled data, which doesn't directly provide the required confidence intervals.
- **Bayesian Analysis:** The Bayesian model setup is fundamentally flawed. It tries to use `titanic['pclass']` and `titanic[['sex', 'age', 'fare']]` directly without proper data preprocessing (e.g., one-hot encoding categorical variables, handling missing values, standardization, especially for `pm.Normal` likelihood). The likelihood `pm.Normal('likelihood', mu=mu, sigma=sigma, observed=titanic['survived'])` implies a continuous outcome, but `survived` is a binary variable (0 or 1). It should be a Bernoulli or Binomial likelihood. The `sigma` parameter makes no sense for a binary outcome. It also lacks a clear modeling purpose for survival data.
- **Model Validation and Sensitivity Analysis:** This section is entirely missing. There is no implementation of model diagnostics (e.g., PH assumption), sensitivity analysis (e.g., imputation strategy), or model validation beyond basic C-index from CoxPH fitter (which is part of the `print_summary()`).

**Code Quality:**
- **Imports:** Imports are mostly present, but `sm` is not imported for `statsmodels` (though `chi2` from `scipy.stats` is imported, it's unused, and `chi2` is not the correct function for contingency tables in `scipy`). `seaborn` is missing, which is a good practice for plots.
- **Structure:** The code is structured into sections but the sections are not fully independent and rely on global `titanic` dataframe without proper preprocessing within each section.
- **Readability:** Variable names are somewhat clear. No comments or explanations are provided.

**Completeness:**
- **Kaplan-Meier survival analysis (partially done):** Does not group K-M by relevant factors (e.g., sex, Pclass). The definition of T/E is likely incorrect.
- **Cox proportional hazards regression (partially done):** Basic fit is there, but no model diagnostics or comparison as requested.
- **Advanced statistical tests (poorly done):** Multivariate analysis is initiated but interaction effects are not explicitly handled. Model diagnostics (specifically for CoxPH and Bayesian models) are missing.
- **Propensity score matching (completely missing):** This critical requirement is not met at all.
- **Bootstrap resampling (incomplete):** The function is there, but its application for confidence intervals on *model parameters* is missing.
- **Bayesian analysis (incorrectly implemented):** The model setup is incorrect for survival/binary data, and its purpose is unclear.
- **Model validation and sensitivity analysis (completely missing):** No implementation of assumptions testing, imputation sensitivity, or prior sensitivity.

Overall, the generated code fails to meet most of the advanced requirements, particularly for propensity score matching, comprehensive Bayesian analysis, and robust model validation/sensitivity analysis, and misinterprets the core survival analysis parameters.",mistral-small-24b-instruct,LLM_judge_config
H50,hard,Statistical Modeling & Visualization,,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Data
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S011', 'S012'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Math', 'Science'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91, 65, 77],
        'Final_Score': [90, 88, 82, 92, 80,...",9,"The generated code is highly correct and addresses almost all requirements of the task.

**Strong Points:**
- **DataFrame Creation:** Correctly creates the pandas DataFrame as specified.
- **Pearson Correlation:** Accurately calculates the Pearson correlation coefficient between 'Attendance_Rate' and 'Final_Score' and prints it.
- **Scatter Plot:**
    - Correctly uses `seaborn.scatterplot` for plotting.
    - Colors points based on 'Subject' using the `hue` parameter.
    - Includes a title and clear axis labels as requested.
    - Includes a legend for subjects.
- **Overall_Grade Calculation:** Ingeniously uses `pd.cut` for assigning `Overall_Grade`, which is a very concise and effective method for binning numerical data into categories. The binning logic `[0, 69, 79, 89, 100]` with labels `['D', 'C', 'B', 'A']` correctly maps to the specified grade criteria: 'A' for >=90 (bins include up to 100 which covers 90-100), 'B' for 80-89, 'C' for 70-79, and 'D' for <70 (0-69).
- **Grade Distribution:** Correctly counts and prints the distribution of students in each grade category.

**Areas for Minor Improvement:**
- **Correlation Output Precision:** The `correlation:.2f` rounds the correlation coefficient to two decimal places. The ground truth used four decimal places (`.4f`), which is generally preferred for statistical outputs to retain more precision. While not a critical error, it's a slight deviation from a common best practice in reporting.
- **Plot Readability/Aesthetics:** The generated plot is functional but could benefit from minor aesthetic enhancements present in the ground truth, such as explicit `figsize`, `s` (marker size), `alpha` (transparency), and `edgecolor` for scatter points, and `plt.grid(True)` for better readability, and `plt.tight_layout()` for better plot fitting.
- **Implicit Legend Placement:** The legend is placed inside the plot, which can sometimes overlap with data points if there are many categories or the plot is dense. The ground truth explicitly placed it outside the plot using `bbox_to_anchor`. This is a matter of best practice for plot clarity.
- **Missing Imports:** `numpy` is not explicitly imported, though `pd.cut` often works without it. If `np.select` were used (as in an alternative grading method), `numpy` would be a direct dependency. Since `pd.cut` was used, it's not strictly necessary for this specific implementation, but good to be aware of.

Overall, the code is very strong, functional, and correctly solves the problem. The minor points are about common best practices and polish rather than core functionality errors.",mistral-small-24b-instruct,LLM_judge_config
H51,hard,Data Visualization & Time Series Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Create the DataFrame
data = {
    'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05',
                           '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12']),
    'Ticker': ['TECHCO']*10,
    'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0, 153.0, 150.0],
    'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5, 155.0, 152.0],
    'Low': ...",8,"The generated code correctly performs most of the required tasks. It successfully creates the DataFrame, calculates daily percentage returns, and computes the 3-day and 5-day simple moving averages. The plotting functionality is also well-implemented, generating a clear plot with appropriate labels and a legend.

However, there is a minor issue in identifying the bullish crossover dates. The condition for `bullish_signals` is not entirely correct for identifying *crossovers*. A crossover implies that the shorter-term SMA (SMA_3Day) was *below or equal to* the longer-term SMA (SMA_5Day) on the previous day, and is *now above* the longer-term SMA. The generated code's condition `df['SMA_3Day'] > df['SMA_3Day'].shift(1) & df['SMA_3Day'].shift(1) <= df['SMA_5Day']` checks if SMA_3Day is increasing and if the *previous* SMA_3Day was less than or equal to the *current* SMA_5Day, which is not the standard definition of a bullish crossover (where a specific relationship between SMA_3Day and SMA_5Day on the previous day is important).

Specifically, it should be: `(df['SMA_3Day'] > df['SMA_5Day']) & (df['SMA_3Day'].shift(1) <= df['SMA_5Day'].shift(1))`.
This is a common logical error in crossover detection. Correcting this would make the solution perfect. The rest of the code is robust and accurate.",mistral-small-24b-instruct,LLM_judge_config
H52,hard,Data Visualization & Analysis,,"```python
import pandas as pd
import matplotlib.pyplot as plt

# Data creation
data = {
    'OrderID': [101, 102, 103, 104, 105, 106, 107],
    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
    'Quantity': [1, 2, 1, 1, 3, 1, 1],
    'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
    'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST0...",9,"The generated code successfully completes all tasks outlined in the prompt. It correctly creates the DataFrame, calculates total sales revenue per product, generates and displays the horizontal bar chart with appropriate labels, calculates the average order value, and identifies customers with more than one purchase. The code is well-structured and uses standard pandas and matplotlib functionalities efficiently.

Minor improvements could include:
- Adding titles/headers for each printed section (e.g., ""Total sales revenue per product:"").
- Formatting the average order value with currency symbol and two decimal places in the print statement. (The current code already does this nicely with `f""{average_order_value:.2f}""`)
- The `plt.gca().invert_yaxis()` is a stylistic choice and acceptable, but not explicitly requested.

Overall, it's a very good solution that fulfills the prompt's requirements.",mistral-small-24b-instruct,LLM_judge_config
H53,hard,Feature Engineering,,"```python
import pandas as pd

# Create the initial DataFrame
data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

# Create a new numerical feature ""DeviceType_Numeric"" by mapping ""Desktop"" to 0 and ""Mobile"" to ...",9,"The generated code is almost entirely correct and addresses all requirements.

**Strong Points:**
- Correctly defines the initial DataFrame.
- Accurately creates the `DeviceType_Numeric` column using a map.
- Correctly calculates `PagesPerMinute`.
- Handles division by zero for positive and negative infinity by correctly replacing them with 0.
- Correctly selects the final feature set.
- Prints the head of the resulting DataFrame as requested.

**Areas for Improvement (Minor):**
- The `replace` method for handling division by zero for `PagesPerMinute` uses `None` in `[float('inf'), float('-inf'), None]`. While this might not cause an error in this specific scenario because `pandas` division typically produces `NaN` for `0/0` and `inf` for `x/0`, it's more robust and standard to replace `np.nan` (or `float('nan')`) directly instead of `None` if the intention is to catch `NaN` values from `0/0` or other operations. The ground truth uses `np.inf`, `-np.inf`, and then `fillna(0)` which is a common and very robust pattern for this exact problem, as `fillna` intrinsically handles `NaN` values, including those resulting from `0/0`.
- The ground truth explicitly imports `numpy as np` for `np.inf` and `np.nan`, which is a more idiomatic way to handle these rather than `float('inf')` or `float('nan')` (though `float` versions work). This is a minor stylistic point.

Overall, the code is highly functional and meets almost all criteria. The minor point about `NaN` handling is a nuance that doesn't break the code for this specific data, but could be slightly improved for maximum robustness.",mistral-small-24b-instruct,LLM_judge_config
H54,hard,Anomaly Detection,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:20"", ""2025-05-24 10:25""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2"", ""S1"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 28.0, 21.9, 22.0, 15.0, 22.1],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59, 70, 58],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26, 45, 28]
}

df = pd.DataFrame(data)...",7,"The generated code correctly defines the DataFrame and calculates the mean and standard deviation for ""TemperatureCelsius"". It also identifies and prints outliers. However, there's a critical logical error in the outlier identification. The condition `df[""TemperatureCelsius""].abs() > mean_temp + threshold * std_temp` only checks if the *absolute value* of the temperature is greater than the upper bound. It completely misses outliers that are *below* the lower bound. For instance, a temperature of 15.0 Celsius is identified as an outlier in the ground truth, but not by the generated code because `15.0` is not greater than `22.86 + 1.5 * 3.42`. The correct approach requires checking values both above the upper bound and below the lower bound.

Specifically:
1.  **Correctness (8/10):** Mostly correct, but the outlier detection logic is flawed. It only identifies outliers on the high side (and only if their absolute value is greater than the upper bound derived from the mean, which is not what's intended). It doesn't correctly identify values significantly *below* the mean.
2.  **Code Quality (8/10):** Good. Clear variable names, standard library usage. Properly imports pandas.
3.  **Completeness (7/10):** Meets most requirements but fails on the specific definition of an outlier (1.5 standard deviations away from the mean, which implies both above and below). The output format is also a bit less descriptive than the ground truth.",mistral-small-24b-instruct,LLM_judge_config
H55,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06"", ""SM07"", ""SM08"", ""SM09""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram"", ""Facebook"", ""Twitter"", ""Facebook""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video"", ""Image"", ""Video"", ""Text""],
    ""Likes"": [150, 80, 300, 200, 90, 450, 120, 250, 70],
    ""Shares"": [20, 15, 50, 30, 10, 60, 18, 40, 12],
    ""Comments"": [10, 5, 25, 15, 8, 30, 9,...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the `EngagementScore` as specified, and accurately determines the `PostType` with the highest average `EngagementScore` for each `Platform`. The use of `groupby`, `mean`, `reset_index`, and `idxmax` is an efficient and idiomatic pandas way to achieve the desired result. The output format is also correct and clearly presents the required information.

Strong points:
- **Correctness:** All calculations and logical steps are accurate.
- **Completeness:** Addresses all three parts of the task: DataFrame creation, `EngagementScore` calculation, and finding/printing the highest average engagement per platform/post type.
- **Code Quality:** The code is clean, concise, and uses appropriate pandas functions. The logic for finding the max engagement per platform is particularly clever and efficient using `idxmax()`.
- **Readability:** The code is easy to understand, even without extensive comments.",mistral-small-24b-instruct,LLM_judge_config
H56,hard,Data Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006"", ""P007"", ""P008""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22"", ""2025-01-23"", ""2025-01-25""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28"", ""2025-01-30"", ""2025-02-02""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu"", ""Injury"", ...",9,"The generated code correctly performs all requested operations: DataFrame creation, `LengthOfStay` calculation, grouping by `Diagnosis` to get average stay and patient count, and identifying the diagnosis with the longest average stay. The code is concise and uses appropriate pandas functionalities.

Minor points for improvement:
- The printing format for the `diagnosis_stats` DataFrame could be more user-friendly, similar to the ground truth which uses f-strings for formatted output.
- The final print statement is slightly less readable than the ground truth's, which breaks it into two lines for clarity.
- The ground truth includes `print` statements throughout to show progress, which is good for user feedback in a benchmark setting. The generated code does not.
- The ground truth encapsulates the logic in a function, which is generally good practice for reusability and organization. While not strictly required by the prompt, it enhances code quality.",mistral-small-24b-instruct,LLM_judge_config
H57,hard,Feature Engineering & Ranking,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06"", ""M07"", ""M08""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar"", ""The Lion King"", ""Fight Club""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi"", ""Animation"", ""Drama""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014, 1994, 1999],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6, 8.5, 8.8],
    ""Votes"": [2200000, 2500000, 19...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the ""WeightedRating"" precisely as specified (Rating * Votes / sum of all Votes), adds it as a new column, and then identifies and prints the ""Title"" and ""WeightedRating"" of the movie with the highest ""WeightedRating"". The code is concise, efficient, and directly addresses all parts of the prompt. No significant improvements are needed for correctness or completeness for this specific task. The use of vectorized operations for calculating `WeightedRating` and `idxmax()` for finding the highest weighted movie are both good pandas practices. The f-string for printing the final output is also clear and well-formatted.",mistral-small-24b-instruct,LLM_judge_config
H58,hard,Data Aggregation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006"", ""C007"", ""C008""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda"", ""Ford"", ""BMW""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord"", ""Mustang"", ""X3""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020, 2021, 2022],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000, 45000, 55000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000, 10000, 8000]
}

df = p...",9,"The generated code correctly defines the DataFrame, calculates the average ""PriceUSD"" and ""Mileage"" for each ""Brand"", and prints the resulting grouped DataFrame as required by the task. The code is concise and uses appropriate pandas methods for the aggregation. The only minor point of improvement is that the column names in the output DataFrame (`PriceUSD`, `Mileage`) for the averages are not as descriptive as `Average_PriceUSD`, `Average_Mileage` which the ground truth used via named aggregation. However, this is a stylistic preference and does not hinder correctness or understanding. It perfectly addresses all requirements.",mistral-small-24b-instruct,LLM_judge_config
H59,hard,Feature Engineering & Aggregation,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8, 9],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle"", ""Blastoise""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59, 79],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63, 83],
    ""De...",10,"The generated code is excellent. It correctly defines the DataFrame, accurately calculates the ""TotalStats"" column, identifies the Pokemon with the highest total stats, and computes the average speed for each ""Type1"". The code is clean, concise, and directly addresses all requirements of the task. The use of `idxmax()` is efficient for finding the Pokemon with the highest total stats. The output is well-formatted.",mistral-small-24b-instruct,LLM_judge_config
H6,hard,Customer Segmentation,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, GaussianMixture
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.stats ...",5,"The generated code attempts to address many parts of the prompt but falls short in several critical areas, reducing its overall correctness and completeness.

**Correctness & Completeness Issues:**

1.  **Feature Engineering:**
    *   Many advanced features from the ground truth like `FarePerPerson`, `TicketGroupSize`, `CabinDeck`, `HasCabin`, and interaction features (`Age_Class`, `Fare_Class`, `FarePerPerson_Class`) are entirely missing.
    *   The `GroupId` feature engineering is not properly implemented for social network analysis as envisioned (it extracts a part of the ticket string, not actual group IDs based on shared tickets, and also incorrectly handles 'CABIN'). The ground truth uses `TicketGroupSize` which is a more direct approach for social network analysis based on ticket sharing.
    *   `IsMaleTitle` is a somewhat arbitrary feature and doesn't fully capture the richness of title extraction for socioeconomic analysis like the ground truth's detailed `Title` grouping.
    *   Socioeconomic indicators (`AgeGroup`, `FareGroup`) are created but then dropped (along with other features) from the `X` dataframe used for clustering, making them effectively useless. This is a major bug.

2.  **Preprocessing:**
    *   The `dropna(subset=['Age', 'Fare'])` is a critical error. The `titanic.csv` dataset has missing `Age` values. The ground truth correctly imputes these, ensuring all passengers are included. The generated code implicitly removes rows with missing age or fare, significantly reducing the dataset size and potentially biasing the analysis. This is a severe data handling flaw.
    *   The preprocessing pipeline is very basic (`StandardScaler` on `X` directly), completely ignoring categorical features (`Pclass`, `Sex`, `Embarked`, `Title`, etc.). These are crucial for passenger segmentation and should be one-hot encoded. The `X` dataframe includes non-numeric columns (`Sex`, `Embarked`, `Pclass` if not converted, feature engineered `AgeGroup`, `FareGroup` if they were kept), which a simple `StandardScaler` cannot handle, leading to an implicit failure or runtime error if not corrected. The ground truth uses `ColumnTransformer` for proper mixed-type data handling.

3.  **Clustering Algorithms:**
    *   The generated code uses `DBSCAN`, but then the `df['DBSCAN_Cluster']` assignment `gmm.predict(X_scaled)` is a copy-paste error. This leads to `DBSCAN_Cluster` containing GMM predictions, not DBSCAN results. This is a significant bug.
    *   The `n_clusters=3` is hardcoded for all algorithms without strong justification from the validation metrics (which would show the specific optimal `k`). The ground truth calculates and then uses an `optimal_k`.

4.  **Optimal Number of Clusters:**
    *   Only `silhouette_score` and `calinski_harabasz_score` are used. `Elbow method` (inertia) and `gap statistic` (requested in the prompt) are not implemented. `Davies-Bouldin` is also missing. The prompt explicitly asked for *multiple* validation metrics.
    *   The `elitbows` list is declared but not used.

5.  **Dimensionality Reduction:**
    *   Visually, the `hue` for plotting uses `df['KMeans_Cluster']`. This means if `df` was truncated by `dropna` earlier, the cluster labels might not align with `X_scaled`, leading to misalignment or errors. This highlights the importance of consistent dataframe handling.

6.  **Cluster Stability Analysis:**
    *   The generated code includes placeholders (`consensus_stability = 0`, commented out assumptions) but no actual implementation of cluster stability analysis. This is a significant omission from the prompt's requirements. The ground truth provides a simplified but concrete `ARI` based stability analysis.

7.  **Consensus Clustering:**
    *   Not mentioned or implemented. The ground truth does not explicitly implement full consensus clustering either due to complexity, but the prompt asks for it.

8.  **Comprehensive Visualizations:**
    *   Visualizations are very basic compared to the ground truth. Missing are:
        *   Detailed feature distributions for each cluster (violin/count plots).
        *   Heatmap of scaled feature means, which is crucial for interpreting cluster characteristics based on scaled features.
    *   The `IsMale` column used in `cluster_metrics` is not defined anywhere, leading to a KeyError. The ground truth correctly identifies `Sex` as a categorical feature.

9.  **Validation using External Criteria:**
    *   Placeholder comments (`validation_results = []`, `Assume validation methodology has been implemented`) indicate no concrete implementation. The ground truth provides a clear numerical and visual breakdown of survival rates per cluster.

10. **Assumptions and Environment:**
    *   The statement ""We need to have a GPGPU environment due to the large computation needed"" is an unsupported and incorrect assumption for this dataset size and task. The titanic dataset is small; CPU is perfectly adequate and fast for all operations. This implies a misunderstanding of computational requirements.

**Code Quality:**

*   **Imports:** Mostly correct, but `GaussianMixture` is imported as a clusterer (which it is), but its primary use is for GMM. `MinMaxScaler` is imported but not used. `zscore`, `RandomForestClassifier`, `SelectFromModel` are imported for feature importance but not strictly for clustering analysis, and `RandomForestClassifier` is then used on `X` when it should be `X_scaled` for consistency with clustering.
*   **Structure:** The flow is generally logical, but the `full_df` creation and subsequent dropping of key columns (`AgeGroup`, `FareGroup`, and critical categorical columns) make the feature engineering largely ineffective.
*   **Readability:** Variable names are okay. Lack of comments for more complex parts is a minor con.

In summary, the code contains significant logical and implementation errors in data preprocessing and feature engineering, leading to an incorrect foundation for clustering. Several key requirements like comprehensive validation metrics, detailed cluster analysis, stability analysis, and external validation are either missing or superficially addressed.",mistral-small-24b-instruct,LLM_judge_config
H60,hard,Advanced Aggregation & Analysis,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006"", ""B007""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm"", ""Brave New World""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell"", ""Aldous Huxley""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian"", ""Dystopian""],
    ""Stock"": [...",7,"The generated code correctly defines the DataFrame and performs the required calculations.

**Strong Points:**
*   **Correct DataFrame Creation:** The DataFrame is created accurately.
*   **Correct Aggregation:** The `groupby().agg()` method is used effectively to calculate the count of books and average price per author.
*   **Correct Author with Most Stock (Calculation):** The logic used to sum stock per author and find the maximum is sound.

**Areas for Improvement:**
*   **Printing Author Statistics (Minor):** The generated code prints the `author_stats` DataFrame directly, which is acceptable but not as human-readable as the formatted output in the ground truth for ""Author, Count of Books, and Average Price"". The request specifically asked to ""print this information, showing Author, Count of Books, and Average Price"", implying a formatted output, not just a DataFrame.
*   **Identifying Author with Most Stock (Efficiency/Clarity):** The approach of adding a `StockTotal` column back to the original `df` using `transform` and then using `idxmax()` on that column is functional but less direct than grouping and summing, then finding the `idxmax()` on the resulting Series, as shown in the ground truth. While not incorrect, it adds a temporary column to the original DataFrame which might not be desired. The ground truth's approach of `total_stock_per_author = df.groupby('Author')['Stock'].sum()` and then `author_most_stock = total_stock_per_author.idxmax()` is more concise and arguably clearer for this specific task.
*   **Clarity of Output:** The output for ""Author with the most books in stock"" only prints the author's name, not the total stock value, which would be more informative for the user. The prompt asked to ""Identify and print the 'Author' who has the most books in stock (sum of 'Stock' for all their books)"", and while the author is identified, the sum of stock isn't explicitly printed for that author.",mistral-small-24b-instruct,LLM_judge_config
H7,hard,Advanced Machine Learning,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve, validation_curve
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.ensemble import RandomFores...",5,"The generated code attempts to address many aspects of the prompt but falls short in several key areas, leading to incorrect or incomplete implementations.

**Correctness Issues:**
*   **Data Preprocessing:**
    *   The `PolynomialFeatures` is applied to `data[numeric_features]` *before* the `ColumnTransformer`, which is then applied within each pipeline. This means `PolynomialFeatures` is applied *twice* or incorrectly handled. The prompt specified `PolynomialFeatures` as part of the *comprehensive data preprocessing* pipeline, which should feed directly into the models. The ground truth correctly integrates it into a preprocessor pipeline applied once.
    *   Outlier removal is done by *filtering rows* (`data = remove_outliers(data, col)`), which implicitly removes data points. This is problematic as it changes the dataset size and might remove too much data, especially in a classification task. The prompt asked for *outlier detection and treatment*, and the ground truth used Winsorization (capping), which is a treatment that retains data points.
    *   The `preprocessor` pipeline in the generated code uses `SimpleImputer` for numerical features. The prompt specifically asked for *multiple imputation strategies*, and the ground truth used `IterativeImputer` (MICE), which is a more advanced technique.
    *   The `ColumnTransformer` is defined but then the `X_train` and `X_test` are passed directly to `fit`/`transform` without applying the `ColumnTransformer` *outside* the model pipelines (before base model optimization). While it's within the model pipelines, the prompt implied a processed `X_train_processed` to optimize models on, and the SHAP explanation relies on this processed data. The ground truth explicitly creates `X_train_processed_df` and `X_test_processed_df`.
*   **Base Model Optimization:**
    *   The `GridSearchCV` instances are fit directly to `X_train` and `y_train` without explicitly processing `X_train` first. Although the `Pipeline` objects `rf`, `xgb`, etc., include `preprocessor`, this is functionally correct for fitting. However, the evaluation metrics for `cross_validate` later might be problematic if `X` and `y` used there are the original, uncleaned `data` rather than the processed `X_train/X_test`.
*   **Ensemble Models:**
    *   **Blending:** The prompt explicitly asked for *blending* as an ensemble technique. The generated code completely omits any implementation of blending.
    *   Stacking tuple error: `('svm', grid_svm.best_estimator_), ('nn', grid_nn.best_estimator_),),` has a syntax error (extra comma and parenthesis).
*   **Evaluation:**
    *   **Bootstrap Sampling:** The `Bootstrap` object is initialized with `y_pred` which is undefined, rendering this section broken. The prompt asks for *bootstrap sampling for prediction confidence intervals* on the *test set*, which involves resampling the *test data* and re-evaluating model performance, not fitting a `Bootstrap` object on `X_train, y_train`. The ground truth correctly implements this for metrics.
    *   **Statistical Significance Testing:** The `ttest_rel(scores_rf, scores_ens)` is problematic. `scores_rf` and `scores_ens` from `cross_validate` are *dictionaries* of scores (`'fit_time'`, `'score_time'`, `'test_accuracy'`, `'test_roc_auc'`). You cannot directly perform a t-test on dictionaries. The ground truth correctly extracts the appropriate scores (e.g., `'test_roc_auc'`) for the t-test and iterates over CV folds.
*   **Model Interpretability (SHAP):**
    *   `explainer = shap.Explainer(grid_rf.best_estimator_, preprocessor.transform(X))`: This is wrong. `grid_rf.best_estimator_` is a `Pipeline`. You need to access the `classifier` within the pipeline for the SHAP explainer, and the background data (`preprocessor.transform(X)`) should be the *processed* training data, not the full X. Furthermore, `tree.shap.TreeExplainer` is specialized for tree-based models and more efficient. The `explainer(X)` call is also likely to fail because `X` is the original unprocessed dataframe while the explainer expects processed features.
*   **Model Calibration:**
    *   The `CalibrationDisplay` usage looks incorrect. It's designed to visualize calibration curves, but `y_probs=fop_ens` is misused. `display.plot()` would then plot for `fop_rf` and `mop_rf` implicitly. The ground truth correctly plots multiple lines for uncalibrated and calibrated models using `matplotlib.pyplot.plot` directly.
    *   The prompt asked for *model calibration and probability calibration plots*, which typically implies showing the improvement *after* applying a calibrator like `CalibratedClassifierCV`. This step is entirely missing in the generated code.

**Completeness Issues:**
*   **Advanced Feature Engineering:** While `PolynomialFeatures` is used, the prompt also asked for *interaction features*. The `PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)` covers polynomial features, but the prompt's wording suggests a more explicit focus on interaction features too. The ground truth correctly uses `interaction_only=False` which generates both.
*   **Multiple Imputation Strategies:** The code only uses `SimpleImputer`. The prompt asked for *multiple imputation strategies*. The ground truth used `IterativeImputer`.
*   **Evaluation Metrics:** The prompt asked for a `comprehenisive evaluation` including `precision`, `recall`, `f1_score`, `roc_auc`, `brier_score_loss`. The generated code only uses `accuracy` and `roc_auc` in `cross_validate` and doesn't explicitly calculate all these for the final test set evaluation.
*   **Learning Curves:** While learning curves are generated, validation curves are only mentioned but not fully implemented (the example `validation_curve` needs a `param_name` and `param_range`).
*   **Confidence Intervals:** Bootstrap sampling was for `prediction confidence intervals`, which usually means confidence around the *performance metrics* (like ROC AUC, F1-score). The generated code's bootstrap implementation is flawed and doesn't produce these.
*   **Comprehensive Model Comparison Dashboard:** The prompt asked for a `dashboard with performance metrics, feature importance, and prediction confidence intervals`. The generated code prints some scores and plots learning/calibration curves, but doesn't build a consolidated dashboard (like a DataFrame summary table with CIs, multiple metric bar plots, etc.) that the ground truth provides.
*   **Prediction Confidence Intervals:** The prompt asked for this. While bootstrap was a way to get CI for *metrics*, for *predictions* it usually refers to uncertainty around individual predictions (e.g., from ensemble models or bayesian methods). This is not explicitly addressed.

**Code Quality (Structure, etc.):**
*   Imports are present but some are unused or incorrectly used.
*   The overall flow is somewhat disjointed due to the incorrect implementations, making it hard to follow.
*   Lack of clear print statements for each stage (e.g., ""Starting Base Model Optimization"") making it less readable for a user.
*   The `xgboost` import block `try-except` is missing, which the ground truth included to handle `xgboost` not being installed.
*   The `StackingClassifier` declaration has a syntax error (`),,`).

Overall, the generated code attempts to hit the keywords but often with incorrect or incomplete implementations, especially for advanced requirements like multiple imputation, proper outlier treatment, blending, confidence intervals via bootstrapping, and valid statistical significance testing.",mistral-small-24b-instruct,LLM_judge_config
H8,hard,Advanced Statistical Modeling,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from statsmodels.formula.api import mlogit
from pymc3 import Model, GLM, find_MAP, traceplot
from sklearn....",3,"The generated code attempts to address some parts of the task but falls severely short on correctness and completeness, particularly for an ""advanced statistical modeling"" task. It misses crucial implementations, misuses libraries, and includes significant errors or vague placeholders.

Here's a breakdown of the issues:

**Correctness:**
*   **Missing Data Imputation Comparison:** The code performs the imputations but lacks any meaningful comparison of their effectiveness. The `compare_models` function is ill-defined and misused (e.g., passing a `model` argument to `compare_models(X_test, y_test, logistic)` then calling `model.fit(X,y)` inside, which isn't the test set).
*   **Statistical Models:**
    *   **GAM:** The `gam_model` function definition and usage are fundamentally flawed. `pygam` is imported but then `pygam.LinearGAM` is used, and the return value is `compare_models(X_test, y_test, model_gam)` which incorrectly attempts to use `compare_models` on a GAM object for `classification_report` (GAMs have their own `predict_proba`). The `gam_check_model` call is completely wrong (`gam_model(train, y_train, test, test)` - `train` and `test` are undefined variables).
    *   **Bayesian Logistic Regression (PyMC3):** This section is highly incorrect. `from pymc3 import Model, GLM, find_MAP, traceplot` is okay, but `potability = Gast()` is not PyMC3 code, and `model.fit()` is a non-existent method for a PyMC3 Model. There's no proper definition of priors, likelihood, or observation, and the sampling is just `model.fit()` which doesn't work.
    *   **Robust Statistical Methods:** It only implements `RobustScaler`, which is a preprocessing step, not a robust statistical model like `statsmodels.robust.robust_linear_model.RLM` which was asked for.
*   **Hypothesis Testing:**
    *   **ANOVA:** The `anova_model = ols('Potability ~ C(pH)', data=data).fit()` assumes 'pH' could be treated as a categorical variable for ANOVA, which is typically not the case for a continuous variable like pH. It asks for Multivariate ANOVA (MANOVA), but performs a univariate ANOVA (ANOVA on 'Potability' with 'pH' as a factor, which doesn't make sense as potability is the target). The task explicitly asked for MANOVA.
    *   **Permutation Test:** The `permutation_test` function is a placeholder that only calls `permutation_importance(X, y, model)` which is for feature importance, not a general permutation test for hypothesis testing (e.g., comparing means).
    *   **Bootstrap CI:** `bootstrap()` is a placeholder function, not a real implementation.
*   **Anomaly Detection:** While the models are instantiated, there's no analysis of their results beyond `fit_predict`, no scoring visualization, or clear interpretation. `data_MICE` is referenced before its definition in some places.
*   **Model Diagnostics:**
    *   **Residual Analysis:** Only `sns.histplot(residuals)` is made, which is basic. No residual plots vs. fitted values, Q-Q plots for residuals against normal distribution (which is inappropriate for logistic regression residuals anyway), or proper interpretation.
    *   **Influence Measures:** The `influence_measure` function is a completely blank placeholder. It mentions `likelihood_ratio` but doesn't implement or explain it.
    *   **Goodness-of-Fit:** There's no specific goodness-of-fit test mentioned or implemented.
*   **Visualizations:**
    *   **A-Q Plots:** `stats.probplot(residuals, dist=""norm"", plot=plt)` is appropriate for *linear regression* residuals if you expect normality, but not directly for logistic regression residuals.
    *   **Leverage Plot:** `sns.residplot(X_test['pH'], residuals)` plots residuals against one feature. A ""leverage plot"" usually refers to `statsmodels.graphics.regressionplots.plot_leverage_resid2` for diagnostics, which is missing.
    *   **Diagnostic Dashboards:** This is left completely open without implementation.

**Code Quality:**
*   **Imports:** Several unnecessary imports or misused imports (e.g., `mirex` which is not a standard library and seems to be a placeholder for complex logic like `bootstrap()`).
*   **Vague Code:** Many functions are placeholders (`gast = Gast()`, `model.fit()`, `bootstrap()`, `influence_measure()`).
*   **Redundancy/Errors:** The code is structured into two nearly identical blocks, which is confusing and redundant. There are copy-paste errors (`data_MICE` vs `data_mice`, `train`, `test` vs `X_train`, `X_test`).
*   **Lack of Output/Interpretation:** Very little print statements or explicit interpretation of the results, which is critical for an expert statistical task.
*   **Variable Scope:** Functions like `gam_model` and `permutation_test` are defined to take `X, y, model` but then call `X_train`, `y_train` or undefined `train`, `test` inside.

**Completeness:**
*   **Multiple Imputation Comparison:** Not implemented effectively.
*   **Logistic Regression with Regularization:** Only `LogisticRegressionCV` is used, specifically checking L2. L1 (Lasso) was also requested.
*   **GAM:** Poorly implemented.
*   **Bayesian Logistic Regression:** Critically flawed.
*   **Advanced Hypothesis Testing:** MANOVA, permutation tests, and bootstrap CIs are either missing for their proper implementation or are placeholders.
*   **Anomaly Detection:** Models are run, but lack analysis/viz.
*   **Dimensionality Reduction:** PCA and Factor Analysis are implemented at a basic level, but the output `print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))` is the extent of it for PCA.
*   **Robust Statistical Methods:** Only `RobustScaler` is implemented, not robust models.
*   **Model Diagnostics:** Residual analysis (basic only), influence measures (placeholder), goodness-of-fit (missing).
*   **Advanced Visualizations:** Only basic Q-Q plot and single-feature resid plot are created. Leverage plot and diagnostic dashboards are missing.

Overall, the code demonstrates a basic understanding of some concepts but fails to deliver the advanced, specific, and correct implementations required by the prompt. Many sections are essentially placeholders or demonstrate a misunderstanding of the libraries/methods requested.",mistral-small-24b-instruct,LLM_judge_config
H9,hard,Customer Segmentation,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from sklearn.random_projection import G...",4,"The generated code attempts to address many parts of the prompt but fails significantly in several critical areas, making it largely incorrect or incomplete for a ""comprehensive"" analysis.

**Correctness & Completeness Issues:**
1.  **Feature Engineering (Major Flaw):** The `water_quality_indices` function introduces features like `Calcium` and `Magnesium` which are not present in the `water_potability.csv` dataset. This will cause `KeyError` at runtime, making the code un-executable without modification. The prompt explicitly states to assume the CSV file is in the current directory and to load `water_potability.csv`, implying reliance on its actual columns. The `df['temp_pattern']` is another example of a non-existent column (`temp`). The `pH_balance` is duplicated. The engineered features are also much simpler than what the prompt asked for (""advanced features including water quality indices, chemical balance ratios, contamination risk scores, and temporal patterns""). The ground truth's features are far more sophisticated.
2.  **Missing Value Handling (Incorrect):** `df.fillna(df.mean(), inplace=True)` is called *after* feature engineering. If `Calcium`, `Magnesium`, `temp` etc. don't exist, this will not correctly handle NaNs for those columns, if they were to exist. More importantly, it uses the global mean, not a robust imputer like `SimpleImputer` (median), and this is before scaling, which is generally fine but the engineered features would potentially have NaNs first if not handled correctly.
3.  **Optimal Number of Clusters (Incomplete/Incorrect Implementation):**
    *   `gap_statistic` is implemented incorrectly. It just returns the elbow method's inertia values. The actual Gap Statistic involves comparing the inertia from clustered data to that from uniformly distributed reference data, and it's a complex statistical method not just plotting inertia.
    *   No automatic determination of optimal K is done, only functions that *return* scores. The prompt asks to ""Determine optimal number of clusters"" and the ground truth implies this choice is to be based on the plots, but the generated code doesn't plot them or make any choice based on them.
4.  **Dimensionality Reduction (Factor Analysis):** The generated code uses `FactorAnalysis` from `sklearn.random_projection.GaussianRandomProjection`, which is incorrect. `FactorAnalysis` is in `sklearn.decomposition`. This will lead to a `NameError` or incorrect module import.
5.  **Cluster Stability Analysis (Very Basic):**
    *   `bootstrap_resampling` is extremely rudimentary. It samples 20% of the data using `train_test_split` and then performs KMeans on that subset. It returns the labels but doesn't perform any comparison (like ARI) or consensus clustering, which the prompt specifically asked for. The ground truth provides a robust implementation of consensus clustering.
6.  **Visualization (Missing/Incorrect Radar Chart):** The `radar_chart` function provided is entirely incorrect. It takes `data`, `labels`, and `features` but then tries to plot `angular_dims` and `radial_dims` which are undefined variables. It also attempts to plot a radar chart for *each* cluster individually rather than a comparative one for all clusters as commonly done.
7.  **Cluster Profiling (Limited):** While `external_validation` roughly attempts a profile by grouping by the K-Means cluster and showing potability, it doesn't print or analyze the chemical composition patterns, contamination risks, or go into detail as specified in the prompt (""Analyze water quality profiles for each cluster including chemical composition patterns, contamination risks, and potability rates""). The ground truth provides a `cluster_profiles_df` with mean values for all features.
8.  **Anomaly Detection (Limited):** It only implements DBSCAN for anomaly detection, which is a valid use, but the prompt asked for ""cluster-based anomaly detection to identify unusual water samples"". This typically means approaches like distance to centroid, Isolation Forest, or one-class SVMs *based on the identified clusters*, which the ground truth implements.
9.  **Imports:** Several `sklearn` imports are missing (e.g., `SimpleImputer`, `IsolationForest`, `NearestNeighbors`, `adjusted_rand_score`, `cdist`, `dendrogram`, `linkage`). It imports `FactorAnalysis` from the wrong module.

**Code Quality:**
*   **No Configuration Section:** The ground truth has a clear configuration section for file paths, seeds, etc., which is good practice.
*   **Lack of Print Statements/Feedback:** The generated code runs silently until the end `print(df)`. The ground truth includes descriptive print statements for each stage of the analysis, which is crucial for understanding progress and debugging in complex scripts.
*   **Hardcoded `k=4`:** The code immediately jumps to `k=4` for clustering algorithms without first validating the optimal number of clusters. The prompt clearly asks to determine optimal K *before* applying algorithms.
*   **Variable Naming:** `df` is used for the raw data, then feature-engineered data, then the original `df` is dropped. Better to use distinct names like `df_raw`, `df_fe`.
*   **Reproducibility:** While `random_state=42` is often used, `n_init=10` is missing for KMeans, which can affect reproducibility and convergence.

In summary, the generated code has fundamental errors in feature engineering and the implementation of key analytical steps (optimal K determination, stability analysis, complex visualizations like radar charts). Many sections are rudimentary or conceptually incorrect compared to the requirements.",mistral-small-24b-instruct,LLM_judge_config
M1,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress

# Load the dataset
df = pd.read_csv('winequality-red.csv', delimiter=';')

# Determine correlations with 'quality'
correlations = df.corr()['quality'].drop('quality')

# Determine the top 3 positively and negatively correlated features
positive_corr = correlations.nlargest(4)  # 4 to include 0 index
negative_corr = correlations.nsmallest(4)  # 4 to include 0 index

print(""Top 3...",8,"The generated code largely fulfills all the requirements of the task.

**Strong Points:**
- Correctly uses `pandas` for data loading and correlation calculation.
- Successfully identifies and prints the top 3 positive and negative correlations.
- Generates a correlation matrix heatmap with appropriate annotations and colormap.
- Creates scatter plots for the most positive and negative features against 'quality'.
- Correctly fits and plots linear regression lines on the scatter plots using `scipy.stats.linregress`.
- Uses `matplotlib.pyplot` and `seaborn` effectively for visualization.

**Areas for Improvement/Minor Issues:**

1.  **Delimiter for CSV:** The ground truth code assumes the default comma delimiter, while the generated code explicitly sets `delimiter=';'`. While the `winequality-red.csv` dataset indeed uses semicolons, the prompt didn't specify this. It's a pragmatic choice, but assuming default unless specified is generally safer in a benchmark scenario. The ground truth handles this implicitly or expects standard CSV.
2.  **Handling `nlargest`/`nsmallest` for top 3:** The generated code uses `nlargest(4)` and `nsmallest(4)` and then slices `[1:]` to get the top 3, because the `quality` feature itself will be included (or excluded, depending on the `drop` placement). This is a bit convoluted. A more direct way is to drop `quality` first, then simply use `nlargest(3)` and `nsmallest(3)`, or use `drop('quality')` before getting `nlargest`/`nsmallest` in the `correlations` variable itself, as done in the ground truth. The ground truth sorts all correlations and then takes `head(3)` and `tail(3)` after dropping 'quality', which is cleaner.
3.  **Regression Line plotting:** The generated code plots `df[most_positive_feature]` directly on the x-axis for the regression line. For a regression line, it's generally better to create a range of x-values (e.g., from min to max of the feature) and then calculate the corresponding y-values using the regression equation (`slope * x + intercept`). While the current approach might coincidentally work for some datasets where the feature values are ordered, it's not robust. The ground truth's `x_pos = np.array([df[most_positive_feature].min(), df[most_positive_feature].max()])` and then plotting `x_pos` against `intercept + slope * x_pos` is the correct and robust way to draw a line segment representing the regression.
4.  **Clarity of Regression Label:** The generated code's label `f'Regression Line\n{slope:.2f}x + {intercept:.2f}'` shows the equation, which is good, but the R-squared value is a more common metric to display to indicate the fit quality, as done in the ground truth.
5.  **Output Clarity:** The ground truth includes more descriptive print statements and data exploration steps (e.g., `df.head()`, `df.info()`, `df.describe()`), which are good practices but might be outside the strict scope of the benchmark ""analysis"" task. For the core task, the generated code's output is sufficient.
6.  **Error Handling**: The ground truth includes basic error handling for file not found, which is a good robustness practice, though not explicitly asked for by the prompt.",mistral-small-24b-instruct,LLM_judge_config
M10,medium,Hypothesis Testing and Statistical Analysis,video_games_sales.csv,"```python
import pandas as pd
import scipy.stats as stats

# Load the dataset
data = pd.read_csv('video_games_sales.csv')

# Clean missing values
data = data.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year'])

# 1. Test whether Nintendo games have significantly higher global sales compared to all other publishers combined
nintendo_sales = data[data['Publisher'] == 'Nintendo']['Global_Sales']
other_sales = data[data['Publisher'] != 'Nintendo']['Global_Sales']

t_stat_ninten...",6,"The generated code addresses most of the requirements but has several issues that prevent it from being a strong solution.

**Positive Aspects:**
*   Correctly uses `pandas` for data loading and manipulation, and `scipy.stats` for statistical tests.
*   Performs all four requested statistical tests (t-test, chi-square, Mann-Whitney U test).
*   Correctly calculates p-values for each test.
*   Includes a basic interpretation block at the end, comparing p-values to the alpha level.
*   Handles the top 5 genres/platforms for the chi-square test as requested.

**Areas for Improvement/Corrections:**
1.  **Data Cleaning/Handling Missing Values (Critical):**
    *   The `dropna` call: `data.dropna(subset=['Global_Sales', 'Publisher', 'Genre', 'Platform', 'Year'])`
        *   It uses `'Year'` instead of `'Year_of_Release'`, which is the correct column name in the dataset. This will likely cause `KeyError` or drop data incorrectly if the column name is precisely 'Year_of_Release'.
        *   It does not cast `Year_of_Release` to integer, which is crucial for numerical comparisons (e.g., `data['Year'] < 2000`). If `Year_of_Release` remains float after `dropna`, comparisons might not be robust, or direct integer comparison for 2000 could lead to precision issues if there were floats like 1999.5.
        *   It misses the important `User_Score` cleaning/conversion to numeric, which, while not directly used in the *requested* tests, is an important part of ""cleaning any missing values"" in general for this dataset.
    *   The generated code assumes `Year` column, whereas the ground truth correctly identified `Year_of_Release`.
2.  **Robustness (Critical):**
    *   There are no checks for empty dataframes or series before performing statistical tests (e.g., `if len(nintendo_sales) < 2`). This can lead to errors if certain filters result in an empty or single-element series, which is a common issue with real-world data. The ground truth correctly includes these checks.
    *   The contingency table for the chi-square test also lacks checks for empty data or zero sums.
3.  **Clarity and Interpretation (Minor but Important):**
    *   The output format is very minimal, just printing p-values and then simple interpretations. It lacks the rich detail provided by the ground truth, such as:
        *   Explicitly stating the Null and Alternative Hypotheses for each test.
        *   Printing the test statistic (t-statistic, U-statistic, Chi2 statistic).
        *   Using f-strings for formatted output consistently.
        *   Clearly demarcating tests with headings.
4.  **T-test Alternative Hypothesis:** The t-test for Nintendo vs. Others was explicitly asked to test ""significantly *higher* global sales"". The generated code uses `alternative='two-sided'` by default for `ttest_ind` (or doesn't specify which defaults to two-sided), whereas the ground truth correctly specified `alternative='greater'` for this specific test. This is an important detail for hypothesis testing.
5.  **Re-reading `Task` vs. `Generated Code` order:** The numbering in the generated code's comments (1, 2, 3, 4) does not perfectly align with the task's numbering (2, 3, 4, 5). This is a minor organizational point but adds to consistency.

Overall, the code is a good first attempt but requires significant refinement in data handling, robustness checks, and detailed output to meet the standards of an expert code evaluator.",mistral-small-24b-instruct,LLM_judge_config
M11,medium,Classification and Machine Learning,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder

# Load the dataset
data = pd.read_csv('video_games_sales.csv')

# Handle missing values
data = data.dropna()

# Create binary target variable 'High_Sales'
data['High_Sales']...",6,"The generated code addresses most of the tasks but has several issues that reduce its correctness and robustness.

**Strengths:**
*   Correctly loads the dataset.
*   Correctly creates the 'High_Sales' target variable.
*   Correctly applies Label Encoding to the specified categorical features.
*   Correctly creates 'Total_Regional_Sales'.
*   Correctly trains a Random Forest Classifier.
*   Calculates and prints the required evaluation metrics (Accuracy, Precision, Recall, F1-score, Confusion Matrix).
*   Correctly displays feature importance.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling:** The generated code uses `data = data.dropna()` which drops *any* row with *any* NaN. The ground truth, correctly, handles missing values more judiciously by only dropping NaNs from columns crucial for the target variable or critical features. For 'Year', using a column name `Year_of_Release` that doesn't exist in the actual dataset `video_games_sales.csv` (it's simply 'Year') causes issues. This approach also implicitly drops rows where `NA_Sales`, `EU_Sales`, etc., are NaN without explicitly addressing their importance. This can lead to significant data loss or errors if a crucial column contains many NaNs.
2.  **Incorrect Column Name for 'Years_Since_Release':** The code uses `data['Years_Since_Release'] = 2023 - data['Year_of_Release']`. The correct column name in the dataset is 'Year', not 'Year_of_Release'. This will likely cause a `KeyError` or result in an 'Years_Since_Release' column filled with NaNs or zeros depending on Pandas version and how it handles missing columns in arithmetic, which would then be silently dropped by `dropna()`.
3.  **Over-dropping Columns:** The line `data = data.drop(columns=['Name', 'Year_of_Release', 'Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'])` is problematic.
    *   `Year_of_Release` again, incorrect column name.
    *   `Global_Sales` is part of the original dataset but not used in the final features, which is okay.
    *   **CRITICALLY**, dropping `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` right *after* calculating `Total_Regional_Sales` leads to removing these individual regional sales figures from the features **after** they've been used to create `Total_Regional_Sales`. These individual sales figures are often powerful features themselves and should be included in `X`. The ground truth explicitly includes these as features. This significantly reduces the predictive power of the model.
4.  **No `random_state` for `train_test_split` stratification:** While `random_state` is provided, `stratify=y` is missing for `train_test_split`. This is important for classification tasks, especially with imbalanced classes (which 'High_Sales' likely is), to ensure that both training and testing sets have a similar proportion of target classes.
5.  **Redundant `classification_report`:** While `classification_report` is useful, the prompt specifically asked for individual metrics (accuracy, precision, recall, f1-score, confusion matrix) and not the holistic report. Including it isn't wrong, but strictly speaking, it's beyond the prompt's explicit ask after separate metrics were already requested.

The main issues are the incorrect and overly aggressive missing value handling, the wrong column name for 'Year', and the erroneous dropping of important regional sales features. These flaws would prevent the code from running correctly or from achieving optimal performance/correct problem representation if it did run.",mistral-small-24b-instruct,LLM_judge_config
M12,medium,Feature Engineering and Advanced Analytics,video_games_sales.csv,"```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

# 1) Load the dataset and handle missing values
df = pd.read_csv('video_games_sales.csv')
df.fillna(method='ffill', inplace=True)

# 2) Create engineered features
df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']
df['Publisher_Avg_Sales'] = df.grou...",6,"The generated code attempts to address all requirements of the task, but several critical issues and inaccuracies prevent it from achieving a higher score.

**Correctness Issues:**
*   **Missing Value Handling (Critical Flaw):** `df.fillna(method='ffill', inplace=True)` is an inappropriate and potentially harmful way to handle missing values for a dataset like `video_games_sales.csv`. Forward fill (`ffill`) assumes a sequential dependency in the data, which is not true for game sales data. More importantly, it doesn't handle `NaN` values at the beginning of columns and `Year` and `Publisher`/`Genre` should be imputed more appropriately (e.g., median for year, mode for categorical). The ground truth correctly uses `SimpleImputer` with appropriate strategies. This single point significantly impacts subsequent steps like feature engineering and PCA.
*   **Sales Ratio Division by Zero:** `df['Sales_Ratio_NA_EU'] = df['NA_Sales'] / df['EU_Sales']` will raise a `ZeroDivisionError` or result in `inf` if `EU_Sales` contains zeros. The ground truth adds a small epsilon to avoid this.
*   **Genre Market Share Calculation:** `df.groupby('Genre')['Global_Sales'].transform(lambda x: x / x.sum()) * 100` is fundamentally incorrect for calculating market share. `x.sum()` within `transform` computes the sum for each *group* (genre), not the total global sales across *all* genres. To get market share, you need `(sum of sales for genre) / (total global sales)`. The ground truth correctly calculates `total_global_sales` first.
*   **Platform Popularity:** `df['Platform'].map(df['Platform'].value_counts())` calculates the count for each platform, which is correct, but the ground truth's `transform('count')` is generally more idiomatic for broadcasting group-wise aggregates.
*   **PCA n_components:** The generated code sets `n_components=5` in PCA without first analyzing the explained variance, which is a key part of ""explain variance"". The task implies understanding how many components are needed to explain variance, not just picking an arbitrary number. The ground truth demonstrates this by plotting cumulative explained variance and printing individual component contributions.
*   **Polynomial Features Input:** Applying `StandardScaler` *before* `PolynomialFeatures` on `sales_columns` and then `pd.concat` with the *original* `df` is unusual. Polynomial features are typically applied directly to the original or scaled values, but the `fit_transform` then creating a new DataFrame `poly_features` *from the scaled data* and concatenating it to the original `df` leads to features being created from scaled data, not original sales values, which could be misleading if the user expects polynomial terms of raw sales. Also, `get_feature_names` is deprecated and should be `get_feature_names_out`.
*   **Interaction Features (Year and Genre):** The code `df['Year_genre_interaction'] = df['Year'].astype(str) + '_' + df['Genre']` simply creates a string concatenation. The task asks for *interaction features*, which typically involves numerical multiplication (e.g., year * one-hot encoded genre). This is a significant misinterpretation.
*   **Outlier Handling:** The generated code only handles outliers for `Global_Sales` and *removes* rows with outliers: `df = df[(df['Global_Sales'] >= lower_bound) & (df['Global_Sales'] <= upper_bound)]`. The task asks to ""Identify and handle outliers"" in general terms, but the ground truth correctly applies it to *multiple* relevant numerical columns and uses *capping* (replacing outliers with boundary values) instead of outright removal, which is often preferred to avoid data loss, especially after feature engineering.

**Code Quality/Completeness:**
*   **Imports:** Proper imports are present.
*   **Structure:** The code generally follows the requested numerical order of steps, which is good.
*   **Missing Intermediate Steps:** The output `print` statements and visualizations (except for the correlation heatmap) are missing, making it harder to verify the results of intermediate steps. The ground truth includes these for better clarity and verification.
*   **Robustness:** Lacks error handling (e.g., `try-except` for file loading) present in the ground truth.
*   **Warnings:** No suppression of warnings, which can sometimes clutter output.

Overall, while the code attempts to implement the requirements, the fundamental errors in missing value handling, feature engineering calculations (especially market share and interaction terms), and the chosen outlier handling strategy make it less correct and robust than the ground truth.",mistral-small-24b-instruct,LLM_judge_config
M13,medium,Hypothesis Testing and Statistical Analysis,diabetes.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, pearsonr, f_oneway

# Load the dataset
data = pd.read_csv('diabetes.csv')

# Replace zero values with NaN
data.replace(0, np.nan, inplace=True)

# Impute missing values with median
data.fillna(data.median(), inplace=True)

# Test if diabetic patients have significantly higher glucose levels (t-test)
glucose_diabetic = data[data['Outcome'] == 1]['Glucose']
glucose_nondiabetic = data...",6,"The generated code attempts to address all requirements of the task, which is good. However, it falls short in several critical areas, particularly regarding data cleaning and the interpretation of results.

1.  **Data Cleaning - Critical Error**: The key instruction for data cleaning was: ""clean zero values by replacing them with NaN and then median imputation"". This implicitly means only *biologically implausible* zeros (like Glucose, BMI, BloodPressure, etc.) should be treated this way. The generated code uses `data.replace(0, np.nan, inplace=True)` which indiscriminately replaces *all* zeros, including legitimate zeros for 'Pregnancies' and 'Outcome'. This fundamentally alters the data for these columns, making subsequent statistical tests on them inaccurate (e.g., Pearson correlation on Pregnancies, Chi-square on Outcome where 0 is non-diabetic). The ground truth correctly identifies `cols_to_impute`.

2.  **Lack of Detailed Interpretation**: The task explicitly asked for ""Display p-values and interpret results with significance level α=0.05"". While the generated code prints p-values and a simple ""Significantly different"" or ""Not significantly different"" message, it lacks the detailed interpretation provided in the ground truth code. For example, for the t-test, it should specify *which* group has higher glucose, or for chi-square *what* the association implies, or for Pearson *the direction* of correlation. The interpretation is too generic.

3.  **Missing Error Handling**: The ground truth includes `try-except` for file loading, which is good practice and addresses potential issues a user might face (though not explicitly asked, it is part of ""proper imports and statistical tests"" implying robustness). The generated code assumes the file exists.

4.  **BMI and Age Categories**: While the `pd.cut` usage is correct, the bin boundaries for BMI and Age are slightly different from the ground truth's specific ranges, potentially leading to slightly different categorizations (e.g., Age 30 falls into 'Middle' in ground truth but 'Young' in generated). This is a minor deviation but could lead to different results.

To improve, the code needs to:
*   Correctly identify and impute only biologically implausible zero values.
*   Provide more descriptive and specific interpretations of statistical test results, detailing the nature of the difference or association.
*   Add robustness like file existence checks.",mistral-small-24b-instruct,LLM_judge_config
M14,medium,Classification and Machine Learning,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestCl...",7,"The generated code addresses most of the requirements but has some significant issues that prevent it from scoring higher.

**Strengths:**
*   **Feature Engineering:** Successfully creates `Glucose_BMI_ratio`, `Age_group`, and `Insulin_level`.
*   **Missing Value Handling:** Correctly replaces 0s with NaNs and imputes with the median.
*   **Model Training & Evaluation:** Trains multiple models and calculates various evaluation metrics (accuracy, precision, recall, F1, ROC-AUC).
*   **Cross-Validation:** Uses `cross_val_score` and `StratifiedKFold`.
*   **Pipelines:** Correctly uses `Pipeline` and `ColumnTransformer` for preprocessing and model training, which is good practice.
*   **Visualization:** Attempts to create confusion matrices and ROC curves.
*   **Feature Importance:** Attempts to show feature importance for tree-based models.

**Weaknesses and Areas for Improvement:**

1.  **Incorrect Categorical Encoding (initial stage):** The code initially uses `LabelEncoder` for `Age_group` and `Insulin_level`. However, the `ColumnTransformer` later applies `OneHotEncoder`. This is redundant and confusing. More critically, if `LabelEncoder` was left as the sole encoding for these features, it would imply ordinality which is incorrect for these nominal categories. The `ColumnTransformer` with `OneHotEncoder` *should* be the primary and only method for encoding categorical features. The ground truth correctly identifies categorical features for `OneHotEncoder` within the `ColumnTransformer` without prior `LabelEncoding`.

2.  **Incorrect `GridSearchCV` Setup:**
    *   **""Best Model"" Selection for Tuning:** The code attempts to find the ""best model"" based on `cross_val_score` *before* tuning and then tunes *only* that model. The task asks to ""Perform hyperparameter tuning for the best model using GridSearchCV"" implying that after all models are run, the one with the best *initial performance* should be chosen for tuning. The current approach is problematic because the initial `cross_val_score` might not reflect the true potential after tuning. A better approach would be to tune *all* models or select a widely recognized strong performer (like Random Forest or Gradient Boosting) for tuning, as the ground truth does.
    *   **`param_grid` application:** `grid_search = GridSearchCV(pipeline, param_grid[best_model_name], ...)` This line attempts to tune only `best_model_name`, but the prompt asks to tune the ""best model"", implying that once it's identified, *that specific model* (e.g., Random Forest) should be tuned, not others.
    *   **Missing Hyperparameter Tuning for Other Models:** The task states ""Perform hyperparameter tuning for the best model,"" which implies only one model is tuned. The ground truth picks Random Forest explicitly. However, the current code's approach of dynamically picking the ""best model"" and then only providing param grids for that model is fine, but the *way* it's done makes it harder to compare.

3.  **Incorrect Feature Importance Logic:**
    *   The `feature_importances_` extraction (`best_model_pipeline.named_steps['classifier'].feature_importances_`) is inside an `if` block that checks if the *tuned best model* is a tree-based model. This means if, say, Logistic Regression or SVM turns out to be `best_model_name`, feature importance won't be displayed, which is correct.
    *   However, the `feature_names` extraction for importance is problematic. `numeric_features` are taken from the *original* `X` dataframe, and `get_feature_names_out(categorical_features)` is called on the `OneHotEncoder` from the *pipeline*. This is correct for the features *after* preprocessing, but for `feature_importances_` to map correctly, the `all_feature_names` should be generated in a way that matches the order and presence of features after *all* preprocessing steps, including the one-hot encoding for the categorical features and potentially scaling on numerical features. The ground truth handles this by getting `all_feature_names` after the `ColumnTransformer` is `fit_transform`ed.

4.  **Redundant Metric Calculation for All Models:** After `GridSearchCV`, the code re-loops through *all* models, fits them again (without tuning), and calculates evaluation metrics. This is not efficient, especially since the `GridSearchCV` already provided the tuned best model. The final evaluation should explicitly include the *tuned* best model alongside the *untuned* versions of other models for a fair comparison, as done in the ground truth.

5.  **Confusion Matrix and ROC Curve Plotting:**
    *   The loop `for name, score in results.items():` for plotting confusion matrices and ROC curves uses `models[name].predict(X_test)` and `pipeline.predict_proba(X_test)[:, 1]`. This means it plots for the *untuned* versions of all models, even if one was tuned. The tuned model's performance needs to be explicitly plotted.
    *   The confusion matrix calculation for `y_pred` uses `models[name].predict(X_test)`. This is a problem because `models[name]` is just the raw model, not the one integrated into a pipeline. It should be `pipeline.predict(X_test)` where `pipeline` is the one that was `fit` for that specific model.

6.  **Missing `classification_report` for ""Tuned"" Model:** While `classification_report` is printed for all *untuned* models, it's not explicitly printed for the tuned best model, and the `classification_report` for the tuned model should use predictions from `best_model_pipeline`.

7.  **Minor Issues:**
    *   `max_iter=1000` for Logistic Regression is good, but `solver='liblinear'` is often preferred for smaller datasets and can sometimes be more stable for default settings. The ground truth uses `liblinear`.
    *   The `Age_group` bins are slightly different from the ground truth (`[0, 20, 35, 50, np.inf]` vs `[0, 20, 30, 40, 50, 60, np.inf]`). While not strictly wrong, consistency with the ground truth's detailed bins is good.
    *   The prompt asked for RocCurveDisplay to be used for consistency with `ConfusionMatrixDisplay`, but the generated code plots manually.

The code is functional and solves many parts of the task, but the way certain steps (like categorical encoding, tuning logic, and plotting) are implemented reduces its correctness and robustness compared to an expert solution.",mistral-small-24b-instruct,LLM_judge_config
M15,medium,Feature Engineering and Advanced Analytics,diabetes.csv,"```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn....",6,"The generated code addresses most of the task requirements but has several issues that reduce its overall quality and correctness.

**Positive Aspects:**
- **Loading and Preprocessing:** Correctly loads the data and attempts to handle missing values, although the strategy could be more robust.
- **Engineered Features:** Successfully creates the requested engineered features.
- **Polynomial Features:** Applies polynomial features correctly.
- **Scaling:** Uses both StandardScaler and MinMaxScaler.
- **PCA:** Implements PCA, although the handling and explanation of variance are incomplete.
- **Feature Selection:** Applies SelectKBest and RFE.
- **Correlation Filtering:** Implements correlation-based filtering.
- **SMOTE:** Applies SMOTE for class imbalance.
- **Feature Importance:** Calculates and visualizes feature importance using Random Forest.
- **Imports:** Most necessary imports are present.

**Areas for Improvement/Critical Issues:**

1.  **Missing Value Handling (Correctness/Completeness):** The prompt specifically mentioned ""Identify columns where 0s represent missing values... Replace 0s with NaN in specified columns. Impute missing values using the median strategy."" The generated code simply uses `SimpleImputer(strategy='mean')` on the entire dataframe, which is a common approach but doesn't follow the specific instruction of handling 0s as NaNs in particular columns and then imputing with median. This is a significant deviation from the prompt's explicit instruction.

2.  **Polynomial Features Application (Correctness/Code Quality):**
    *   The `num_cols` for polynomial features includes *all* columns, including the newly engineered features. While not strictly wrong, the ground truth code was more discerning, only applying to the core numerical features and then concatenating. The generated code replaces the original `poly_df` (which contains *only* polynomial features) directly in the scaling step, losing all other non-polynomial features and the target variable. This makes subsequent steps like PCA, scaling, and feature selection operate on a subset of the data, which is incorrect.
    *   The polynomial features are generated from `data_imputed[num_cols]`, but then `poly_df` *replaces* the `data_imputed` dataframe in the scaling step (`data_scaled_std = scaler_std.fit_transform(poly_df)`). This means all original features (like the target variable `Outcome`) and engineered features not included in `num_cols` are lost before PCA and feature selection. This is a critical logical error. The features should be concatenated back into the main DataFrame *after* polynomial transformation and *before* scaling.

3.  **PCA Application (Correctness/Completeness):**
    *   The task asks to ""explain variance ratios."" The generated code calculates `explained_variance_std` and `explained_variance_mm` but doesn't print or visualize them, failing to fully address the ""explain variance ratios"" requirement.
    *   PCA is applied to `data_scaled_std` and `data_scaled_mm` separately, but then `X` for feature selection is arbitrarily set to `data_scaled_std`, seemingly ignoring the PCA results. The task implies PCA as a dimensionality reduction step to be used if useful for subsequent processes, or at least its output should be considered a ""final"" feature set path. The generated code effectively performs PCA but then proceeds with feature selection on the *scaled data before PCA*.

4.  **Feature Selection Input (Correctness):** Both `SelectKBest` and `RFE` are applied to `data_scaled_std` (assigned to `X`) which includes all features (if the polynomial step was corrected). However, it should ideally be applied *after* the correlation filtering step, or PCA if PCA was chosen as the primary dimensionality reduction. The order of operations is somewhat muddled. The ground truth applied these to the `X_filtered_corr` which was a more refined set.

5.  **Correlation Filtering (Correctness/Code Quality):**
    *   `data_corr_filtered = pd.DataFrame(X).drop(columns=to_drop)`: `X` is already a DataFrame from `data_scaled_std`. Converting it to `pd.DataFrame(X)` isn't necessary but harmless. However, `X` (which should now be `data_scaled_std`) contains the polynomial features only in the generated code's current flawed flow. If the polynomial feature merging was fixed, this step would be more appropriate.
    *   The ground truth applied correlation filtering *before* PCA and feature selection using `SelectKBest` and `RFE`, which is a logical flow. The generated code applies it after PCA and feature selection, but the feature selection steps (`X_kbest`, `X_rfe`) are never actually used in subsequent steps, and `X` is used for feature selection, not the `data_corr_filtered`. This creates a confusing and incorrect pipeline.

6.  **Overall Pipeline Flow (Correctness/Completeness):** The sequence of operations is not optimally structured. For example, PCA and feature selection methods like SelectKBest/RFE are dimensionality reduction/feature selection techniques that typically lead to a reduced feature set. However, the code then proceeds to correlation filtering, and then SMOTE on the *correlation-filtered data*, not necessarily the output of PCA or the `SelectKBest`/`RFE` results. This indicates a lack of a clear, coherent pipeline for the final feature set. The task description implies using *these techniques* to arrive at a better feature set, not just running them in isolation.

7.  **RFE Estimator (Code Quality):** `RFE(estimator=RandomForestClassifier())` without `random_state` can lead to non-reproducible results. The ground truth used `LogisticRegression` which is more common for feature selection in RFE due to its interpretability of coefficients (though RF can work).

8.  **Feature Importance Visualization (Correctness/Completeness):** While the visualization is created, the label `top_features` is generated using indices `data_corr_filtered.columns[i]`. If `data_corr_filtered` itself is `pd.DataFrame(X)` and `X` refers to `data_scaled_std` (which only contains polynomial features if the earlier issue is not fixed), the feature names might be incorrect or misleading. The ground truth creates a DataFrame with explicit 'Feature' and 'Importance' columns for clarity.",mistral-small-24b-instruct,LLM_judge_config
M16,medium,Hypothesis Testing and Statistical Analysis,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
import scipy.stats as stats
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# 1) Load the dataset and handle any missing values
df = pd.read_csv('world-happiness-report-2021.csv')
df = df.dropna()

# Define regions for Western Europe
western_europe = ['Austria', 'Belgium', 'Denmark', 'Finland', 'France', 'Germany', 'Ireland', 'Italy', 'Luxembourg',
                  'Netherlands', 'Norway', 'Portugal', 'Spain', 'Sweden', 'Switzerland', ...",4,"The generated code attempts to address all parts of the prompt but falls short in several critical areas, leading to incorrect or incomplete analysis.

**Major Issues:**

*   **Missing Value Handling (Critical):** The generated code simply uses `df.dropna()`. While this handles missing values, the prompt specifically asked to handle missing values, implying a more robust approach if specific columns are critical or to report how many rows were dropped. The ground truth identified critical columns and only dropped NaNs in those, which is a more refined approach.
*   **Interpretation (Major Fail):** The prompt explicitly requested ""Display p-values and interpret results with significance level α=0.05."" The generated code prints p-values but provides *no interpretation whatsoever* of whether to reject or fail to reject the null hypothesis, or what that means in plain language for each test. This is a core requirement of hypothesis testing. The ground truth provides detailed interpretations for each test.
*   **Task 2 - Western Europe T-test (Incorrect Implementation):**
    *   The `t_test_group` function takes `group_name` as a list of strings but then checks `df[column].isin(group_name)`. For 'Regional indicator', `group_name` should be `['Western Europe']`, not a list of countries. The actual list of countries provided (`western_europe`) is for identifying countries *within* Western Europe, not the `Regional indicator` itself. This means the t-test for Western Europe vs. others will likely be incorrect as it's checking individual country names against the 'Regional indicator' column directly.
    *   It hardcodes 'Ladder score' inside the function, making it less flexible than implied by the `column` parameter, though for this specific task, it might pass incidentally.
*   **Task 3 - Mann-Whitney U Test (Incorrect Column Name):** The code attempts to use `'Log GDP per capita'` which is incorrect; the actual column name is `'Logged GDP per capita'`. This will cause a `KeyError`.
*   **Task 4 - ANOVA (Hardcoded Regions & Missing Regions):** The ANOVA test hardcodes a specific list of regional indicators, which might not be exhaustive (missing regions from the dataset) and is not robust if the dataset changes. A better approach, like the ground truth, would be to dynamically get unique regions.
*   **Task 5 - Correlation (Correct Column Name Issue):** Similar to Task 3, it uses `'Log GDP per capita'` instead of `'Logged GDP per capita'`.
*   **Task 6 - Chi-square Test (Missing Happiness Categorization):** The most significant issue here is that the generated code attempts to use a column named `'Ladder score (High/Medium/Low)'` for the chi-square test. This column *does not exist* in the original DataFrame and is never created. It implies the user intended to categorize happiness, but failed to actually implement it. The ground truth correctly defines and creates this column. This makes the chi-square test completely non-functional.
*   **Hardcoding `alpha`:** While the ground truth also defines `alpha` at the start, the generated code only defines it *after* all tests are run, making it impossible to use for conditional interpretation without re-running. It also doesn't use `alpha` anywhere in its current state as it lacks interpretations.
*   **Lack of Readability and Output Formatting:** The output is very basic. The ground truth's use of print statements, separators, and clear headings makes the output much more readable and professional.

**Minor Issues:**

*   Imports: `pairwise_tukeyhsd` is imported but never used. While not a failure, it's unnecessary clutter.
*   No error handling for file not found: The ground truth includes a `try-except` for file loading, which is good practice.

**Summary of Deficiencies:**

The generated code fails to correctly execute several statistical tests due to incorrect column names or missing data preparation steps (like happiness categorization). Crucially, it completely misses the requirement for interpretation of results, rendering the statistical output largely useless for the given task. The structure and robustness are also significantly weaker than the ground truth.",mistral-small-24b-instruct,LLM_judge_config
M17,medium,Regression and Predictive Modeling,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('worl...",7,"The generated code addresses most of the task requirements but has several areas for improvement in terms of correctness, completeness, and adherence to specific instructions.

**Strengths:**
*   Correctly loads the dataset.
*   Performs data splitting.
*   Trains multiple regression models.
*   Performs hyperparameter tuning for one model using GridSearchCV.
*   Evaluates models with MAE, MSE, RMSE, and R² score.
*   Generates residual plots and feature importance visualizations.
*   Implements cross-validation.
*   Makes predictions for hypothetical new data.
*   Good use of standard ML libraries (sklearn, pandas, numpy, matplotlib, seaborn).

**Areas for Improvement / Issues:**

1.  **Data Preprocessing (Handling Missing Values):**
    *   **Issue:** The generated code uses `data = data.dropna()` which drops *all* rows with *any* missing value in *any* column. The ground truth specifically focuses on dropping rows only when missing values are present in the *target or feature columns*. While often similar, this can lead to unnecessarily dropping more data if irrelevant columns have NaNs.
    *   **Improvement:** Focus `dropna` on the specific columns relevant to the model (`df[FEATURE_COLUMNS + [TARGET_COLUMN]].dropna()`).

2.  **Data Preprocessing (Categorical Variables):**
    *   **Issue:** The generated code applies `pd.get_dummies(data, columns=['Regional indicator'], drop_first=True)` and then *throws away* this encoding by selecting only the numerical columns for `X` afterwards: `X = data_encoded[['Logged GDP per capita', ...]]`. The prompt implicitly stated that features were 'GDP per capita', 'Social support', etc., which are all numerical. The ground truth correctly identifies that no categorical encoding is needed for the *specified features*. While encoding `Regional indicator` is not wrong, it's not used.
    *   **Improvement:** Either correctly include the one-hot encoded 'Regional indicator' in `X` if it's meant to be a feature, or remove the `get_dummies` step as it's not utilized for the specified features.

3.  **Feature and Target Variable Names:**
    *   **Issue:** The prompt specified specific feature names like ""GDP per capita"", ""Freedom"" and ""Perceptions of corruption"". The dataset column names are ""Logged GDP per capita"", ""Freedom to make life choices"", and ""Perceptions of corruption"". The ground truth clarifies this mapping and uses the exact column names from the dataset. The generated code correctly uses the dataset's exact column names ('Logged GDP per capita', 'Freedom to make life choices', 'Perceptions of corruption'), which is good, but it's worth noting the slight discrepancy with the prompt's simplified wording. However, the ground truth made this explicit.

4.  **Scaling and Feature Importance/Predictions:**
    *   **Issue:** The features `X_train` and `X_test` are scaled using `StandardScaler`. This is a good practice. However, when performing cross-validation (`cross_val_score(best_model, X, y, ...)`) and making predictions for new hypothetical countries (`new_data_scaled = scaler.transform(new_data_df)`), the original, *unscaled* `X` and `y` are passed to `cross_val_score`, and the `new_data_df` is scaled with a scaler trained on `X_train`. This is a mismatch.
        *   Cross-validation should ideally be done on the scaled data if the models are trained on scaled data, or a `Pipeline` should be used to encapsulate scaling within each fold.
        *   More critically, `best_model` (a `GradientBoostingRegressor`) was trained on *scaled* data. When predicting `new_data_df`, it must be scaled using the *same scaler* trained on `X_train`. The features in `new_data_df` typically need to be in a similar range to the scaled `X_train` data for correct prediction. The current ""new\_data"" values (`Logged GDP per capita`: [1.0, 1.5], etc.) are *already* in a ""scaled-like"" range, which is problematic. Actual GDP values are around 8-11. Scaling 1.0 would result in a very different value than scaling 8.0.
    *   **Improvement:** For cross-validation, either pass scaled `X` (after fitting a scaler to the *entire* `X` or handling scaling within cross-validation folds using a pipeline), or ensure `X` remains unscaled if the models are not sensitive to scaling (though they are here). For new predictions, the hypothetical values provided should be *realistic unscaled values* from the original domain, and then `scaler.transform()` should be applied. The provided hypothetical data seems to be *already scaled* in a strange way, which is a misunderstanding.

5.  **Best Model Selection for Hyperparameter Tuning:**
    *   **Issue:** The code determines `best_model` based on R² during the initial model training loop, but then *hardcodes* `GradientBoostingRegressor(random_state=42)` for the hyperparameter tuning step (`grid_search = GridSearchCV(estimator=GradientBoostingRegressor(...))`). The prompt stated ""Perform hyperparameter tuning for the *best model* using GridSearchCV"". While Gradient Boosting is a good choice for tuning, the code doesn't dynamically use the `best_model` found in the previous step.
    *   **Improvement:** Dynamically select the `best_model` from the initial comparison loop to be tuned, or add a comment explaining why a specific model (e.g., GBR) was chosen if the intention was to always tune that one regardless of initial R2.

6.  **Residual Plot Labels:**
    *   **Minor Issue:** The residual plot labels ""Predicted values"" and ""Residuals"" are generic.
    *   **Improvement:** More descriptive labels like ""Predicted Ladder Score"" and ""Residuals (Actual - Predicted)"" as used in the ground truth would be better.

7.  **Feature Importance Visualization:**
    *   **Missing:** The ground truth correctly checks `if hasattr(best_tuned_model, 'feature_importances_')`. Linear models don't have `feature_importances_`, so this check is important for robustness. The generated code assumes `best_model` will always have this attribute (which it will if `best_model` is a GBR/RF, but not if it were Linear/Ridge/Lasso).
    *   **Improvement:** Add the `hasattr` check for robustness.

8.  **Prediction Analysis:**
    *   **Missing:** The ground truth includes a simple analysis of how the predicted values for hypothetical countries align with expectations based on their input features. This demonstrates understanding of the model's output. The generated code only prints the predictions.
    *   **Improvement:** Add a brief analytical comment on the predictions.

9.  **Random State for Ridge/Lasso:**
    *   **Minor Issue:** Ridge and Lasso models do not have `random_state` parameters. While the ground truth code includes it, it's unnecessary noise. The generated code correctly omits it. (This is a very minor point for the generated code's benefit).

Overall, the generated code provides a functional solution to many aspects of the task, but several details in preprocessing, data handling consistency (scaling), and adherence to the ""best model"" for tuning could be refined for higher quality and completeness.",mistral-small-24b-instruct,LLM_judge_config
M18,medium,Feature Engineering and Advanced Analytics,world-happiness-report-2021.csv,"```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as...",6,"The generated code attempts to address many of the requirements but falls short in several key areas of correctness, completeness, and robustness, particularly when compared to the ground truth.

**Correctness Issues:**
*   **NaN Handling:** The generated code uses a simple `data = data.dropna()` right after loading. This is problematic because it removes all rows with *any* NaN, which can drastically reduce the dataset size, especially if NaNs are sparse. The ground truth specifically handles `Perceptions of corruption` NaNs by filling them, and then drops remaining NaNs only in relevant columns for calculations, which is a more robust approach.
*   **Polynomial Features Application:** The generated code applies `PolynomialFeatures` to `['Ladder score', 'Logged GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Perceptions of corruption']`. The `Ladder score` is the target variable (`y`) and should not be included in feature engineering for the input `X`. Also, `PolynomialFeatures` creates a NumPy array, but the code doesn't integrate these new polynomial features back into the main DataFrame (`data`). The ground truth correctly integrates them and avoids including the target variable.
*   **Feature Scaling Input:** The generated code applies `fit_transform` of scalers to `data` directly: `scaled_data[name] = scaler.fit_transform(data)`. `data` at this point still contains non-numeric columns (`Country name`, `Regional indicator`) and the target variable (`Ladder score`). Scaling should only be applied to numerical features for `X`. The ground truth correctly separates `X` (numerical features) and `y` (target) before scaling `X`.
*   **PCA/t-SNE Input:** Similar to scaling, PCA/t-SNE are applied on `scaled_data['StandardScaler']`, which still refers to the potentially problematic scaled version of the entire `data` DataFrame, not just the features. This could lead to errors or incorrect dimensionality reduction.
*   **SelectKBest `f_classif`:** The task specifies ""f_regression"" for SelectKBest, as 'Ladder score' is a regression target. The generated code uses `f_classif`, which is for classification tasks. This is a fundamental error for a regression problem.
*   **Mutual Information Scoring:** The ground truth uses `mutual_info_regression` from `sklearn.feature_selection`. The generated code imports `mutual_info_score` from `sklearn.metrics`, which is typically used for classification (discrete labels) or evaluation, and its application `mutual_info_score(X, y, discrete_features='auto')` is incorrect for regression. It's missing `mutual_info_regression` from `sklearn.feature_selection`.
*   **RFE Estimator:** The ground truth uses `LinearRegression` as the RFE estimator, which is a common and efficient choice. The generated code uses `RandomForestRegressor()`. While not strictly wrong, a simpler estimator is often preferred for RFE due to computational cost and interpretability.
*   **Visualization:** The generated code only includes the Random Forest feature importance plot. It completely misses the requested PCA and t-SNE visualizations.

**Completeness Issues:**
*   **Interaction Features:** The ground truth creates two specific interaction features (`GDP_x_SocialSupport`, `GDP_x_HealthyLifeExpectancy`). The generated code only creates one, `Economic_Social_Interaction`, which is `Logged GDP per capita * Social support`.
*   **Specific `PolynomialFeatures` columns:** The ground truth correctly identifies specific key factors for polynomial features (`Logged GDP per capita`, `Social support`, `Healthy life expectancy`, `Freedom to make life choices`, `Generosity`, `Perceptions of corruption`). The generated code incorrectly includes 'Ladder score' and omits 'Generosity'.
*   **Robust NaN/Inf Handling during X identification:** The ground truth includes an important step to replace `np.inf` and `-np.inf` with `np.nan` and then fill them with the mean after some feature engineering (like `Happiness_Efficiency` which involves division) to prevent errors in subsequent scaling and modeling steps. The generated code does not have this.
*   **Code Structure/Comments:** The ground truth is very well-structured with clear comments for each step, making it easy to follow. The generated code is less organized and less commented, though it does group some logical steps.

**Code Quality:**
*   Lack of robustness for potential errors (e.g., file not found handled in ground truth).
*   Inefficient `dropna()` at the start.
*   Misuse of `f_classif` and `mutual_info_score`.
*   Lack of visualization for PCA/t-SNE as requested.

Overall, while the code attempts most steps, the crucial correctness errors in applying transformations and selecting features, especially the use of classification methods for a regression task, along with incomplete visualizations and kurang robust NaN handling, degrade its quality significantly.",mistral-small-24b-instruct,LLM_judge_config
M19,medium,Time Series Analysis,synthetic_covid19_data.csv,"```python
import pandas as pd
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
import matplotlib.pyplot as plt

# 1) Load the dataset, parse the 'date' column, and filter for 'location' == 'USA'
data = pd.read_csv('synthetic_covid19_data.csv', parse_dates=['date'])
usa_data = data[data['location'] == 'USA']

# 2) Set the 'date' column as the index for the filtered data
usa_data.set_index('date', inplace=True)

# 3...",7,"The generated code addresses most of the requirements but has several areas for improvement in terms of robustness, completeness, and plotting.

**Strengths:**
*   Correctly loads, parses, and filters the data.
*   Correctly sets the date as the index.
*   Performs weekly resampling with `sum`.
*   Performs additive time series decomposition.
*   Conducts the ADF test and correctly interprets the p-value for differencing.
*   Plots ACF and PACF, choosing between original or differenced series based on stationarity.

**Areas for Improvement:**

1.  **Decomposition Period:** The `seasonal_decompose` function is called without specifying the `period` argument. For weekly data, a period of 52 (weeks in a year) is typically appropriate for annual seasonality. Without this, `seasonal_decompose` will attempt to infer it, which might not be accurate or could lead to errors if the data length is insufficient. The ground truth explicitly sets `period=52`.
2.  **Missing Imports for Plotting:** The generated code uses `sm.graphics.tsa.plot_acf` and `sm.graphics.tsa.plot_pacf` but `from statsmodels.graphics.tsaplots import plot_acf, plot_pacf` is missing. Instead it relies on `import statsmodels.api as sm`, which is less direct for these specific plotting functions and could lead to issues if `sm` doesn't expose them in the expected way, or simply means more verbose calls.
3.  **Error Handling/Robustness:** The code lacks proper error handling for file not found, empty datasets, or insufficient data points for certain operations (e.g., decomposition, ADF test, or plots after differencing). The ground truth includes `try-except` blocks and checks for empty DataFrames/Series.
4.  **Plotting Details:**
    *   The `decomposition.plot()` in the generated code does not get a custom title or size, which is a minor aesthetic point.
    *   The `plot_acf` and `plot_pacf` calls hardcode `lags=40`. It's generally better to let the function default to reasonable lags or compute them dynamically based on data length, as done in the ground truth (`max_lags` calculation).
5.  **Readability/Comments:** While the prompt requested only code, a few more comments explaining the steps within the code itself would enhance readability, especially around the stationarity checks and differencing logic. The ground truth provides more comprehensive comments and print statements indicating progress.
6.  **`dropna()` before ADF Test:** The `weekly_cases` might contain `NaN` values, especially after `resample().sum()` if a week has no data. While `adfuller` can handle some `NaN`s, explicitly dropping them before the test (e.g., `weekly_cases.dropna()`) makes the analysis more robust and prevents potential issues, as done in the ground truth.

The code is functional for ideal input but could be more robust and complete to handle real-world data variations and edge cases.",mistral-small-24b-instruct,LLM_judge_config
M2,medium,Statistical Analysis,winequality-red.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Categorize wines based on 'pH' values
data['pH_group'] = pd.cut(data['pH'], bins=[0, 3.2, 3.4, float('inf')],
                           labels=['Low pH', 'Medium pH', 'High pH'])

# Calculate average 'quality' for each pH group
average_quality = data.groupby('pH_group')['quality'].mean()

# Perform ANOVA test
anova_result =...",6,"The generated code correctly performs the basic categorization, mean calculation, and ANOVA test. However, it fails to fully meet the visualization requirement regarding confidence intervals. The request was for ""error bars representing 95% confidence intervals,"" but the generated code uses `yerr=sem_quality`, which represents the standard error of the mean, not the confidence interval. Calculating and plotting actual 95% CIs requires a bit more statistical calculation (usually involving a t-distribution and degrees of freedom). Additionally, the pH group labels are less informative in the generated code compared to the ground truth. The labels `Low pH (< 3.2)`, `Medium pH (3.2 to 3.4)`, and `High pH (> 3.4)` would be more descriptive. The output of ANOVA is not interpreted as well as in the ground truth. Also, the ground truth includes robust error handling for file loading, which is good practice.",mistral-small-24b-instruct,LLM_judge_config
M20,medium,Regression,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.compose import ma...",9,"The generated code impressively uses `Pipeline` and `ColumnTransformer` for a clean and efficient data preprocessing workflow, which is a strong point. It correctly handles all the requirements: loading data, feature selection, handling missing values with median imputation, one-hot encoding 'continent', scaling numerical features, splitting data, training Linear Regression and Random Forest models, and evaluating them.

One minor area for improvement is handling missing values in the target variable (`new_deaths`), which was explicitly mentioned in the prompt (step 3: ""Handle missing values in selected features and target by imputing with the median""). The current pipeline correctly imputes numerical features, but if `new_deaths` itself had missing values, the current setup would fail or lead to errors. While the ground truth handles this by applying median imputation to both features and the target *before* splitting and processing, the generated code relies on `SimpleImputer` within the `ColumnTransformer`, which only applies to X. For a production scenario where Y might also have NaNs, a separate imputation step for Y or ensuring Y is clean upfront would be necessary. However, for this specific problem, assuming `new_deaths` doesn't have missing values in the given synthetic data, the solution works.

Another subtle point is that `StandardScaler` is applied to columns *after* `SimpleImputer` and `OneHotEncoder` have transformed them. While this is generally correct for numerical features transformed by `SimpleImputer`, `OneHotEncoder` outputs binary columns which typically don't need scaling, and scaling them can sometimes be counterproductive depending on the model. However, `StandardScaler` within a pipeline will attempt to scale all numerical columns it receives, including the one-hot encoded ones. This is a common practice and not necessarily an error, but it's worth noting. The `ColumnTransformer` handles this by applying the scaler only to the numerical features (which are then combined with the categorical features). The pipeline's `scaler` step after `preprocessor` acts on *all* output features of the preprocessor, including the OHE features. This is a slight deviation from the ground truth which scales features *before* training after all preprocessing. In general, it's better to scale *only* the truly numerical features. This would require the scaler to be part of the `ColumnTransformer` itself, applied only to the numeric features bundle.

Despite these minor points, the code is very high quality, follows best practices with pipelines, and correctly implements all major requirements.",mistral-small-24b-instruct,LLM_judge_config
M21,medium,Hypothesis Testing,synthetic_covid19_data.csv,"```python
import pandas as pd
import numpy as np
from scipy.stats import f_oneway, pearsonr, ttest_ind
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Load the dataset
data = pd.read_csv('synthetic_covid19_data.csv')

# Handle missing values with median imputation
columns_to_impute = ['reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases']
for column in columns_to_impute:
    data[column].fillna(data[column].median(), inplace=True)

# 1) Tes...",6,"The generated code addresses most of the requirements but has several critical issues that prevent it from being production-ready or fully reliable for a data science benchmark.

**Correctness Issues:**

1.  **Imputation:** While it performs median imputation, it does so for `data[column].fillna(data[column].median())`. This is problematic because `data[column].median()` calculates the median *before* imputation. If the column mostly contains `NaN` values, its median might be `NaN` itself, leading to no imputation. The correct approach (as in the ground truth) is to calculate the median *once* and then use that computed median for `fillna`.
2.  **ANOVA Data Preparation:** The ANOVA calculation for `reproduction_rate` does not properly handle `NaN` values *after* the initial imputation when filtering for continents. If a 'reproduction_rate' is NaN for a specific continent, `f_oneway` will likely behave unexpectedly or raise an error. The `dropna()` method should be applied to the specific series being passed to `f_oneway` for each group.
3.  **T-test Data Preparation for 'high_death_rate_country':**
    *   `latest_data = data.drop_duplicates(subset=['country', 'date'], keep='last')` is incorrect. The task specifies ""latest entry per country"" which means grouping by country and then selecting the last entry, typically after sorting by date (`data.sort_values(by=['country', 'date']).groupby('country').last().reset_index()`). `drop_duplicates` might not give the *latest* entry if the data isn't pre-sorted by date.
    *   The calculation `np.percentile(ratio, 75)` on `ratio = latest_data['total_deaths'] / latest_data['population']` does *not* handle `Inf` or `NaN` values that can arise from division by zero or missing population data. This can lead to incorrect percentile calculations and subsequent classification. The ground truth explicitly handles `inf` and `nan` for this ratio.
    *   It assumes `median_age` will be non-null in the `latest_data` after its initial imputation. While the initial imputation helps, it's safer to ensure there are no NaNs in `median_age` for the groups being passed to `ttest_ind`, especially if the initial imputation was flawed or if the `latest_data` subset introduces new `NaN` contexts.
4.  **Missing Null Hypothesis and Conclusion String Prints:** The prompt specifically asks to ""state the null hypothesis, print the p-value, and conclude whether to reject the null hypothesis at α=0.05"". While it prints the p-value and a conclusion, it doesn't clearly print *the statement of the null hypothesis* before each test's results. It assigns the null hypothesis string to a variable but doesn't print it.

**Code Quality and Completeness:**

1.  **Missing Imports (minor):** It imports `statsmodels.stats.multicomp.pairwise_tukeyhsd` but doesn't use it.
2.  **No Alpha Configuration:** The `ALPHA` constant is missing, making the significance level less flexible.
3.  **Error Handling:** Lacks robust error handling, e.g., for `FileNotFoundError` when loading the CSV, or for cases where there's insufficient data for a statistical test (e.g., empty groups for ANOVA/t-test, or all NaNs after filtering for ANOVA). The ground truth handles this much better.
4.  **Output Clarity:** The output is very minimal compared to the ground truth, which provides clear headings, states explicit null hypotheses and conclusions, and offers more context for each test.
5.  **Task Fulfillment:**
    *   ""Provide only the Python code without explanations."" - This was followed.
    *   ""Imputation ('reproduction_rate', 'stringency_index', 'median_age', 'total_deaths', 'population', 'new_cases')."" - Done, but flawed.
    *   ""ANOVA for 'reproduction_rate' by continent"" - Done, but potentially flawed data prep.
    *   ""Pearson correlation for 'stringency_index' and 'new_cases'"" - Done, but assumes no NaNs in the final selected series.
    *   ""t-test for 'median_age' based on a derived high death rate category"" - Done, but flawed in how the 'latest entry' and 'death rate ratio' are calculated and handled.
    *   ""For each test, state the null hypothesis, print the p-value, and conclude"" - P-value and conclusion are printed, but null hypothesis statement is not *printed* for each test.

Overall, the code provides a basic structure and attempts to perform the required analyses, but the data preprocessing steps for deriving the death rate category and ensuring valid inputs for statistical tests are not robust enough, leading to potential inaccuracies or errors in a real-world scenario.",mistral-small-24b-instruct,LLM_judge_config
M22,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# Select the numerical columns
columns = ['elixirCost', 'Win Rate', 'Usage', 'Win Rate Change', 'Usage Change']
selected_data = data[columns]

# Calculate the pairwise correlation matrix
correlation_matrix = selected_data.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=Tru...",9,"The generated code correctly performs all the required steps: loading the CSV, selecting the specified numerical columns, calculating the correlation matrix, and visualizing it as a heatmap using Seaborn and Matplotlib. It includes proper imports and produces the expected output.

Strong points:
- All task requirements are met.
- Code is concise and easy to understand.
- Proper use of pandas for data manipulation and seaborn/matplotlib for visualization.

Areas for improvement (minor):
- The generated code is missing error handling (e.g., `FileNotFoundError`, `KeyError` for missing columns), which is present in the ground truth. While not explicitly asked for, robust code often includes this.
- Does not wrap the logic in a function, which is good practice for reusability and organization, as demonstrated in the ground truth.",mistral-small-24b-instruct,LLM_judge_config
M23,medium,Exploratory Data Analysis (EDA),clash_royale_cards.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('clash_royale_cards.csv')

# Group the data by the 'rarity' column
grouped = df.groupby('rarity')

# Calculate the average 'elixirCost', 'Win Rate', and 'Usage' for each card rarity
result = grouped.agg({
    'elixirCost': 'mean',
    'Win Rate': 'mean',
    'Usage': 'mean'
}).reset_index()

# Display the calculated average values for each rarity
print(result)
```",9,"The generated code is highly correct and complete. It effectively loads the dataset, groups it by 'rarity', and calculates the average 'elixirCost', 'Win Rate', and 'Usage' for each rarity, then displays the results. The use of `.agg()` is an excellent and explicit way to specify the aggregation for multiple columns, and `.reset_index()` makes the output cleaner by turning 'rarity' back into a regular column, which can be preferable for further processing or display.

The only minor difference from the ground truth, which does not impact correctness or functionality, is that the ground truth directly selects the columns before calling `.mean()`, resulting in 'rarity' as the index. The generated code uses `.agg()` and then `.reset_index()`, achieving the same result but with 'rarity' as a column. Both are perfectly valid and common pandas idioms. The generated code is arguably more explicit about the aggregation functions.",mistral-small-24b-instruct,LLM_judge_config
M24,medium,Feature Engineering,clash_royale_cards.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# 1) Load the dataset
data = pd.read_csv('clash_royale_cards.csv')

# 2) Create a new binary target column named `is_high_win_rate`
data['is_high_win_rate'] = (data['Win Rate'] > 55).astype(int)

# 3) Select the specified features
features = data[['elixirCost', 'Usage', 'rarity']]

# 4) Apply one-hot encoding to the categorical `rarity` column
encoder = OneHotEncode...",7,"The generated code correctly performs most of the required steps: loading the data, creating the target variable, and splitting the data. The OneHotEncoder approach for 'rarity' is also correct, but the implementation of concatenating the encoded features back into the DataFrame (`features = pd.concat([features.drop(columns=['rarity']), rarity_encoded_df], axis=1)`) has a potential issue. If the original `features` DataFrame does not have a continuous, 0-indexed integer index (which it might not if rows were dropped or reordered earlier), the concatenation could misalign the data, leading to incorrect feature-target pairings.

While `pd.get_dummies` (used in the ground truth) is often simpler and less prone to indexing issues for one-off encoding directly on the DataFrame, the OneHotEncoder approach is not inherently wrong if implemented carefully. The provided solution misses setting the index for `rarity_encoded_df` to match `features.index` before concatenation, which is crucial for robust merging. This could lead to data misalignment if `features` does not have a default integer index. Additionally, the import of `OneHotEncoder` implies a likely use case for `sklearn` for encoding, so using it rather than `pd.get_dummies` is acceptable given the import.

To improve, ensure the `rarity_encoded_df` inherits the index from the original `features` DataFrame to ensure correct alignment during concatenation. This would make the encoding step more robust.",mistral-small-24b-instruct,LLM_judge_config
M25,medium,Classification,iris.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('iris.csv')

# Define features X
X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]

# Define the target y
y = data['Species']

# Encode the categorical target variable into numerical labels
label_encoder = Labe...",9,"The generated code correctly implements all specified requirements: loading data, defining features and target, encoding the target, splitting the data with the correct test size and random state, training a KNN classifier with `n_neighbors=3`, and reporting accuracy.

The code is well-structured and uses appropriate imports.

The only minor difference from the ground truth, which does not affect correctness or essential functionality, is the omission of `stratify=y_encoded` in the `train_test_split` function. While `stratify` is good practice for classification tasks to ensure class distribution parity in splits, it wasn't explicitly mentioned in the task description as a *required* parameter for `train_test_split`. Given the task constraints, the omission is minor. Also, the ground truth includes error handling for `FileNotFoundError` and a more descriptive print statement for accuracy, which are good additions but not strictly required by the prompt. Because the core requirements are fully met, the score remains high.",mistral-small-24b-instruct,LLM_judge_config
M26,medium,Hypothesis Testing,iris.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
data = pd.read_csv('iris.csv')

# Filter the data for the two species
versicolor = data[data['Species'] == 'Iris-versicolor']['PetalLengthCm']
virginica = data[data['Species'] == 'Iris-virginica']['PetalLengthCm']

# Perform the independent two-sample t-test
t_statistic, p_value = ttest_ind(versicolor, virginica)

# Null and alternative hypotheses
# Null hypothesis (H0): The mean PetalLengthCm of Iris-versicolor ...",8,"The generated code correctly performs the t-test, extracts the relevant data, and interprets the results based on the p-value and alpha. It demonstrates a clear understanding of the task requirements.

However, there are a few areas for improvement to reach a perfect score:
1.  **Hypotheses statement:** While the hypotheses are correctly stated as comments in the code, the ground truth explicitly prints them as part of the output, which was a requirement in step 4 (""Clearly state the null and alternative hypotheses for this test.""). The generated code only has them as comments.
2.  **Robustness (File Not Found):** The generated code lacks error handling for `FileNotFoundError` if 'iris.csv' is not present, which is a good practice for real-world applications and was included in the ground truth for robustness.
3.  **Completeness of output:** The request asked to ""clearly state the null and alternative hypotheses"" and the ground truth printed them. The generated code only includes them as comments, which isn't the same as stating them in the output.
4.  **Formatting of output:** The ground truth includes more contextual print statements and formatting (e.g., using `f""Calculated T-statistic: {t_statistic:.4f}""`) which makes the output more readable and professional. The generated code's output is a bit more concise but could benefit from more descriptive labels and formatting.

Overall, it's a very good attempt that functionally solves the core problem but could be enhanced in terms of output completeness and robustness.",mistral-small-24b-instruct,LLM_judge_config
M27,medium,Data Visualization,iris.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
iris = pd.read_csv('iris.csv')

# Ensure the correct columns are selected
selected_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']

# Filter the dataframe to include only the selected columns
iris = iris[selected_columns]

# Set the style of seaborn
sns.set(style=""whitegrid"")

# Create box plots for each numerical feature
for column in ['SepalLengthCm', ...",10,"The generated code correctly addresses all requirements of the task. It properly loads the `iris.csv` dataset, iterates through each of the four specified numerical features, and generates a box plot for each. Each box plot correctly groups the data by the `Species` column, showing separate distributions for each species. The plots also have appropriate titles indicating the feature being plotted. The use of `pandas`, `seaborn`, and `matplotlib.pyplot` is correct, and the code is well-structured and easy to understand. The `FileNotFoundError` handling is not present in the generated code, but the prompt specified that the CSV file is assumed to be in the current directory and did not explicitly require error handling. Therefore, its absence does not detract from the score given the prompt's constraints. The `selected_columns` filtering is redundant as the task implies working with these columns, but it does no harm. Overall, it's a strong solution.",mistral-small-24b-instruct,LLM_judge_config
M28,medium,Classification,heart.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('heart.csv')

# Select the features and target variable
features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Sex', 'ChestPainType']
target = 'Heart...",9,"The generated code is almost identical to the ground truth and correctly implements all task requirements. It loads the data, selects the specified features and target, performs one-hot encoding on `Sex` and `ChestPainType` using `ColumnTransformer` within a `Pipeline`, splits the data correctly, trains a `LogisticRegression` model, and reports the accuracy.

Minor points for improvement:
1.  The `OneHotEncoder` in the generated code doesn't include `handle_unknown='ignore'`, which is a good practice for cases where the test set might contain categories not seen in the training set.
2.  The `LogisticRegression` model in the ground truth includes `max_iter=1000` which is often necessary for convergence with datasets of this nature, though it might not always be strictly required. The generated code uses default parameters, which might lead to a `ConvergenceWarning` for some datasets if `max_iter` is too low.
3.  The print statement for accuracy formats to two decimal places (`.2f`), while the ground truth uses four (`.4f`). This is a minor stylistic difference and doesn't impact correctness.

Overall, it's a very strong submission, demonstrating a clear understanding of the data science workflow for this problem.",mistral-small-24b-instruct,LLM_judge_config
M29,medium,Hypothesis Testing,heart.csv,"```python
import pandas as pd
from scipy import stats

# 1) Load the dataset
data = pd.read_csv('heart.csv')

# 2) Create two groups based on the `Sex` column: one for males ('M') and one for females ('F')
males = data[data['Sex'] == 'M']['Cholesterol'].dropna()
females = data[data['Sex'] == 'F']['Cholesterol'].dropna()

# 3) Perform an independent two-sample t-test
t_statistic, p_value = stats.ttest_ind(males, females, equal_var=False)

# 4) Null and alternative hypotheses:
#  Null hypothesis (...",9,"The generated code correctly performs all the required steps: loading the data, creating groups, performing the t-test (using `equal_var=False` which is good practice), printing the results, and interpreting them based on the significance level. The hypotheses are correctly stated as comments.

However, the code lacks robustness (e.g., error handling for `FileNotFoundError`) and does not encapsulate the logic within a function, which makes it less reusable and harder to test compared to the ground truth. The ground truth also provides more detailed print statements for clarity and includes checks for sufficient data in groups. These are good practices for production-ready code. Despite these minor omissions, the core task requirements are met accurately.",mistral-small-24b-instruct,LLM_judge_config
M3,medium,Data Preprocessing,winequality-red.csv,"```python
import pandas as pd
import numpy as np

# Load the dataset
data = pd.read_csv('winequality-red.csv')

# Function to identify and cap outliers using IQR method
def cap_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    num_outliers = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()
    df[column] = np.where(df[column] < lower_bound, lower_bound,...",5,"The generated code attempts to address all parts of the task but has a critical flaw in its approach to comparing ""before"" and ""after"" statistics.

1.  **Correctness Issue (Major Flaw):** The `cap_outliers` function modifies the `data` DataFrame in place. This means that when the code calculates `before_volatile_acidity_mean`, etc., the outliers have *already* been capped within the `data` DataFrame. Consequently, `before_volatile_acidity_mean` and `after_volatile_acidity_mean` will be identical (or very nearly identical due to floating point variations), making the ""comparison"" meaningless. The task explicitly asks to ""Compare the mean and standard deviation of these two columns before and after outlier treatment,"" which requires keeping the original data separate from the treated data for comparison.

2.  **Completeness:**
    *   It identifies outliers and reports their count.
    *   It performs winsorization (capping).
    *   It attempts to compare statistics, but the method is flawed as described above.
    *   It doesn't explicitly create a ""new dataset version"" where outliers are treated; it modifies the original `data` variable. While this technically *is* a new version in memory, the lack of an explicit `df_winsorized = df.copy()` step makes the ""before/after"" comparison impossible if done sequentially on the same DataFrame.

3.  **Code Quality:**
    *   Uses functions, which is good for modularity.
    *   Proper imports.
    *   Lack of comments or print statements to indicate progress or clarity of operations (e.g., when the data is effectively ""after"" treatment). This is less critical but affects readability compared to the ground truth.
    *   The `cap_outliers` function modifies the DataFrame passed to it, which is a side effect that can lead to bugs, as seen here. It would be better if it returned a modified copy or processed a copy internally.

To improve, the code needs to make a `copy()` of the original DataFrame *before* applying the outlier capping, and then perform the ""after"" calculations on this copied (winsorized) DataFrame. The ""before"" calculations should be done on the *unmodified* original DataFrame.",mistral-small-24b-instruct,LLM_judge_config
M30,medium,Data Visualization,heart.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('heart.csv')

# Group the data
grouped_data = data.groupby(['ChestPainType', 'HeartDisease']).size().unstack(fill_value=0)

# Create the grouped bar chart
grouped_data.plot(kind='bar', stacked=False, figsize=(10, 6))

# Add labels and title
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.title('Heart Disease Count by Chest Pain Type')
plt.legend(['HeartDisease = 0', 'H...",7,"The generated code correctly loads the dataset and creates a grouped bar chart as requested. It includes proper imports, a title, and labeled axes. However, there are a few areas for improvement:

1.  **Legend Clarity:** The legend `['HeartDisease = 0', 'HeartDisease = 1']` is manually created and relies on the order of columns after `unstack()`. While it works, it's less robust than mapping the `HeartDisease` values to more descriptive labels (e.g., 'No Heart Disease', 'Heart Disease') as done in the ground truth, especially for a broader audience.
2.  **Robustness (Error Handling):** The code does not include any error handling for `FileNotFoundError` or missing columns, which is a good practice for general-purpose scripts.
3.  **ChestPainType Ordering:** The `ChestPainType` categories might not be ordered as 'TA, ATA, NAP, ASY' if the CSV has them in a different order or if not all types are present. The ground truth explicitly orders these categories.
4.  **Plotting Method:** While `grouped_data.plot(kind='bar', ...)` works, using Seaborn's `countplot` with the `hue` parameter is often more direct and flexible for this type of visualization (counting categories within groups) and less verbose when it comes to setting up `groupby` and `unstack` manually.
5.  **Readability/Best Practices:** Encapsulating the logic within a function (as in the ground truth) with a clear `if __name__ == '__main__':` block is a better practice for reusability and organization.",mistral-small-24b-instruct,LLM_judge_config
M31,medium,Data Cleaning,spotify_2023.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Clean the `in_shazam_charts` column
df['in_shazam_charts'] = df['in_shazam_charts'].str.replace(',', '').astype(float)
nan_count = df['in_shazam_charts'].isna().sum()

# Print the count of NaNs
print(f""Count of NaNs in 'in_shazam_charts': {nan_count}"")

# Group by `key` and `mode`
grouped = df.groupby(['key', 'mode'])

# Calculate and display the average `danceability_%` and `valence_%`
result = grouped[['d...",8,"The generated code correctly loads the dataset, cleans the `in_shazam_charts` column by removing commas and converting it to numeric type, and correctly reports the NaN count. It also groups the data by `key` and `mode` and calculates the average `danceability_%` and `valence_%`.

Strong points:
- Proper use of pandas for data loading and manipulation.
- Correctly handles the cleaning and type conversion of `in_shazam_charts`, and reports NaNs.
- Efficiently groups and calculates the required averages.

Areas for minor improvement:
- The `astype(float)` in cleaning `in_shazam_charts` implicitly handles errors by coercing to NaN if the conversion fails. While this works, explicitly using `pd.to_numeric(..., errors='coerce')` is often clearer and more robust, especially if there were intermediate non-string values.
- The `try-except FileNotFoundError` block for `pd.read_csv`, while not strictly required by the prompt, is good practice for robust code, as seen in the ground truth.
- The final `print(result)` displays the grouped data correctly, but the ground truth adds more descriptive print statements, which improves readability for the user. `reset_index()` is not strictly necessary as the `.mean()` result by default has `key` and `mode` as index, but it makes the output look like a flat table, which is fine.",mistral-small-24b-instruct,LLM_judge_config
M32,medium,Regression,spotify_2023.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Load the dataset
df = pd.read_csv('spotify_2023.csv')

# Ensure the 'streams' column is numeric and handle errors by coercing to NaN, then drop rows with NaN
df['streams'] = pd.to_numeric(df['streams'], errors='coerce')
df = df.dropna(subset=['streams', 'in_spotify_playlists', 'energy_%'])

# Select featur...",9,"The generated code is almost perfect and correctly addresses all requirements of the task.

**Strong points:**
*   **Correctness:** The code correctly loads the dataset, handles the `streams` column conversion to numeric, selects the specified features and target, performs the train-test split with the correct `random_state` and `test_size`, trains a Linear Regression model, and evaluates it reporting R2 and MSE.
*   **Completeness:** All steps outlined in the task description are implemented.
*   **Code Quality:** The code is clean, readable, and uses proper imports.

**Minor Area for Improvement (leading to a score of 9 instead of 10):**
*   **Robustness in File Loading:** The generated code lacks a `try-except` block for `pd.read_csv`, which is present in the ground truth. This means if `spotify_2023.csv` is not found, the script will crash with a `FileNotFoundError` instead of gracefully handling it and providing a user-friendly message. While the prompt stated ""assume the CSV file is in the current directory,"" robust production-ready code typically includes such error handling. For a benchmark evaluation, this is a minor deviation unless explicitly asked for error handling.

Overall, it's a very strong performance, demonstrating a good understanding of data preprocessing and basic machine learning workflow.",mistral-small-24b-instruct,LLM_judge_config
M33,medium,Feature Engineering,spotify_2023.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('spotify_2023.csv')

# 2. Combine `released_year`, `released_month`, and `released_day` into a single `release_date` column
df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']].assign(day=lambda x: x.where(x['released_day'] > 0, 1)), errors='coerce').dtype('datetime64[ns]')

# 3. Create a new feature named `days_since_release`
fixed_date = pd.to_datetime('2023-12-31')
df['days_since_relea...",6,"The generated code attempts to solve the problem and gets most of the way there, but has a critical error in the date conversion and some minor issues with data types and robustness.

**Strengths:**
*   Correctly loads the CSV file.
*   Correctly identifies the need for `pd.to_datetime` for `release_date`.
*   Correctly uses `errors='coerce'` for handling invalid dates.
*   Correctly calculates `days_since_release` using a fixed date.
*   Correctly uses `.dt.days` to get the number of days.
*   Correctly displays the requested columns for the first 5 rows.

**Weaknesses and Areas for Improvement:**
*   **Critical Error in Date Conversion:** The line `df['release_date'] = pd.to_datetime(df[['released_year', 'released_month', 'released_day']].assign(day=lambda x: x.where(x['released_day'] > 0, 1)), errors='coerce').dtype('datetime64[ns]')` is incorrect and will likely raise an error or produce incorrect results.
    *   `pd.to_datetime` expects either a single string, a datetime object, or components (`year`, `month`, `day`) as individual arguments, or a DataFrame with named `year`, `month`, `day` columns. Creating a sub-DataFrame `df[['released_year', 'released_month', 'released_day']]` works when the columns are correctly named `year`, `month`, `day` respectively, but `df[['released_year', 'released_month', 'released_day']]` does not automatically rename these columns to 'year', 'month', 'day' for `to_datetime`.
    *   The `assign(day=lambda x: x.where(x['released_day'] > 0, 1))` part is confusing and unnecessary, especially if `released_day` is already the day column. It seems like an attempt to handle zero days, but `errors='coerce'` is the more robust way for general invalid date components.
    *   `.dtype('datetime64[ns]')` is applying the `.dtype` attribute directly to the result of `pd.to_datetime`, which is a Series. This will lead to an error as `Series.dtype` returns the Series' data type, not a method to cast it. The cast should be `Series.astype('datetime64[ns]')` if it's even needed after `to_datetime` (which usually already returns datetime64[ns]).
    *   The correct approach for combining year, month, day columns is typically to construct a string 'YYYY-MM-DD' first, or to pass a dictionary/DataFrame with columns explicitly named 'year', 'month', 'day'. The ground truth demonstrates the string concatenation method which is very common and robust.
*   **`days_since_release` Data Type:** The `astype(int)` for `days_since_release` will convert `NaN` values (which result from `NaT` in `release_date`) to 0, which is incorrect. The requirement specified that `days_since_release` should be an integer, but implicitly allows for missing values (since `release_date` can be `NaT`). `astype('Int64')` (pandas nullable integer) is the correct way to handle this, preserving `NaN` for invalid dates while keeping the column type as integer.
*   **Lack of Robustness (Missing File Handling):** The code assumes the file exists and doesn't include any `try-except` blocks for `FileNotFoundError`, which was demonstrated in the ground truth as good practice. While not explicitly asked for in the prompt, it significantly improves code quality and is a common expectation in data science tasks.",mistral-small-24b-instruct,LLM_judge_config
M34,medium,Data Cleaning & Aggregation,youtube_trendings.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# Clean 'Video views' and 'Likes' columns
df['Video views'] = pd.to_numeric(df['Video views'].str.replace(',', ''), errors='coerce').fillna(0)
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)

# Fill missing values in 'Category' column
df['Category'] = df['Category'].fillna('Unknown')

# Group by 'Category' and calculate sums
grouped = df.groupby('Category').agg({
...",9,"The generated code correctly implements all the requirements of the task. It properly loads the CSV, cleans ""Video views"" and ""Likes"" columns by removing commas, converting to numeric, and filling NaNs with 0. It also correctly fills missing ""Category"" values with 'Unknown'. Finally, it groups the data by ""Category"" and calculates the sum of cleaned views and likes per category.

**Strong points:**
*   **Concise and efficient:** The code for cleaning ""Video views"" and ""Likes"" columns is very compact and uses method chaining effectively (`.str.replace(',', '').fillna(0)`).
*   **Correct data type handling:** `pd.to_numeric` with `errors='coerce'` correctly handles non-numeric values by converting them to `NaN`, which are then filled with 0.
*   **Correct aggregation:** Using `.agg` for grouping is a clean way to apply sum functions to multiple columns.
*   **Completeness:** Addresses all specified requirements.

**Areas for minor improvement (leading to a score of 9 instead of 10):**
*   **Error Handling:** The ground truth includes `try-except` blocks for `FileNotFoundError` and general exceptions, as well as checks for column existence. While the prompt didn't explicitly ask for robust error handling, it's good practice in production-level code. The generated code assumes the file exists and the columns are present. In a real-world scenario, this could lead to runtime errors if the dataset is missing or column names are misspelled.
*   **Output Format:** The prompt asked to ""display the sum of cleaned Video views and the sum of cleaned Likes."" The ground truth explicitly prints ""Sum of Video Views and Likes per Category:"" before the table, which is a slightly more user-friendly output compared to just printing the DataFrame.

Overall, the generated code is excellent and solves the core problem very well.",mistral-small-24b-instruct,LLM_judge_config
M35,medium,Hypothesis Testing,youtube_trendings.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
data = pd.read_csv('youtube_trendings.csv')

# Clean the `Likes` column
data['Likes'] = data['Likes'].str.replace(',', '', regex=False).apply(pd.to_numeric, errors='coerce').fillna(0)

# Fill missing values in the `Category` column
data['Category'] = data['Category'].fillna('Unknown')

# Create groups
music_likes = data[data['Category'] == 'Music']['Likes']
sports_likes = data[data['Category'] == 'Sports']['Likes...",9,"The generated code is almost entirely correct and fulfills nearly all requirements of the task.

**Strong Points:**
*   **Correctness:**
    *   Successfully loads the dataset.
    *   Correctly cleans the `Likes` column by removing commas, converting to numeric (handling errors), and filling NaNs with 0 in a concise chain.
    *   Correctly fills missing `Category` values with 'Unknown'.
    *   Correctly creates the 'Music' and 'Sports' `Likes` groups.
    *   Performs the independent two-sample t-test using `ttest_ind`.
    *   Correctly states the null and alternative hypotheses.
    *   Prints the t-statistic and p-value.
    *   Correctly interprets the p-value against the alpha of 0.05.
*   **Code Quality:**
    *   Uses appropriate imports (`pandas`, `scipy.stats.ttest_ind`).
    *   Code is clean and readable.
    *   The cleaning of the `Likes` column is done very concisely in a single line, which is efficient.
*   **Completeness:** Addresses most of the requirements.

**Areas for Improvement (minor nuances):**
*   **`equal_var` parameter in `ttest_ind`:** The ground truth explicitly sets `equal_var=False` (Welch's t-test), which is generally recommended when population variances are not assumed to be equal, and is often a safer default. The generated code implicitly uses `equal_var=True` (Student's t-test) by defaulting to it. While not strictly ""wrong"" without explicit instruction, best practice in statistical testing often leans towards Welch's test if there's no strong reason to assume equal variances.
*   **`nan_policy='omit'`:** This is redundant when the `Likes` column has already been filled with 0s. The groups `music_likes` and `sports_likes` will not contain NaNs, so `nan_policy` will have no effect. While not harmful, it's unnecessary in this specific context.
*   **Verbosity in Output:** The ground truth code provides more detailed print statements about the steps being performed and the final interpretation, which can be useful for debugging and understanding the flow. The generated code is a bit more terse.
*   **Error Handling for File Not Found:** The ground truth includes a `try-except` block for file loading, which is robust. The generated code assumes the file exists. While the prompt assumes the CSV is in the current directory, robust code would include this.

Overall, the generated code is highly functional and correct for the primary statistical task. The noted improvements are minor and relate to best practices and robustness rather than fundamental errors in the statistical analysis.",mistral-small-24b-instruct,LLM_judge_config
M36,medium,Feature Engineering,youtube_trendings.csv,"```python
import pandas as pd

# 1) Load the dataset
df = pd.read_csv('youtube_trendings.csv')

# 2) Clean Likes and Dislikes columns
df['Likes'] = pd.to_numeric(df['Likes'].str.replace(',', ''), errors='coerce').fillna(0)
df['Dislikes'] = pd.to_numeric(df['Dislikes'].str.replace(',', ''), errors='coerce').fillna(0)

# 3) Create a new column TotalEngagement
df['TotalEngagement'] = df['Likes'] + df['Dislikes']

# 4) Create a new column LikeDislikeRatio
df['LikeDislikeRatio'] = df['Likes'] / (df['...",10,"The generated code is excellent. It correctly loads the dataset, cleans the 'Likes' and 'Dislikes' columns by removing commas, converting to numeric, and filling NaNs with 0 in a concise manner. It then accurately creates the `TotalEngagement` and `LikeDislikeRatio` columns according to the specified formulas. Finally, it displays the required columns. The imports are proper, and the code is well-structured and complete, addressing all requirements of the task. The implicit assumption that `df['Likes']` and `df['Dislikes']` might contain non-string values initially by handling `.str.replace` without `astype(str)` could theoretically cause an error if a column contained purely numeric types from the start, but for typical CSV loading where these columns are often treated as objects due to commas, it robustly handles the conversion. However, the ground truth explicitly uses `astype(str)` which is more robust. Given the context of the problem description, the generated solution is perfectly acceptable.",mistral-small-24b-instruct,LLM_judge_config
M37,medium,Data Aggregation,mobiles_2025.csv,"```python
import pandas as pd

# Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# Clean the 'Launched Price (USA)' column
df['Launched Price (USA)'] = df['Launched Price (USA)""].str.replace('USD ', '').str.replace(',', '').apply(
    lambda x: pd.to_numeric(x, errors='coerce'))

# Clean the 'Screen Size' column
df['Screen Size'] = df['Screen Size""].str.replace(' inches', '').apply(
    lambda x: pd.to_numeric(x, errors='coerce'))

# Group the data by 'Company Name' and 'Launched Year'
gr...",6,"The generated code correctly loads the CSV, groups the data, and displays the aggregated results. However, there are critical issues in the data cleaning steps that lead to incorrect output for the `Launched Price (USA)` and `Screen Size` columns.

**Issues:**

1.  **Typo in column access:** The code has a typo in accessing both columns (`df['Launched Price (USA)""` and `df['Screen Size""`), using `""` instead of `'` at the end, leading to `KeyError` if executed as is. This is a fatal error.
2.  **Incorrect cleaning for `Launched Price (USA)`:**
    *   The `str.replace` methods are chained directly, which is good.
    *   However, the `apply(lambda x: pd.to_numeric(x, errors='coerce'))` is applied to the *entire column*, not element-wise in a way that respects the previous string operations when chained this way. `pd.to_numeric` can be applied directly to a Series after string cleaning. The current approach with `apply` on the Series will likely fail to convert correctly or introduce NaNs where not intended.
3.  **Incorrect cleaning for `Screen Size`:** Similar to the price column, `apply(lambda x: pd.to_numeric(x, errors='coerce'))` is misapplied. `pd.to_numeric` should be applied directly to the series after the `str.replace`.
4.  **No new cleaned columns:** The cleaning attempts overwrite the original columns (`df['Launched Price (USA)']` and `df['Screen Size']`). While not strictly against the prompt (which just said ""clean the column""), it's better practice to create new columns for cleaned data, especially if the original raw data might be needed later or if the cleaning process is multi-stepped. The ground truth, for example, creates `Cleaned Price (USD)` and `Cleaned Screen Size (inches)`. Using the original column names for the mean calculation after overwriting them with potentially bad conversions might lead to confusion.

**Areas for Improvement:**

*   **Syntax Correction:** Fix the typographical errors in column access.
*   **Direct `pd.to_numeric` application:** Apply `pd.to_numeric` directly to the Series after the string cleaning operations for `Launched Price (USA)` and `Screen Size`. This is the standard and most efficient way to achieve the conversion.
*   **Robustness:** While not explicitly asked for, adding error handling for file loading (like the ground truth) would make the code more robust.
*   **Clarity:** Creating new columns for cleaned data improves readability and maintains the original data if needed.",mistral-small-24b-instruct,LLM_judge_config
M38,medium,Feature Engineering,mobiles_2025.csv,"```python
import pandas as pd
import re

# 1. Load the dataset
df = pd.read_csv('mobiles_2025.csv')

# 2. Define a function to clean camera columns
def extract_mp(value):
    if isinstance(value, str):
        match = re.search(r'\d+', value)
        if match:
            return int(match.group())
    return 0

# Apply the function to create cleaned `Cleaned Front Camera` and `Cleaned Back Camera` numeric columns
df['Cleaned Front Camera'] = df['Front Camera'].apply(extract_mp)
df['Cleaned Back ...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the dataset, defines and applies a function to extract MP values from camera specifications (handling non-string values by returning 0), creates the `Total Camera MP` feature, and displays the specified columns for the first 10 rows. The imports are correct and the code structure is logical.

One minor area for improvement, though not critical for correctness in this specific task, is the handling of `NaN` or non-string values in the `extract_mp` function. While `isinstance(value, str)` covers many cases, `pd.isna()` is often more robust for DataFrame columns that might contain pandas' NaN type. However, for the given problem description, the current implementation is practically effective. The ground truth's inclusion of `try-except` for `FileNotFoundError` is also a good practice for robustness, but the task description didn't explicitly ask for error handling beyond assuming the CSV is present. Overall, it's a very good solution.",mistral-small-24b-instruct,LLM_judge_config
M39,medium,Regression,mobiles_2025.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load the dataset
data = pd.read_csv('mobiles_2025.csv')

# 2. Clean the data
data['RAM'] = pd.to_numeric(data['RAM'].replace('GB', '', regex=True), errors='coerce')
data['Battery Capacity'] = pd.to_numeric(data['Battery Capacity'].replace('mAh', '', regex=True), errors='coerce')
data['Screen Size'] = p...",9,"The generated code correctly addresses all requirements of the task: loading the dataset, cleaning specified columns, handling `NaN` values, splitting the data, training a linear regression model, and reporting R2 and MAE. The approach to cleaning is concise and effective using `replace` with `regex=True` and `pd.to_numeric` with `errors='coerce'`, followed by `dropna`. The imports are correct, and the code structure is logical and easy to follow.

One minor suggestion for improvement would be to add a `try-except` block for `pd.read_csv` to handle potential `FileNotFoundError`, as was done in the ground truth. This adds robustness to the script. Also, formatting the output scores to a specific number of decimal places (e.g., `:.4f` for R2 and `:.2f` for MAE) would align better with typical reporting standards and the ground truth's output format. However, these are minor quality-of-life improvements and do not affect the core functionality or correctness of the solution.",mistral-small-24b-instruct,LLM_judge_config
M4,medium,Hypothesis Testing,titanic.csv,"```python
import pandas as pd
from scipy.stats import chi2_contingency, ttest_ind, f_oneway
import numpy as np

# Load the dataset
data = pd.read_csv('titanic.csv')

# 1. Chi-square test for association between Pclass and Survival
contingency_table = pd.crosstab(data['Survived'], data['Pclass'])
chi2, p_value, dof, expected = chi2_contingency(contingency_table)

# Effect size for chi-square test (Cramer's V)
def cramers_v(confusion_matrix):
    chi2 = chi2
    n = confusion_matrix.sum()
    phi2...",6,"The generated code correctly implements the three statistical tests (chi-square, t-test, ANOVA) and calculates effect sizes for each. It also displays the results clearly with p-values and interprets the conclusions based on the alpha level.

However, there are several areas for improvement:

1.  **Cramer's V Calculation (Incorrect):** The `cramers_v` function definition is incorrect. It tries to use `chi2` (which is a global variable from the `chi2_contingency` call) but also passes `confusion_matrix` to itself, making it confusing. More importantly, the formula `phi2corr / min( (kcorr-1), (rcorr-1))` is a variant, but the previous `chi2 / n` and then `sqrt` is the standard formula, and the one used in the ground truth is often preferred for simplicity. The `min_dim` part in the ground truth is `min(rows-1, cols-1)`, while the generated code uses `(kcorr-1), (rcorr-1)`. This often results in a wrong Cramer's V value. Test with known values would show this.

2.  **Eta-squared Calculation (Incorrect):** The `eta_squared` function is fundamentally flawed. `N` is `sum(fares.shape)`, which will be `2` for a 1D array. It should be the total number of observations. The formula `f / (f + N - k)` is for partial eta-squared or a different context, not the standard eta-squared (SS_between / SS_total) calculated in ANOVA. The generated code computes `f_stat` and `N` (incorrectly) and `k` (correctly) for a formula that does not yield the correct eta-squared for basic one-way ANOVA.

3.  **Missing Error Handling/Robustness:**
    *   No handling for `titanic.csv` not found.
    *   No check for empty groups before performing t-test or ANOVA, which can lead to errors if data is sparse or missing (`dropna()` helps but doesn't prevent empty groups).
    *   The t-test does not incorporate Levene's test for equality of variances, assuming equal variances (`equal_var=True` by default in `ttest_ind`), which might not always be appropriate.

4.  **Clarity and Structure:** The initial loading of data and then diving straight into calculations could be better structured with clearer print statements separating each test, as in the ground truth. Also, the `cramers_v` function shadows the variable, which is a minor issue but can lead to confusion.

5.  **Interpretation Detail:** The conclusions are very brief (""Significant association"" vs. ""No significant association""). While correct, more descriptive conclusions are preferred, especially mentioning which hypothesis is rejected/failed to be rejected.

The code is a good start, but the incorrect effect size calculations and lack of robustness reduce its quality and correctness for a complete solution.",mistral-small-24b-instruct,LLM_judge_config
M40,medium,Time Series EDA,imdb_top_250.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# Group by Year and calculate average Rating and Runtime
grouped = data.groupby('Year').agg({
    'Rating': 'mean',
    'Runtime': 'mean'
}).reset_index()

# Create a line plot for average Rating over the years
plt.figure(figsize=(10, 6))
sns.lineplot(data=grouped, x='Year', y='Rating', marker='o')

# Add title and labels
plt.title('Trend of Average Rati...",7,"The generated code correctly loads the dataset, groups by year to calculate average Rating and Runtime, and generates a line plot for average Rating over the years with a title and labeled axes. All core requirements are met.

However, it lacks the robustness of the ground truth code, specifically in data cleaning. The 'Year' and 'Runtime' columns in the dataset might contain non-numeric characters (e.g., ""(2000)"" for Year, ""142 min"" for Runtime), which the generated code does not handle. This could lead to errors if the columns are not purely numeric, or incorrect calculations if the string values are not properly parsed. The ground truth code includes specific pre-processing steps using `re` and `astype`/`to_numeric` with error handling to clean these columns, making it more resilient to real-world data variations.

To achieve a higher score, the generated code would benefit significantly from adding data cleaning steps for 'Year' and 'Runtime' to ensure they are in a suitable numerical format before aggregation.",mistral-small-24b-instruct,LLM_judge_config
M41,medium,Hypothesis Testing,imdb_top_250.csv,"```python
import pandas as pd
from scipy.stats import ttest_ind

# 1) Load the dataset
data = pd.read_csv('imdb_top_250.csv')

# 2) Create two groups of movies based on the `Year` column
movies_before_1990 = data[data['Year'] < 1990]['Rating']
movies_1990_and_after = data[data['Year'] >= 1990]['Rating']

# 3) Perform an independent two-sample t-test
t_statistic, p_value = ttest_ind(movies_before_1990, movies_1990_and_after)

# 4) Null and alternative hypotheses
null_hypothesis = ""The mean Rating...",7,"The generated code correctly performs the core task of loading data, splitting into groups, and running a t-test. It also correctly states hypotheses and interprets the p-value.

However, there are several areas for improvement and a few missing pieces that prevent a higher score:
1.  **Robustness (Data Type Conversion & Error Handling):** The ground truth includes robust handling for potential `FileNotFoundError` and explicit conversion of 'Year' and 'Rating' to numeric types with `errors='coerce'` and subsequent `dropna`. The generated code assumes the CSV file exists and that the 'Year' and 'Rating' columns are already in a clean numeric format. This is a common pitfall in real-world data science.
2.  **Hypotheses Statement:** While the hypotheses are conceptually correct and present as strings, the ground truth explicitly prints them as part of the output, which was a requirement (""Clearly state the null and alternative hypotheses for this test.""). The generated code stores them in variables but doesn't print them.
3.  **Clarity of Interpretation:** The interpretation only states ""Reject the null hypothesis."" or ""Fail to reject the null hypothesis."" The ground truth provides a more complete sentence relating back to the original context of the ratings, which is crucial for a data scientist interpreting the results clearly.
4.  **`equal_var` parameter in `ttest_ind`:** The `ttest_ind` function defaults to `equal_var=True`. While this might be the intended assumption, the ground truth explicitly sets it and includes a comment about when `equal_var=False` might be more appropriate (Welch's t-test), demonstrating a deeper understanding of the statistical test.
5.  **Output Formatting:** The ground truth formats the t-statistic and p-value to a specific number of decimal places, improving readability.

Overall, it's a functional solution that addresses the main statistical task, but lacks the robustness, completeness, and clarity often expected in production-ready or thoroughly explained data science code.",mistral-small-24b-instruct,LLM_judge_config
M42,medium,Feature Engineering,imdb_top_250.csv,"```python
import pandas as pd

# 1. Load the dataset
df = pd.read_csv('imdb_top_250.csv')

# 2. Create a new column named `Decade` by categorizing the `Year` column
df['Decade'] = pd.cut(df['Year'], bins=[1900, 1909, 1919, 1929, 1939, 1949, 1959, 1969, 1979, 1989, 1999, 2009, 2019, 2029, 2039], labels=[
    '1900s', '1910s', '1920s', '1930s', '1940s', '1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s'])

# 3. Display the `Title`, `Year`, and the new `Decade` column for the fi...",7,"The generated code correctly loads the dataset and displays the requested columns. The approach to create the `Decade` column using `pd.cut` is conceptually sound and achieves the desired grouping. However, it's not as robust or flexible as the ground truth's floor division method, which handles any year without needing to pre-define all possible decade bins. For instance, if the dataset contained a movie from 1895, the ground truth's method would still correctly assign it to the '1890s', whereas the generated code's `pd.cut` would throw an error or assign it to an 'NaN' bin if not explicitly handled. Also, the output formatting for the display isn't as clean as the ground truth's `to_string(index=False)`. While the code fulfills the core requirements, its method for decade creation is less generalizable.",mistral-small-24b-instruct,LLM_judge_config
M43,medium,Data Cleaning,penguins.csv,"```python
import pandas as pd

# Load the dataset
penguins = pd.read_csv('penguins.csv')

# Impute missing values for numerical columns
numeric_columns = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']
for col in numeric_columns:
    penguins[col].fillna(penguins[col].mean(), inplace=True)

# Impute missing values for the 'sex' column
penguins['sex'].fillna(penguins['sex'].mode()[0], inplace=True)

# Group by 'species' and 'sex' and calculate the average 'body_mass_g'...",10,"The generated code correctly addresses all requirements of the task. It loads the dataset, performs mean imputation for the specified numerical columns, mode imputation for the 'sex' column, and then correctly groups the data by 'species' and 'sex' to calculate and display the average 'body_mass_g'. The code is concise, well-structured, and uses appropriate pandas functions. It handles the imputation and aggregation steps accurately. A strong point is its directness and efficiency in applying the required transformations.",mistral-small-24b-instruct,LLM_judge_config
M44,medium,Hypothesis Testing,penguins.csv,"```python
import pandas as pd
from scipy import stats

# Load the dataset
data = pd.read_csv('penguins.csv')

# Filter the data to include only records where species is 'Adelie' or 'Gentoo'
filtered_data = data[data['species'].isin(['Adelie', 'Gentoo'])]

# Within this filtered dataset, remove any rows where flipper_length_mm or species is missing (NA)
cleaned_data = filtered_data.dropna(subset=['flipper_length_mm', 'species'])

# Perform an independent two-sample t-test
adelie_flipper_lengths =...",9,"The generated code is largely correct and addresses most of the requirements. It correctly loads the data, filters by species, handles NAs, performs the t-test, and interprets the results.

**Strong Points:**
- Correctly uses `pandas` for data loading and filtering.
- Correctly uses `dropna` on the specified subset.
- Correctly separates data for the two groups for the t-test.
- Correctly uses `scipy.stats.ttest_ind` for the independent t-test.
- Correctly interprets the p-value against `alpha = 0.05`.
- Clearly states the hypotheses and the conclusion.

**Areas for Improvement (Why not a 10):**
- **Robustness (Missing File Handling):** The ground truth includes `try-except` blocks for file loading, which is a good practice for robustness in real-world scenarios. The generated code assumes the file will always be present. While not explicitly asked for in the prompt's core task, it's a mark of more robust code.
- **Explicit Alpha Definition:** The `alpha = 0.05` is hardcoded in the `if` statement for interpretation. Defining it as a variable (e.g., `ALPHA = 0.05`) at the beginning, like in the ground truth, makes the code more readable, maintainable, and easier to modify if the significance level needs to change.
- **Hypotheses Printing:** The hypotheses are stated as comments, not printed to the console as explicitly requested (""Clearly state the null and alternative hypotheses for this test""). The ground truth prints them, which is a better fulfillment of ""state"".
- **`equal_var` parameter:** The `ttest_ind` defaults to `equal_var=True`. While not explicitly stated in the prompt, for robust real-world analysis, `equal_var=False` (Welch's t-test) is often preferred unless equality of variances is known or confirmed. The ground truth code mentions and implicitly uses `equal_var=False`. The generated code uses the default, which might be acceptable but isn't as robust as Welch's. However, given the prompt didn't specify Welch's test, sticking to the default is not strictly incorrect.
- **Output Formatting:** The output for t-statistic and p-value is not formatted to a specific number of decimal places, which can make it less readable for statistical results. The ground truth uses f-strings with formatting, providing cleaner output. For instance, `f""T-Statistic: {t_statistic:.4f}""`.

Overall, a very good solution that successfully completes the core task. The suggested improvements are mainly for code quality, robustness, and full adherence to the exact wording of all sub-requirements.",mistral-small-24b-instruct,LLM_judge_config
M45,medium,Classification,penguins.csv,"```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression

# 1) Load the dataset
data = pd.read_csv('penguins.csv')

# 2) Select the features and target
X = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']]
y = data['species']

# 3) Drop rows with missing values in the target or selected features
data_cleaned = data.dropna(subset=['species', ...",9,"The generated code correctly addresses all requirements of the task. It loads the data, selects the specified features and target, correctly handles missing values by dropping rows where specified columns have NAs, encodes the target variable, splits the data into training and testing sets with the correct ratio and random state, trains a Logistic Regression model with `max_iter=1000`, and reports the accuracy.

Minor improvements could be:
- The `accuracy_score` function from `sklearn.metrics` is a standard way to report accuracy, although `model.score()` achieves the same result for this task.
- The `X` and `y` variables are created before the `dropna` step, but then `X_cleaned` and `y_encoded` are derived from `data_cleaned`. While functionally correct, it's slightly less direct than creating `X` and `y` directly from the cleaned DataFrame, as shown in the ground truth. Specifically, the line `y = data['species']` is overwritten by `y_encoded` which is derived from `data_cleaned['species']`, and `X` is never used after `data_cleaned` is created. This could lead to confusion.
- The print statements are minimal, which is acceptable but the ground truth's verbose print statements could be considered better for debugging and clarity in a real-world scenario.

Overall, it's a very solid and correct solution.",mistral-small-24b-instruct,LLM_judge_config
M46,medium,Exploratory Data Analysis (EDA),penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
df = pd.read_csv('penguins.csv')

# 2) Drop rows where `body_mass_g` or `species` is NA
df_cleaned = df.dropna(subset=['body_mass_g', 'species'])

# 3) For each `species`, calculate the average `body_mass_g`
average_body_mass = df_cleaned.groupby('species')['body_mass_g'].mean()

# 4) Create a bar plot showing the average `body_mass_g` for each `species`
plt.figure(figsize=(10, 6))
sns.barp...",9,"The generated code correctly addresses all requirements of the task. It successfully loads the data, handles NAs in the specified columns, calculates the average body mass per species, and generates a bar plot with a title and labeled axes. The use of `df.dropna(subset=...)` is correct for dropping NAs in specific columns. The `groupby().mean()` operation is accurate for calculating averages. The plot is generated using seaborn and matplotlib correctly.

**Strong points:**
*   Correctly loads the CSV.
*   Correctly drops NA rows specifically for `body_mass_g` and `species`.
*   Correctly calculates the average `body_mass_g` per `species`.
*   Successfully generates a bar plot with appropriate title and axis labels.
*   Uses `sns.barplot` effectively, extracting `index` and `values` from the series which is a common and valid approach.

**Minor areas for improvement (not critical for this task, hence the high score):**
*   The ground truth includes error handling for `FileNotFoundError`, which is good practice for robustness, though not explicitly required by the prompt.
*   The ground truth uses more explicit variable names for configuration and print statements for debugging/verification, which improves code readability and maintainability in larger projects.
*   The ground truth applies `palette` and `xticks(rotation)` in the plot, which can enhance visualization, but are not strictly required by the prompt's output specifications.",mistral-small-24b-instruct,LLM_judge_config
M47,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 1) Load the dataset
penguins = pd.read_csv('penguins.csv')

# 2) Drop rows where culmen_length_mm, culmen_depth_mm, flipper_length_mm, or body_mass_g are NA
penguins = penguins.dropna(subset=['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'])

# 3) Create a new feature culmen_area calculated as culmen_length_mm * culmen_depth_mm
penguins['culmen_area'] = penguins['culmen_length_mm'] * ...",9,"The generated code correctly performs all the required steps: loading the data, dropping NA values from the specified columns, creating the `culmen_area` feature, calculating the Pearson correlation matrix for the specified columns, and visualizing it as an annotated heatmap with the correct title. The use of `pandas`, `seaborn`, and `matplotlib.pyplot` is appropriate.

The code is concise and directly addresses the task requirements. The only minor improvement would be to include error handling for file not found, similar to the ground truth, for robustness in a real-world scenario. However, given the prompt's assumption about the CSV file being in the current directory, this isn't a critical omission for correctness of the core task. The `figsize` used in the generated code (10,8) is also slightly better for readability than the ground truth's (8,6).",mistral-small-24b-instruct,LLM_judge_config
M48,medium,Feature Engineering,penguins.csv,"```python
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Step 1: Load the dataset
data = pd.read_csv('penguins.csv')

# Step 2: Select the features and target variable
features = data[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g', 'island', 'sex']]
target = data['species']

# Step 3: Impute NA values for numerical features with the mean
num_features = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_len...",8,"The generated code addresses most of the task requirements correctly. It loads the data, selects features, performs numerical imputation, handles the 'sex' column as specified, and applies one-hot encoding. The use of `sklearn` imputers and `OneHotEncoder` is good practice.

However, there are a few issues that prevent a perfect score:

1.  **Index Mismatch after One-Hot Encoding**: When `pd.concat` is used with `features` and `island_encoded_df`, if `features` has a non-default or interrupted index (e.g., due to dropped rows for NAs, although not explicitly done here, it's a common pitfall), the concatenation can lead to NaN values or incorrect alignment. It's safer to ensure indices are aligned or reset when combining dataframes that underwent transformations that might alter their index. In this specific case, `features[num_features] = num_imputer.fit_transform(features[num_features])` might also cause `SettingWithCopyWarning` if `features` is a slice, potentially leading to issues. Pandas operations like `df[cols] = ...` can sometimes result in this. Using `.loc` or `.copy()` more strategically would avoid this. A more robust way to handle the imputation and then one-hot encoding within the original `features` DataFrame would be to process step-by-step without reassigning entire slices in a way that might create fragmented DataFrames. A common robust pipeline would maintain one DataFrame and modify columns in place or create new ones which prevent index alignment issues.

2.  **`SettingWithCopyWarning` potential**: The lines `features[num_features] = num_imputer.fit_transform(features[num_features])` and `features['sex'] = sex_imputer.fit_transform(features[['sex']])` are prone to `SettingWithCopyWarning` if `features` was created as a view rather than a copy from `data`. While it might work in this exact scenario, it's considered bad practice and can lead to unexpected behavior. Using `.loc` or explicitly using `.copy()` when slicing would be safer.

3.  **Redundant Imports**: `OneHotEncoder` is imported, but `sparse=False` creates a dense NumPy array, which is then converted to a DataFrame. `pd.get_dummies` is often simpler and more direct for one-hot encoding columns in a DataFrame, and was used in the ground truth. While using `OneHotEncoder` is not wrong, `pd.get_dummies` is more idiomatic for this specific use case in pandas.

4.  **No `target` head/shape printed**: The task asked to print the shape and head of the *feature matrix*. The code correctly does this for `features`. It separates `target` but doesn't show its shape or head, which wasn't explicitly asked for the target but could be a good addition for completeness in a real scenario. However, this is a minor point.

The core logic for imputation and encoding is correct. The `sex` mapping is handled well. The overall code structure is clean.",mistral-small-24b-instruct,LLM_judge_config
M49,medium,Customer Segmentation,,"```python
import pandas as pd

# Create the DataFrame
data = {'OrderID': [101, 102, 103, 104, 105, 106, 107],
        'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Mouse', 'Laptop'],
        'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics'],
        'Quantity': [1, 2, 1, 1, 3, 1, 1],
        'Price_USD': [1200, 25, 75, 300, 50, 25, 1150],
        'CustomerID': ['CUST001', 'CUST002', 'CUST001', 'CUST003', 'CUST...",9,"The generated code correctly performs all the required tasks: it creates the DataFrame, calculates total spending per customer, identifies and prints the top 3 customers by spending, and calculates the total quantity sold per category. The imports are correct, and the code structure is logical.

A minor point for improvement, although not critical to correctness, is that the 'Total_Spending' column added to the original `df` is an item-level calculation (`Quantity * Price_USD`), not the *customer's* total spending. While this column is correctly used to calculate the *customer's* total spending later via `groupby().sum()`, it could potentially be misleadingly named. The ground truth, which calculates `LineTotal` for each row, handles this more precisely before summing.

Another minor point is that the `reset_index()` calls in the `groupby` operations are often not strictly necessary for the subsequent steps unless you explicitly need the grouped result as a DataFrame with default integer index for other operations. Here, `Series` objects would also work.

Finally, the ground truth code includes helpful print statements explaining each step, which improved clarity and debugging for a user. The generated code is more concise, but less verbose in its output.",mistral-small-24b-instruct,LLM_judge_config
M5,medium,Classification,titanic.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, classification_report
from sklearn.impute import SimpleImputer

# Load the dataset
data ...",7,"The generated code largely addresses the requirements of the task, demonstrating a good understanding of the steps involved in building and evaluating a logistic regression model.

**Strong points:**
*   Correctly loads data and handles basic missing values (`Age`, `Embarked`, `Fare`).
*   Performs categorical encoding (`Sex`, `Embarked`) and feature scaling.
*   Splits data into training and testing sets with a correct ratio and stratification.
*   Trains a Logistic Regression model.
*   Evaluates the model using the requested metrics (accuracy, precision, recall, F1-score, ROC-AUC) and prints them out.
*   Calculates and displays feature coefficients.
*   Includes visualizations for Confusion Matrix and ROC Curve.
*   Performs cross-validation for model stability.

**Areas for improvement to achieve a higher score:**

1.  **Preprocessing Pipeline (Major Improvement):** The generated code performs preprocessing steps (`LabelEncoder`, `StandardScaler`) *before* the train-test split, and outside of a `Pipeline`. This is a common mistake that leads to data leakage. The scaler is fitted on the entire `features` dataset, meaning information from the test set is used during the scaling process, which can artificially inflate performance metrics. The ground truth correctly uses `ColumnTransformer` and `Pipeline` to ensure preprocessing is applied only on the training data and then consistently on the test data, preventing leakage.
2.  **Categorical Variable Encoding:** Using `LabelEncoder` for `Pclass` (which is often treated as categorical) and `Embarked` could imply an ordinal relationship where none exists. `OneHotEncoder` (as used in the ground truth) is generally preferred for nominal categorical variables to avoid this false ordinality. The generated code also implicitly drops `Pclass` from encoding by not including it in `categorical_cols`, but `Pclass` is indeed a feature that needs preprocessing.
3.  **Feature Interpretation:** While coefficients are displayed, the interpretation provided in the ground truth code is missing. This is a subtle but important part of understanding the model.
4.  **Confusion Matrix Display:** The `matplotlib.pyplot.imshow` approach for the confusion matrix is less intuitive and readable than `ConfusionMatrixDisplay.from_estimator` used in the ground truth, which provides labels and a clearer visualization directly from scikit-learn.
5.  **Cross-Validation Metrics:** The cross-validation in the generated code only calculates accuracy. The requirement was to evaluate the model using *all* specified metrics (accuracy, precision, recall, F1-score, ROC-AUC) in cross-validation as well, which the ground truth accomplishes using `cross_validate` with multiple scoring metrics.
6.  **Irrelevant Column Dropping:** The ground truth explicitly handles 'PassengerId', 'Name', 'Ticket', and 'Cabin' (due to too many missing values or irrelevance). The generated code also drops 'Cabin', 'Name', 'Ticket', but the strategy for 'Fare' missing values is less common (median imputation, while not necessarily wrong, `Fare` rarely has missing values on Titanic).
7.  **Feature Engineering:** The ground truth includes a simple but effective feature engineering step (`FamilySize`). While not strictly required, it's a good practice and often improves model performance for this dataset. Its absence is a minor point.
8.  **Output Clarity:** The ground truth code provides more descriptive print statements and clear headings for each section, improving readability.",mistral-small-24b-instruct,LLM_judge_config
M50,medium,Time Series Analysis,,"```python
import pandas as pd

# Create the DataFrame
data = {'Date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05', '2024-01-08', '2024-01-09', '2024-01-10']),
        'Ticker': ['TECHCO']*8,
        'Open': [150.0, 152.5, 151.0, 153.0, 155.5, 154.0, 156.0, 155.0],
        'High': [153.0, 153.5, 154.0, 156.0, 157.0, 156.5, 157.5, 156.5],
        'Low': [149.5, 150.5, 150.0, 152.5, 154.5, 153.0, 155.0, 154.0],
        'Close': [152.0, 151.0, 153.5, 155.0, 1...",9,"The generated code is very good and correctly solves almost all aspects of the task.

**Strong Points:**
- **Correct DataFrame Creation and Indexing:** The DataFrame is created correctly, and 'Date' is set as the index using `set_index('Date', inplace=True)`, as required.
- **Correct SMA Calculation:** The 3-day SMA of the 'Close' price (`df['Close'].rolling(window=3).mean()`) is calculated perfectly, and it correctly handles the initial `NaN` values.
- **Correct Volume Percentage Change Calculation (mostly):** The `pct_change()` method is correctly used to calculate the percentage change in 'Volume', and it properly introduces a `NaN` for the first day.
- **Proper Imports:** Only `pandas` is imported, which is correct and sufficient.
- **Clear and Concise:** The code is well-structured and easy to understand.

**Area for Minor Improvement:**
- **Percentage Format for Volume_Pct_Change:** The task asked for ""percentage change"", and typically in finance, this is represented as a decimal (e.g., 0.05 for 5%). The generated code multiplies by 100 (`df['Volume'].pct_change() * 100`), which explicitly converts it to a whole number percentage (e.g., 5.0 for 5%). While this isn't strictly incorrect, `pct_change()` itself outputs a decimal, which is often preferred for further calculations. The ground truth code assumes the decimal representation is sufficient. If the intent was a displayed percentage (e.g., ""5%""), then formatting would typically be applied during printing, not during calculation. This is a very minor point and depends on specific interpretation, but it deviates slightly from the common output of `pct_change()`.

Overall, the code is highly effective and meets most of the requirements comprehensively.",mistral-small-24b-instruct,LLM_judge_config
M51,medium,Data Transformation & Analysis,,"```python
import pandas as pd

# Create DataFrame
data = {'StudentID': ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010'],
        'Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math', 'English', 'Science', 'Math', 'English'],
        'Midterm_Score': [85, 92, 78, 88, 76, 95, 82, 89, 70, 91],
        'Final_Score': [90, 88, 82, 92, 80, 91, 85, 93, 75, 89],
        'Attendance_Rate': [0.95, 0.98, 0.85, 0.92, 0.80, 0.99, 0.88, 0.96, 0.75, 0.93]}
df = pd...",10,"The generated code correctly performs all requested operations:
1. It successfully creates the pandas DataFrame.
2. It correctly calculates and adds the 'Score_Improvement' column.
3. It accurately calculates and prints the average 'Score_Improvement' for each subject.
4. It correctly identifies and prints the 'StudentID' of students who meet both the 'Final_Score' and 'Attendance_Rate' criteria.

The code is well-structured, uses appropriate pandas functions, and is efficient. The imports are correct. The use of `.tolist()` for printing the student IDs is a good touch for clear output.",mistral-small-24b-instruct,LLM_judge_config
M52,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""EmployeeID"": [""E001"", ""E002"", ""E003"", ""E004"", ""E005"", ""E006"", ""E007"", ""E008""],
    ""Department"": [""Sales"", ""HR"", ""Tech"", ""Sales"", ""Tech"", ""HR"", ""Tech"", ""Sales""],
    ""PerformanceScore"": [4, 3, 5, 2, 4, 3, 5, 4],
    ""YearsExperience"": [5, 2, 8, 3, 6, 2, 10, 4],
    ""ProjectsCompleted"": [10, 5, 15, 4, 12, 6, 20, 8],
    ""TrainingHours"": [20, 40, 10, 50, 20, 30, 15, 25]
}
df = pd.DataFrame(data)

# Filter the DataFrame
filtered_df = df[(df['Department']...",9,"The generated code correctly defines the DataFrame, filters it accurately based on the specified conditions (""Department"" == 'Tech' and ""PerformanceScore"" > 3), and then correctly calculates and prints the average number of ""ProjectsCompleted"" for the filtered subset.

Strong points:
- **Correctness:** The code fully and accurately solves the problem as described in the task.
- **Clarity:** The variable names are descriptive, and the steps (DataFrame definition, filtering, calculation, printing) are clearly distinct.
- **Conciseness:** The code is efficient and to the point.

Areas for minor improvement (leading to a score of 9 instead of 10):
- **Edge Case Handling:** The ground truth includes an `if not filtered_df.empty:` check, which is good practice for robustness in real-world applications where a filter might result in an empty DataFrame. While for this specific dataset and query it will not be empty, considering such edge cases makes the code more robust.
- **Output Formatting:** The ground truth requests `average_projects_completed:.2f` for formatting, which was not explicitly done in the generated code's print statement, though this is a minor detail.",mistral-small-24b-instruct,LLM_judge_config
M53,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""UserID"": [""User1"", ""User2"", ""User3"", ""User4"", ""User5"", ""User6""],
    ""SessionDurationMinutes"": [15, 25, 10, 30, 22, 18],
    ""PagesViewed"": [5, 8, 3, 10, 7, 6],
    ""DeviceType"": [""Desktop"", ""Mobile"", ""Desktop"", ""Mobile"", ""Desktop"", ""Mobile""],
    ""Converted"": [True, False, False, True, True, False]
}
df = pd.DataFrame(data)

desktop_conversion_rate = df[df[""DeviceType""] == ""Desktop""][""Converted""].mean() * 100
mobile_conversion_rate = df[df[""DeviceTyp...",9,"The generated code correctly defines the DataFrame, calculates the conversion rates for Desktop and Mobile users, and prints them clearly labeled, matching the ground truth's core functionality. The code is concise and efficient for the given task.

The only minor area for improvement, which prevents a perfect score, is the lack of explicit handling for edge cases where `df[df[""DeviceType""] == ""Desktop""]` or `df[df[""DeviceType""] == ""Mobile""]` might result in an empty DataFrame. While `mean()` on an empty Series returns `NaN`, which is technically correct, the ground truth explicitly handles this by setting the rate to `0.0` and printing a warning, which is a more robust approach for real-world applications where data might be sparse. For this specific dataset, this edge case doesn't occur, so the generated code works perfectly.",mistral-small-24b-instruct,LLM_judge_config
M54,medium,Data Aggregation,,"```python
import pandas as pd

data = {
    ""Timestamp"": [""2025-05-24 10:00"", ""2025-05-24 10:05"", ""2025-05-24 10:10"", ""2025-05-24 10:15"", ""2025-05-24 10:00"", ""2025-05-24 10:05""],
    ""SensorID"": [""S1"", ""S1"", ""S1"", ""S1"", ""S2"", ""S2""],
    ""TemperatureCelsius"": [22.5, 22.7, 22.6, 22.8, 21.9, 22.0],
    ""HumidityPercent"": [55, 56, 55, 57, 60, 59],
    ""AirQualityIndex"": [30, 32, 31, 33, 25, 26]
}

df = pd.DataFrame(data)

average_air_quality = df.groupby('SensorID')['AirQualityIndex'].mean()

for se...",9,"The generated code correctly defines the DataFrame and accurately calculates the average ""AirQualityIndex"" for each unique ""SensorID"". The output clearly indicates which average belongs to which sensor, fulfilling all requirements of the task. The code is clean, concise, and uses appropriate pandas functions.

One minor suggestion for improvement, though not strictly required by the prompt, would be to format the average `AirQualityIndex` to a consistent number of decimal places, similar to the ground truth (e.g., `""{avg_air_quality_index:.2f}""`). This improves readability of numerical output. However, this oversight does not detract significantly from the correctness or completeness of the solution.

Strong points:
- Correct DataFrame creation.
- Accurate use of `groupby()` and `mean()` for the calculation.
- Clear and correct printing of results.
- Adherence to all specified task requirements.",mistral-small-24b-instruct,LLM_judge_config
M55,medium,Feature Engineering & Filtering,,"Sure, here is the Python code:

```python
import pandas as pd

data = {
    ""PostID"": [""SM01"", ""SM02"", ""SM03"", ""SM04"", ""SM05"", ""SM06""],
    ""Platform"": [""Instagram"", ""Twitter"", ""Facebook"", ""Instagram"", ""Twitter"", ""Instagram""],
    ""PostType"": [""Image"", ""Text"", ""Video"", ""Image"", ""Text"", ""Video""],
    ""Likes"": [150, 80, 300, 200, 90, 450],
    ""Shares"": [20, 15, 50, 30, 10, 60],
    ""Comments"": [10, 5, 25, 15, 8, 30]
}
df = pd.DataFrame(data)

# Create EngagementScore
df[""EngagementScore""] = df[""L...",9,"The generated code correctly defines the DataFrame, calculates the ""EngagementScore"" as specified, filters the DataFrame for Instagram posts, and accurately calculates and prints their average ""EngagementScore"". The code is concise and directly addresses all parts of the task.

One minor improvement, though not strictly required by the prompt, would be to include a check for an empty `instagram_df` before calculating the mean, similar to the ground truth. This makes the code more robust against edge cases where no Instagram posts might be present in the initial data. Also, formatting the output string with an f-string and limiting decimal places for the average would enhance readability, as done in the ground truth. However, these are minor points and do not detract from the core correctness and completeness of the solution.",mistral-small-24b-instruct,LLM_judge_config
M56,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""PatientID"": [""P001"", ""P002"", ""P003"", ""P004"", ""P005"", ""P006""],
    ""AdmissionDate"": pd.to_datetime([""2025-01-10"", ""2025-01-12"", ""2025-01-15"", ""2025-01-18"", ""2025-01-20"", ""2025-01-22""]),
    ""DischargeDate"": pd.to_datetime([""2025-01-15"", ""2025-01-16"", ""2025-01-22"", ""2025-01-25"", ""2025-01-24"", ""2025-01-28""]),
    ""Diagnosis"": [""Flu"", ""Cold"", ""Flu"", ""Injury"", ""Cold"", ""Flu""],
    ""Age"": [34, 56, 23, 45, 67, 30],
    ""Gender"": [""Male"", ""Female"", ""Female"", ""...",10,"The generated code is excellent. It correctly defines the DataFrame, calculates the `LengthOfStay` column by subtracting dates and extracting days, accurately filters the DataFrame for patients with a ""Flu"" diagnosis, and then correctly calculates and prints the average `LengthOfStay` for these specific patients. The code is clean, concise, uses appropriate pandas functions, and directly addresses all parts of the prompt. No improvements are needed for correctness or completeness.",mistral-small-24b-instruct,LLM_judge_config
M57,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""MovieID"": [""M01"", ""M02"", ""M03"", ""M04"", ""M05"", ""M06""],
    ""Title"": [""Inception"", ""The Dark Knight"", ""Pulp Fiction"", ""Forrest Gump"", ""The Matrix"", ""Interstellar""],
    ""Genre"": [""Sci-Fi"", ""Action"", ""Crime"", ""Drama"", ""Sci-Fi"", ""Sci-Fi""],
    ""Year"": [2010, 2008, 1994, 1994, 1999, 2014],
    ""Rating"": [8.8, 9.0, 8.9, 8.8, 8.7, 8.6],
    ""Votes"": [2200000, 2500000, 1900000, 1800000, 1700000, 1600000]
}

df = pd.DataFrame(data)

sci_fi_movies = df[df[""Genr...",9,"The generated code correctly defines the DataFrame, filters for 'Sci-Fi' movies, and accurately calculates their average rating, fulfilling all task requirements. It is concise and directly addresses the problem.

The only minor area for improvement, especially for a production-level or more robust solution, would be to include an explicit check for the edge case where no 'Sci-Fi' movies might be found in the DataFrame (e.g., if `sci_fi_movies` ends up empty). In such a scenario, `mean()` would return `NaN`, which might not always be the desired behavior and could lead to unexpected results if not handled. The ground truth code demonstrates such a check. However, for the given specific dataset and problem description, the generated code is perfectly functional and correct.",mistral-small-24b-instruct,LLM_judge_config
M58,medium,Data Manipulation,,"```python
import pandas as pd

data = {
    ""CarID"": [""C001"", ""C002"", ""C003"", ""C004"", ""C005"", ""C006""],
    ""Brand"": [""Toyota"", ""Honda"", ""Ford"", ""Toyota"", ""BMW"", ""Honda""],
    ""Model"": [""Camry"", ""Civic"", ""Focus"", ""Rav4"", ""X5"", ""Accord""],
    ""Year"": [2020, 2021, 2019, 2022, 2023, 2020],
    ""PriceUSD"": [25000, 22000, 18000, 30000, 60000, 24000],
    ""Mileage"": [30000, 20000, 45000, 15000, 5000, 35000]
}

df = pd.DataFrame(data)

# 1) Filter the DataFrame to include only cars of the ""Toyota"" brand...",10,"The generated code is correct, complete, and follows good coding practices. It accurately defines the DataFrame, filters for 'Toyota' brand cars, and correctly calculates and prints their average mileage. The code is concise and directly addresses all requirements of the task.",mistral-small-24b-instruct,LLM_judge_config
M59,medium,Data Filtering,,"```python
import pandas as pd

data = {
    ""PokemonID"": [1, 2, 3, 4, 5, 6, 7, 8],
    ""Name"": [""Bulbasaur"", ""Ivysaur"", ""Venusaur"", ""Charmander"", ""Charmeleon"", ""Charizard"", ""Squirtle"", ""Wartortle""],
    ""Type1"": [""Grass"", ""Grass"", ""Grass"", ""Fire"", ""Fire"", ""Fire"", ""Water"", ""Water""],
    ""Type2"": [""Poison"", ""Poison"", ""Poison"", None, None, ""Flying"", None, None],
    ""HP"": [45, 60, 80, 39, 58, 78, 44, 59],
    ""Attack"": [49, 62, 82, 52, 64, 84, 48, 63],
    ""Defense"": [49, 63, 83, 43, 58, 78, 65, 80...",9,"The generated code correctly defines the DataFrame, filters for Fire-type Pokemon, and accurately calculates their average Attack stat, addressing all requirements of the task. The code is clean and follows standard pandas practices.

The primary area for improvement is the lack of explicit handling for the edge case where no Fire-type Pokemon might be found in the `fire_pokemon` DataFrame. While in this specific dataset, Fire-type Pokemon exist, robust code should account for such scenarios to prevent potential runtime errors if the dataset were different (e.g., `fire_pokemon['Attack'].mean()` would return `NaN` or raise an error if `fire_pokemon` were entirely empty and the column didn't exist). The ground truth code includes an `if not fire_pokemon_df.empty:` check, which is a good practice for production-ready code.

Another minor point is the formatting of the printed output. The ground truth uses `:.2f` for two decimal places, which can be useful for numerical results. However, this is a minor stylistic choice and doesn't affect correctness.",mistral-small-24b-instruct,LLM_judge_config
M6,medium,Data Visualization,titanic.csv,"```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Load the dataset
data = pd.read_csv('titanic.csv')

# 1) Multi-panel figure showing survival rates
plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
sns.barplot(data=data, x='Pclass', y='Survived')
plt.title('Survival Rate by Class')

plt.subplot(1, 3, 2)
sns.barplot(data=data, x='Sex', y='Survived')
plt.title('Survival Rate by Gender')

plt.subplot...",6,"The generated code attempts to address all parts of the task, but several implementations are incomplete, incorrect, or lack the sophistication requested.

**Strengths:**
*   Successfully loads the data and performs basic plotting.
*   Correctly implements the first three visualization requirements using Matplotlib and Seaborn, although with some basic customization.
*   The interactive scatter plot using Plotly Express is correctly implemented as requested.

**Weaknesses and Areas for Improvement:**

1.  **Correctness & Completeness (Multi-panel figure):**
    *   The first multi-panel figure is created but lacks consistent `ylim` and appropriate labeling for better comparison, as well as a common title for the panels.
    *   The `AgeGroup` creation could be done once at the beginning during data preprocessing for consistency.

2.  **Correctness (Violin Plots):**
    *   The violin plot section is significantly incomplete. It creates two subplots, but only the first one is used for a violin plot, and the second one has no visualization at all (`plt.title('Age Distribution of Passengers')` without any plot).
    *   The request was for ""violin plots comparing age distributions of survivors vs non-survivors by gender,"" which implies a comparison on the survival status. Only one plot is shown, and while it uses `hue='Sex'`, it doesn't clearly separate survivors vs. non-survivors as distinct violin plots for each gender. The `split=True` helps, but the overall structure (two subplots) is not fully utilized.

3.  **Completeness & Sophistication (Interactive Scatter Plot):**
    *   The `size='Survived'` in `px.scatter` is not ideal for categorizing survival; `symbol='Survived'` would be more appropriate (as in the ground truth) to visually differentiate survivors from non-survivors. The current size mapping doesn't effectively convey survival status.
    *   No custom color mapping as in the ground truth was used.

4.  **Correctness & Completeness (Dashboard):**
    *   This is the weakest part of the generated code. The request was for a ""comprehensive dashboard-style figure combining multiple visualizations with proper annotations and insights.""
    *   The generated dashboard uses `go.FigureWidget()` and `fig.add_trace()`, but it doesn't properly combine *multiple visualizations* into a single coherent figure using subplots or a grid layout as expected for a dashboard.
    *   It tries to add bar charts for survival rates, a heatmap, and violin plots, but `fig.add_plot(heatmap_trace)` and `fig.add_trace(scatter_trace)` will simply layer these plots on top of each other in a single facet, not create a multi-panel dashboard. `go.FigureWidget()` is typically for interactive dashboards but requires specific layout configuration (e.g., `make_subplots`) to show multiple distinct plots.
    *   It fails to include the requested ""proper annotations and insights"" which is a key part of ""storytelling.""
    *   The `size='Survived'` in the scatter plot is not a good mapping for a discrete categorical variable (0 or 1).

5.  **Code Quality & Preprocessing:**
    *   Data preprocessing (handling missing values, creating categorical variables consistently) is done inline with the plotting for `AgeGroup` but not explicitly for `Pclass` or `Sex` in a reusable way. The ground truth demonstrates better practice by performing preprocessing once at the beginning.
    *   No error handling for file loading.
    *   Lack of consistent styling and `rcParams` configuration for Matplotlib.

Overall, the code demonstrates an understanding of the individual plotting requirements (except for the violin plot implementation), but the dashboard component is largely misimplemented, failing to create a multi-panel layout with embedded insights.",mistral-small-24b-instruct,LLM_judge_config
M60,medium,Data Filtering & Aggregation,,"```python
import pandas as pd

data = {
    ""BookID"": [""B001"", ""B002"", ""B003"", ""B004"", ""B005"", ""B006""],
    ""Title"": [""The Great Gatsby"", ""To Kill a Mockingbird"", ""1984"", ""Pride and Prejudice"", ""The Hobbit"", ""Animal Farm""],
    ""Author"": [""F. Scott Fitzgerald"", ""Harper Lee"", ""George Orwell"", ""Jane Austen"", ""J.R.R. Tolkien"", ""George Orwell""],
    ""Genre"": [""Classic"", ""Classic"", ""Dystopian"", ""Romance"", ""Fantasy"", ""Dystopian""],
    ""Stock"": [12, 8, 15, 7, 20, 10],
    ""Price"": [10.99, 9.50, 12.00, ...",9,"The generated code correctly defines the DataFrame, filters for Dystopian genre, and accurately calculates the average stock for that genre, fulfilling all task requirements. It demonstrates good understanding of pandas operations.

The only minor area for improvement, making the ground truth slightly more robust, is the addition of a check for an empty filtered DataFrame (`if not dystopian_books_df.empty:`). While for this specific dataset it's not strictly necessary (as Dystopian books exist), in a more general scenario, such a check would prevent a `NaN` result or potential errors if no books of a specific genre were found. However, for the given problem statement and data, the current solution is perfectly functional and correct. The output formatting for printing the average stock in the ground truth also uses f-string with formatting (e.g., `:.2f`), which is a good practice for numerical outputs, but not a strict requirement for correctness.",mistral-small-24b-instruct,LLM_judge_config
M7,medium,Hypothesis Testing,water_potability.csv,"```python
import pandas as pd
import numpy as np
from scipy import stats

# Load the dataset
data = pd.read_csv('water_potability.csv')

# 1) Conduct t-tests to compare mean values of each water quality parameter
# between potable and non-potable water samples
potable = data[data['Potability'] == 1]
non_potable = data[data['Potability'] == 0]

parameters = ['pH', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']
results = []

for pa...",6,"The generated code addresses most of the requirements but has significant flaws and omissions that prevent it from being a fully correct and robust solution.

**Strengths:**
*   **Structure:** It attempts to follow the numbered requirements of the prompt.
*   **Basic Tests Performed:** It correctly attempts t-tests, chi-square, and Shapiro-Wilk tests.
*   **Cohen's d Calculation:** It correctly attempts to calculate Cohen's d for significant differences.
*   **Imports:** Necessary libraries are imported.

**Weaknesses and Areas for Improvement:**

1.  **Missing Value Handling (Critical):** The most significant issue is the lack of proper missing value handling *before* performing statistical tests.
    *   The `ttest_ind` function, by default, will drop NaNs, but this is not always the best strategy (could lead to different sample sizes for different parameters).
    *   The `shapiro` test explicitly drops NaNs (`.dropna()`), but this is done *inside* the loop. A better practice is to handle all missing values at the beginning, uniformly, perhaps by mean imputation, as done in the ground truth. This is crucial for consistent analysis.
    *   The prompt implies that the analysis should be robust/correct, and ignoring missing values or handling them implicitly per test is not robust.

2.  **Chi-square Goodness-of-Fit Test (Incorrect Implementation):**
    *   The `f_exp` argument for `stats.chisquare` expects an array (or list) of expected frequencies for *each category*.
    *   `expected = [sum(observed)]` is fundamentally incorrect for a goodness-of-fit test. It expects an array like `[expected_count_for_0, expected_count_for_1]`. If the assumption is a uniform 50/50 distribution, it should be `[total_samples / 2, total_samples / 2]`. This error leads to an incorrect test calculation.
    *   The interpretation of the chi-square results is missing.

3.  **T-test Details (Missing `equal_var` consideration):**
    *   The `stats.ttest_ind` function defaults to `equal_var=True` (Student's t-test). For real-world data, assuming equal variances is often incorrect. Welch's t-test (`equal_var=False`) is more robust. The ground truth uses `equal_var=False`, which is better practice.

4.  **Normality Test Interpretation:**
    *   Similar to the chi-square, the interpretation of normality tests is summarized in a DataFrame but lacks explicit print statements like ""data IS normally distributed"" or ""data is NOT normally distributed"" for each feature based on the p-value against alpha.

5.  **Effect Size Interpretation (Missing):**
    *   While Cohen's d is calculated, there is no interpretation of its magnitude (e.g., small, medium, large effect) as requested by the prompt (""interpretation of results"").

6.  **Readability/Output Format:**
    *   The output is raw DataFrames. While functional, it doesn't provide the detailed, clear, and user-friendly interpretation requested (""Display p-values, test statistics, and interpretation of results for each test""). The ground truth provides much clearer structured output.

7.  **Robustness (Minor):**
    *   The `parameters` list is hardcoded. It would be more robust to dynamically identify numerical columns, excluding 'Potability', similar to the ground truth.
    *   No `try-except` block for file loading.

Overall, the code provides a basic attempt at fulfilling the requirements, but the critical error in the chi-square test and the inadequate missing value handling make it unreliable for statistical analysis, especially for a benchmark task. The lack of detailed interpretation for all tests requested by the prompt also reduces its completeness score.",mistral-small-24b-instruct,LLM_judge_config
M8,medium,Classification,water_potability.csv,"```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, RocCurveDisplay, classi...",6,"The generated code addresses most of the requirements but has significant issues regarding data preprocessing and cross-validation methodology, leading to potential data leakage and incorrect assessment of model performance.

**Strong Points:**
*   Correctly loads data and separates features/target.
*   Implements all three requested models (Logistic Regression, Random Forest, SVM).
*   Calculates and prints multiple evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC).
*   Generates ROC curves for each model.
*   Attempts k-fold cross-validation.
*   Compares models to select the best one based on CV accuracy.
*   Displays feature importance for Random Forest (if it's the best model).
*   Uses appropriate libraries.

**Areas for Improvement/Correction:**

1.  **Data Preprocessing Order (Critical):**
    *   **Missing Value Imputation before Train-Test Split:** The code imputes missing values and scales the *entire* dataset (`X_imputed = imputer.fit_transform(X)`, `X_scaled = scaler.fit_transform(X_imputed)`) *before* the train-test split. This is a severe form of data leakage. The imputer and scaler learn parameters (like mean/median for imputation, mean/std for scaling) from the test set data, which contaminates the training process and leads to an overoptimistic evaluation of model performance.
    *   **Correction:** Imputation and scaling should be applied *within* the train-test split (fit on `X_train`, transform `X_train` and `X_test`). The ground truth code correctly demonstrates this, or a Pipeline can be used for robust handling.

2.  **Cross-Validation Implementation (Critical):**
    *   **Data Leakage in CV:** The `cross_val_score` is performed on `X_scaled` (the entire dataset *after* global imputation and scaling). This means that during cross-validation, the imputer and scaler are *not* refit on each fold's training data. This again leads to data leakage and an unrealistic assessment of generalizability.
    *   **Recommendation:** For correct cross-validation, a `Pipeline` should be used that encapsulates the imputer, scaler, and classifier. This ensures that preprocessing steps are correctly applied within each CV fold, fitting only on the training data of that fold. The ground truth code correctly implements a pipeline for CV.
    *   **Metrics in CV:** Only accuracy is used for cross-validation evaluation. The prompt asks for k-fold cross-validation to assess model stability, implying using the comprehensive set of metrics as well (`accuracy, precision, recall, F1-score, and ROC-AUC`).

3.  **Confusion Matrix Visualization:**
    *   The prompt specifically asks for ""Create confusion matrices... for each model"". The generated code calculates `confusion_matrix(y_test, y_pred)` and prints it, but it does *not* visualize it using `ConfusionMatrixDisplay` or `seaborn.heatmap` as requested and as done in the ground truth code.

4.  **ROC Curve Display:**
    *   `RocCurveDisplay.from_estimator` is called inside the loop, and `plt.show()` is called immediately after it. This will show a separate ROC curve plot for each model, which is acceptable. However, the ground truth code also shows a combined ROC curve plot for all models on a single figure, which is a nice touch for comparison.

5.  **Best Model Selection:**
    *   The `best_model` comparison relies solely on `cv_scores.mean()` which is accuracy. While accuracy is a common metric, for binary classification (especially with potential class imbalance, though not explicitly stated for this dataset, it's good practice), F1-score or ROC-AUC might be more robust choices, as mentioned in the ground truth's best model selection based on F1-score.

6.  **Feature Importance for non-tree models:**
    *   The code only implements feature importance for `RandomForestClassifier`. The prompt asks for feature importance for ""the best model"". If Logistic Regression were the best model, it should also display its coefficients as a proxy for importance. SVM generally doesn't have direct feature importance attributes. The ground truth handles this comprehensively.

The issues with data leakage during preprocessing and cross-validation are critical for the correctness of the model evaluation and the validity of the results.",mistral-small-24b-instruct,LLM_judge_config
M9,medium,Feature Engineering,water_potability.csv,"```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.preprocessing import PolynomialFeatures

# Load dataset
df = pd.read_csv('water_potability.csv')

# Create new features
df['PH_Chloramines_Ratio'] = df['ph'] / df['Chloramines']
df['Hardness_Solids_Ratio'] = df['Hardness'] / df['...",5,"The generated code attempts to address the task but falls short on several critical aspects, leading to a score of 5.

**Correctness:**
*   **Feature Engineering Errors:** The code makes incorrect assumptions about column names (e.g., `df['Total Solids']`, `df['Sulfate']` used for pH ratio) that are not present or correctly used in the `water_potability.csv` dataset. This will cause `KeyError` or incorrect calculations.
*   **Missing Value Handling:** The code completely omits handling missing values, which is crucial for the `water_potability.csv` dataset (pH, Sulfate, Trihalomethanes have NaNs). This will lead to errors or incorrect results in subsequent calculations (e.g., correlation, feature selection).
*   **Polynomial Features:** The `PolynomialFeatures` application is flawed. `poly.fit_transform(df)` will apply polynomial features to *all* columns, including 'Potability' and previously engineered features, which is not standard practice and can lead to issues with target leakage or an explosion of features. The `get_feature_names_input` method is also not suitable for direct column names when dealing with a DataFrame.
*   **Correlation Matrix Visualization:** Using `annot=True` on a heatmap with potentially hundreds of features (after aggressive polynomial feature engineering) will produce an unreadable plot.
*   **Feature Importance Plot:** The x-axis labels for feature importance are unreadable due to too many features and rotation.
*   **Pairwise Scatter Plot:** `sns.pairplot(df[top_5_features])` will result in an empty plot if 'Potability' is not included in `top_5_features`, as `pairplot` needs the target for hue or at least more than one feature to show relationships. The request specifically asks for *pairwise scatter plots for top correlated features*, typically visualized with the target variable as hue. The generated code also didn't consider the case where `top_5_features` might not contain 'Potability' (which is correct to exclude it from the ranked list) but it *is* needed for the `hue` in `pairplot`.

**Code Quality:**
*   **Readability/Clarity:** The feature engineering section invents column names, making it hard to follow or directly run.
*   **Hardcoded Values:** `k=5` for `SelectKBest` and `n_features_to_select=5` for RFE might be too aggressive for a dataset with many features, especially after engineering. The ground truth used `k=15`, which is more reasonable.
*   **No Error Handling:** No `try-except` block for file loading.
*   **Redundant Model Fitting:** `model.fit(X, y)` is called twice for `RandomForestClassifier`.

**Completeness:**
*   **Meaningful Engineered Features:** While it attempts to create ratios and interactions, the implementation is weak and makes incorrect assumptions about column names (e.g., ""Total Solids""). It also lacks the variety of interaction terms and a more controlled application of polynomial features as requested and demonstrated in the ground truth.
*   **Top 5 Features:** Correctly identified but the `pairplot` usage is flawed for visualization.
*   **Feature Selection Overlap Analysis:** This critical part of the prompt was completely missed. The generated code only shows ""Common features selected by both methods"" but does not perform the comprehensive analysis of overlap, unique features, etc., as specified.
*   **Standardization:** Features are not scaled/standardized before applying `SelectKBest` and RFE, which can negatively impact these methods, especially RFE with tree-based models and others that are sensitive to feature scales.
*   **Visualization Coverage:** While it attempts all types, the quality and effectiveness of some visualizations (heatmap, feature importance, pairplot) are poor or potentially problematic as implemented.

Overall, the code provides a skeletal structure but fails on crucial implementation details, data handling, and comprehensive analysis/visualization.",mistral-small-24b-instruct,LLM_judge_config
